From michele.furlanetto at aleagames.com  Tue Aug  1 04:30:24 2017
From: michele.furlanetto at aleagames.com (Michele Furlanetto)
Date: Tue, 1 Aug 2017 10:30:24 +0200
Subject: [Project Clearwater] [Clearwater] AS Configuration and
 untrusted sources
In-Reply-To: <BLUPR02MB4371DC95390C2D4E830905AE5B20@BLUPR02MB437.namprd02.prod.outlook.com>
References: <7296A131-7DDD-43F1-9F00-5F85FCF40FF7@aleagames.com>
	<3ACF4837-B492-45A2-8B83-14202C9CFDEE@aleagames.com>
	<BLUPR02MB43730770DB57A115A4B287BE5BB0@BLUPR02MB437.namprd02.prod.outlook.com>
	<AAB4BECC-919D-43AB-A00A-78C1110C3244@aleagames.com>
	<BLUPR02MB437901136FF4E64F7DC3D14E5B80@BLUPR02MB437.namprd02.prod.outlook.com>
	<4AACAE0B-0199-49B4-9234-1FF4FFE2AEFC@aleagames.com>
	<6E2AAAE8-F6FB-4E4A-97B0-0480FC01EAA1@aleagames.com>
	<BLUPR02MB4371DC95390C2D4E830905AE5B20@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <20107419-E39A-47C3-BF90-346E7938536D@aleagames.com>

Hi Andrew,

About the third-party registration I had misconfigurated something. So I restarted from the .ova image and now works.
The last thing I have to configure is to enable communication between as1 and as2.

In attachment you can find the files /etc/hosts, /etc/dnsmasq.conf and the logs of bono and sprout.


Thanks for the help,
Michele




> Il giorno 31 lug 2017, alle ore 11:07, Andrew Edmonds <Andrew.Edmonds at metaswitch.com> ha scritto:
> 
> Hi Michele,
>  
> Could I get some more diagnostics to try and work out why third-party registration is no longer working. The Sprout logs (which can be found on SPNs under /var/log/sprout/sprout_current.txt) for a registration may be helpful here.
>  
> Thanks,
>  
> Andrew
>  
> From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Michele Furlanetto
> Sent: Wednesday, July 26, 2017 4:40 PM
> To: clearwater at lists.projectclearwater.org
> Subject: Re: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources
>  
> I went a little forward, removing the line relative to as1.service.example.com <http://as1.service.example.com/> in file /etc/hosts and adding 
>  
> srv-host=_sip._udp.scscf.cw-aio,scscf.cw-aio,5054
> srv-host=_sip._udp.as1.service.example.com <http://udp.as1.service.example.com/>,example.com <http://example.com/>,5071
>  
> to /etc/dnsmasq.conf.
>  
> now the SUBSCRIBEs (and other messages) are delivered to as1, but I?ve lost the working third-party registration.
>  
> After fixing this, the next step for me will be adding another AS, as2, which will dialog only with as1.
> The simplest think I could try is adding 
> srv-host=_sip._udp.as2.service.example.com <http://udp.as2.service.example.com/>,example.com <http://example.com/>,5072
> but is not enough. What configuration I?m missing?
>  
> Thank you,
> Michele
>  
>  
> Il giorno 26 lug 2017, alle ore 11:27, Michele Furlanetto <michele.furlanetto at aleagames.com <mailto:michele.furlanetto at aleagames.com>> ha scritto:
>  
> Hi Andrew,
>  
> As I said, I didn?t configure any DNS service to support our AS, 
> all I did in this way was adding the lines to /etc/hosts
>  
> 10.2.0.91 as1.service.example.com <http://participant.mcptt.example.com/>
> 10.2.0.127 example.com <http://example.com/>
>  
> In attachment you can find the local_config and shared_config files.
>  
> Referring the previous message, my SIP client does use the same port for both REGISTER and SUBSCRIBE.
>  
> Thank you for your support,
> Michele
>  
> <local and shared configs.zip>
>  
>  
> Il giorno 25 lug 2017, alle ore 15:40, Andrew Edmonds <Andrew.Edmonds at metaswitch.com <mailto:Andrew.Edmonds at metaswitch.com>> ha scritto:
>  
> Hi Michele,
>  
> I?ve spoken to a colleague about this issue and have a new idea now as to why the SUBSCRIBE message is being rejected.
>  
> After Bono receives the SUBSCRIBE from the SIP device we can see the log:
>  
> 24-07-2017 10:38:25.414 UTC Verbose pjsip: tcpc0x7f094c03 TCP transport 10.2.0.127:5058 is connecting to 10.2.0.127:5060..
>  
> 10.2.0.127 is the Bono node?s IP address. So this is telling us that Bono is forwarding the SUBSCRIBE to itself from its trusted port 5058 (used to communicate with the IMS core) to its untrusted port 5060 (which SIP devices send their requests to). We then see in the logs Bono receiving this SUBSCRIBE and then rejecting it.
>  
> So the question we need to resolve now becomes why is Bono routing the SUBSCRIBE to itself? To answer this we need to understand how SIP messages are routed. Messages containing SIP methods (like INVITEs, SUBSCRIBEs, REGISTERs etc.) are routed based off of the Route-header. If we look at the route headers for one of the SUBSCRIBERs we see:
>  
> Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690 <sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690>
>  
> Here the port of the S-CSCF (5054) has not been included in the SIP address of the S-CSCF. We can also see from the logs that we do not retrieve a port number via SRV DNS lookup:
>  
> 24-07-2017 10:38:25.414 UTC Debug dnscachedresolver.cpp:686: Received DNS response for _sip._udp.scscf.cw-aio type SRV
> 24-07-2017 10:38:25.414 UTC Warning dnscachedresolver.cpp:828: Failed to retrieve record for _sip._udp.scscf.cw-aio: Domain name not found
>  
> This means that Bono does not know which port to forward the message to and so it chooses 5060 which routes the message back to itself.
>  
> The next thing to consider then is why does the Route-Header not contain the port number? The Route header which is to be applied to future requests for a subscriber is communicated to Bono by the S-CSCF in a Service-Route header after it successfully authenticates a subscriber. If we have a look at the 200 OK in response to the registration we see:
>  
> Service-Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690 <sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690>
>  
> This means that it is the S-CSCF that is causing Bono to attach a Route header to the SUBSCRIBE which does not contain a port number. To figure out why the S-CSCF is doing this could you please send me a copy of /etc/clearwater/shared_config and /etc/clearwater/local_config from the SPN and a copy of the SPN log file during a REGISTRATION and SUBSCRIBE.
>  
> Thanks,
>  
> Andrew
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org <mailto:Clearwater at lists.projectclearwater.org>
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org <http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170801/7b73d906/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: AIO- configurations and logs.zip
Type: application/zip
Size: 117560 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170801/7b73d906/attachment.zip>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170801/7b73d906/attachment-0001.html>

From Jobin.Johnson at G3TI.NET  Tue Aug  1 07:51:22 2017
From: Jobin.Johnson at G3TI.NET (Johnson, Jobin)
Date: Tue, 1 Aug 2017 11:51:22 +0000
Subject: [Project Clearwater] External PCRF/ Rx interface Support
Message-ID: <aa2c2cf549754dd6b7e598d4e2d8a30b@S02-MAIL002.G3TI.Local>

I was interested in trying out VoLTE with Project Clearwater. However, I can't seem to find any integration between the p-cscf/Bono and an external PCRF or something equivalent to tell the LTE network to setup a dedicated bearer with appropriate QoS for the UE during call setup. Are there any additional resources I can look into for this or did I miss it in the documentation?

Thank you in advance for any responses!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170801/55f1f00d/attachment.html>

From Adam.Lindley at metaswitch.com  Tue Aug  1 13:03:36 2017
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Tue, 1 Aug 2017 17:03:36 +0000
Subject: [Project Clearwater] =?iso-8859-1?q?_Release_note_for_sprint_K=ED?=
 =?iso-8859-1?q?li?=
Message-ID: <BY2PR02MB20379F71BC49BA618A2ED23DE2B30@BY2PR02MB2037.namprd02.prod.outlook.com>

Hi all,

We have cut the latest Project Clearwater Release: "K?li". The code for this release has been tagged as release-127 in GitHub.
This release includes the following bug fixes:


?         Crest build failure due to cryptography 2..0 release, and setup.py ignoring pinned dependencies

?         clearwater-config-manager may not be restarted when plugin crashes very early

?         Unnecessary repeat logging when running with a misconfigured Application Server

?         Stuck alarm for shared config not being up to date

?         No useful diagnostics when Sprout decides to ignore Authorization header

?         The Queue manager's force_restart_queue_state script gives the force_queue_state script an incorrect argument

?         You can upload a fallback iFC file larger than the speced limit of 0.5MB

?         Unnecessarily verbose errors when you try to upload invalid Shared iFCs, masking useful errors

?         Inconsistent logs when using standard iFCs and Default iFCs

?         Default iFCs are not being invoked when a subscriber doesn't have any iFCs provisioned in the HSS

?         404s cause "cURL failure" logs even though cURL succeeded

?         Bulk_create.py incorrectly creating only barred identities, causing registration failure with error 500

?         poll_homestead-prov attempts poll, even if no hostname provided, and reports status failed

?         Poor diagnostics when a node fails to join the etcd cluster

?         Un-useful 'cURL failure with cURL error code 6' logs

?         Sprout fails to parse star access codes in To headers

To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.

Adam
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170801/e14df217/attachment.html>

From michele.furlanetto at aleagames.com  Thu Aug  3 05:05:17 2017
From: michele.furlanetto at aleagames.com (Michele Furlanetto)
Date: Thu, 3 Aug 2017 11:05:17 +0200
Subject: [Project Clearwater] [Clearwater] AS Configuration and
 untrusted sources
In-Reply-To: <BLUPR02MB4371DC95390C2D4E830905AE5B20@BLUPR02MB437.namprd02.prod.outlook.com>
References: <7296A131-7DDD-43F1-9F00-5F85FCF40FF7@aleagames.com>
	<3ACF4837-B492-45A2-8B83-14202C9CFDEE@aleagames.com>
	<BLUPR02MB43730770DB57A115A4B287BE5BB0@BLUPR02MB437.namprd02.prod.outlook.com>
	<AAB4BECC-919D-43AB-A00A-78C1110C3244@aleagames.com>
	<BLUPR02MB437901136FF4E64F7DC3D14E5B80@BLUPR02MB437.namprd02.prod.outlook.com>
	<4AACAE0B-0199-49B4-9234-1FF4FFE2AEFC@aleagames.com>
	<6E2AAAE8-F6FB-4E4A-97B0-0480FC01EAA1@aleagames.com>
	<BLUPR02MB4371DC95390C2D4E830905AE5B20@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <CB76FEAE-A8B6-43A8-9F3E-9FADA45CEB3C@aleagames.com>

Hi Andrew,

About the third-party registration I had misconfigurated something. So I restarted from the .ova image and now works.
The last thing I have to configure is to enable communication between as1 and as2.

In attachment you can find the files /etc/hosts, /etc/dnsmasq.conf and the logs of bono and sprout.


Thanks for the help,
Michele



> Il giorno 31 lug 2017, alle ore 11:07, Andrew Edmonds <Andrew.Edmonds at metaswitch.com> ha scritto:
> 
> Hi Michele,
>  
> Could I get some more diagnostics to try and work out why third-party registration is no longer working. The Sprout logs (which can be found on SPNs under /var/log/sprout/sprout_current.txt) for a registration may be helpful here.
>  
> Thanks,
>  
> Andrew
>  
> From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Michele Furlanetto
> Sent: Wednesday, July 26, 2017 4:40 PM
> To: clearwater at lists.projectclearwater.org
> Subject: Re: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources
>  
> I went a little forward, removing the line relative to as1.service.example.com <http://as1.service.example.com/> in file /etc/hosts and adding 
>  
> srv-host=_sip._udp.scscf.cw-aio,scscf.cw-aio,5054
> srv-host=_sip._udp.as1.service.example.com <http://udp.as1.service.example.com/>,example.com <http://example.com/>,5071
>  
> to /etc/dnsmasq.conf.
>  
> now the SUBSCRIBEs (and other messages) are delivered to as1, but I?ve lost the working third-party registration.
>  
> After fixing this, the next step for me will be adding another AS, as2, which will dialog only with as1.
> The simplest think I could try is adding 
> srv-host=_sip._udp.as2.service.example.com <http://udp.as2.service.example.com/>,example.com <http://example.com/>,5072
> but is not enough. What configuration I?m missing?
>  
> Thank you,
> Michele
>  
>  
> Il giorno 26 lug 2017, alle ore 11:27, Michele Furlanetto <michele.furlanetto at aleagames.com <mailto:michele.furlanetto at aleagames.com>> ha scritto:
>  
> Hi Andrew,
>  
> As I said, I didn?t configure any DNS service to support our AS, 
> all I did in this way was adding the lines to /etc/hosts
>  
> 10.2.0.91 as1.service.example.com <http://participant.mcptt.example.com/>
> 10.2.0.127 example.com <http://example.com/>
>  
> In attachment you can find the local_config and shared_config files.
>  
> Referring the previous message, my SIP client does use the same port for both REGISTER and SUBSCRIBE.
>  
> Thank you for your support,
> Michele
>  
> <local and shared configs.zip>
>  
>  
> Il giorno 25 lug 2017, alle ore 15:40, Andrew Edmonds <Andrew.Edmonds at metaswitch.com <mailto:Andrew.Edmonds at metaswitch.com>> ha scritto:
>  
> Hi Michele,
>  
> I?ve spoken to a colleague about this issue and have a new idea now as to why the SUBSCRIBE message is being rejected.
>  
> After Bono receives the SUBSCRIBE from the SIP device we can see the log:
>  
> 24-07-2017 10:38:25.414 UTC Verbose pjsip: tcpc0x7f094c03 TCP transport 10.2.0.127:5058 is connecting to 10.2.0.127:5060..
>  
> 10.2.0.127 is the Bono node?s IP address. So this is telling us that Bono is forwarding the SUBSCRIBE to itself from its trusted port 5058 (used to communicate with the IMS core) to its untrusted port 5060 (which SIP devices send their requests to). We then see in the logs Bono receiving this SUBSCRIBE and then rejecting it.
>  
> So the question we need to resolve now becomes why is Bono routing the SUBSCRIBE to itself? To answer this we need to understand how SIP messages are routed. Messages containing SIP methods (like INVITEs, SUBSCRIBEs, REGISTERs etc.) are routed based off of the Route-header. If we look at the route headers for one of the SUBSCRIBERs we see:
>  
> Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690 <sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690>
>  
> Here the port of the S-CSCF (5054) has not been included in the SIP address of the S-CSCF. We can also see from the logs that we do not retrieve a port number via SRV DNS lookup:
>  
> 24-07-2017 10:38:25.414 UTC Debug dnscachedresolver.cpp:686: Received DNS response for _sip._udp.scscf.cw-aio type SRV
> 24-07-2017 10:38:25.414 UTC Warning dnscachedresolver.cpp:828: Failed to retrieve record for _sip._udp.scscf.cw-aio: Domain name not found
>  
> This means that Bono does not know which port to forward the message to and so it chooses 5060 which routes the message back to itself.
>  
> The next thing to consider then is why does the Route-Header not contain the port number? The Route header which is to be applied to future requests for a subscriber is communicated to Bono by the S-CSCF in a Service-Route header after it successfully authenticates a subscriber. If we have a look at the 200 OK in response to the registration we see:
>  
> Service-Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690 <sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690>
>  
> This means that it is the S-CSCF that is causing Bono to attach a Route header to the SUBSCRIBE which does not contain a port number. To figure out why the S-CSCF is doing this could you please send me a copy of /etc/clearwater/shared_config and /etc/clearwater/local_config from the SPN and a copy of the SPN log file during a REGISTRATION and SUBSCRIBE.
>  
> Thanks,
>  
> Andrew
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org <mailto:Clearwater at lists.projectclearwater.org>
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org <http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170803/6f2b02be/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: AIO- configurations and logs.zip
Type: application/zip
Size: 117560 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170803/6f2b02be/attachment.zip>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170803/6f2b02be/attachment-0001.html>

From hkaranjikar at apm.com  Thu Aug  3 05:28:33 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Thu, 3 Aug 2017 14:58:33 +0530
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <BY2PR02MB2149BFFB2D1407E8078AD0FAF4B20@BY2PR02MB2149.namprd02.prod.outlook.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
	<BY2PR02MB2149BFFB2D1407E8078AD0FAF4B20@BY2PR02MB2149.namprd02.prod.outlook.com>
Message-ID: <CABmBNEaUiYg4kSqcr5qq=Fwr1y4qL9qrEkV59NkPu8P5=X1aTw@mail.gmail.com>

Hi,

Thanks for you response.
Here are the logs after I enable debug on bono node.
I am also attaching the packet capture herwith.

03-08-2017 09:24:11.633 UTC Verbose pjsip:    tcplis:5058 TCP listener
192.168.56.106:5058: got incoming TCP connection from 192.168.56.106:55980,
sock=93
03-08-2017 09:24:11.633 UTC Verbose pjsip: tcps0x7f926800
tcp->base.local_name: 192.168.56.106
03-08-2017 09:24:11.633 UTC Verbose pjsip: tcps0x7f926800 TCP server
transport created
03-08-2017 09:24:11.637 UTC Debug pjsip: sip_endpoint.c Processing incoming
message: Request msg OPTIONS/cseq=610944 (rdata0x7f92680012b0)
03-08-2017 09:24:11.637 UTC Verbose common_sip_processing.cpp:90: RX 361
bytes Request msg OPTIONS/cseq=610944 (rdata0x7f92680012b0) from TCP
192.168.56.106:55980:
--start msg--

OPTIONS sip:poll-sip at 192.168.56.106:5058 SIP/2.0
Via: SIP/2.0/TCP 192.168.56.106;rport;branch=z9hG4bK-610944
Max-Forwards: 2
To: <sip:poll-sip at 192.168.56.106:5058>
From: poll-sip <sip:poll-sip at 192.168.56.106>;tag=610944
Call-ID: poll-sip-610944
CSeq: 610944 OPTIONS
Contact: <sip:192.168.56.106>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
03-08-2017 09:24:11.637 UTC Debug uri_classifier.cpp:160: home domain:
false, local_to_node: true, is_gruu: false, enforce_user_phone: false,
prefer_sip: true, treat_number_as_phone: false
03-08-2017 09:24:11.637 UTC Debug uri_classifier.cpp:206: Classified URI as
3
03-08-2017 09:24:11.637 UTC Debug common_sip_processing.cpp:183: Skipping
SAS logging for OPTIONS request
03-08-2017 09:24:11.637 UTC Debug thread_dispatcher.cpp:268: Queuing cloned
received message 0x7f9268003888 for worker threads
03-08-2017 09:24:11.637 UTC Debug thread_dispatcher.cpp:145: Worker thread
dequeue message 0x7f9268003888
03-08-2017 09:24:11.637 UTC Debug pjsip: sip_endpoint.c Distributing rdata
to modules: Request msg OPTIONS/cseq=610944 (rdata0x7f9268003888)
03-08-2017 09:24:11.637 UTC Debug uri_classifier.cpp:160: home domain:
false, local_to_node: true, is_gruu: false, enforce_user_phone: false,
prefer_sip: true, treat_number_as_phone: false
03-08-2017 09:24:11.637 UTC Debug uri_classifier.cpp:206: Classified URI as
3
03-08-2017 09:24:11.637 UTC Debug pjsip:       endpoint Response msg
200/OPTIONS/cseq=610944 (tdta0x7f926c0008e0) created
03-08-2017 09:24:11.637 UTC Verbose common_sip_processing.cpp:106: TX 290
bytes Response msg 200/OPTIONS/cseq=610944 (tdta0x7f926c0008e0) to TCP
192.168.56.106:55980:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP
192.168.56.106;rport=55980;received=192.168.56.106;branch=z9hG4bK-610944
Call-ID: poll-sip-610944
From: "poll-sip" <sip:poll-sip at 192.168.56.106>;tag=610944
To: <sip:poll-sip at 192.168.56.106>;tag=z9hG4bK-610944
CSeq: 610944 OPTIONS
Content-Length:  0


--end msg--
03-08-2017 09:24:11.637 UTC Debug common_sip_processing.cpp:232: Skipping
SAS logging for OPTIONS response
03-08-2017 09:24:11.637 UTC Debug pjsip: tdta0x7f926c00 Destroying txdata
Response msg 200/OPTIONS/cseq=610944 (tdta0x7f926c0008e0)
03-08-2017 09:24:11.637 UTC Debug thread_dispatcher.cpp:195: Worker thread
completed processing message 0x7f9268003888
03-08-2017 09:24:11.637 UTC Debug thread_dispatcher.cpp:201: Request
latency = 84us
03-08-2017 09:24:12.064 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.072 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.072 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.072 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.072 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1
03-08-2017 09:24:12.073 UTC Verbose dnscachedresolver.cpp:468: Check cache
for icscf.cwsprout1 type 1
03-08-2017 09:24:12.073 UTC Debug dnscachedresolver.cpp:570: Pulling 0
records from cache for icscf.cwsprout1 A
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:341: Found 0 A/AAAA
records, creating iterator
03-08-2017 09:24:12.073 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:61: SIPResolver::resolve
for name icscf.cwsprout1, port 5052, transport 6, family 2
03-08-2017 09:24:12.073 UTC Debug baseresolver.cpp:400: Attempt to parse
icscf.cwsprout1 as IP address
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:103: Port is specified
03-08-2017 09:24:12.073 UTC Debug sipresolver.cpp:271: Perform A/AAAA
record lookup only, name = icscf.cwsprout1


Thanks
Hrishikesh

On Mon, Jul 31, 2017 at 2:43 PM, Robert Day (projectclearwater.org) <
rob at projectclearwater.org> wrote:

> Hi Hrishikesh,
>
>
>
> I think there could be one of two problems here:
>
> ?         The request could be failing to reach your Bono node at all,
> due to incorrect port forwarding, local firewall rules, etc. ? so Zoiper is
> generating a 408 Request Timeout locally
>
> ?         Or the request could be reaching Bono, but it could be failing
> to pass it on, so Bono is generating the 408 Request Timeout
>
>
>
> To see which of these is happening, could you do one of the following
> (whatever?s easiest)?
>
> ?         Take packet capture on your local PC (e.g. with Wireshark), to
> see if Zoiper is successfully sending messages to Bono and getting the 408
> response (or if it?s just retransmitting them until a timeout)
>
> ?         Turn on debug logging on Bono (see
> http://clearwater.readthedocs.io/en/stable/Troubleshooting_
> and_Recovery.html#bono), and check the Bono logs to see if there are
> records of it receiving the SIP messages
>
>
>
> I saw from your other emails to the mailing list that you?re using
> /etc/hosts for DNS records, not an external DNS server. Is this still the
> case? Bono will attempt to look up SRV records (which /etc/hosts doesn?t
> provide) in order to reach Sprout nodes by default, which could be the
> problem here, and setting scscf_uri=?sip: cwsprout1:5054;transport=tcp?
> should resolve this (as explicitly specifying the port removes the need for
> an A record lookup).
>
>
>
> Thanks,
>
> Rob
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* 27 July 2017 12:22
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi,
>
> I have used manual installation for clearwater.
>
> I have created 6 VMs using Virtualbox and using host only network.
>
> All 6 nodes are running fine as per monit logs.
>
> I have installed Zoiper client and tried to add new account as follows,
>
> ===================================================
> Zoiper Preferences
>
> Domain        -    example.com
> Username    -    6505550708
> Password    -
> Auth. Uname    -    6505550708 at example.com
> Outbound Proxy    -    192.168.56.105:8060
>
> Error        -     SIP 408 - Request Timeout
>
>
> Private Identity Generated by Ellis
>
> Private Identity:
>
> 6505550028 at example.com
> Password:Na5ZWdQj4
>
> ===================================================
>
> How ever I am getting error mentioned in subject line.
>
> Here are other logs for each node,
>
> ===================================================
>
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
> local_ip=192.168.56.105
> public_ip=192.168.56.105
> public_hostname=cwellis1
> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,
> 192.168.56.108,192.168.56.109,192.168.56.110"
>
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout1
> sprout_registration_store=192.168.56.110 #vellum
> hs_hostname=192.168.56.109:8888 #dime
> hs_provisioning_hostname=192.168.56.109:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.108:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=192.168.56.110 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
> [sudo] password for cwellis1:
> Monit 5.18.1 uptime: 3h 33m
>  Service Name                     Status
> Type
>  node-cwellis1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [ellis]cwellis1 at cwellis1:~$
>
>
> [bono]cwbono1 at cwbono1:~$ sudo monit summary
> [sudo] password for cwbono1:
> Monit 5.18.1 uptime: 23m
>  Service Name                     Status
> Type
>  node-cwbono1                     Running
> System
>  restund_process                  Running
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat
> /etc/clearwater/local_config
> local_ip=192.168.56.105
> public_ip=192.168.56.105
> public_hostname=cwellis1
> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,
> 192.168.56.108,192.168.56.109,192.168.56.110"
>
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout1
> sprout_registration_store=192.168.56.110 #vellum
> hs_hostname=192.168.56.109:8888 #dime
> hs_provisioning_hostname=192.168.56.109:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.108:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=192.168.56.110 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
> [sudo] password for cwellis1:
> Monit 5.18.1 uptime: 3h 33m
>  Service Name                     Status
> Type
>  node-cwellis1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [ellis]cwellis1 at cwellis1:~$
>
>
> [bono]cwbono1 at cwbono1:~$ sudo monit summary
> [sudo] password for cwbono1:
> Monit 5.18.1 uptime: 23m
>  Service Name                     Status
> Type
>  node-cwbono1                     Running
> System
>  restund_process                  Running
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono1 at cwbono1:~$
>
>
>
> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
> [sudo] password for cwsprout1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwsprout1                   Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
>
>
>
>
> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
> [sudo] password for cwhomer1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwhomer1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [homer]cwhomer1 at cwhomer1:~$
>
>
> [dime]cwdime1 at cwdime1:~$ sudo monit summary
> [sudo] password for cwdime1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwdime1                     Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [dime]cwdime1 at cwdime1:~$
>
>
>
> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
> [sudo] password for cwvellum1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwvellum1                   Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [vellum]cwvellum1 at cwvellum1:~$
> 192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono1 at cwbono1:~$
>
>
>
> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
> [sudo] password for cwsprout1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwsprout1                   Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
>
>
>
>
> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
> [sudo] password for cwhomer1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwhomer1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [homer]cwhomer1 at cwhomer1:~$
>
>
> [dime]cwdime1 at cwdime1:~$ sudo monit summary
> [sudo] password for cwdime1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwdime1                     Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [dime]cwdime1 at cwdime1:~$
>
>
>
> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
> [sudo] password for cwvellum1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwvellum1                   Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [vellum]cwvellum1 at cwvellum1:~$
>
>
> ===================================================
>
> Please let me know what is wrong with the setup?
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170803/f412113c/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Zoiper.pcapng
Type: application/x-pcapng
Size: 57268 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170803/f412113c/attachment.bin>

From hkaranjikar at apm.com  Thu Aug  3 05:29:30 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Thu, 3 Aug 2017 14:59:30 +0530
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <BLUPR02MB437E06C13B70FCC491E0A27E5B20@BLUPR02MB437.namprd02.prod.outlook.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
	<BLUPR02MB437E06C13B70FCC491E0A27E5B20@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <CABmBNEanzSPTx5=QUczx34_=WLa3b0Ztbt3wZ57-AYx39UN0QQ@mail.gmail.com>

Hi Andrew,

Thanks a lot for your reply.
I did not mention the password but while doing the configuration I am using
it.

Thanks
Hrishikesh

On Mon, Jul 31, 2017 at 2:44 PM, Andrew Edmonds <
Andrew.Edmonds at metaswitch.com> wrote:

> Hi Hrishikesh,
>
>
>
> Thank you for your question.
>
>
>
> I?m not sure whether this is deliberate but it looks like you haven?t
> included the password in your Zoiper configuration. You must make sure you
> use the same password the Ellis client gives you.
>
>
>
> We have this guide
> <https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html>
> that might help you with the configuration you need to use with Zoiper.
>
>
>
> If this guide goes not help could I please have Bono and Sprout logs
> during the time of the registration.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, July 27, 2017 12:22 PM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi,
>
> I have used manual installation for clearwater.
>
> I have created 6 VMs using Virtualbox and using host only network.
>
> All 6 nodes are running fine as per monit logs.
>
> I have installed Zoiper client and tried to add new account as follows,
>
> ===================================================
> Zoiper Preferences
>
> Domain        -    example.com
> Username    -    6505550708
> Password    -
> Auth. Uname    -    6505550708 at example.com
> Outbound Proxy    -    192.168.56.105:8060
>
> Error        -     SIP 408 - Request Timeout
>
>
> Private Identity Generated by Ellis
>
> Private Identity:
>
> 6505550028 at example.com
> Password:Na5ZWdQj4
>
> ===================================================
>
> How ever I am getting error mentioned in subject line.
>
> Here are other logs for each node,
>
> ===================================================
>
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
> local_ip=192.168.56.105
> public_ip=192.168.56.105
> public_hostname=cwellis1
> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,
> 192.168.56.108,192.168.56.109,192.168.56.110"
>
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout1
> sprout_registration_store=192.168.56.110 #vellum
> hs_hostname=192.168.56.109:8888 #dime
> hs_provisioning_hostname=192.168.56.109:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.108:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=192.168.56.110 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
> [sudo] password for cwellis1:
> Monit 5.18.1 uptime: 3h 33m
>  Service Name                     Status
> Type
>  node-cwellis1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [ellis]cwellis1 at cwellis1:~$
>
>
> [bono]cwbono1 at cwbono1:~$ sudo monit summary
> [sudo] password for cwbono1:
> Monit 5.18.1 uptime: 23m
>  Service Name                     Status
> Type
>  node-cwbono1                     Running
> System
>  restund_process                  Running
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat
> /etc/clearwater/local_config
> local_ip=192.168.56.105
> public_ip=192.168.56.105
> public_hostname=cwellis1
> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,
> 192.168.56.108,192.168.56.109,192.168.56.110"
>
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout1
> sprout_registration_store=192.168.56.110 #vellum
> hs_hostname=192.168.56.109:8888 #dime
> hs_provisioning_hostname=192.168.56.109:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.108:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=192.168.56.110 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
> [sudo] password for cwellis1:
> Monit 5.18.1 uptime: 3h 33m
>  Service Name                     Status
> Type
>  node-cwellis1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [ellis]cwellis1 at cwellis1:~$
>
>
> [bono]cwbono1 at cwbono1:~$ sudo monit summary
> [sudo] password for cwbono1:
> Monit 5.18.1 uptime: 23m
>  Service Name                     Status
> Type
>  node-cwbono1                     Running
> System
>  restund_process                  Running
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono1 at cwbono1:~$
>
>
>
> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
> [sudo] password for cwsprout1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwsprout1                   Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
>
>
>
>
> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
> [sudo] password for cwhomer1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwhomer1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [homer]cwhomer1 at cwhomer1:~$
>
>
> [dime]cwdime1 at cwdime1:~$ sudo monit summary
> [sudo] password for cwdime1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwdime1                     Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [dime]cwdime1 at cwdime1:~$
>
>
>
> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
> [sudo] password for cwvellum1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwvellum1                   Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [vellum]cwvellum1 at cwvellum1:~$
> 192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono1 at cwbono1:~$
>
>
>
> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
> [sudo] password for cwsprout1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwsprout1                   Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
>
>
>
>
> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
> [sudo] password for cwhomer1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwhomer1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [homer]cwhomer1 at cwhomer1:~$
>
>
> [dime]cwdime1 at cwdime1:~$ sudo monit summary
> [sudo] password for cwdime1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwdime1                     Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [dime]cwdime1 at cwdime1:~$
>
>
>
> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
> [sudo] password for cwvellum1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwvellum1                   Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [vellum]cwvellum1 at cwvellum1:~$
>
>
> ===================================================
>
> Please let me know what is wrong with the setup?
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170803/926f94ae/attachment.html>

From Andrew.Edmonds at metaswitch.com  Thu Aug  3 06:00:47 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Thu, 3 Aug 2017 10:00:47 +0000
Subject: [Project Clearwater] [Clearwater] AS Configuration and
 untrusted sources
In-Reply-To: <CB76FEAE-A8B6-43A8-9F3E-9FADA45CEB3C@aleagames.com>
References: <7296A131-7DDD-43F1-9F00-5F85FCF40FF7@aleagames.com>
	<3ACF4837-B492-45A2-8B83-14202C9CFDEE@aleagames.com>
	<BLUPR02MB43730770DB57A115A4B287BE5BB0@BLUPR02MB437.namprd02.prod.outlook.com>
	<AAB4BECC-919D-43AB-A00A-78C1110C3244@aleagames.com>
	<BLUPR02MB437901136FF4E64F7DC3D14E5B80@BLUPR02MB437.namprd02.prod.outlook.com>
	<4AACAE0B-0199-49B4-9234-1FF4FFE2AEFC@aleagames.com>
	<6E2AAAE8-F6FB-4E4A-97B0-0480FC01EAA1@aleagames.com>
	<BLUPR02MB4371DC95390C2D4E830905AE5B20@BLUPR02MB437.namprd02.prod.outlook.com>
	<CB76FEAE-A8B6-43A8-9F3E-9FADA45CEB3C@aleagames.com>
Message-ID: <BLUPR02MB437C33565BDDFF0BC0B37C0E5B10@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Michele,

Thanks for the additional diagnostics.

In order to diagnose the problem I need to understand what you expect the flow between as1 and as2 to look like. How do you expect Clearwater will be involved in the routing here? Typically we route to application servers by using iFCs but from the looks of the sprout log the iFC used here doesn?t include as2. How do you expect calls to be routed to as2?

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Michele Furlanetto
Sent: Thursday, August 3, 2017 10:05 AM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources

Hi Andrew,

About the third-party registration I had misconfigurated something. So I restarted from the .ova image and now works.
The last thing I have to configure is to enable communication between as1 and as2.

In attachment you can find the files /etc/hosts, /etc/dnsmasq.conf and the logs of bono and sprout.


Thanks for the help,
Michele


Il giorno 31 lug 2017, alle ore 11:07, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> ha scritto:

Hi Michele,

Could I get some more diagnostics to try and work out why third-party registration is no longer working. The Sprout logs (which can be found on SPNs under /var/log/sprout/sprout_current.txt) for a registration may be helpful here.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Michele Furlanetto
Sent: Wednesday, July 26, 2017 4:40 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources

I went a little forward, removing the line relative to as1.service.example.com<http://as1.service.example.com/> in file /etc/hosts and adding

srv-host=_sip._udp.scscf.cw-aio,scscf.cw-aio,5054
srv-host=_sip._udp.as1.service.example.com<http://udp.as1.service.example.com/>,example.com<http://example.com/>,5071

to /etc/dnsmasq.conf.

now the SUBSCRIBEs (and other messages) are delivered to as1, but I?ve lost the working third-party registration.

After fixing this, the next step for me will be adding another AS, as2, which will dialog only with as1.
The simplest think I could try is adding
srv-host=_sip._udp.as2.service.example.com<http://udp.as2.service.example.com/>,example.com<http://example.com/>,5072
but is not enough. What configuration I?m missing?

Thank you,
Michele


Il giorno 26 lug 2017, alle ore 11:27, Michele Furlanetto <michele.furlanetto at aleagames.com<mailto:michele.furlanetto at aleagames.com>> ha scritto:

Hi Andrew,

As I said, I didn?t configure any DNS service to support our AS,
all I did in this way was adding the lines to /etc/hosts

10.2.0.91 as1.service.example.com<http://participant.mcptt.example.com/>
10.2.0.127 example.com<http://example.com/>

In attachment you can find the local_config and shared_config files.

Referring the previous message, my SIP client does use the same port for both REGISTER and SUBSCRIBE.

Thank you for your support,
Michele

<local and shared configs.zip>


Il giorno 25 lug 2017, alle ore 15:40, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> ha scritto:

Hi Michele,

I?ve spoken to a colleague about this issue and have a new idea now as to why the SUBSCRIBE message is being rejected.

After Bono receives the SUBSCRIBE from the SIP device we can see the log:

24-07-2017 10:38:25.414 UTC Verbose pjsip: tcpc0x7f094c03 TCP transport 10.2.0.127:5058 is connecting to 10.2.0.127:5060..

10.2.0.127 is the Bono node?s IP address. So this is telling us that Bono is forwarding the SUBSCRIBE to itself from its trusted port 5058 (used to communicate with the IMS core) to its untrusted port 5060 (which SIP devices send their requests to). We then see in the logs Bono receiving this SUBSCRIBE and then rejecting it.

So the question we need to resolve now becomes why is Bono routing the SUBSCRIBE to itself? To answer this we need to understand how SIP messages are routed. Messages containing SIP methods (like INVITEs, SUBSCRIBEs, REGISTERs etc.) are routed based off of the Route-header. If we look at the route headers for one of the SUBSCRIBERs we see:

Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690

Here the port of the S-CSCF (5054) has not been included in the SIP address of the S-CSCF. We can also see from the logs that we do not retrieve a port number via SRV DNS lookup:

24-07-2017 10:38:25.414 UTC Debug dnscachedresolver.cpp:686: Received DNS response for _sip._udp.scscf.cw-aio type SRV
24-07-2017 10:38:25.414 UTC Warning dnscachedresolver.cpp:828: Failed to retrieve record for _sip._udp.scscf.cw-aio: Domain name not found

This means that Bono does not know which port to forward the message to and so it chooses 5060 which routes the message back to itself.

The next thing to consider then is why does the Route-Header not contain the port number? The Route header which is to be applied to future requests for a subscriber is communicated to Bono by the S-CSCF in a Service-Route header after it successfully authenticates a subscriber. If we have a look at the 200 OK in response to the registration we see:

Service-Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690

This means that it is the S-CSCF that is causing Bono to attach a Route header to the SUBSCRIBE which does not contain a port number. To figure out why the S-CSCF is doing this could you please send me a copy of /etc/clearwater/shared_config and /etc/clearwater/local_config from the SPN and a copy of the SPN log file during a REGISTRATION and SUBSCRIBE.

Thanks,

Andrew
_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170803/5558cc20/attachment.html>

From Andrew.Edmonds at metaswitch.com  Mon Aug  7 06:58:22 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Mon, 7 Aug 2017 10:58:22 +0000
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <CABmBNEanzSPTx5=QUczx34_=WLa3b0Ztbt3wZ57-AYx39UN0QQ@mail.gmail.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
	<BLUPR02MB437E06C13B70FCC491E0A27E5B20@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEanzSPTx5=QUczx34_=WLa3b0Ztbt3wZ57-AYx39UN0QQ@mail.gmail.com>
Message-ID: <BLUPR02MB4379BCC901095CF549CF27FE5B50@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Hrishikesh,

Thanks for the updated diags.

One thing that can be useful when you are reading through a log file is to search for the pattern ?Error?. This will highlight those logs which indicate an error. If we do that here we see:

03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)

This suggests that Bono is unable to resolve the hostname ?icscf.cwsprout1?. We?ve spoken in the past about how you are unable to configure a DNS. Hence I would suggest configuring /etc/hosts to resolve icscf.cwsprout1.

Please let me know if this works.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, August 3, 2017 10:30 AM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] SIP 408 - Request Timeout

Hi Andrew,
Thanks a lot for your reply.
I did not mention the password but while doing the configuration I am using it.
Thanks
Hrishikesh

On Mon, Jul 31, 2017 at 2:44 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Thank you for your question.

I?m not sure whether this is deliberate but it looks like you haven?t included the password in your Zoiper configuration. You must make sure you use the same password the Ellis client gives you.

We have this guide<https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html> that might help you with the configuration you need to use with Zoiper.

If this guide goes not help could I please have Bono and Sprout logs during the time of the registration.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, July 27, 2017 12:22 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] SIP 408 - Request Timeout

Hi,
I have used manual installation for clearwater.
I have created 6 VMs using Virtualbox and using host only network.
All 6 nodes are running fine as per monit logs.
I have installed Zoiper client and tried to add new account as follows,

===================================================
Zoiper Preferences

Domain        -    example.com<http://example.com>
Username    -    6505550708
Password    -
Auth. Uname    -    6505550708 at example.com<mailto:6505550708 at example.com>
Outbound Proxy    -    192.168.56.105:8060<http://192.168.56.105:8060>

Error        -     SIP 408 - Request Timeout


Private Identity Generated by Ellis

Private Identity:

6505550028 at example.com<mailto:6505550028 at example.com>
Password:Na5ZWdQj4

===================================================
How ever I am getting error mentioned in subject line.
Here are other logs for each node,

===================================================

[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888<http://192.168.56.109:8888> #dime
hs_provisioning_hostname=192.168.56.109:8889<http://192.168.56.109:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888<http://192.168.56.108:7888> #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888<http://192.168.56.109:8888> #dime
hs_provisioning_hostname=192.168.56.109:8889<http://192.168.56.109:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888<http://192.168.56.108:7888> #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$
192.168.56.109:4000<http://192.168.56.109:4000> isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$


===================================================
Please let me know what is wrong with the setup?
Thanks
Hrishikesh



_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170807/025abd3d/attachment.html>

From hkaranjikar at apm.com  Tue Aug  8 05:15:25 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Tue, 8 Aug 2017 14:45:25 +0530
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <BLUPR02MB4379BCC901095CF549CF27FE5B50@BLUPR02MB437.namprd02.prod.outlook.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
	<BLUPR02MB437E06C13B70FCC491E0A27E5B20@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEanzSPTx5=QUczx34_=WLa3b0Ztbt3wZ57-AYx39UN0QQ@mail.gmail.com>
	<BLUPR02MB4379BCC901095CF549CF27FE5B50@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <CABmBNEb+4xu-qBE7Sq_cfO13GMc6WBxJBEpC8YqPrCGj_D=F8Q@mail.gmail.com>

Hi Andrew,

Thanks for your help.
I already have configured etc/hosts with the entry. After I added the entry
restund process started executing.
Bono can ping icscf.cwsprout1

[bono]cwbono1 at cwbono1:~$ ping icscf.cwsprout1
PING icscf.cwsprout1 (192.168.56.107) 56(84) bytes of data.
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=1 ttl=64
time=0.218 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=2 ttl=64
time=0.311 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=3 ttl=64
time=0.224 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=4 ttl=64
time=0.188 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=5 ttl=64
time=0.187 ms
^C

Thanks
Hrishikesh


On Mon, Aug 7, 2017 at 4:28 PM, Andrew Edmonds <
Andrew.Edmonds at metaswitch.com> wrote:

> Hi Hrishikesh,
>
>
>
> Thanks for the updated diags.
>
>
>
> One thing that can be useful when you are reading through a log file is to
> search for the pattern ?Error?. This will highlight those logs which
> indicate an error. If we do that here we see:
>
>
>
> 03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
>
>
>
> This suggests that Bono is unable to resolve the hostname
> ?icscf.cwsprout1?. We?ve spoken in the past about how you are unable to
> configure a DNS. Hence I would suggest configuring /etc/hosts to resolve
> icscf.cwsprout1.
>
>
>
> Please let me know if this works.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, August 3, 2017 10:30 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi Andrew,
>
> Thanks a lot for your reply.
>
> I did not mention the password but while doing the configuration I am
> using it.
>
> Thanks
>
> Hrishikesh
>
>
>
> On Mon, Jul 31, 2017 at 2:44 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
> Hi Hrishikesh,
>
>
>
> Thank you for your question.
>
>
>
> I?m not sure whether this is deliberate but it looks like you haven?t
> included the password in your Zoiper configuration. You must make sure you
> use the same password the Ellis client gives you.
>
>
>
> We have this guide
> <https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html>
> that might help you with the configuration you need to use with Zoiper.
>
>
>
> If this guide goes not help could I please have Bono and Sprout logs
> during the time of the registration.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, July 27, 2017 12:22 PM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi,
>
> I have used manual installation for clearwater.
>
> I have created 6 VMs using Virtualbox and using host only network.
>
> All 6 nodes are running fine as per monit logs.
>
> I have installed Zoiper client and tried to add new account as follows,
>
> ===================================================
> Zoiper Preferences
>
> Domain        -    example.com
> Username    -    6505550708
> Password    -
> Auth. Uname    -    6505550708 at example.com
> Outbound Proxy    -    192.168.56.105:8060
>
> Error        -     SIP 408 - Request Timeout
>
>
> Private Identity Generated by Ellis
>
> Private Identity:
>
> 6505550028 at example.com
> Password:Na5ZWdQj4
>
> ===================================================
>
> How ever I am getting error mentioned in subject line.
>
> Here are other logs for each node,
>
> ===================================================
>
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
> local_ip=192.168.56.105
> public_ip=192.168.56.105
> public_hostname=cwellis1
> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,
> 192.168.56.108,192.168.56.109,192.168.56.110"
>
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout1
> sprout_registration_store=192.168.56.110 #vellum
> hs_hostname=192.168.56.109:8888 #dime
> hs_provisioning_hostname=192.168.56.109:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.108:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=192.168.56.110 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
> [sudo] password for cwellis1:
> Monit 5.18.1 uptime: 3h 33m
>  Service Name                     Status
> Type
>  node-cwellis1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [ellis]cwellis1 at cwellis1:~$
>
>
> [bono]cwbono1 at cwbono1:~$ sudo monit summary
> [sudo] password for cwbono1:
> Monit 5.18.1 uptime: 23m
>  Service Name                     Status
> Type
>  node-cwbono1                     Running
> System
>  restund_process                  Running
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat
> /etc/clearwater/local_config
> local_ip=192.168.56.105
> public_ip=192.168.56.105
> public_hostname=cwellis1
> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,
> 192.168.56.108,192.168.56.109,192.168.56.110"
>
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout1
> sprout_registration_store=192.168.56.110 #vellum
> hs_hostname=192.168.56.109:8888 #dime
> hs_provisioning_hostname=192.168.56.109:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.108:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=192.168.56.110 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
> [sudo] password for cwellis1:
> Monit 5.18.1 uptime: 3h 33m
>  Service Name                     Status
> Type
>  node-cwellis1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [ellis]cwellis1 at cwellis1:~$
>
>
> [bono]cwbono1 at cwbono1:~$ sudo monit summary
> [sudo] password for cwbono1:
> Monit 5.18.1 uptime: 23m
>  Service Name                     Status
> Type
>  node-cwbono1                     Running
> System
>  restund_process                  Running
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono1 at cwbono1:~$
>
>
>
> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
> [sudo] password for cwsprout1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwsprout1                   Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
>
>
>
>
> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
> [sudo] password for cwhomer1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwhomer1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [homer]cwhomer1 at cwhomer1:~$
>
>
> [dime]cwdime1 at cwdime1:~$ sudo monit summary
> [sudo] password for cwdime1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwdime1                     Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [dime]cwdime1 at cwdime1:~$
>
>
>
> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
> [sudo] password for cwvellum1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwvellum1                   Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [vellum]cwvellum1 at cwvellum1:~$
> 192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono1 at cwbono1:~$
>
>
>
> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
> [sudo] password for cwsprout1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwsprout1                   Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
>
>
>
>
> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
> [sudo] password for cwhomer1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwhomer1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [homer]cwhomer1 at cwhomer1:~$
>
>
> [dime]cwdime1 at cwdime1:~$ sudo monit summary
> [sudo] password for cwdime1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwdime1                     Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [dime]cwdime1 at cwdime1:~$
>
>
>
> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
> [sudo] password for cwvellum1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwvellum1                   Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [vellum]cwvellum1 at cwvellum1:~$
>
>
> ===================================================
>
> Please let me know what is wrong with the setup?
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170808/b9172ce4/attachment.html>

From Andrew.Edmonds at metaswitch.com  Wed Aug  9 05:12:18 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Wed, 9 Aug 2017 09:12:18 +0000
Subject: [Project Clearwater] External PCRF/ Rx interface Support
In-Reply-To: <aa2c2cf549754dd6b7e598d4e2d8a30b@S02-MAIL002.G3TI.Local>
References: <aa2c2cf549754dd6b7e598d4e2d8a30b@S02-MAIL002.G3TI.Local>
Message-ID: <BLUPR02MB437FD4A8E9C3E6A9DC3E6CCE58B0@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Jobin,

Thank you for your interested in Clearwater.

Bono, which is part of our open source solution Project Clearwater, does not support an external PCRF. Metaswitch offers a P-CSCF which does support using an external PCRF, this is called Perimeta and you can read more about it here<https://www.metaswitch.com/perimeta-session-border-controller-sbc>.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Johnson, Jobin
Sent: Tuesday, August 1, 2017 12:51 PM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] External PCRF/ Rx interface Support


I was interested in trying out VoLTE with Project Clearwater. However, I can't seem to find any integration between the p-cscf/Bono and an external PCRF or something equivalent to tell the LTE network to setup a dedicated bearer with appropriate QoS for the UE during call setup. Are there any additional resources I can look into for this or did I miss it in the documentation?

Thank you in advance for any responses!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170809/c9268206/attachment.html>

From hkaranjikar at apm.com  Thu Aug 10 08:48:35 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Thu, 10 Aug 2017 18:18:35 +0530
Subject: [Project Clearwater] Sprout compilation failed on ARM64 platform
Message-ID: <CABmBNEbLNk7Ds-=hKsFfv1B6GgY=vyaqz3NeuOu4hW6wT8sx0A@mail.gmail.com>

Hi,

I am trying to compile sprout as per the instructions given in below link,

https://github.com/Metaswitch/sprout/blob/master/docs/Development.md

But I am getting error as follows,

===================================================================

cp -r --preserve=timestamps
/home/hrishikesh/Hrishikesh/sprout/build/module-install/usr/
/home/hrishikesh/Hrishikesh/sprout/usr/
make -C /home/hrishikesh/Hrishikesh/sprout/src
make[1]: Entering directory '/home/hrishikesh/Hrishikesh/sprout/src'
g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings
-I../include -I../modules/cpp-common/include
-I../modules/app-servers/include -I../usr/include
-I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig
pkg-config --cflags libpjproject`  -c ../modules/cpp-common/src/logger.cpp
-o ../build/sprout/logger.o
g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings
-I../include -I../modules/cpp-common/include
-I../modules/app-servers/include -I../usr/include
-I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig
pkg-config --cflags libpjproject`  -c
../modules/cpp-common/src/saslogger.cpp -o ../build/sprout/saslogger.o
g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings
-I../include -I../modules/cpp-common/include
-I../modules/app-servers/include -I../usr/include
-I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig
pkg-config --cflags libpjproject`  -c ../modules/cpp-common/src/utils.cpp
-o ../build/sprout/utils.o
g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings
-I../include -I../modules/cpp-common/include
-I../modules/app-servers/include -I../usr/include
-I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig
pkg-config --cflags libpjproject`  -c analyticslogger.cpp -o
../build/sprout/analyticslogger.o
g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings
-I../include -I../modules/cpp-common/include
-I../modules/app-servers/include -I../usr/include
-I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig
pkg-config --cflags libpjproject`  -c stack.cpp -o ../build/sprout/stack.o
In file included from stack.cpp:30:0:
../include/constants.h:183:27: error: ???METHOD_UPDATE??? defined but not
used [-Werror=unused-variable]
 const static pjsip_method METHOD_UPDATE = { PJSIP_OTHER_METHOD,
pj_str((char*)"
                           ^
../include/constants.h:184:27: error: ???METHOD_INFO??? defined but not
used [-Werror=unused-variable]
 const static pjsip_method METHOD_INFO = { PJSIP_OTHER_METHOD,
pj_str((char*)"IN
                           ^
cc1plus: all warnings being treated as errors
../build-infra/cpp.mk:222: recipe for target '../build/sprout/stack.o'
failed
make[1]: *** [../build/sprout/stack.o] Error 1
make[1]: Leaving directory '/home/hrishikesh/Hrishikesh/sprout/src'
/home/hrishikesh/Hrishikesh/sprout/mk/sprout.mk:14: recipe for target
'sprout' failed
make: *** [sprout] Error 2

===================================================================

Can you help me out with these?


Thanks
Hrishikesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170810/b12eab4c/attachment.html>

From Andrew.Edmonds at metaswitch.com  Thu Aug 10 08:57:10 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Thu, 10 Aug 2017 12:57:10 +0000
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <CABmBNEb+4xu-qBE7Sq_cfO13GMc6WBxJBEpC8YqPrCGj_D=F8Q@mail.gmail.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
	<BLUPR02MB437E06C13B70FCC491E0A27E5B20@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEanzSPTx5=QUczx34_=WLa3b0Ztbt3wZ57-AYx39UN0QQ@mail.gmail.com>
	<BLUPR02MB4379BCC901095CF549CF27FE5B50@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEb+4xu-qBE7Sq_cfO13GMc6WBxJBEpC8YqPrCGj_D=F8Q@mail.gmail.com>
Message-ID: <BLUPR02MB437E492B6592B3B3EBBEB99E5880@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Hrishikesh,

The process that Bono uses to resolve a DNS request is to:


?         make a DNS lookup - by default, to dnsmasq running on 127.0.0.1 (local host)

?         dnsmasq reads /etc/hosts

?         dnsmasq responds with an answer

The reason why this might be failing but pings are working is that pings may just read /etc/hosts directly rather than using dnsmasq.

Can you please try:


?         Running `dig icscf.cwsprout1 @127.0.0.1` - this will show whether dnsmasq is responding with an answer (and therefore whether the problem is in bono or dnsmasq)

?         Running `sudo service dnsmasq restart` - this might trigger it to pick up a change in the /etc/hosts file

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Hrishikesh Karanjikar
Sent: Tuesday, August 8, 2017 10:15 AM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] SIP 408 - Request Timeout

Hi Andrew,
Thanks for your help.
I already have configured etc/hosts with the entry. After I added the entry restund process started executing.
Bono can ping icscf.cwsprout1

[bono]cwbono1 at cwbono1:~$ ping icscf.cwsprout1
PING icscf.cwsprout1 (192.168.56.107) 56(84) bytes of data.
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=1 ttl=64 time=0.218 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=2 ttl=64 time=0.311 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=3 ttl=64 time=0.224 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=4 ttl=64 time=0.188 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=5 ttl=64 time=0.187 ms
^C
Thanks
Hrishikesh


On Mon, Aug 7, 2017 at 4:28 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Thanks for the updated diags.

One thing that can be useful when you are reading through a log file is to search for the pattern ?Error?. This will highlight those logs which indicate an error. If we do that here we see:

03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)

This suggests that Bono is unable to resolve the hostname ?icscf.cwsprout1?. We?ve spoken in the past about how you are unable to configure a DNS. Hence I would suggest configuring /etc/hosts to resolve icscf.cwsprout1.

Please let me know if this works.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, August 3, 2017 10:30 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP 408 - Request Timeout

Hi Andrew,
Thanks a lot for your reply.
I did not mention the password but while doing the configuration I am using it.
Thanks
Hrishikesh

On Mon, Jul 31, 2017 at 2:44 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Thank you for your question.

I?m not sure whether this is deliberate but it looks like you haven?t included the password in your Zoiper configuration. You must make sure you use the same password the Ellis client gives you.

We have this guide<https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html> that might help you with the configuration you need to use with Zoiper.

If this guide goes not help could I please have Bono and Sprout logs during the time of the registration.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, July 27, 2017 12:22 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] SIP 408 - Request Timeout

Hi,
I have used manual installation for clearwater.
I have created 6 VMs using Virtualbox and using host only network.
All 6 nodes are running fine as per monit logs.
I have installed Zoiper client and tried to add new account as follows,

===================================================
Zoiper Preferences

Domain        -    example.com<http://example.com>
Username    -    6505550708
Password    -
Auth. Uname    -    6505550708 at example.com<mailto:6505550708 at example.com>
Outbound Proxy    -    192.168.56.105:8060<http://192.168.56.105:8060>

Error        -     SIP 408 - Request Timeout


Private Identity Generated by Ellis

Private Identity:

6505550028 at example.com<mailto:6505550028 at example.com>
Password:Na5ZWdQj4

===================================================
How ever I am getting error mentioned in subject line.
Here are other logs for each node,

===================================================

[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888<http://192.168.56.109:8888> #dime
hs_provisioning_hostname=192.168.56.109:8889<http://192.168.56.109:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888<http://192.168.56.108:7888> #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888<http://192.168.56.109:8888> #dime
hs_provisioning_hostname=192.168.56.109:8889<http://192.168.56.109:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888<http://192.168.56.108:7888> #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$
192.168.56.109:4000<http://192.168.56.109:4000> isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$


===================================================
Please let me know what is wrong with the setup?
Thanks
Hrishikesh



_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170810/25658d09/attachment.html>

From Andrew.Edmonds at metaswitch.com  Fri Aug 11 07:02:33 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Fri, 11 Aug 2017 11:02:33 +0000
Subject: [Project Clearwater] Sprout compilation failed on ARM64 platform
In-Reply-To: <CABmBNEbLNk7Ds-=hKsFfv1B6GgY=vyaqz3NeuOu4hW6wT8sx0A@mail.gmail.com>
References: <CABmBNEbLNk7Ds-=hKsFfv1B6GgY=vyaqz3NeuOu4hW6wT8sx0A@mail.gmail.com>
Message-ID: <BLUPR02MB4375FC5773B4E1F703A71EBE5890@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Hrishikish,

Thank you for your continued support of Project Clearwater.

Having a look through the output of the make command it looks as though warnings (which would not usually cause the compilation to abort) are being treated as errors and causing the compiler to stop. This suggests that the ?-Werror? flag has been enabled, perhaps it?s a default on your compiler. We can see more evidence for this here:

cc1plus: all warnings being treated as errors

The instructions on how to disable ?Werror may change depending on which compiler you are using. You can try including the ?-i? option in the make command (i.e. ?make -i??).

Please let me know if this works

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, August 10, 2017 1:49 PM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Sprout compilation failed on ARM64 platform

Hi,
I am trying to compile sprout as per the instructions given in below link,

https://github.com/Metaswitch/sprout/blob/master/docs/Development.md
But I am getting error as follows,

===================================================================

cp -r --preserve=timestamps /home/hrishikesh/Hrishikesh/sprout/build/module-install/usr/ /home/hrishikesh/Hrishikesh/sprout/usr/
make -C /home/hrishikesh/Hrishikesh/sprout/src
make[1]: Entering directory '/home/hrishikesh/Hrishikesh/sprout/src'
g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings -I../include -I../modules/cpp-common/include -I../modules/app-servers/include -I../usr/include -I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig pkg-config --cflags libpjproject`  -c ../modules/cpp-common/src/logger.cpp -o ../build/sprout/logger.o
g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings -I../include -I../modules/cpp-common/include -I../modules/app-servers/include -I../usr/include -I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig pkg-config --cflags libpjproject`  -c ../modules/cpp-common/src/saslogger.cpp -o ../build/sprout/saslogger.o
g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings -I../include -I../modules/cpp-common/include -I../modules/app-servers/include -I../usr/include -I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig pkg-config --cflags libpjproject`  -c ../modules/cpp-common/src/utils.cpp -o ../build/sprout/utils.o
g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings -I../include -I../modules/cpp-common/include -I../modules/app-servers/include -I../usr/include -I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig pkg-config --cflags libpjproject`  -c analyticslogger.cpp -o ../build/sprout/analyticslogger.o
g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings -I../include -I../modules/cpp-common/include -I../modules/app-servers/include -I../usr/include -I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig pkg-config --cflags libpjproject`  -c stack.cpp -o ../build/sprout/stack.o
In file included from stack.cpp:30:0:
../include/constants.h:183:27: error: ???METHOD_UPDATE??? defined but not used [-Werror=unused-variable]
 const static pjsip_method METHOD_UPDATE = { PJSIP_OTHER_METHOD, pj_str((char*)"
                           ^
../include/constants.h:184:27: error: ???METHOD_INFO??? defined but not used [-Werror=unused-variable]
 const static pjsip_method METHOD_INFO = { PJSIP_OTHER_METHOD, pj_str((char*)"IN
                           ^
cc1plus: all warnings being treated as errors
../build-infra/cpp.mk:222<http://cpp.mk:222>: recipe for target '../build/sprout/stack.o' failed
make[1]: *** [../build/sprout/stack.o] Error 1
make[1]: Leaving directory '/home/hrishikesh/Hrishikesh/sprout/src'
/home/hrishikesh/Hrishikesh/sprout/mk/sprout.mk:14<http://sprout.mk:14>: recipe for target 'sprout' failed
make: *** [sprout] Error 2

===================================================================
Can you help me out with these?

Thanks
Hrishikesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170811/98988f32/attachment.html>

From hkaranjikar at apm.com  Mon Aug 14 05:49:11 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Mon, 14 Aug 2017 15:19:11 +0530
Subject: [Project Clearwater] Sprout compilation failed on ARM64 platform
In-Reply-To: <BLUPR02MB4375FC5773B4E1F703A71EBE5890@BLUPR02MB437.namprd02.prod.outlook.com>
References: <CABmBNEbLNk7Ds-=hKsFfv1B6GgY=vyaqz3NeuOu4hW6wT8sx0A@mail.gmail.com>
	<BLUPR02MB4375FC5773B4E1F703A71EBE5890@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <CABmBNEaAc6OSTkC49a3Gnw_Z5_PknL4oi9Gv5WycTqcMfORrrA@mail.gmail.com>

Hi Andrew,

With -Wno-error the compilation moved ahead. But now I am getting some
issue as follows.
Is it anything to do with specific curl version? My current version is
7.47.0

=================================================================================

make[3]: Entering directory
'/home/hrishikesh/Hrishikesh/sprout/modules/curl/src'
/bin/bash ../libtool  --tag=CC   --mode=link gcc  -Wno-error -O2
-Wno-system-headers   -L/home/hrishikesh/Hrishikesh/sprout/usr/lib -ldl -o
curl curl-tool_binmode.o curl-tool_bname.o curl-tool_cb_dbg.o
curl-tool_cb_hdr.o curl-tool_cb_prg.o curl-tool_cb_rea.o curl-tool_cb_see.o
curl-tool_cb_wrt.o curl-tool_cfgable.o curl-tool_convert.o
curl-tool_dirhie.o curl-tool_doswin.o curl-tool_easysrc.o
curl-tool_formparse.o curl-tool_getparam.o curl-tool_getpass.o
curl-tool_help.o curl-tool_helpers.o curl-tool_homedir.o
curl-tool_hugehelp.o curl-tool_libinfo.o curl-tool_main.o
curl-tool_metalink.o curl-tool_mfiles.o curl-tool_msgs.o
curl-tool_operate.o curl-tool_operhlp.o curl-tool_panykey.o
curl-tool_paramhlp.o curl-tool_parsecfg.o curl-tool_strdup.o
curl-tool_setopt.o curl-tool_sleep.o curl-tool_urlglob.o curl-tool_util.o
curl-tool_vms.o curl-tool_writeenv.o curl-tool_writeout.o curl-tool_xattr.o
../lib/curl-strtoofft.o ../lib/curl-rawstr.o ../lib/curl-nonblock.o
../lib/curl-warnless.o  ../lib/libcurl.la   -lssl -lcrypto -lz
libtool: link: gcc -Wno-error -O2 -Wno-system-headers -o .libs/curl
curl-tool_binmode.o curl-tool_bname.o curl-tool_cb_dbg.o curl-tool_cb_hdr.o
curl-tool_cb_prg.o curl-tool_cb_rea.o curl-tool_cb_see.o curl-tool_cb_wrt.o
curl-tool_cfgable.o curl-tool_convert.o curl-tool_dirhie.o
curl-tool_doswin.o curl-tool_easysrc.o curl-tool_formparse.o
curl-tool_getparam.o curl-tool_getpass.o curl-tool_help.o
curl-tool_helpers.o curl-tool_homedir.o curl-tool_hugehelp.o
curl-tool_libinfo.o curl-tool_main.o curl-tool_metalink.o
curl-tool_mfiles.o curl-tool_msgs.o curl-tool_operate.o curl-tool_operhlp.o
curl-tool_panykey.o curl-tool_paramhlp.o curl-tool_parsecfg.o
curl-tool_strdup.o curl-tool_setopt.o curl-tool_sleep.o curl-tool_urlglob.o
curl-tool_util.o curl-tool_vms.o curl-tool_writeenv.o curl-tool_writeout.o
curl-tool_xattr.o ../lib/curl-strtoofft.o ../lib/curl-rawstr.o
../lib/curl-nonblock.o ../lib/curl-warnless.o
-L/home/hrishikesh/Hrishikesh/sprout/usr/lib -ldl ../lib/.libs/libcurl.so
-lssl -lcrypto -lz -Wl,-rpath -Wl,/home/hrishikesh/Hrishikesh/sprout/usr/lib
../lib/.libs/libcurl.so: undefined reference to `SSL_get0_alpn_selected'
../lib/.libs/libcurl.so: undefined reference to `SSL_CTX_set_alpn_protos'
collect2: error: ld returned 1 exit status
Makefile:770: recipe for target 'curl' failed
make[3]: *** [curl] Error 1
make[3]: Leaving directory
'/home/hrishikesh/Hrishikesh/sprout/modules/curl/src'
Makefile:649: recipe for target 'all' failed
make[2]: *** [all] Error 2
make[2]: Leaving directory
'/home/hrishikesh/Hrishikesh/sprout/modules/curl/src'
Makefile:862: recipe for target 'all-recursive' failed
make[1]: *** [all-recursive] Error 1
make[1]: Leaving directory '/home/hrishikesh/Hrishikesh/sprout/modules/curl'
/home/hrishikesh/Hrishikesh/sprout/mk/curl.mk:14: recipe for target 'curl'
failed
make: *** [curl] Error 2


=================================================================================


Thanks
Hrishikesh

On Fri, Aug 11, 2017 at 4:32 PM, Andrew Edmonds <
Andrew.Edmonds at metaswitch.com> wrote:

> Hi Hrishikish,
>
>
>
> Thank you for your continued support of Project Clearwater.
>
>
>
> Having a look through the output of the make command it looks as though
> warnings (which would not usually cause the compilation to abort) are being
> treated as errors and causing the compiler to stop. This suggests that the
> ?-Werror? flag has been enabled, perhaps it?s a default on your compiler.
> We can see more evidence for this here:
>
>
>
> cc1plus: all warnings being treated as errors
>
>
>
> The instructions on how to disable ?Werror may change depending on which
> compiler you are using. You can try including the ?-i? option in the make
> command (i.e. ?make -i??).
>
>
>
> Please let me know if this works
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, August 10, 2017 1:49 PM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Sprout compilation failed on ARM64
> platform
>
>
>
> Hi,
>
> I am trying to compile sprout as per the instructions given in below link,
>
> https://github.com/Metaswitch/sprout/blob/master/docs/Development.md
>
> But I am getting error as follows,
>
> ===================================================================
>
> cp -r --preserve=timestamps /home/hrishikesh/Hrishikesh/
> sprout/build/module-install/usr/ /home/hrishikesh/Hrishikesh/sprout/usr/
> make -C /home/hrishikesh/Hrishikesh/sprout/src
> make[1]: Entering directory '/home/hrishikesh/Hrishikesh/sprout/src'
> g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings
> -I../include -I../modules/cpp-common/include -I../modules/app-servers/include
> -I../usr/include -I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig
> pkg-config --cflags libpjproject`  -c ../modules/cpp-common/src/logger.cpp
> -o ../build/sprout/logger.o
> g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings
> -I../include -I../modules/cpp-common/include -I../modules/app-servers/include
> -I../usr/include -I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig
> pkg-config --cflags libpjproject`  -c ../modules/cpp-common/src/saslogger.cpp
> -o ../build/sprout/saslogger.o
> g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings
> -I../include -I../modules/cpp-common/include -I../modules/app-servers/include
> -I../usr/include -I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig
> pkg-config --cflags libpjproject`  -c ../modules/cpp-common/src/utils.cpp
> -o ../build/sprout/utils.o
> g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings
> -I../include -I../modules/cpp-common/include -I../modules/app-servers/include
> -I../usr/include -I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig
> pkg-config --cflags libpjproject`  -c analyticslogger.cpp -o
> ../build/sprout/analyticslogger.o
> g++   -MMD -MP -O2 -ggdb3 -std=c++11 -Wall -Werror -Wno-write-strings
> -I../include -I../modules/cpp-common/include -I../modules/app-servers/include
> -I../usr/include -I../modules/rapidjson/include `PKG_CONFIG_PATH=../usr/lib/pkgconfig
> pkg-config --cflags libpjproject`  -c stack.cpp -o ../build/sprout/stack.o
> In file included from stack.cpp:30:0:
> ../include/constants.h:183:27: error: ???METHOD_UPDATE??? defined but not
> used [-Werror=unused-variable]
>  const static pjsip_method METHOD_UPDATE = { PJSIP_OTHER_METHOD,
> pj_str((char*)"
>                            ^
> ../include/constants.h:184:27: error: ???METHOD_INFO??? defined but not
> used [-Werror=unused-variable]
>  const static pjsip_method METHOD_INFO = { PJSIP_OTHER_METHOD,
> pj_str((char*)"IN
>                            ^
> cc1plus: all warnings being treated as errors
> ../build-infra/cpp.mk:222: recipe for target '../build/sprout/stack.o'
> failed
> make[1]: *** [../build/sprout/stack.o] Error 1
> make[1]: Leaving directory '/home/hrishikesh/Hrishikesh/sprout/src'
> /home/hrishikesh/Hrishikesh/sprout/mk/sprout.mk:14: recipe for target
> 'sprout' failed
> make: *** [sprout] Error 2
>
> ===================================================================
>
> Can you help me out with these?
>
>
>
> Thanks
>
> Hrishikesh
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170814/49e09a64/attachment.html>

From pramchan at yahoo.com  Tue Aug 15 18:39:55 2017
From: pramchan at yahoo.com (prakash RAMCHANDRAN)
Date: Tue, 15 Aug 2017 22:39:55 +0000 (UTC)
Subject: [Project Clearwater] [Installation] Has any one tried Clearwater
 vIMS on OpenStack Ocata with Heat Orchestrator
References: <1995159145.2409936.1502836795446.ref@mail.yahoo.com>
Message-ID: <1995159145.2409936.1502836795446@mail.yahoo.com>

Hi all,
I am looking to install Clearwater vIMS on a? Ubuntu 140.04 Cloud image on Rackspce generation 1 server with 1-30 GB RAM with Ceph / Swift back end for Nova/Heat.Refer to bottom of the link below.
https://github.com/Metaswitch/clearwater-heat/
Q1. Will ubuntu Cloudimage just needs a single external network to hang on as it states
Q2. Will the other private networks be automatically created by using clearwater.yaml?
Q3. If not do we have to prepare a separate user-data file for cloud-int or it will create that out from the associated yaml files and scripts. If that happens do we have to loginto ubunto cloud image and get clone the clearwater repo on to the server before it can reboot and create all cool vIMS components and we can jsut test it as needed using various sip tools?
For now this should suffice for me to attempt launch one and see what happens.
Any words of advice from experienced? or even hackers welcome.
ReagrdsPrakash



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170815/080aec15/attachment.html>

From pramchan at yahoo.com  Wed Aug 16 13:27:15 2017
From: pramchan at yahoo.com (prakash RAMCHANDRAN)
Date: Wed, 16 Aug 2017 17:27:15 +0000 (UTC)
Subject: [Project Clearwater] [Installation] Has any one tried
 Clearwater vIMS on OpenStack Ocata with Heat Orchestrator
In-Reply-To: <1995159145.2409936.1502836795446@mail.yahoo.com>
References: <1995159145.2409936.1502836795446.ref@mail.yahoo.com>
	<1995159145.2409936.1502836795446@mail.yahoo.com>
Message-ID: <695899428.3059667.1502904435489@mail.yahoo.com>




On Tuesday, August 15, 2017, 3:39:55 PM PDT, prakash RAMCHANDRAN <pramchan at yahoo.com> wrote:

Hi all,
I am looking to install Clearwater vIMS on a? Ubuntu 140.04 Cloud image on Rackspce generation 1 server with 1-30 GB RAM with Ceph / Swift back end for Nova/Heat.Refer to bottom of the link below.
https://github.com/Metaswitch/clearwater-heat/
Q1. Will ubuntu Cloudimage just needs a single external network to hang on as it statesAns 1
I tried yes thats correct and used a single public network to point to both mgmt and sig ntworks in Openstack Oacta Stable with heat orchestration.it creates two private networks for use and adds all compments and stiches them together using private networks it creates on two submets 192.168.0.0/24 and 192.168.1.0/24 ;listed in mail clearwater.yaml file.

Q2. Will the other private networks be automatically created by using clearwater.yaml?Ans 2
yes as above in Ans1.

Q3. If not do we have to prepare a separate user-data file for cloud-int or it will create that out from the associated yaml files and scripts. If that happens do we have to loginto ubunto cloud image and get clone the clearwater repo on to the server before it can reboot and create all cool vIMS components and we can jsut test it as needed using various sip tools?
Ans 3
Heat though Nova addresses it as part of cloud-init the necessary userdata aor config drive. Still the part of logging into vIMS I am working on, but on dashboard it shows some failure due to quota exceeded for security groups and working on it. Any pointers to fixing that welcome. 


For now this should suffice for me to attempt launch one and see what happens.Ans: Tried and stack goes active but due to resource limitations fails on quota for now. Will update once have better resource or workaround.

used command: (Please comment if this needs change)1. Install devstack with heat referhttps://docs.openstack.org/devstack/latest/
2. Set up Demo project env using$ source openrc demo demo
3. git clone the celarwater repogit clone https://github.com/Metaswitch/clearwater-heat.git
$ cd clearwarer-heat
4, Follow procedures on Clearwater heat template on link (see at end of page on link)
https://github.com/Metaswitch/clearwater-heat
5. run clearwater vIMS using heat
openstack stack create -t clearwater.yaml --parameter "image=trustyVM;flavor=m1.small;key_name=demokey04;public_mgmt_net_id=bb888011-1579-45f4-9c65-14883fd77426;public_sig_net_id=bb888011-1579-45f4-9c65-14883fd77426;dnssec_key=H/PVeRl7SBFpOwPTVCoxCQoq8bFu2AEsX+vb1H7j5Kpvdgfh6ykU0uwQKtbsSaONESMOtl" vIMS
openstack stack list
+----------------+------------+---------------+----------------+--------------+
| ID???????????? | Stack Name | Stack Status??| Creation Time??| Updated Time |
+----------------+------------+---------------+----------------+--------------+
| 4dee9e81-a87f- | vIMS?????? | CREATE_FAILED | 2017-08-16T09: | None???????? |
| 47b9-ab92-0818 |????????????|?????????????? | 06:48Z???????? |??????????????|
| 676c3df5?????? |????????????|?????????????? |????????????????|??????????????|

Any words of advice from experienced? or even hackers welcome.

ReagardsPrakash



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170816/56aaa480/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Selection_277.png
Type: image/png
Size: 55499 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170816/56aaa480/attachment.png>

From Robert.Day at metaswitch.com  Fri Aug 18 11:35:01 2017
From: Robert.Day at metaswitch.com (Robert Day)
Date: Fri, 18 Aug 2017 15:35:01 +0000
Subject: [Project Clearwater] [Installation] Has any one tried
 Clearwater vIMS on OpenStack Ocata with Heat Orchestrator
In-Reply-To: <1995159145.2409936.1502836795446@mail.yahoo.com>
References: <1995159145.2409936.1502836795446.ref@mail.yahoo.com>
	<1995159145.2409936.1502836795446@mail.yahoo.com>
Message-ID: <BY2PR02MB21494F73CB5CD485DCE7A5ECF4800@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Prakash,

Yes, the Heat stack should create the private networks it needs (that?s the networks.yaml file) ? it then allocates public floating IPs from the provided public network.

You shouldn?t need a separate user-data file ? the ?heat stack-create? command should create a complete set of working VMs.

Good luck, and let us know how it goes!

Best,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of prakash RAMCHANDRAN
Sent: 15 August 2017 23:40
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] [Installation] Has any one tried Clearwater vIMS on OpenStack Ocata with Heat Orchestrator

Hi all,

I am looking to install Clearwater vIMS on a  Ubuntu 140.04 Cloud image on Rackspce generation 1 server with 1-30 GB RAM with Ceph / Swift back end for Nova/Heat.Refer to bottom of the link below.
https://github.com/Metaswitch/clearwater-heat/

Q1. Will ubuntu Cloudimage just needs a single external network to hang on as it states

Q2. Will the other private networks be automatically created by using clearwater.yaml?

Q3. If not do we have to prepare a separate user-data file for cloud-int or it will create that out from the associated yaml files and scripts. If that happens do we have to loginto ubunto cloud image and get clone the clearwater repo on to the server before it can reboot and create all cool vIMS components and we can jsut test it as needed using various sip tools?

For now this should suffice for me to attempt launch one and see what happens.

Any words of advice from experienced  or even hackers welcome.

Reagrds
Prakash



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170818/c0fe75f1/attachment.html>

From hkaranjikar at apm.com  Tue Aug 22 05:51:10 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Tue, 22 Aug 2017 15:21:10 +0530
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <BLUPR02MB437E492B6592B3B3EBBEB99E5880@BLUPR02MB437.namprd02.prod.outlook.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
	<BLUPR02MB437E06C13B70FCC491E0A27E5B20@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEanzSPTx5=QUczx34_=WLa3b0Ztbt3wZ57-AYx39UN0QQ@mail.gmail.com>
	<BLUPR02MB4379BCC901095CF549CF27FE5B50@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEb+4xu-qBE7Sq_cfO13GMc6WBxJBEpC8YqPrCGj_D=F8Q@mail.gmail.com>
	<BLUPR02MB437E492B6592B3B3EBBEB99E5880@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <CABmBNEaT9TZ1h94v7P5n67qY8ZL3bGyac2Ucr_ALiKp=g55HFg@mail.gmail.com>

Hi Andrew,

Thanks for your constant support. I am facing a weired problem. All of my
nodes were working fine. I sent you the logs earlier.
I restarted all nodes and suddenly etcd_process on all nodes is failed to
execute. I don't know what is the reason. Will you please guide me on this.
All nodes are able to ping one another.

Here are some logs

[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
cluster may be unhealthy: failed to list members
Error:  client: etcd cluster is unavailable or misconfigured; error #0:
dial tcp 192.168.56.105:4000: getsockopt: connection refused

error #0: dial tcp 192.168.56.105:4000: getsockopt: connection refused

[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
Error:  client: etcd cluster is unavailable or misconfigured; error #0:
dial tcp 192.168.56.105:4000: getsockopt: connection refused

error #0: dial tcp 192.168.56.105:4000: getsockopt: connection refused

2017-08-22 14:42:44.966660634 Running etcdctl cluster-health
cluster may be unhealthy: failed to list members
Error:  client: etcd cluster is unavailable or misconfigured; error #0:
client: endpoint http://192.168.56.110:4000 exceeded header timeout

#cat /var/log/clearwater-etcd/clearwater-etcd-initd.log

2017-08-22 15:17:22.731984254 etcdctl returned 4
2017-08-22 15:17:22.732730248 Not joining an unhealthy cluster
2017-08-22 15:18:02.429508523 Restarting etcd clearwater-etcd
2017-08-22 15:18:02.433629983 Configured ETCDCTL_PEERS: 192.168.56.107:4000,
192.168.56.109:4000,192.168.56.110:4000,192.168.56.108:4000,
2017-08-22 15:18:02.434237379 Check for previous failed startup attempt
2017-08-22 15:18:02.435071900 Running etcdctl member list
client: etcd cluster is unavailable or misconfigured; error #0: dial tcp
192.168.56.110:4000: getsockopt: connection refused
; error #1: dial tcp 192.168.56.107:4000: getsockopt: connection refused
; error #2: dial tcp 192.168.56.109:4000: getsockopt: connection refused
; error #3: dial tcp 192.168.56.108:4000: getsockopt: connection refused
; error #4: http: no Host in request URL

2017-08-22 15:18:02.447231505 etcdctl returned 1
2017-08-22 15:18:02.453120361 Joining existing cluster...
2017-08-22 15:18:13.456387174 Configured ETCDCTL_PEERS: 192.168.56.107:4000,
192.168.56.109:4000,192.168.56.110:4000,192.168.56.108:4000,
2017-08-22 15:18:13.457279845 Check cluster is healthy
2017-08-22 15:18:13.458636020 Running etcdctl cluster-health
cluster may be unhealthy: failed to list members
Error:  client: etcd cluster is unavailable or misconfigured; error #0:
http: no Host in request URL
; error #1: dial tcp 192.168.56.109:4000: getsockopt: connection refused
; error #2: dial tcp 192.168.56.108:4000: getsockopt: connection refused
; error #3: dial tcp 192.168.56.110:4000: getsockopt: connection refused
; error #4: dial tcp 192.168.56.107:4000: getsockopt: connection refused

error #0: http: no Host in request URL
error #1: dial tcp 192.168.56.109:4000: getsockopt: connection refused
error #2: dial tcp 192.168.56.108:4000: getsockopt: connection refused
error #3: dial tcp 192.168.56.110:4000: getsockopt: connection refused
error #4: dial tcp 192.168.56.107:4000: getsockopt: connection refused

2017-08-22 15:18:13.470356301 etcdctl returned 4
2017-08-22 15:18:13.471296869 Not joining an unhealthy cluster
2017-08-22 15:18:43.177856227 Restarting etcd clearwater-etcd
2017-08-22 15:18:43.182076210 Configured ETCDCTL_PEERS: 192.168.56.107:4000,
192.168.56.109:4000,192.168.56.110:4000,192.168.56.108:4000,
2017-08-22 15:18:43.182623728 Check for previous failed startup attempt
2017-08-22 15:18:43.183439470 Running etcdctl member list
client: etcd cluster is unavailable or misconfigured; error #0: dial tcp
192.168.56.107:4000: getsockopt: connection refused
; error #1: dial tcp 192.168.56.108:4000: getsockopt: connection refused
; error #2: dial tcp 192.168.56.109:4000: getsockopt: connection refused
; error #3: http: no Host in request URL
; error #4: dial tcp 192.168.56.110:4000: getsockopt: connection refused

2017-08-22 15:18:43.195788447 etcdctl returned 1
2017-08-22 15:18:43.201223078 Joining existing cluster...


Thanks
Hrishikesh


On Thu, Aug 10, 2017 at 6:27 PM, Andrew Edmonds <
Andrew.Edmonds at metaswitch.com> wrote:

> Hi Hrishikesh,
>
>
>
> The process that Bono uses to resolve a DNS request is to:
>
>
>
> ?         make a DNS lookup - by default, to dnsmasq running on 127.0.0.1
> (local host)
>
> ?         dnsmasq reads /etc/hosts
>
> ?         dnsmasq responds with an answer
>
>
>
> The reason why this might be failing but pings are working is that pings
> may just read /etc/hosts directly rather than using dnsmasq.
>
>
>
> Can you please try:
>
>
>
> ?         Running `dig icscf.cwsprout1 @127.0.0.1` - this will show
> whether dnsmasq is responding with an answer (and therefore whether the
> problem is in bono or dnsmasq)
>
> ?         Running `sudo service dnsmasq restart` - this might trigger it
> to pick up a change in the /etc/hosts file
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Tuesday, August 8, 2017 10:15 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi Andrew,
>
> Thanks for your help.
>
> I already have configured etc/hosts with the entry. After I added the
> entry restund process started executing.
>
> Bono can ping icscf.cwsprout1
>
> [bono]cwbono1 at cwbono1:~$ ping icscf.cwsprout1
> PING icscf.cwsprout1 (192.168.56.107) 56(84) bytes of data.
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=1 ttl=64
> time=0.218 ms
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=2 ttl=64
> time=0.311 ms
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=3 ttl=64
> time=0.224 ms
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=4 ttl=64
> time=0.188 ms
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=5 ttl=64
> time=0.187 ms
> ^C
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
> On Mon, Aug 7, 2017 at 4:28 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
> Hi Hrishikesh,
>
>
>
> Thanks for the updated diags.
>
>
>
> One thing that can be useful when you are reading through a log file is to
> search for the pattern ?Error?. This will highlight those logs which
> indicate an error. If we do that here we see:
>
>
>
> 03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
>
>
>
> This suggests that Bono is unable to resolve the hostname
> ?icscf.cwsprout1?. We?ve spoken in the past about how you are unable to
> configure a DNS. Hence I would suggest configuring /etc/hosts to resolve
> icscf.cwsprout1.
>
>
>
> Please let me know if this works.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, August 3, 2017 10:30 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi Andrew,
>
> Thanks a lot for your reply.
>
> I did not mention the password but while doing the configuration I am
> using it.
>
> Thanks
>
> Hrishikesh
>
>
>
> On Mon, Jul 31, 2017 at 2:44 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
> Hi Hrishikesh,
>
>
>
> Thank you for your question.
>
>
>
> I?m not sure whether this is deliberate but it looks like you haven?t
> included the password in your Zoiper configuration. You must make sure you
> use the same password the Ellis client gives you.
>
>
>
> We have this guide
> <https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html>
> that might help you with the configuration you need to use with Zoiper.
>
>
>
> If this guide goes not help could I please have Bono and Sprout logs
> during the time of the registration.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, July 27, 2017 12:22 PM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi,
>
> I have used manual installation for clearwater.
>
> I have created 6 VMs using Virtualbox and using host only network.
>
> All 6 nodes are running fine as per monit logs.
>
> I have installed Zoiper client and tried to add new account as follows,
>
> ===================================================
> Zoiper Preferences
>
> Domain        -    example.com
> Username    -    6505550708
> Password    -
> Auth. Uname    -    6505550708 at example.com
> Outbound Proxy    -    192.168.56.105:8060
>
> Error        -     SIP 408 - Request Timeout
>
>
> Private Identity Generated by Ellis
>
> Private Identity:
>
> 6505550028 at example.com
> Password:Na5ZWdQj4
>
> ===================================================
>
> How ever I am getting error mentioned in subject line.
>
> Here are other logs for each node,
>
> ===================================================
>
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
> local_ip=192.168.56.105
> public_ip=192.168.56.105
> public_hostname=cwellis1
> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,
> 192.168.56.108,192.168.56.109,192.168.56.110"
>
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout1
> sprout_registration_store=192.168.56.110 #vellum
> hs_hostname=192.168.56.109:8888 #dime
> hs_provisioning_hostname=192.168.56.109:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.108:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=192.168.56.110 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
> [sudo] password for cwellis1:
> Monit 5.18.1 uptime: 3h 33m
>  Service Name                     Status
> Type
>  node-cwellis1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [ellis]cwellis1 at cwellis1:~$
>
>
> [bono]cwbono1 at cwbono1:~$ sudo monit summary
> [sudo] password for cwbono1:
> Monit 5.18.1 uptime: 23m
>  Service Name                     Status
> Type
>  node-cwbono1                     Running
> System
>  restund_process                  Running
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat
> /etc/clearwater/local_config
> local_ip=192.168.56.105
> public_ip=192.168.56.105
> public_hostname=cwellis1
> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,
> 192.168.56.108,192.168.56.109,192.168.56.110"
>
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout1
> sprout_registration_store=192.168.56.110 #vellum
> hs_hostname=192.168.56.109:8888 #dime
> hs_provisioning_hostname=192.168.56.109:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.108:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=192.168.56.110 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
> [sudo] password for cwellis1:
> Monit 5.18.1 uptime: 3h 33m
>  Service Name                     Status
> Type
>  node-cwellis1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [ellis]cwellis1 at cwellis1:~$
>
>
> [bono]cwbono1 at cwbono1:~$ sudo monit summary
> [sudo] password for cwbono1:
> Monit 5.18.1 uptime: 23m
>  Service Name                     Status
> Type
>  node-cwbono1                     Running
> System
>  restund_process                  Running
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono1 at cwbono1:~$
>
>
>
> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
> [sudo] password for cwsprout1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwsprout1                   Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
>
>
>
>
> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
> [sudo] password for cwhomer1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwhomer1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [homer]cwhomer1 at cwhomer1:~$
>
>
> [dime]cwdime1 at cwdime1:~$ sudo monit summary
> [sudo] password for cwdime1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwdime1                     Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [dime]cwdime1 at cwdime1:~$
>
>
>
> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
> [sudo] password for cwvellum1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwvellum1                   Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [vellum]cwvellum1 at cwvellum1:~$
> 192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono1 at cwbono1:~$
>
>
>
> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
> [sudo] password for cwsprout1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwsprout1                   Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
>
>
>
>
> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
> [sudo] password for cwhomer1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwhomer1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [homer]cwhomer1 at cwhomer1:~$
>
>
> [dime]cwdime1 at cwdime1:~$ sudo monit summary
> [sudo] password for cwdime1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwdime1                     Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [dime]cwdime1 at cwdime1:~$
>
>
>
> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
> [sudo] password for cwvellum1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwvellum1                   Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [vellum]cwvellum1 at cwvellum1:~$
>
>
> ===================================================
>
> Please let me know what is wrong with the setup?
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170822/1960ffa1/attachment.html>

From Robert.Day at metaswitch.com  Fri Aug 25 15:17:49 2017
From: Robert.Day at metaswitch.com (Robert Day)
Date: Fri, 25 Aug 2017 19:17:49 +0000
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <CABmBNEaT9TZ1h94v7P5n67qY8ZL3bGyac2Ucr_ALiKp=g55HFg@mail.gmail.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
	<BLUPR02MB437E06C13B70FCC491E0A27E5B20@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEanzSPTx5=QUczx34_=WLa3b0Ztbt3wZ57-AYx39UN0QQ@mail.gmail.com>
	<BLUPR02MB4379BCC901095CF549CF27FE5B50@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEb+4xu-qBE7Sq_cfO13GMc6WBxJBEpC8YqPrCGj_D=F8Q@mail.gmail.com>
	<BLUPR02MB437E492B6592B3B3EBBEB99E5880@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEaT9TZ1h94v7P5n67qY8ZL3bGyac2Ucr_ALiKp=g55HFg@mail.gmail.com>
Message-ID: <BY2PR02MB214906F5CE748977C1F924F3F49B0@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Hrishikesh,

Could you send /var/log/clearwater-etcd/clearwater-etcd.log (which is different to /var/log/clearwater-etcd/clearwater-etcd-initd.log) from each of your nodes? That should help tell what?s going on here.

Thanks,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Hrishikesh Karanjikar
Sent: 22 August 2017 10:51
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] SIP 408 - Request Timeout

Hi Andrew,
Thanks for your constant support. I am facing a weired problem. All of my nodes were working fine. I sent you the logs earlier.
I restarted all nodes and suddenly etcd_process on all nodes is failed to execute. I don't know what is the reason. Will you please guide me on this.
All nodes are able to ping one another.
Here are some logs

[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
cluster may be unhealthy: failed to list members
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 192.168.56.105:4000<http://192.168.56.105:4000>: getsockopt: connection refused

error #0: dial tcp 192.168.56.105:4000<http://192.168.56.105:4000>: getsockopt: connection refused

[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 192.168.56.105:4000<http://192.168.56.105:4000>: getsockopt: connection refused

error #0: dial tcp 192.168.56.105:4000<http://192.168.56.105:4000>: getsockopt: connection refused

2017-08-22 14:42:44.966660634 Running etcdctl cluster-health
cluster may be unhealthy: failed to list members
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint http://192.168.56.110:4000 exceeded header timeout

#cat /var/log/clearwater-etcd/clearwater-etcd-initd.log

2017-08-22 15:17:22.731984254 etcdctl returned 4
2017-08-22 15:17:22.732730248 Not joining an unhealthy cluster
2017-08-22 15:18:02.429508523 Restarting etcd clearwater-etcd
2017-08-22 15:18:02.433629983 Configured ETCDCTL_PEERS: 192.168.56.107:4000<http://192.168.56.107:4000>,192.168.56.109:4000<http://192.168.56.109:4000>,192.168.56.110:4000<http://192.168.56.110:4000>,192.168.56.108:4000<http://192.168.56.108:4000>,
2017-08-22 15:18:02.434237379 Check for previous failed startup attempt
2017-08-22 15:18:02.435071900 Running etcdctl member list
client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 192.168.56.110:4000<http://192.168.56.110:4000>: getsockopt: connection refused
; error #1: dial tcp 192.168.56.107:4000<http://192.168.56.107:4000>: getsockopt: connection refused
; error #2: dial tcp 192.168.56.109:4000<http://192.168.56.109:4000>: getsockopt: connection refused
; error #3: dial tcp 192.168.56.108:4000<http://192.168.56.108:4000>: getsockopt: connection refused
; error #4: http: no Host in request URL

2017-08-22 15:18:02.447231505 etcdctl returned 1
2017-08-22 15:18:02.453120361 Joining existing cluster...
2017-08-22 15:18:13.456387174 Configured ETCDCTL_PEERS: 192.168.56.107:4000<http://192.168.56.107:4000>,192.168.56.109:4000<http://192.168.56.109:4000>,192.168.56.110:4000<http://192.168.56.110:4000>,192.168.56.108:4000<http://192.168.56.108:4000>,
2017-08-22 15:18:13.457279845 Check cluster is healthy
2017-08-22 15:18:13.458636020 Running etcdctl cluster-health
cluster may be unhealthy: failed to list members
Error:  client: etcd cluster is unavailable or misconfigured; error #0: http: no Host in request URL
; error #1: dial tcp 192.168.56.109:4000<http://192.168.56.109:4000>: getsockopt: connection refused
; error #2: dial tcp 192.168.56.108:4000<http://192.168.56.108:4000>: getsockopt: connection refused
; error #3: dial tcp 192.168.56.110:4000<http://192.168.56.110:4000>: getsockopt: connection refused
; error #4: dial tcp 192.168.56.107:4000<http://192.168.56.107:4000>: getsockopt: connection refused

error #0: http: no Host in request URL
error #1: dial tcp 192.168.56.109:4000<http://192.168.56.109:4000>: getsockopt: connection refused
error #2: dial tcp 192.168.56.108:4000<http://192.168.56.108:4000>: getsockopt: connection refused
error #3: dial tcp 192.168.56.110:4000<http://192.168.56.110:4000>: getsockopt: connection refused
error #4: dial tcp 192.168.56.107:4000<http://192.168.56.107:4000>: getsockopt: connection refused

2017-08-22 15:18:13.470356301 etcdctl returned 4
2017-08-22 15:18:13.471296869 Not joining an unhealthy cluster
2017-08-22 15:18:43.177856227 Restarting etcd clearwater-etcd
2017-08-22 15:18:43.182076210 Configured ETCDCTL_PEERS: 192.168.56.107:4000<http://192.168.56.107:4000>,192.168.56.109:4000<http://192.168.56.109:4000>,192.168.56.110:4000<http://192.168.56.110:4000>,192.168.56.108:4000<http://192.168.56.108:4000>,
2017-08-22 15:18:43.182623728 Check for previous failed startup attempt
2017-08-22 15:18:43.183439470 Running etcdctl member list
client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 192.168.56.107:4000<http://192.168.56.107:4000>: getsockopt: connection refused
; error #1: dial tcp 192.168.56.108:4000<http://192.168.56.108:4000>: getsockopt: connection refused
; error #2: dial tcp 192.168.56.109:4000<http://192.168.56.109:4000>: getsockopt: connection refused
; error #3: http: no Host in request URL
; error #4: dial tcp 192.168.56.110:4000<http://192.168.56.110:4000>: getsockopt: connection refused

2017-08-22 15:18:43.195788447 etcdctl returned 1
2017-08-22 15:18:43.201223078 Joining existing cluster...

Thanks
Hrishikesh


On Thu, Aug 10, 2017 at 6:27 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

The process that Bono uses to resolve a DNS request is to:


?         make a DNS lookup - by default, to dnsmasq running on 127.0.0.1 (local host)

?         dnsmasq reads /etc/hosts

?         dnsmasq responds with an answer

The reason why this might be failing but pings are working is that pings may just read /etc/hosts directly rather than using dnsmasq.

Can you please try:


?         Running `dig icscf.cwsprout1 @127.0.0.1<http://127.0.0.1>` - this will show whether dnsmasq is responding with an answer (and therefore whether the problem is in bono or dnsmasq)

?         Running `sudo service dnsmasq restart` - this might trigger it to pick up a change in the /etc/hosts file

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Tuesday, August 8, 2017 10:15 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP 408 - Request Timeout

Hi Andrew,
Thanks for your help.
I already have configured etc/hosts with the entry. After I added the entry restund process started executing.
Bono can ping icscf.cwsprout1

[bono]cwbono1 at cwbono1:~$ ping icscf.cwsprout1
PING icscf.cwsprout1 (192.168.56.107) 56(84) bytes of data.
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=1 ttl=64 time=0.218 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=2 ttl=64 time=0.311 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=3 ttl=64 time=0.224 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=4 ttl=64 time=0.188 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=5 ttl=64 time=0.187 ms
^C
Thanks
Hrishikesh


On Mon, Aug 7, 2017 at 4:28 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Thanks for the updated diags.

One thing that can be useful when you are reading through a log file is to search for the pattern ?Error?. This will highlight those logs which indicate an error. If we do that here we see:

03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)

This suggests that Bono is unable to resolve the hostname ?icscf.cwsprout1?. We?ve spoken in the past about how you are unable to configure a DNS. Hence I would suggest configuring /etc/hosts to resolve icscf.cwsprout1.

Please let me know if this works.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, August 3, 2017 10:30 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP 408 - Request Timeout

Hi Andrew,
Thanks a lot for your reply.
I did not mention the password but while doing the configuration I am using it.
Thanks
Hrishikesh

On Mon, Jul 31, 2017 at 2:44 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Thank you for your question.

I?m not sure whether this is deliberate but it looks like you haven?t included the password in your Zoiper configuration. You must make sure you use the same password the Ellis client gives you.

We have this guide<https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html> that might help you with the configuration you need to use with Zoiper.

If this guide goes not help could I please have Bono and Sprout logs during the time of the registration.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, July 27, 2017 12:22 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] SIP 408 - Request Timeout

Hi,
I have used manual installation for clearwater.
I have created 6 VMs using Virtualbox and using host only network.
All 6 nodes are running fine as per monit logs.
I have installed Zoiper client and tried to add new account as follows,

===================================================
Zoiper Preferences

Domain        -    example.com<http://example.com>
Username    -    6505550708
Password    -
Auth. Uname    -    6505550708 at example.com<mailto:6505550708 at example.com>
Outbound Proxy    -    192.168.56.105:8060<http://192.168.56.105:8060>

Error        -     SIP 408 - Request Timeout


Private Identity Generated by Ellis

Private Identity:

6505550028 at example.com<mailto:6505550028 at example.com>
Password:Na5ZWdQj4

===================================================
How ever I am getting error mentioned in subject line.
Here are other logs for each node,

===================================================

[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888<http://192.168.56.109:8888> #dime
hs_provisioning_hostname=192.168.56.109:8889<http://192.168.56.109:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888<http://192.168.56.108:7888> #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888<http://192.168.56.109:8888> #dime
hs_provisioning_hostname=192.168.56.109:8889<http://192.168.56.109:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888<http://192.168.56.108:7888> #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$
192.168.56.109:4000<http://192.168.56.109:4000> isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$


===================================================
Please let me know what is wrong with the setup?
Thanks
Hrishikesh



_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170825/71710200/attachment.html>

From pramchan at yahoo.com  Fri Aug 25 16:12:14 2017
From: pramchan at yahoo.com (prakash RAMCHANDRAN)
Date: Fri, 25 Aug 2017 20:12:14 +0000 (UTC)
Subject: [Project Clearwater] [installation & testing] Need help on
 Clearwater
References: <4342232.2512519.1503691934048.ref@mail.yahoo.com>
Message-ID: <4342232.2512519.1503691934048@mail.yahoo.com>

Hi all,
[installation] [DevStack on RackSpace Gen1 servers] - Stlll has DNS bind issues 
Previously I had send an email thread on Install and had couple issues and successfully over come them.The one I had on Quota was easy to set through
openstck quota set --security group 100 <tenant>
openstack quota set --secgroup-rules 100 <tenant>


The recognition of DNS zone was not resolvable either by installing bind9 with local zone or enabling designate through DevStack on RackSpace Gen1 server or memory intensive servers in Gen1. So if some one tries better use Gen2 servers as Gen1 doe not appear to scale for even minimum all-in-one install of Clearwater-heat template based. If any one succeeds with gen2 servers on Rack space please share the information.

[Installation] OPNFV os-nosdn-nofeature-ha with Compass4nfv installer - Working Successful

The compass4nfv is a open source Installer with great features and is able to install in an OPNFV vPOD using Docker container on Jumphost calling host1 to install openstack ocata with both Haet and Ceph like a charm. I used the latest Danube main and you can try Danube 3.0 or 3.1 and create the Clearwater vIMS VNF. 
Having crossed this milestone looking for test suggestions and in do's and dont's from community involved.

[Testing] Celarwater vIMS in OPNFV ( should be usable for both OPNAP and OpenBaton Integration - Integrated with OPNFV (NFVI+VIM) as a System under Test(SUT) with vIMS being a use case.

OPNFV has FUNCTEST and that is usable throgh API calls and uses Swagger API and SNAP api calls to Openstack modules. That runs and able to test few test cases simple ones and working to fix others.
Help needed in:? Clearwater Functionla Verification test

1. https://github.com/Metaswitch/clearwater-fv-test
Possibly recursive one does not go enough or not allowed to go deep enough to get module codes. Is that correct?git clone --recursive https://github.com/Metaswitch/clearwater-fv-test.git
Thus although it says sucessfully cloned the make fails?
The 'mk test" on root fails at first module

module Astaire not accesible or you are not permitted to access?You mean these are not open source?


2. https://github.com/Metaswitch/cpp-common/tree/master/test_utils

This is cpp tests and mock library is used in OPNFV FUNCTEST. Now not sure why SNMP here, as most use some servers like Zabbix (in OPNFV Doctor) and telemetry from OpenStack in OPNFV (Cielomeer, AODH etc.) So are there any comon tests that we can use or euse with variations of these. Any DevOps folks if yu tried this give some insights to me.

3. The rake test I have read both manual and automated and are in Ruby, some of them OPNFV has adapted through python, but like to hear if we consider any vIMS (Clearwater or compatible commercial offerings) do you see role for Rake test in that or Clearwater-live-tests as everyone calls it?
Hope to hear your inputs and lets build on this and OPNFV MANO WG is trying to support all MANO stacks to embrace the rich Scenarios and flexibility available through Clearwater vIMS to community of Vendors and Service providers.
ThanksPrakash





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170825/69222c9e/attachment.html>

From ankitpatwa92 at gmail.com  Thu Aug 24 11:32:51 2017
From: ankitpatwa92 at gmail.com (Ankit Patwa)
Date: Thu, 24 Aug 2017 21:02:51 +0530
Subject: [Project Clearwater] SIP Registeration Failed
Message-ID: <CA+qu+1fersTYSCD5YDBnCekzjoP1j3amG_Lo=vOmpmhFUc__rA@mail.gmail.com>

Hi,

we have installed all in one image on linux 14.04 serer.
After login on ellis public and private identifier generated.

Private identifier
6505550677 at 192.168.5.20
Password rnqYR4s7d

Public identifier
sip:6505550677 at 192.168.5.20

we are giving

username                 6505550677
password                  rnqYR4s7d
server                       192.168.5.20
Authentication user   sip:6505550677 at 192.168.5.20
Transport Type          TCP

*Device    Redmi  *
*Android  marshmallow*

*But I am getting registration failed error.*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170824/16b4dda9/attachment.html>

From abdulbasit.ta at gmail.com  Sun Aug 27 01:24:03 2017
From: abdulbasit.ta at gmail.com (Abdul Basit Alvi)
Date: Sun, 27 Aug 2017 10:24:03 +0500
Subject: [Project Clearwater] Cassandra and etcd clustering problem
Message-ID: <CAEAO5tZ3Bdm+b=BHDkag7dYztDeqJJyoatVMAuwFLrk+x9XA9w@mail.gmail.com>

Hi,

I have been trying to make the IMS work via manual install. I have followed
all the instructions to the dot and have tried starting from scratch
multiple times, but somehow I cant figure out why Cassandra and etcd
clustering are not working properly.

In the Dime node homestead process is not running, this I know is because
it cant connect to the vellum node cassandra via the thrift port 9160.

Next in the Vellum node the cassandra process is running but not working at
all. I have attached the system log files as well as cassandra.yaml and the
cassandra-env.sh file. For test purposes I have allowed all incomming
TCP/UDP traffic to and from all nodes.

Can you kindly look at the logs and outputs and point out what am I doing
wrong?

*[Monit summary of dime]*
root at dime-0:/home/ubuntu# monit summary
Monit 5.18.1 uptime: 6h 27m
* Service Name                     Status                      Type   *

 node-dime-0.test.com             Running                     System
 snmpd_process                    Running                     Process
 ralf_process                     Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Does not exist              Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 ralf_uptime                      Status ok                   Program
 poll_ralf                        Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Wait parent                 Program
 poll_homestead                   Wait parent                 Program
 check_cx_health                  Wait parent                 Program
 poll_homestead-prov              Status failed               Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status failed               Program
 poll_etcd                        Status ok                   Program
[Dime Local config]

*[etcd cluster health dime]*
root at dime-0:/home/ubuntu# clearwater-etcdctl cluster-health
member 5cd7042180fbb2a is unhealthy: got unhealthy result from
http://192.168.0.6:4000
failed to check the health of member 208dd0fbcefb149c on
http://192.168.0.8:4000: Get http://192.168.0.8:4000/health: dial tcp
192.168.0.8:4000: getsockopt: connection refused
member 208dd0fbcefb149c is unreachable: [http://192.168.0.8:4000] are all
unreachable
member 2457d2c8a20fc738 is unhealthy: got unhealthy result from
http://192.168.0.5:4000
member 48fa49be2b2ae2c2 is unhealthy: got unhealthy result from
http://192.168.0.3:4000
failed to check the health of member a48134f48b185ad9 on
http://192.168.0.7:4000: Get http://192.168.0.7:4000/health: dial tcp
192.168.0.7:4000: getsockopt: connection refused
member a48134f48b185ad9 is unreachable: [http://192.168.0.7:4000] are all
unreachable
member e7db4eebbdb94a11 is unhealthy: got unhealthy result from
http://192.168.0.4:4000
cluster is unhealthy

*[local config file dime]*
root at dime-0:/home/ubuntu# cat /etc/clearwater/local_config
local_ip=192.168.0.4
public_ip=10.1.10.192
public_hostname=dime-0.test.com
etcd_cluster=192.168.0.3,192.168.0.4,192.168.0.5,192.168.0.6,192.168.0.7,192.168.0.8

*[shared config file dime]*
root at vellum-0:/home/ubuntu# cat /etc/clearwater/shared_config
# Deployment definitions
home_domain=test.com
sprout_hostname=sprout.test.com
hs_hostname=hs.test.com:8888
hs_provisioning_hostname=hs-prov.test.com:8889
dime_hostname=dime.test.com:10888
xdms_hostname=homer.test.com:7888
sprout_impi_store=vellum.test.com
sprout_registration_store=vellum.test.com
cassandra_hostname=vellum.test.com
chronos_hostname=vellum.test.com
dime_session_store=vellum.test.com

upstream_port=0

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

*[Error Log Homestead]*
Thrift: Sun Aug 27 05:04:01 2017 TSocket::open() error on socket (after
THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:04:02 2017 TSocket::open() error on socket (after
THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:04:44 2017 TSocket::open() error on socket (after
THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:04:44 2017 TSocket::open() error on socket (after
THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:05:08 2017 TSocket::open() error on socket (after
THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:05:08 2017 TSocket::open() error on socket (after
THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:05:30 2017 TSocket::open() error on socket (after
THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:05:30 2017 TSocket::open() error on socket (after
THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:05:38 2017 TSocket::open() error on socket (after
THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:05:38 2017 TSocket::open() error on socket (after
THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused

*[Running cqlsh on vellum]*
root at vellum-0:/home/ubuntu# cqlsh
Connection error: ('Unable to connect to any servers', {'127.0.0.1':
error(111, "Tried connecting to [('127.0.0.1', 9042)]. Last error:
Connection refused")}

*[Monit summary of vellum]*
root at vellum-0:/home/ubuntu# monit summary
Monit 5.18.1 uptime: 6h 39m
 Service Name                     Status                      Type
 node-vellum-0.test.com           Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Execution failed | Does...  Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Wait parent                 Program
 poll_etcd_cluster                Wait parent                 Program
 poll_etcd                        Wait parent                 Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status failed               Program
 astaire_uptime                   Status ok                   Program

*[local config file vellum]*
root at vellum-0:/home/ubuntu# cat /etc/clearwater/local_config
local_ip=192.168.0.8
public_ip=10.1.10.204
public_hostname=vellum-0.test.com
etcd_cluster=192.168.0.3,192.168.0.4,192.168.0.5,192.168.0.6,192.168.0.7,192.168.0.8


*[shared config file vellum]*
root at vellum-0:/home/ubuntu# cat /etc/clearwater/shared_config
# Deployment definitions
home_domain=test.com
sprout_hostname=sprout.test.com
hs_hostname=hs.test.com:8888
hs_provisioning_hostname=hs-prov.test.com:8889
dime_hostname=dime.test.com:10888
xdms_hostname=homer.test.com:7888
sprout_impi_store=vellum.test.com
sprout_registration_store=vellum.test.com
cassandra_hostname=vellum.test.com
chronos_hostname=vellum.test.com
dime_session_store=vellum.test.com

upstream_port=0

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

*[netstat on vellum]*
root at vellum-0:/home/ubuntu# netstat -tulnap
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address
State       PID/Program name
tcp        0      0 192.168.0.8:11211       0.0.0.0:*
LISTEN      27005/memcached
tcp        0      0 127.0.0.1:7253          0.0.0.0:*
LISTEN      27610/chronos
tcp        0      0 255.255.255.255:7253    0.0.0.0:*
LISTEN      27610/chronos
tcp        0      0 127.0.0.1:53            0.0.0.0:*
LISTEN      7718/dnsmasq
tcp        0      0 0.0.0.0:22              0.0.0.0:*
LISTEN      1189/sshd
tcp        0      0 127.0.0.1:2812          0.0.0.0:*
LISTEN      7833/monit
tcp        0      0 192.168.0.8:44035       192.168.0.8:11211
TIME_WAIT   -
tcp        0      0 127.0.0.1:54026         127.0.0.1:7253
TIME_WAIT   -
tcp        0      0 192.168.0.8:44064       192.168.0.8:11211
TIME_WAIT   -
tcp        0      0 192.168.0.8:44081       192.168.0.8:11211
TIME_WAIT   -
tcp        0      0 192.168.0.8:44053       192.168.0.8:11211
TIME_WAIT   -
tcp        0      0 192.168.0.8:44054       192.168.0.8:11211
TIME_WAIT   -
tcp        0      0 192.168.0.8:44048       192.168.0.8:11211
TIME_WAIT   -
tcp        0      0 192.168.0.8:22          10.1.10.112:51998
ESTABLISHED 26454/sshd: ubuntu
tcp        0    268 192.168.0.8:22          10.1.10.112:51933
ESTABLISHED 22779/sshd: ubuntu
tcp6       0      0 :::11311                :::*
LISTEN      26657/astaire
tcp6       0      0 ::1:53                  :::*
LISTEN      7718/dnsmasq
tcp6       0      0 :::22                   :::*
LISTEN      1189/sshd
udp        0      0 127.0.0.1:53            0.0.0.0:*
7718/dnsmasq
udp        0      0 0.0.0.0:68              0.0.0.0:*
601/dhclient
udp        0      0 192.168.0.8:123         0.0.0.0:*
7362/ntpd
udp        0      0 127.0.0.1:123           0.0.0.0:*
7362/ntpd
udp        0      0 0.0.0.0:123             0.0.0.0:*
7362/ntpd
udp        0      0 0.0.0.0:161             0.0.0.0:*
7472/snmpd
udp        0      0 0.0.0.0:55423           0.0.0.0:*
601/dhclient
udp6       0      0 :::23767
:::*                                601/dhclient
udp6       0      0 ::1:53
:::*                                7718/dnsmasq
udp6       0      0 ::1:123
:::*                                7362/ntpd
udp6       0      0 fe80::f816:3eff:fe3:123
:::*                                7362/ntpd
udp6       0      0 :::123
:::*                                7362/ntpd
udp6       0      0 :::161
:::*                                7472/snmpd

*[Ping results]*
root at vellum-0:/home/ubuntu# ping hs-prov.test.com
PING hs-prov.test.com (192.168.0.4) 56(84) bytes of data.
64 bytes from 192.168.0.4: icmp_seq=1 ttl=64 time=10.3 ms
64 bytes from 192.168.0.4: icmp_seq=2 ttl=64 time=12.1 ms
64 bytes from 192.168.0.4: icmp_seq=3 ttl=64 time=9.06 ms
^C
--- hs-prov.test.com ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2032ms
rtt min/avg/max/mdev = 9.063/10.529/12.151/1.270 ms
root at vellum-0:/home/ubuntu#
root at vellum-0:/home/ubuntu# ping hs.test.com
PING hs.test.com (192.168.0.4) 56(84) bytes of data.
64 bytes from 192.168.0.4: icmp_seq=1 ttl=64 time=21.3 ms
64 bytes from 192.168.0.4: icmp_seq=2 ttl=64 time=4.58 ms
64 bytes from 192.168.0.4: icmp_seq=3 ttl=64 time=20.6 ms
^C
--- hs.test.com ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2070ms
rtt min/avg/max/mdev = 4.584/15.523/21.363/7.741 ms
root at vellum-0:/home/ubuntu# ping dime-0.test.com
PING dime-0.test.com (10.1.10.192) 56(84) bytes of data.
64 bytes from 10.1.10.192: icmp_seq=1 ttl=63 time=25.9 ms
64 bytes from 10.1.10.192: icmp_seq=2 ttl=63 time=58.9 ms
^C
--- dime-0.test.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1029ms
rtt min/avg/max/mdev = 25.954/42.459/58.964/16.505 ms

root at bono-0:/home/ubuntu# ping vellum-0.test.com
PING vellum-0.test.com (10.1.10.204) 56(84) bytes of data.
64 bytes from 10.1.10.204: icmp_seq=1 ttl=63 time=36.7 ms
64 bytes from 10.1.10.204: icmp_seq=2 ttl=63 time=25.2 ms
^C
--- vellum-0.test.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 25.240/30.985/36.730/5.745 ms
root at bono-0:/home/ubuntu# ping vellum.test.com
PING vellum.test.com (192.168.0.8) 56(84) bytes of data.
64 bytes from 192.168.0.8: icmp_seq=1 ttl=64 time=51.4 ms
64 bytes from 192.168.0.8: icmp_seq=2 ttl=64 time=3.93 ms
64 bytes from 192.168.0.8: icmp_seq=3 ttl=64 time=4.22 ms

Regards,

Abdul Basit Alvi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170827/fac689e0/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: cassandra-env.sh
Type: application/x-sh
Size: 12132 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170827/fac689e0/attachment.sh>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: cassandra.yaml
Type: application/octet-stream
Size: 3014 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170827/fac689e0/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: system.log
Type: application/octet-stream
Size: 323169 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170827/fac689e0/attachment-0001.obj>

From hkaranjikar at apm.com  Tue Aug 29 05:25:13 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Tue, 29 Aug 2017 14:55:13 +0530
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <BY2PR02MB214906F5CE748977C1F924F3F49B0@BY2PR02MB2149.namprd02.prod.outlook.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
	<BLUPR02MB437E06C13B70FCC491E0A27E5B20@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEanzSPTx5=QUczx34_=WLa3b0Ztbt3wZ57-AYx39UN0QQ@mail.gmail.com>
	<BLUPR02MB4379BCC901095CF549CF27FE5B50@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEb+4xu-qBE7Sq_cfO13GMc6WBxJBEpC8YqPrCGj_D=F8Q@mail.gmail.com>
	<BLUPR02MB437E492B6592B3B3EBBEB99E5880@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEaT9TZ1h94v7P5n67qY8ZL3bGyac2Ucr_ALiKp=g55HFg@mail.gmail.com>
	<BY2PR02MB214906F5CE748977C1F924F3F49B0@BY2PR02MB2149.namprd02.prod.outlook.com>
Message-ID: <CABmBNEZ-g4AtJC5dcVoykVSn__hWj2jgJ1Xsq_4yDH1L_b12YQ@mail.gmail.com>

Hi Rob,

Thanks for your email.
File /var/log/clearwater-etcd/clearwater-etcd.log does not show any logs.
However I have checked file /var/log/clearwater-etcd/clearwater-etcd.log,1.
The size of these files is huge in 100s of Mbs.
I won't be able to send you all the logs. Which section of logs will you
prefer? The logs immediately after system restart or logs at the end of the
file or any particular scenario?

Thanks
Hrishikesh


On Sat, Aug 26, 2017 at 12:47 AM, Robert Day <Robert.Day at metaswitch.com>
wrote:

> Hi Hrishikesh,
>
>
>
> Could you send /var/log/clearwater-etcd/clearwater-etcd.log (which is
> different to /var/log/clearwater-etcd/clearwater-etcd-initd.log) from
> each of your nodes? That should help tell what?s going on here.
>
>
>
> Thanks,
>
> Rob
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* 22 August 2017 10:51
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi Andrew,
>
> Thanks for your constant support. I am facing a weired problem. All of my
> nodes were working fine. I sent you the logs earlier.
>
> I restarted all nodes and suddenly etcd_process on all nodes is failed to
> execute. I don't know what is the reason. Will you please guide me on this.
>
> All nodes are able to ping one another.
>
> Here are some logs
>
>
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> cluster may be unhealthy: failed to list members
> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
> dial tcp 192.168.56.105:4000: getsockopt: connection refused
>
> error #0: dial tcp 192.168.56.105:4000: getsockopt: connection refused
>
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
> dial tcp 192.168.56.105:4000: getsockopt: connection refused
>
> error #0: dial tcp 192.168.56.105:4000: getsockopt: connection refused
>
> 2017-08-22 14:42:44.966660634 Running etcdctl cluster-health
> cluster may be unhealthy: failed to list members
> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
> client: endpoint http://192.168.56.110:4000 exceeded header timeout
>
> #cat /var/log/clearwater-etcd/clearwater-etcd-initd.log
>
> 2017-08-22 15:17:22.731984254 etcdctl returned 4
> 2017-08-22 15:17:22.732730248 Not joining an unhealthy cluster
> 2017-08-22 15:18:02.429508523 Restarting etcd clearwater-etcd
> 2017-08-22 15:18:02.433629983 Configured ETCDCTL_PEERS:
> 192.168.56.107:4000,192.168.56.109:4000,192.168.56.110:4000,
> 192.168.56.108:4000,
> 2017-08-22 15:18:02.434237379 Check for previous failed startup attempt
> 2017-08-22 15:18:02.435071900 Running etcdctl member list
> client: etcd cluster is unavailable or misconfigured; error #0: dial tcp
> 192.168.56.110:4000: getsockopt: connection refused
> ; error #1: dial tcp 192.168.56.107:4000: getsockopt: connection refused
> ; error #2: dial tcp 192.168.56.109:4000: getsockopt: connection refused
> ; error #3: dial tcp 192.168.56.108:4000: getsockopt: connection refused
> ; error #4: http: no Host in request URL
>
> 2017-08-22 15:18:02.447231505 etcdctl returned 1
> 2017-08-22 15:18:02.453120361 Joining existing cluster...
> 2017-08-22 15:18:13.456387174 Configured ETCDCTL_PEERS:
> 192.168.56.107:4000,192.168.56.109:4000,192.168.56.110:4000,
> 192.168.56.108:4000,
> 2017-08-22 15:18:13.457279845 Check cluster is healthy
> 2017-08-22 15:18:13.458636020 Running etcdctl cluster-health
> cluster may be unhealthy: failed to list members
> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
> http: no Host in request URL
> ; error #1: dial tcp 192.168.56.109:4000: getsockopt: connection refused
> ; error #2: dial tcp 192.168.56.108:4000: getsockopt: connection refused
> ; error #3: dial tcp 192.168.56.110:4000: getsockopt: connection refused
> ; error #4: dial tcp 192.168.56.107:4000: getsockopt: connection refused
>
> error #0: http: no Host in request URL
> error #1: dial tcp 192.168.56.109:4000: getsockopt: connection refused
> error #2: dial tcp 192.168.56.108:4000: getsockopt: connection refused
> error #3: dial tcp 192.168.56.110:4000: getsockopt: connection refused
> error #4: dial tcp 192.168.56.107:4000: getsockopt: connection refused
>
> 2017-08-22 15:18:13.470356301 etcdctl returned 4
> 2017-08-22 15:18:13.471296869 Not joining an unhealthy cluster
> 2017-08-22 15:18:43.177856227 Restarting etcd clearwater-etcd
> 2017-08-22 15:18:43.182076210 Configured ETCDCTL_PEERS:
> 192.168.56.107:4000,192.168.56.109:4000,192.168.56.110:4000,
> 192.168.56.108:4000,
> 2017-08-22 15:18:43.182623728 Check for previous failed startup attempt
> 2017-08-22 15:18:43.183439470 Running etcdctl member list
> client: etcd cluster is unavailable or misconfigured; error #0: dial tcp
> 192.168.56.107:4000: getsockopt: connection refused
> ; error #1: dial tcp 192.168.56.108:4000: getsockopt: connection refused
> ; error #2: dial tcp 192.168.56.109:4000: getsockopt: connection refused
> ; error #3: http: no Host in request URL
> ; error #4: dial tcp 192.168.56.110:4000: getsockopt: connection refused
>
> 2017-08-22 15:18:43.195788447 etcdctl returned 1
> 2017-08-22 15:18:43.201223078 Joining existing cluster...
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
> On Thu, Aug 10, 2017 at 6:27 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
> Hi Hrishikesh,
>
>
>
> The process that Bono uses to resolve a DNS request is to:
>
>
>
> ?         make a DNS lookup - by default, to dnsmasq running on 127.0.0.1
> (local host)
>
> ?         dnsmasq reads /etc/hosts
>
> ?         dnsmasq responds with an answer
>
>
>
> The reason why this might be failing but pings are working is that pings
> may just read /etc/hosts directly rather than using dnsmasq.
>
>
>
> Can you please try:
>
>
>
> ?         Running `dig icscf.cwsprout1 @127.0.0.1` - this will show
> whether dnsmasq is responding with an answer (and therefore whether the
> problem is in bono or dnsmasq)
>
> ?         Running `sudo service dnsmasq restart` - this might trigger it
> to pick up a change in the /etc/hosts file
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Tuesday, August 8, 2017 10:15 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi Andrew,
>
> Thanks for your help.
>
> I already have configured etc/hosts with the entry. After I added the
> entry restund process started executing.
>
> Bono can ping icscf.cwsprout1
>
> [bono]cwbono1 at cwbono1:~$ ping icscf.cwsprout1
> PING icscf.cwsprout1 (192.168.56.107) 56(84) bytes of data.
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=1 ttl=64
> time=0.218 ms
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=2 ttl=64
> time=0.311 ms
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=3 ttl=64
> time=0.224 ms
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=4 ttl=64
> time=0.188 ms
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=5 ttl=64
> time=0.187 ms
> ^C
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
> On Mon, Aug 7, 2017 at 4:28 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
> Hi Hrishikesh,
>
>
>
> Thanks for the updated diags.
>
>
>
> One thing that can be useful when you are reading through a log file is to
> search for the pattern ?Error?. This will highlight those logs which
> indicate an error. If we do that here we see:
>
>
>
> 03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
>
>
>
> This suggests that Bono is unable to resolve the hostname
> ?icscf.cwsprout1?. We?ve spoken in the past about how you are unable to
> configure a DNS. Hence I would suggest configuring /etc/hosts to resolve
> icscf.cwsprout1.
>
>
>
> Please let me know if this works.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, August 3, 2017 10:30 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi Andrew,
>
> Thanks a lot for your reply.
>
> I did not mention the password but while doing the configuration I am
> using it.
>
> Thanks
>
> Hrishikesh
>
>
>
> On Mon, Jul 31, 2017 at 2:44 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
> Hi Hrishikesh,
>
>
>
> Thank you for your question.
>
>
>
> I?m not sure whether this is deliberate but it looks like you haven?t
> included the password in your Zoiper configuration. You must make sure you
> use the same password the Ellis client gives you.
>
>
>
> We have this guide
> <https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html>
> that might help you with the configuration you need to use with Zoiper.
>
>
>
> If this guide goes not help could I please have Bono and Sprout logs
> during the time of the registration.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, July 27, 2017 12:22 PM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi,
>
> I have used manual installation for clearwater.
>
> I have created 6 VMs using Virtualbox and using host only network.
>
> All 6 nodes are running fine as per monit logs.
>
> I have installed Zoiper client and tried to add new account as follows,
>
> ===================================================
> Zoiper Preferences
>
> Domain        -    example.com
> Username    -    6505550708
> Password    -
> Auth. Uname    -    6505550708 at example.com
> Outbound Proxy    -    192.168.56.105:8060
>
> Error        -     SIP 408 - Request Timeout
>
>
> Private Identity Generated by Ellis
>
> Private Identity:
>
> 6505550028 at example.com
> Password:Na5ZWdQj4
>
> ===================================================
>
> How ever I am getting error mentioned in subject line.
>
> Here are other logs for each node,
>
> ===================================================
>
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
> local_ip=192.168.56.105
> public_ip=192.168.56.105
> public_hostname=cwellis1
> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,
> 192.168.56.108,192.168.56.109,192.168.56.110"
>
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout1
> sprout_registration_store=192.168.56.110 #vellum
> hs_hostname=192.168.56.109:8888 #dime
> hs_provisioning_hostname=192.168.56.109:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.108:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=192.168.56.110 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
> [sudo] password for cwellis1:
> Monit 5.18.1 uptime: 3h 33m
>  Service Name                     Status
> Type
>  node-cwellis1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [ellis]cwellis1 at cwellis1:~$
>
>
> [bono]cwbono1 at cwbono1:~$ sudo monit summary
> [sudo] password for cwbono1:
> Monit 5.18.1 uptime: 23m
>  Service Name                     Status
> Type
>  node-cwbono1                     Running
> System
>  restund_process                  Running
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat
> /etc/clearwater/local_config
> local_ip=192.168.56.105
> public_ip=192.168.56.105
> public_hostname=cwellis1
> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,
> 192.168.56.108,192.168.56.109,192.168.56.110"
>
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout1
> sprout_registration_store=192.168.56.110 #vellum
> hs_hostname=192.168.56.109:8888 #dime
> hs_provisioning_hostname=192.168.56.109:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.108:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=192.168.56.110 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
> [sudo] password for cwellis1:
> Monit 5.18.1 uptime: 3h 33m
>  Service Name                     Status
> Type
>  node-cwellis1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [ellis]cwellis1 at cwellis1:~$
>
>
> [bono]cwbono1 at cwbono1:~$ sudo monit summary
> [sudo] password for cwbono1:
> Monit 5.18.1 uptime: 23m
>  Service Name                     Status
> Type
>  node-cwbono1                     Running
> System
>  restund_process                  Running
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono1 at cwbono1:~$
>
>
>
> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
> [sudo] password for cwsprout1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwsprout1                   Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
>
>
>
>
> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
> [sudo] password for cwhomer1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwhomer1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [homer]cwhomer1 at cwhomer1:~$
>
>
> [dime]cwdime1 at cwdime1:~$ sudo monit summary
> [sudo] password for cwdime1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwdime1                     Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [dime]cwdime1 at cwdime1:~$
>
>
>
> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
> [sudo] password for cwvellum1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwvellum1                   Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [vellum]cwvellum1 at cwvellum1:~$
> 192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono1 at cwbono1:~$
>
>
>
> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
> [sudo] password for cwsprout1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwsprout1                   Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
>
>
>
>
> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
> [sudo] password for cwhomer1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwhomer1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [homer]cwhomer1 at cwhomer1:~$
>
>
> [dime]cwdime1 at cwdime1:~$ sudo monit summary
> [sudo] password for cwdime1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwdime1                     Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [dime]cwdime1 at cwdime1:~$
>
>
>
> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
> [sudo] password for cwvellum1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwvellum1                   Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [vellum]cwvellum1 at cwvellum1:~$
>
>
> ===================================================
>
> Please let me know what is wrong with the setup?
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170829/1717b18e/attachment.html>

From omordi at coure-tech.com  Tue Aug 29 05:30:05 2017
From: omordi at coure-tech.com (MORDI Obim)
Date: Tue, 29 Aug 2017 10:30:05 +0100
Subject: [Project Clearwater] ENUM: Multiple @ dig queries
Message-ID: <008201d320a9$66dc6cc0$34954640$@coure-tech.com>

Good Morning Clearwater Team,

 

My name is Obim, mailing from Lagos - Nigeria,

My company has a web solution that helps our clients run number lookups
against our database, connecting either via the API channel or FTP channel.

Now, we recently heard of the ENUM channel (using the BIND tool) and we were
able to set is up successfully, but we can only run number lookup at a time.
See below our used syntax: 

 

dig @ourURL 3.8.0.6.6.8.8.6.0.8.4.3.2.e164.arpa.4.username.password IN NAPTR


 

now we have clients that want to run multiple queries at the same, please
can you help point us in the right direction concerning which tool to use
that can help solve our problem..

 

thank you. 

 

 

 

 

Regards...

MORDI Obim
Lead Application Support/Testing
COURE Software & Systems Ltd
T: +234-1-631-1910 Office  +234-1-631-1925 Direct 320 Ext*
+234-806-886-6190 NG Cell (Nigeria)
E:
<javascript:OpenNewWindow('/Mondo/lang/sys/Forms/MAI/compose.aspx?MsgTo=omor
di%40coure-tech.com&MsgSubject=&MsgCc=&MsgBcc=&MsgBody=',570,450)>
omordi at coure-tech.com || W:  <http://www.coure-tech.com/>
http://www.coure-tech.com || S: omordi

 <http://www.coure-tech.com/>  <http://www.anq.nms.com.ng/>
<http://www.tendersvault.com/> 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170829/f6afee5e/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.gif
Type: image/gif
Size: 1741 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170829/f6afee5e/attachment.gif>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 6736 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170829/f6afee5e/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 6452 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170829/f6afee5e/attachment-0001.png>

From Robert.Day at metaswitch.com  Wed Aug 30 05:18:45 2017
From: Robert.Day at metaswitch.com (Robert Day)
Date: Wed, 30 Aug 2017 09:18:45 +0000
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <CABmBNEZ-g4AtJC5dcVoykVSn__hWj2jgJ1Xsq_4yDH1L_b12YQ@mail.gmail.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
	<BLUPR02MB437E06C13B70FCC491E0A27E5B20@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEanzSPTx5=QUczx34_=WLa3b0Ztbt3wZ57-AYx39UN0QQ@mail.gmail.com>
	<BLUPR02MB4379BCC901095CF549CF27FE5B50@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEb+4xu-qBE7Sq_cfO13GMc6WBxJBEpC8YqPrCGj_D=F8Q@mail.gmail.com>
	<BLUPR02MB437E492B6592B3B3EBBEB99E5880@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEaT9TZ1h94v7P5n67qY8ZL3bGyac2Ucr_ALiKp=g55HFg@mail.gmail.com>
	<BY2PR02MB214906F5CE748977C1F924F3F49B0@BY2PR02MB2149.namprd02.prod.outlook.com>
	<CABmBNEZ-g4AtJC5dcVoykVSn__hWj2jgJ1Xsq_4yDH1L_b12YQ@mail.gmail.com>
Message-ID: <BY2PR02MB214936A7D8033E5BBBA88575F49C0@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Hrishikesh,

Is it possible for you to compress it and send the compressed version? For example, with `bzip2 /var/log/clearwater-etcd/clearwater-etcd.log.1`?

Thanks,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Hrishikesh Karanjikar
Sent: 29 August 2017 10:25
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] SIP 408 - Request Timeout

Hi Rob,
Thanks for your email.
File /var/log/clearwater-etcd/clearwater-etcd.log does not show any logs.
However I have checked file /var/log/clearwater-etcd/clearwater-etcd.log,1. The size of these files is huge in 100s of Mbs.
I won't be able to send you all the logs. Which section of logs will you prefer? The logs immediately after system restart or logs at the end of the file or any particular scenario?
Thanks
Hrishikesh


On Sat, Aug 26, 2017 at 12:47 AM, Robert Day <Robert.Day at metaswitch.com<mailto:Robert.Day at metaswitch.com>> wrote:
Hi Hrishikesh,

Could you send /var/log/clearwater-etcd/clearwater-etcd.log (which is different to /var/log/clearwater-etcd/clearwater-etcd-initd.log) from each of your nodes? That should help tell what?s going on here.

Thanks,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: 22 August 2017 10:51
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP 408 - Request Timeout

Hi Andrew,
Thanks for your constant support. I am facing a weired problem. All of my nodes were working fine. I sent you the logs earlier.
I restarted all nodes and suddenly etcd_process on all nodes is failed to execute. I don't know what is the reason. Will you please guide me on this.
All nodes are able to ping one another.
Here are some logs

[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
cluster may be unhealthy: failed to list members
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 192.168.56.105:4000<http://192.168.56.105:4000>: getsockopt: connection refused

error #0: dial tcp 192.168.56.105:4000<http://192.168.56.105:4000>: getsockopt: connection refused

[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 192.168.56.105:4000<http://192.168.56.105:4000>: getsockopt: connection refused

error #0: dial tcp 192.168.56.105:4000<http://192.168.56.105:4000>: getsockopt: connection refused

2017-08-22 14:42:44.966660634 Running etcdctl cluster-health
cluster may be unhealthy: failed to list members
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint http://192.168.56.110:4000 exceeded header timeout

#cat /var/log/clearwater-etcd/clearwater-etcd-initd.log

2017-08-22 15:17:22.731984254 etcdctl returned 4
2017-08-22 15:17:22.732730248 Not joining an unhealthy cluster
2017-08-22 15:18:02.429508523 Restarting etcd clearwater-etcd
2017-08-22 15:18:02.433629983 Configured ETCDCTL_PEERS: 192.168.56.107:4000<http://192.168.56.107:4000>,192.168.56.109:4000<http://192.168.56.109:4000>,192.168.56.110:4000<http://192.168.56.110:4000>,192.168.56.108:4000<http://192.168.56.108:4000>,
2017-08-22 15:18:02.434237379 Check for previous failed startup attempt
2017-08-22 15:18:02.435071900 Running etcdctl member list
client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 192.168.56.110:4000<http://192.168.56.110:4000>: getsockopt: connection refused
; error #1: dial tcp 192.168.56.107:4000<http://192.168.56.107:4000>: getsockopt: connection refused
; error #2: dial tcp 192.168.56.109:4000<http://192.168.56.109:4000>: getsockopt: connection refused
; error #3: dial tcp 192.168.56.108:4000<http://192.168.56.108:4000>: getsockopt: connection refused
; error #4: http: no Host in request URL

2017-08-22 15:18:02.447231505 etcdctl returned 1
2017-08-22 15:18:02.453120361 Joining existing cluster...
2017-08-22 15:18:13.456387174 Configured ETCDCTL_PEERS: 192.168.56.107:4000<http://192.168.56.107:4000>,192.168.56.109:4000<http://192.168.56.109:4000>,192.168.56.110:4000<http://192.168.56.110:4000>,192.168.56.108:4000<http://192.168.56.108:4000>,
2017-08-22 15:18:13.457279845 Check cluster is healthy
2017-08-22 15:18:13.458636020 Running etcdctl cluster-health
cluster may be unhealthy: failed to list members
Error:  client: etcd cluster is unavailable or misconfigured; error #0: http: no Host in request URL
; error #1: dial tcp 192.168.56.109:4000<http://192.168.56.109:4000>: getsockopt: connection refused
; error #2: dial tcp 192.168.56.108:4000<http://192.168.56.108:4000>: getsockopt: connection refused
; error #3: dial tcp 192.168.56.110:4000<http://192.168.56.110:4000>: getsockopt: connection refused
; error #4: dial tcp 192.168.56.107:4000<http://192.168.56.107:4000>: getsockopt: connection refused

error #0: http: no Host in request URL
error #1: dial tcp 192.168.56.109:4000<http://192.168.56.109:4000>: getsockopt: connection refused
error #2: dial tcp 192.168.56.108:4000<http://192.168.56.108:4000>: getsockopt: connection refused
error #3: dial tcp 192.168.56.110:4000<http://192.168.56.110:4000>: getsockopt: connection refused
error #4: dial tcp 192.168.56.107:4000<http://192.168.56.107:4000>: getsockopt: connection refused

2017-08-22 15:18:13.470356301 etcdctl returned 4
2017-08-22 15:18:13.471296869 Not joining an unhealthy cluster
2017-08-22 15:18:43.177856227 Restarting etcd clearwater-etcd
2017-08-22 15:18:43.182076210 Configured ETCDCTL_PEERS: 192.168.56.107:4000<http://192.168.56.107:4000>,192.168.56.109:4000<http://192.168.56.109:4000>,192.168.56.110:4000<http://192.168.56.110:4000>,192.168.56.108:4000<http://192.168.56.108:4000>,
2017-08-22 15:18:43.182623728 Check for previous failed startup attempt
2017-08-22 15:18:43.183439470 Running etcdctl member list
client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 192.168.56.107:4000<http://192.168.56.107:4000>: getsockopt: connection refused
; error #1: dial tcp 192.168.56.108:4000<http://192.168.56.108:4000>: getsockopt: connection refused
; error #2: dial tcp 192.168.56.109:4000<http://192.168.56.109:4000>: getsockopt: connection refused
; error #3: http: no Host in request URL
; error #4: dial tcp 192.168.56.110:4000<http://192.168.56.110:4000>: getsockopt: connection refused

2017-08-22 15:18:43.195788447 etcdctl returned 1
2017-08-22 15:18:43.201223078 Joining existing cluster...
Thanks
Hrishikesh


On Thu, Aug 10, 2017 at 6:27 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

The process that Bono uses to resolve a DNS request is to:


?         make a DNS lookup - by default, to dnsmasq running on 127.0.0.1 (local host)

?         dnsmasq reads /etc/hosts

?         dnsmasq responds with an answer

The reason why this might be failing but pings are working is that pings may just read /etc/hosts directly rather than using dnsmasq.

Can you please try:


?         Running `dig icscf.cwsprout1 @127.0.0.1<http://127.0.0.1>` - this will show whether dnsmasq is responding with an answer (and therefore whether the problem is in bono or dnsmasq)

?         Running `sudo service dnsmasq restart` - this might trigger it to pick up a change in the /etc/hosts file

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Tuesday, August 8, 2017 10:15 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP 408 - Request Timeout

Hi Andrew,
Thanks for your help.
I already have configured etc/hosts with the entry. After I added the entry restund process started executing.
Bono can ping icscf.cwsprout1

[bono]cwbono1 at cwbono1:~$ ping icscf.cwsprout1
PING icscf.cwsprout1 (192.168.56.107) 56(84) bytes of data.
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=1 ttl=64 time=0.218 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=2 ttl=64 time=0.311 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=3 ttl=64 time=0.224 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=4 ttl=64 time=0.188 ms
64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=5 ttl=64 time=0.187 ms
^C
Thanks
Hrishikesh


On Mon, Aug 7, 2017 at 4:28 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Thanks for the updated diags.

One thing that can be useful when you are reading through a log file is to search for the pattern ?Error?. This will highlight those logs which indicate an error. If we do that here we see:

03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)

This suggests that Bono is unable to resolve the hostname ?icscf.cwsprout1?. We?ve spoken in the past about how you are unable to configure a DNS. Hence I would suggest configuring /etc/hosts to resolve icscf.cwsprout1.

Please let me know if this works.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, August 3, 2017 10:30 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP 408 - Request Timeout

Hi Andrew,
Thanks a lot for your reply.
I did not mention the password but while doing the configuration I am using it.
Thanks
Hrishikesh

On Mon, Jul 31, 2017 at 2:44 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Thank you for your question.

I?m not sure whether this is deliberate but it looks like you haven?t included the password in your Zoiper configuration. You must make sure you use the same password the Ellis client gives you.

We have this guide<https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html> that might help you with the configuration you need to use with Zoiper.

If this guide goes not help could I please have Bono and Sprout logs during the time of the registration.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, July 27, 2017 12:22 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] SIP 408 - Request Timeout

Hi,
I have used manual installation for clearwater.
I have created 6 VMs using Virtualbox and using host only network.
All 6 nodes are running fine as per monit logs.
I have installed Zoiper client and tried to add new account as follows,

===================================================
Zoiper Preferences

Domain        -    example.com<http://example.com>
Username    -    6505550708
Password    -
Auth. Uname    -    6505550708 at example.com<mailto:6505550708 at example.com>
Outbound Proxy    -    192.168.56.105:8060<http://192.168.56.105:8060>

Error        -     SIP 408 - Request Timeout


Private Identity Generated by Ellis

Private Identity:

6505550028 at example.com<mailto:6505550028 at example.com>
Password:Na5ZWdQj4

===================================================
How ever I am getting error mentioned in subject line.
Here are other logs for each node,

===================================================

[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888<http://192.168.56.109:8888> #dime
hs_provisioning_hostname=192.168.56.109:8889<http://192.168.56.109:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888<http://192.168.56.108:7888> #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888<http://192.168.56.109:8888> #dime
hs_provisioning_hostname=192.168.56.109:8889<http://192.168.56.109:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888<http://192.168.56.108:7888> #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$
192.168.56.109:4000<http://192.168.56.109:4000> isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$


===================================================
Please let me know what is wrong with the setup?
Thanks
Hrishikesh



_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/566b6370/attachment.html>

From Robert.Day at metaswitch.com  Wed Aug 30 05:26:37 2017
From: Robert.Day at metaswitch.com (Robert Day)
Date: Wed, 30 Aug 2017 09:26:37 +0000
Subject: [Project Clearwater] [installation & testing] Need help on
 Clearwater
In-Reply-To: <4342232.2512519.1503691934048@mail.yahoo.com>
References: <4342232.2512519.1503691934048.ref@mail.yahoo.com>
	<4342232.2512519.1503691934048@mail.yahoo.com>
Message-ID: <BY2PR02MB214928DF59777EF7E04E5EFFF49C0@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Prakash,

Answering your questions in turn:


1.       This sounds like the problem might be trying to clone Astaire over SSH without an SSH key. https://github.com/Metaswitch/sprout/blob/dev/docs/Development.md#getting-the-code suggests running the following command before cloning to force access over HTTPS instead:

git config --global url."https://github.com/".insteadOf git at github.com:


2.       We?ve found that SNMP is standard and widely used, particularly for telecoms VNFs. Zabbix in particular has good SNMP integration, documented at https://www.zabbix.com/documentation/2.4/manual/config/items/itemtypes/snmp, and we?ve previously demonstrated Zabbix integration at https://www.youtube.com/watch?v=IF7bUgDCYMM

3.       I?m not quite sure if I?ve understood the question, but https://github.com/Metaswitch/clearwater-live-test#usage-options describes how to run our live tests ?as a continuous verification VNF which can be installed alongside a Clearwater deployment to provide continuous, service-level verification of the deployment's basic functionality?. Does that help?

Best,
Rob


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of prakash RAMCHANDRAN
Sent: 25 August 2017 21:12
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] [installation & testing] Need help on Clearwater

Hi all,

[installation] [DevStack on RackSpace Gen1 servers] - Stlll has DNS bind issues
Previously I had send an email thread on Install and had couple issues and successfully over come them.
The one I had on Quota was easy to set through

openstck quota set --security group 100 <tenant>
openstack quota set --secgroup-rules 100 <tenant>

The recognition of DNS zone was not resolvable either by installing bind9 with local zone or enabling designate through DevStack on RackSpace Gen1 server or memory intensive servers in Gen1. So if some one tries better use Gen2 servers as Gen1 doe not appear to scale for even minimum all-in-one install of Clearwater-heat template based. If any one succeeds with gen2 servers on Rack space please share the information.

[Installation] OPNFV os-nosdn-nofeature-ha with Compass4nfv<http://artifacts.opnfv.org/compass4nfv/docs/release_installation/index.html> installer - Working Successful

The compass4nfv is a open source Installer with great features and is able to install in an OPNFV vPOD using Docker container on Jumphost calling host1 to install openstack ocata with both Haet and Ceph like a charm. I used the latest Danube main and you can try Danube 3.0 or 3.1 and create the Clearwater vIMS VNF.

Having crossed this milestone looking for test suggestions and in do's and dont's from community involved.

[Testing] Celarwater vIMS in OPNFV
( should be usable for both OPNAP and OpenBaton Integration - Integrated with OPNFV (NFVI+VIM) as a System under Test(SUT) with vIMS being a use case.

OPNFV has FUNCTEST and that is usable throgh API calls and uses Swagger API and SNAP api calls to Openstack modules. That runs and able to test few test cases simple ones and working to fix others.

Help needed in:  Clearwater Functionla Verification test

1. https://github.com/Metaswitch/clearwater-fv-test

Possibly recursive one does not go enough or not allowed to go deep enough to get module codes. Is that correct?

git clone --recursive https://github.com/Metaswitch/clearwater-fv-test.git



Thus although it says sucessfully cloned the make fails?
The 'mk test" on root fails at first module

module Astaire not accesible or you are not permitted to access?
You mean these are not open source?


2. https://github.com/Metaswitch/cpp-common/tree/master/test_utils


This is cpp tests and mock library is used in OPNFV FUNCTEST. Now not sure why SNMP here, as most use some servers like Zabbix (in OPNFV Doctor) and telemetry from OpenStack in OPNFV (Cielomeer, AODH etc.) So are there any comon tests that we can use or euse with variations of these. Any DevOps folks if yu tried this give some insights to me.


3. The rake test I have read both manual and automated and are in Ruby, some of them OPNFV has adapted through python, but like to hear if we consider any vIMS (Clearwater or compatible commercial offerings) do you see role for Rake test in that or Clearwater-live-tests as everyone calls it?

Hope to hear your inputs and lets build on this and OPNFV MANO WG is trying to support all MANO stacks to embrace the rich Scenarios and flexibility available through Clearwater vIMS to community of Vendors and Service providers.

Thanks
Prakash





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/873b1436/attachment.html>

From Robert.Day at metaswitch.com  Wed Aug 30 05:28:27 2017
From: Robert.Day at metaswitch.com (Robert Day)
Date: Wed, 30 Aug 2017 09:28:27 +0000
Subject: [Project Clearwater] SIP Registeration Failed
In-Reply-To: <CA+qu+1fersTYSCD5YDBnCekzjoP1j3amG_Lo=vOmpmhFUc__rA@mail.gmail.com>
References: <CA+qu+1fersTYSCD5YDBnCekzjoP1j3amG_Lo=vOmpmhFUc__rA@mail.gmail.com>
Message-ID: <BY2PR02MB2149CEB2085D23CB16CB8095F49C0@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Ankit,

Have you tried the steps at http://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html, particularly turning on debug logging on Bono and Sprout to see if your request is getting to them and where it is failing?

Thanks,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Ankit Patwa
Sent: 24 August 2017 16:33
To: Clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP Registeration Failed

Hi,

we have installed all in one image on linux 14.04 serer.
After login on ellis public and private identifier generated.

Private identifier
6505550677 at 192.168.5.20<mailto:6505550677 at 192.168.5.20>
Password rnqYR4s7d

Public identifier
sip:6505550677 at 192.168.5.20<mailto:sip%3A6505550677 at 192.168.5.20>

we are giving

username                 6505550677
password                  rnqYR4s7d
server                       192.168.5.20
Authentication user   sip:6505550677 at 192.168.5.20<mailto:sip%3A6505550677 at 192.168.5.20>
Transport Type          TCP

Device    Redmi
Android  marshmallow

But I am getting registration failed error.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/62955453/attachment.html>

From Robert.Day at metaswitch.com  Wed Aug 30 05:49:29 2017
From: Robert.Day at metaswitch.com (Robert Day)
Date: Wed, 30 Aug 2017 09:49:29 +0000
Subject: [Project Clearwater] Cassandra and etcd clustering problem
In-Reply-To: <CAEAO5tZ3Bdm+b=BHDkag7dYztDeqJJyoatVMAuwFLrk+x9XA9w@mail.gmail.com>
References: <CAEAO5tZ3Bdm+b=BHDkag7dYztDeqJJyoatVMAuwFLrk+x9XA9w@mail.gmail.com>
Message-ID: <BY2PR02MB2149C9285739587B7DBBA88FF49C0@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Abdul,

The etcd output you?ve provided lists a few nodes as unreachable, which is likely to cause problems:

root at dime-0:/home/ubuntu# clearwater-etcdctl cluster-health
member 5cd7042180fbb2a is unhealthy: got unhealthy result from http://192.168.0.6:4000
failed to check the health of member 208dd0fbcefb149c on http://192.168.0.8:4000: Get http://192.168.0.8:4000/health: dial tcp 192.168.0.8:4000<http://192.168.0.8:4000>: getsockopt: connection refused
member 208dd0fbcefb149c is unreachable: [http://192.168.0.8:4000] are all unreachable
member 2457d2c8a20fc738 is unhealthy: got unhealthy result from http://192.168.0.5:4000
member 48fa49be2b2ae2c2 is unhealthy: got unhealthy result from http://192.168.0.3:4000
failed to check the health of member a48134f48b185ad9 on http://192.168.0.7:4000: Get http://192.168.0.7:4000/health: dial tcp 192.168.0.7:4000<http://192.168.0.7:4000>: getsockopt: connection refused
member a48134f48b185ad9 is unreachable: [http://192.168.0.7:4000] are all unreachable
member e7db4eebbdb94a11 is unhealthy: got unhealthy result from http://192.168.0.4:4000
cluster is unhealthy

Are you expecting 192.168.0.9 and 192.168.0.7 to be reachable? Which nodes are they?

Also, the Cassandra logs you?ve sent over have quite a few ten-minute gaps in them like this, which is interesting:

INFO  [main] 2017-08-27 04:47:56,225 GossipingPropertyFileSnitch.java:64 - Loaded cassandra-topology.properties for compatibility
INFO  [ScheduledTasks:1] 2017-08-27 04:48:11,720 TokenMetadata.java:433 - Updating topology for all endpoints that have changed
INFO  [main] 2017-08-27 04:56:10,295 CassandraDaemon.java:155 - Hostname: vellum-0.test.com
INFO  [main] 2017-08-27 04:56:17,803 YamlConfigurationLoader.java:92 - Loading settings from file:/etc/cassandra/cassandra.yaml

Could you send over /var/log/monit.log (and possibly a complete diagnostics package, created by sudo cw-gather_diags)? That should give more information on when and why Cassandra is being restarted.

Thanks,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Abdul Basit Alvi
Sent: 27 August 2017 06:24
To: Clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Cassandra and etcd clustering problem

Hi,
I have been trying to make the IMS work via manual install. I have followed all the instructions to the dot and have tried starting from scratch multiple times, but somehow I cant figure out why Cassandra and etcd clustering are not working properly.

In the Dime node homestead process is not running, this I know is because it cant connect to the vellum node cassandra via the thrift port 9160.

Next in the Vellum node the cassandra process is running but not working at all. I have attached the system log files as well as cassandra.yaml and the cassandra-env.sh file. For test purposes I have allowed all incomming TCP/UDP traffic to and from all nodes.

Can you kindly look at the logs and outputs and point out what am I doing wrong?

[Monit summary of dime]
root at dime-0:/home/ubuntu# monit summary
Monit 5.18.1 uptime: 6h 27m
 Service Name                     Status                      Type
 node-dime-0.test.com<http://node-dime-0.test.com>             Running                     System
 snmpd_process                    Running                     Process
 ralf_process                     Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Does not exist              Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 ralf_uptime                      Status ok                   Program
 poll_ralf                        Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Wait parent                 Program
 poll_homestead                   Wait parent                 Program
 check_cx_health                  Wait parent                 Program
 poll_homestead-prov              Status failed               Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status failed               Program
 poll_etcd                        Status ok                   Program
[Dime Local config]

[etcd cluster health dime]
root at dime-0:/home/ubuntu# clearwater-etcdctl cluster-health
member 5cd7042180fbb2a is unhealthy: got unhealthy result from http://192.168.0.6:4000
failed to check the health of member 208dd0fbcefb149c on http://192.168.0.8:4000: Get http://192.168.0.8:4000/health: dial tcp 192.168.0.8:4000<http://192.168.0.8:4000>: getsockopt: connection refused
member 208dd0fbcefb149c is unreachable: [http://192.168.0.8:4000] are all unreachable
member 2457d2c8a20fc738 is unhealthy: got unhealthy result from http://192.168.0.5:4000
member 48fa49be2b2ae2c2 is unhealthy: got unhealthy result from http://192.168.0.3:4000
failed to check the health of member a48134f48b185ad9 on http://192.168.0.7:4000: Get http://192.168.0.7:4000/health: dial tcp 192.168.0.7:4000<http://192.168.0.7:4000>: getsockopt: connection refused
member a48134f48b185ad9 is unreachable: [http://192.168.0.7:4000] are all unreachable
member e7db4eebbdb94a11 is unhealthy: got unhealthy result from http://192.168.0.4:4000
cluster is unhealthy

[local config file dime]
root at dime-0:/home/ubuntu# cat /etc/clearwater/local_config
local_ip=192.168.0.4
public_ip=10.1.10.192
public_hostname=dime-0.test.com<http://dime-0.test.com>
etcd_cluster=192.168.0.3,192.168.0.4,192.168.0.5,192.168.0.6,192.168.0.7,192.168.0.8

[shared config file dime]
root at vellum-0:/home/ubuntu# cat /etc/clearwater/shared_config
# Deployment definitions
home_domain=test.com<http://test.com>
sprout_hostname=sprout.test.com<http://sprout.test.com>
hs_hostname=hs.test.com:8888<http://hs.test.com:8888>
hs_provisioning_hostname=hs-prov.test.com:8889<http://hs-prov.test.com:8889>
dime_hostname=dime.test.com:10888<http://dime.test.com:10888>
xdms_hostname=homer.test.com:7888<http://homer.test.com:7888>
sprout_impi_store=vellum.test.com<http://vellum.test.com>
sprout_registration_store=vellum.test.com<http://vellum.test.com>
cassandra_hostname=vellum.test.com<http://vellum.test.com>
chronos_hostname=vellum.test.com<http://vellum.test.com>
dime_session_store=vellum.test.com<http://vellum.test.com>

upstream_port=0

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

[Error Log Homestead]
Thrift: Sun Aug 27 05:04:01 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:04:02 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:04:44 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:04:44 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:05:08 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:05:08 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:05:30 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:05:30 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:05:38 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
Thrift: Sun Aug 27 05:05:38 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused

[Running cqlsh on vellum]
root at vellum-0:/home/ubuntu# cqlsh
Connection error: ('Unable to connect to any servers', {'127.0.0.1': error(111, "Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused")}

[Monit summary of vellum]
root at vellum-0:/home/ubuntu# monit summary
Monit 5.18.1 uptime: 6h 39m
 Service Name                     Status                      Type
 node-vellum-0.test.com<http://node-vellum-0.test.com>           Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Execution failed | Does...  Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Wait parent                 Program
 poll_etcd_cluster                Wait parent                 Program
 poll_etcd                        Wait parent                 Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status failed               Program
 astaire_uptime                   Status ok                   Program

[local config file vellum]
root at vellum-0:/home/ubuntu# cat /etc/clearwater/local_config
local_ip=192.168.0.8
public_ip=10.1.10.204
public_hostname=vellum-0.test.com<http://vellum-0.test.com>
etcd_cluster=192.168.0.3,192.168.0.4,192.168.0.5,192.168.0.6,192.168.0.7,192.168.0.8


[shared config file vellum]
root at vellum-0:/home/ubuntu# cat /etc/clearwater/shared_config
# Deployment definitions
home_domain=test.com<http://test.com>
sprout_hostname=sprout.test.com<http://sprout.test.com>
hs_hostname=hs.test.com:8888<http://hs.test.com:8888>
hs_provisioning_hostname=hs-prov.test.com:8889<http://hs-prov.test.com:8889>
dime_hostname=dime.test.com:10888<http://dime.test.com:10888>
xdms_hostname=homer.test.com:7888<http://homer.test.com:7888>
sprout_impi_store=vellum.test.com<http://vellum.test.com>
sprout_registration_store=vellum.test.com<http://vellum.test.com>
cassandra_hostname=vellum.test.com<http://vellum.test.com>
chronos_hostname=vellum.test.com<http://vellum.test.com>
dime_session_store=vellum.test.com<http://vellum.test.com>

upstream_port=0

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

[netstat on vellum]
root at vellum-0:/home/ubuntu# netstat -tulnap
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 192.168.0.8:11211<http://192.168.0.8:11211>       0.0.0.0:*               LISTEN      27005/memcached
tcp        0      0 127.0.0.1:7253<http://127.0.0.1:7253>          0.0.0.0:*               LISTEN      27610/chronos
tcp        0      0 255.255.255.255:7253<http://255.255.255.255:7253>    0.0.0.0:*               LISTEN      27610/chronos
tcp        0      0 127.0.0.1:53<http://127.0.0.1:53>            0.0.0.0:*               LISTEN      7718/dnsmasq
tcp        0      0 0.0.0.0:22<http://0.0.0.0:22>              0.0.0.0:*               LISTEN      1189/sshd
tcp        0      0 127.0.0.1:2812<http://127.0.0.1:2812>          0.0.0.0:*               LISTEN      7833/monit
tcp        0      0 192.168.0.8:44035<http://192.168.0.8:44035>       192.168.0.8:11211<http://192.168.0.8:11211>       TIME_WAIT   -
tcp        0      0 127.0.0.1:54026<http://127.0.0.1:54026>         127.0.0.1:7253<http://127.0.0.1:7253>          TIME_WAIT   -
tcp        0      0 192.168.0.8:44064<http://192.168.0.8:44064>       192.168.0.8:11211<http://192.168.0.8:11211>       TIME_WAIT   -
tcp        0      0 192.168.0.8:44081<http://192.168.0.8:44081>       192.168.0.8:11211<http://192.168.0.8:11211>       TIME_WAIT   -
tcp        0      0 192.168.0.8:44053<http://192.168.0.8:44053>       192.168.0.8:11211<http://192.168.0.8:11211>       TIME_WAIT   -
tcp        0      0 192.168.0.8:44054<http://192.168.0.8:44054>       192.168.0.8:11211<http://192.168.0.8:11211>       TIME_WAIT   -
tcp        0      0 192.168.0.8:44048<http://192.168.0.8:44048>       192.168.0.8:11211<http://192.168.0.8:11211>       TIME_WAIT   -
tcp        0      0 192.168.0.8:22<http://192.168.0.8:22>          10.1.10.112:51998<http://10.1.10.112:51998>       ESTABLISHED 26454/sshd: ubuntu
tcp        0    268 192.168.0.8:22<http://192.168.0.8:22>          10.1.10.112:51933<http://10.1.10.112:51933>       ESTABLISHED 22779/sshd: ubuntu
tcp6       0      0 :::11311                :::*                    LISTEN      26657/astaire
tcp6       0      0 ::1:53                  :::*                    LISTEN      7718/dnsmasq
tcp6       0      0 :::22                   :::*                    LISTEN      1189/sshd
udp        0      0 127.0.0.1:53<http://127.0.0.1:53>            0.0.0.0:*                           7718/dnsmasq
udp        0      0 0.0.0.0:68<http://0.0.0.0:68>              0.0.0.0:*                           601/dhclient
udp        0      0 192.168.0.8:123<http://192.168.0.8:123>         0.0.0.0:*                           7362/ntpd
udp        0      0 127.0.0.1:123<http://127.0.0.1:123>           0.0.0.0:*                           7362/ntpd
udp        0      0 0.0.0.0:123<http://0.0.0.0:123>             0.0.0.0:*                           7362/ntpd
udp        0      0 0.0.0.0:161<http://0.0.0.0:161>             0.0.0.0:*                           7472/snmpd
udp        0      0 0.0.0.0:55423<http://0.0.0.0:55423>           0.0.0.0:*                           601/dhclient
udp6       0      0 :::23767                :::*                                601/dhclient
udp6       0      0 ::1:53                  :::*                                7718/dnsmasq
udp6       0      0 ::1:123                 :::*                                7362/ntpd
udp6       0      0 fe80::f816:3eff:fe3:123 :::*                                7362/ntpd
udp6       0      0 :::123                  :::*                                7362/ntpd
udp6       0      0 :::161                  :::*                                7472/snmpd

[Ping results]
root at vellum-0:/home/ubuntu# ping hs-prov.test.com<http://hs-prov.test.com>
PING hs-prov.test.com<http://hs-prov.test.com> (192.168.0.4) 56(84) bytes of data.
64 bytes from 192.168.0.4<http://192.168.0.4>: icmp_seq=1 ttl=64 time=10.3 ms
64 bytes from 192.168.0.4<http://192.168.0.4>: icmp_seq=2 ttl=64 time=12.1 ms
64 bytes from 192.168.0.4<http://192.168.0.4>: icmp_seq=3 ttl=64 time=9.06 ms
^C
--- hs-prov.test.com<http://hs-prov.test.com> ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2032ms
rtt min/avg/max/mdev = 9.063/10.529/12.151/1.270 ms
root at vellum-0:/home/ubuntu#
root at vellum-0:/home/ubuntu# ping hs.test.com<http://hs.test.com>
PING hs.test.com<http://hs.test.com> (192.168.0.4) 56(84) bytes of data.
64 bytes from 192.168.0.4<http://192.168.0.4>: icmp_seq=1 ttl=64 time=21.3 ms
64 bytes from 192.168.0.4<http://192.168.0.4>: icmp_seq=2 ttl=64 time=4.58 ms
64 bytes from 192.168.0.4<http://192.168.0.4>: icmp_seq=3 ttl=64 time=20.6 ms
^C
--- hs.test.com<http://hs.test.com> ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2070ms
rtt min/avg/max/mdev = 4.584/15.523/21.363/7.741 ms
root at vellum-0:/home/ubuntu# ping dime-0.test.com<http://dime-0.test.com>
PING dime-0.test.com<http://dime-0.test.com> (10.1.10.192) 56(84) bytes of data.
64 bytes from 10.1.10.192<http://10.1.10.192>: icmp_seq=1 ttl=63 time=25.9 ms
64 bytes from 10.1.10.192<http://10.1.10.192>: icmp_seq=2 ttl=63 time=58.9 ms
^C
--- dime-0.test.com<http://dime-0.test.com> ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1029ms
rtt min/avg/max/mdev = 25.954/42.459/58.964/16.505 ms

root at bono-0:/home/ubuntu# ping vellum-0.test.com<http://vellum-0.test.com>
PING vellum-0.test.com<http://vellum-0.test.com> (10.1.10.204) 56(84) bytes of data.
64 bytes from 10.1.10.204<http://10.1.10.204>: icmp_seq=1 ttl=63 time=36.7 ms
64 bytes from 10.1.10.204<http://10.1.10.204>: icmp_seq=2 ttl=63 time=25.2 ms
^C
--- vellum-0.test.com<http://vellum-0.test.com> ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 25.240/30.985/36.730/5.745 ms
root at bono-0:/home/ubuntu# ping vellum.test.com<http://vellum.test.com>
PING vellum.test.com<http://vellum.test.com> (192.168.0.8) 56(84) bytes of data.
64 bytes from 192.168.0.8<http://192.168.0.8>: icmp_seq=1 ttl=64 time=51.4 ms
64 bytes from 192.168.0.8<http://192.168.0.8>: icmp_seq=2 ttl=64 time=3.93 ms
64 bytes from 192.168.0.8<http://192.168.0.8>: icmp_seq=3 ttl=64 time=4.22 ms

Regards,

Abdul Basit Alvi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/840ad171/attachment.html>

From Robert.Day at metaswitch.com  Wed Aug 30 05:52:06 2017
From: Robert.Day at metaswitch.com (Robert Day)
Date: Wed, 30 Aug 2017 09:52:06 +0000
Subject: [Project Clearwater] SIP Registeration Failed
In-Reply-To: <BY2PR02MB2149CEB2085D23CB16CB8095F49C0@BY2PR02MB2149.namprd02.prod.outlook.com>
References: <CA+qu+1fersTYSCD5YDBnCekzjoP1j3amG_Lo=vOmpmhFUc__rA@mail.gmail.com>
	<BY2PR02MB2149CEB2085D23CB16CB8095F49C0@BY2PR02MB2149.namprd02.prod.outlook.com>
Message-ID: <BY2PR02MB2149419D247D29CB04EBB24FF49C0@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Ankit,

The problem is probably that you have ?sip:? in the authentication username, which you don?t need.

Best,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Robert Day
Sent: 30 August 2017 10:28
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] SIP Registeration Failed

Hi Ankit,

Have you tried the steps at http://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html, particularly turning on debug logging on Bono and Sprout to see if your request is getting to them and where it is failing?

Thanks,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Ankit Patwa
Sent: 24 August 2017 16:33
To: Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] SIP Registeration Failed

Hi,

we have installed all in one image on linux 14.04 serer.
After login on ellis public and private identifier generated.

Private identifier
6505550677 at 192.168.5.20<mailto:6505550677 at 192.168.5.20>
Password rnqYR4s7d

Public identifier
sip:6505550677 at 192.168.5.20<mailto:sip%3A6505550677 at 192.168.5.20>

we are giving

username                 6505550677
password                  rnqYR4s7d
server                       192.168.5.20
Authentication user   sip:6505550677 at 192.168.5.20<mailto:sip%3A6505550677 at 192.168.5.20>
Transport Type          TCP

Device    Redmi
Android  marshmallow

But I am getting registration failed error.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/a8d58a75/attachment.html>

From Robert.Day at metaswitch.com  Wed Aug 30 05:52:09 2017
From: Robert.Day at metaswitch.com (Robert Day)
Date: Wed, 30 Aug 2017 09:52:09 +0000
Subject: [Project Clearwater] ENUM: Multiple @ dig queries
In-Reply-To: <008201d320a9$66dc6cc0$34954640$@coure-tech.com>
References: <008201d320a9$66dc6cc0$34954640$@coure-tech.com>
Message-ID: <BY2PR02MB2149BE8D98BBA69F1BF0B3FDF49C0@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Mordi,

It's not clear to me whether you're using Clearwater or not (and if not, this may be the wrong mailing list). http://clearwater.readthedocs.io/en/stable/ENUM.html describes Clearwater's use of ENUM, in case that's helpful. Note that Clearwater doesn't include a username or password in the ENUM request.

Best,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of MORDI Obim
Sent: 29 August 2017 10:30
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] ENUM: Multiple @ dig queries

Good Morning Clearwater Team,

My name is Obim, mailing from Lagos - Nigeria,
My company has a web solution that helps our clients run number lookups against our database, connecting either via the API channel or FTP channel.
Now, we recently heard of the ENUM channel (using the BIND tool) and we were able to set is up successfully, but we can only run number lookup at a time. See below our used syntax:

dig @ourURL 3.8.0.6.6.8.8.6.0.8.4.3.2.e164.arpa.4.username.password IN NAPTR

now we have clients that want to run multiple queries at the same, please can you help point us in the right direction concerning which tool to use that can help solve our problem..

thank you.




Regards...

MORDI Obim
Lead Application Support/Testing
COURE Software & Systems Ltd
T: +234-1-631-1910 Office  +234-1-631-1925 Direct 320 Ext*
+234-806-886-6190 NG Cell (Nigeria)
E: omordi at coure-tech.com<javascript:OpenNewWindow('/Mondo/lang/sys/Forms/MAI/compose.aspx?MsgTo=omordi%40coure-tech.com&MsgSubject=&MsgCc=&MsgBcc=&MsgBody=',570,450)> || W: http://www.coure-tech.com<http://www.coure-tech.com/> || S: omordi
[CoureCompanyLogo1]<http://www.coure-tech.com/>[anq1]<http://www.anq.nms.com.ng/>[tv]<http://www.tendersvault.com/>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/17e748f2/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.gif
Type: image/gif
Size: 1741 bytes
Desc: image001.gif
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/17e748f2/attachment.gif>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 6736 bytes
Desc: image002.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/17e748f2/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 6452 bytes
Desc: image003.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/17e748f2/attachment-0001.png>

From hkaranjikar at apm.com  Wed Aug 30 06:22:13 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Wed, 30 Aug 2017 15:52:13 +0530
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <BY2PR02MB214936A7D8033E5BBBA88575F49C0@BY2PR02MB2149.namprd02.prod.outlook.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
	<BLUPR02MB437E06C13B70FCC491E0A27E5B20@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEanzSPTx5=QUczx34_=WLa3b0Ztbt3wZ57-AYx39UN0QQ@mail.gmail.com>
	<BLUPR02MB4379BCC901095CF549CF27FE5B50@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEb+4xu-qBE7Sq_cfO13GMc6WBxJBEpC8YqPrCGj_D=F8Q@mail.gmail.com>
	<BLUPR02MB437E492B6592B3B3EBBEB99E5880@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEaT9TZ1h94v7P5n67qY8ZL3bGyac2Ucr_ALiKp=g55HFg@mail.gmail.com>
	<BY2PR02MB214906F5CE748977C1F924F3F49B0@BY2PR02MB2149.namprd02.prod.outlook.com>
	<CABmBNEZ-g4AtJC5dcVoykVSn__hWj2jgJ1Xsq_4yDH1L_b12YQ@mail.gmail.com>
	<BY2PR02MB214936A7D8033E5BBBA88575F49C0@BY2PR02MB2149.namprd02.prod.outlook.com>
Message-ID: <CABmBNEYxGsjFCm2xuFD3ODGv4r86-ugw6tWM0dMNr3B22WBJ_g@mail.gmail.com>

Please find attached compressed logs of
vellum, sprout, homer, ellis, bono

On Wed, Aug 30, 2017 at 2:48 PM, Robert Day <Robert.Day at metaswitch.com>
wrote:

> Hi Hrishikesh,
>
>
>
> Is it possible for you to compress it and send the compressed version? For
> example, with `bzip2 /var/log/clearwater-etcd/clearwater-etcd.log.1`?
>
>
>
> Thanks,
>
> Rob
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* 29 August 2017 10:25
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi Rob,
>
> Thanks for your email.
>
> File /var/log/clearwater-etcd/clearwater-etcd.log does not show any logs.
>
> However I have checked file /var/log/clearwater-etcd/clearwater-etcd.log,1.
> The size of these files is huge in 100s of Mbs.
>
> I won't be able to send you all the logs. Which section of logs will you
> prefer? The logs immediately after system restart or logs at the end of the
> file or any particular scenario?
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
> On Sat, Aug 26, 2017 at 12:47 AM, Robert Day <Robert.Day at metaswitch.com>
> wrote:
>
> Hi Hrishikesh,
>
>
>
> Could you send /var/log/clearwater-etcd/clearwater-etcd.log (which is
> different to /var/log/clearwater-etcd/clearwater-etcd-initd.log) from
> each of your nodes? That should help tell what?s going on here.
>
>
>
> Thanks,
>
> Rob
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* 22 August 2017 10:51
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi Andrew,
>
> Thanks for your constant support. I am facing a weired problem. All of my
> nodes were working fine. I sent you the logs earlier.
>
> I restarted all nodes and suddenly etcd_process on all nodes is failed to
> execute. I don't know what is the reason. Will you please guide me on this.
>
> All nodes are able to ping one another.
>
> Here are some logs
>
>
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> cluster may be unhealthy: failed to list members
> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
> dial tcp 192.168.56.105:4000: getsockopt: connection refused
>
> error #0: dial tcp 192.168.56.105:4000: getsockopt: connection refused
>
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
> dial tcp 192.168.56.105:4000: getsockopt: connection refused
>
> error #0: dial tcp 192.168.56.105:4000: getsockopt: connection refused
>
> 2017-08-22 14:42:44.966660634 Running etcdctl cluster-health
> cluster may be unhealthy: failed to list members
> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
> client: endpoint http://192.168.56.110:4000 exceeded header timeout
>
> #cat /var/log/clearwater-etcd/clearwater-etcd-initd.log
>
> 2017-08-22 15:17:22.731984254 etcdctl returned 4
> 2017-08-22 15:17:22.732730248 Not joining an unhealthy cluster
> 2017-08-22 15:18:02.429508523 Restarting etcd clearwater-etcd
> 2017-08-22 15:18:02.433629983 Configured ETCDCTL_PEERS:
> 192.168.56.107:4000,192.168.56.109:4000,192.168.56.110:4000,
> 192.168.56.108:4000,
> 2017-08-22 15:18:02.434237379 Check for previous failed startup attempt
> 2017-08-22 15:18:02.435071900 Running etcdctl member list
> client: etcd cluster is unavailable or misconfigured; error #0: dial tcp
> 192.168.56.110:4000: getsockopt: connection refused
> ; error #1: dial tcp 192.168.56.107:4000: getsockopt: connection refused
> ; error #2: dial tcp 192.168.56.109:4000: getsockopt: connection refused
> ; error #3: dial tcp 192.168.56.108:4000: getsockopt: connection refused
> ; error #4: http: no Host in request URL
>
> 2017-08-22 15:18:02.447231505 etcdctl returned 1
> 2017-08-22 15:18:02.453120361 Joining existing cluster...
> 2017-08-22 15:18:13.456387174 Configured ETCDCTL_PEERS:
> 192.168.56.107:4000,192.168.56.109:4000,192.168.56.110:4000,
> 192.168.56.108:4000,
> 2017-08-22 15:18:13.457279845 Check cluster is healthy
> 2017-08-22 15:18:13.458636020 Running etcdctl cluster-health
> cluster may be unhealthy: failed to list members
> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
> http: no Host in request URL
> ; error #1: dial tcp 192.168.56.109:4000: getsockopt: connection refused
> ; error #2: dial tcp 192.168.56.108:4000: getsockopt: connection refused
> ; error #3: dial tcp 192.168.56.110:4000: getsockopt: connection refused
> ; error #4: dial tcp 192.168.56.107:4000: getsockopt: connection refused
>
> error #0: http: no Host in request URL
> error #1: dial tcp 192.168.56.109:4000: getsockopt: connection refused
> error #2: dial tcp 192.168.56.108:4000: getsockopt: connection refused
> error #3: dial tcp 192.168.56.110:4000: getsockopt: connection refused
> error #4: dial tcp 192.168.56.107:4000: getsockopt: connection refused
>
> 2017-08-22 15:18:13.470356301 etcdctl returned 4
> 2017-08-22 15:18:13.471296869 Not joining an unhealthy cluster
> 2017-08-22 15:18:43.177856227 Restarting etcd clearwater-etcd
> 2017-08-22 15:18:43.182076210 Configured ETCDCTL_PEERS:
> 192.168.56.107:4000,192.168.56.109:4000,192.168.56.110:4000,
> 192.168.56.108:4000,
> 2017-08-22 15:18:43.182623728 Check for previous failed startup attempt
> 2017-08-22 15:18:43.183439470 Running etcdctl member list
> client: etcd cluster is unavailable or misconfigured; error #0: dial tcp
> 192.168.56.107:4000: getsockopt: connection refused
> ; error #1: dial tcp 192.168.56.108:4000: getsockopt: connection refused
> ; error #2: dial tcp 192.168.56.109:4000: getsockopt: connection refused
> ; error #3: http: no Host in request URL
> ; error #4: dial tcp 192.168.56.110:4000: getsockopt: connection refused
>
> 2017-08-22 15:18:43.195788447 etcdctl returned 1
> 2017-08-22 15:18:43.201223078 Joining existing cluster...
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
> On Thu, Aug 10, 2017 at 6:27 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
> Hi Hrishikesh,
>
>
>
> The process that Bono uses to resolve a DNS request is to:
>
>
>
> ?         make a DNS lookup - by default, to dnsmasq running on 127.0.0.1
> (local host)
>
> ?         dnsmasq reads /etc/hosts
>
> ?         dnsmasq responds with an answer
>
>
>
> The reason why this might be failing but pings are working is that pings
> may just read /etc/hosts directly rather than using dnsmasq.
>
>
>
> Can you please try:
>
>
>
> ?         Running `dig icscf.cwsprout1 @127.0.0.1` - this will show
> whether dnsmasq is responding with an answer (and therefore whether the
> problem is in bono or dnsmasq)
>
> ?         Running `sudo service dnsmasq restart` - this might trigger it
> to pick up a change in the /etc/hosts file
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Tuesday, August 8, 2017 10:15 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi Andrew,
>
> Thanks for your help.
>
> I already have configured etc/hosts with the entry. After I added the
> entry restund process started executing.
>
> Bono can ping icscf.cwsprout1
>
> [bono]cwbono1 at cwbono1:~$ ping icscf.cwsprout1
> PING icscf.cwsprout1 (192.168.56.107) 56(84) bytes of data.
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=1 ttl=64
> time=0.218 ms
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=2 ttl=64
> time=0.311 ms
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=3 ttl=64
> time=0.224 ms
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=4 ttl=64
> time=0.188 ms
> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=5 ttl=64
> time=0.187 ms
> ^C
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
> On Mon, Aug 7, 2017 at 4:28 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
> Hi Hrishikesh,
>
>
>
> Thanks for the updated diags.
>
>
>
> One thing that can be useful when you are reading through a log file is to
> search for the pattern ?Error?. This will highlight those logs which
> indicate an error. If we do that here we see:
>
>
>
> 03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
>
>
>
> This suggests that Bono is unable to resolve the hostname
> ?icscf.cwsprout1?. We?ve spoken in the past about how you are unable to
> configure a DNS. Hence I would suggest configuring /etc/hosts to resolve
> icscf.cwsprout1.
>
>
>
> Please let me know if this works.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, August 3, 2017 10:30 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi Andrew,
>
> Thanks a lot for your reply.
>
> I did not mention the password but while doing the configuration I am
> using it.
>
> Thanks
>
> Hrishikesh
>
>
>
> On Mon, Jul 31, 2017 at 2:44 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
> Hi Hrishikesh,
>
>
>
> Thank you for your question.
>
>
>
> I?m not sure whether this is deliberate but it looks like you haven?t
> included the password in your Zoiper configuration. You must make sure you
> use the same password the Ellis client gives you.
>
>
>
> We have this guide
> <https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html>
> that might help you with the configuration you need to use with Zoiper.
>
>
>
> If this guide goes not help could I please have Bono and Sprout logs
> during the time of the registration.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, July 27, 2017 12:22 PM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] SIP 408 - Request Timeout
>
>
>
> Hi,
>
> I have used manual installation for clearwater.
>
> I have created 6 VMs using Virtualbox and using host only network.
>
> All 6 nodes are running fine as per monit logs.
>
> I have installed Zoiper client and tried to add new account as follows,
>
> ===================================================
> Zoiper Preferences
>
> Domain        -    example.com
> Username    -    6505550708
> Password    -
> Auth. Uname    -    6505550708 at example.com
> Outbound Proxy    -    192.168.56.105:8060
>
> Error        -     SIP 408 - Request Timeout
>
>
> Private Identity Generated by Ellis
>
> Private Identity:
>
> 6505550028 at example.com
> Password:Na5ZWdQj4
>
> ===================================================
>
> How ever I am getting error mentioned in subject line.
>
> Here are other logs for each node,
>
> ===================================================
>
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
> local_ip=192.168.56.105
> public_ip=192.168.56.105
> public_hostname=cwellis1
> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,
> 192.168.56.108,192.168.56.109,192.168.56.110"
>
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout1
> sprout_registration_store=192.168.56.110 #vellum
> hs_hostname=192.168.56.109:8888 #dime
> hs_provisioning_hostname=192.168.56.109:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.108:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=192.168.56.110 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
> [sudo] password for cwellis1:
> Monit 5.18.1 uptime: 3h 33m
>  Service Name                     Status
> Type
>  node-cwellis1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [ellis]cwellis1 at cwellis1:~$
>
>
> [bono]cwbono1 at cwbono1:~$ sudo monit summary
> [sudo] password for cwbono1:
> Monit 5.18.1 uptime: 23m
>  Service Name                     Status
> Type
>  node-cwbono1                     Running
> System
>  restund_process                  Running
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat
> /etc/clearwater/local_config
> local_ip=192.168.56.105
> public_ip=192.168.56.105
> public_hostname=cwellis1
> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,
> 192.168.56.108,192.168.56.109,192.168.56.110"
>
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout1
> sprout_registration_store=192.168.56.110 #vellum
> hs_hostname=192.168.56.109:8888 #dime
> hs_provisioning_hostname=192.168.56.109:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.108:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=192.168.56.110 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
> [sudo] password for cwellis1:
> Monit 5.18.1 uptime: 3h 33m
>  Service Name                     Status
> Type
>  node-cwellis1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [ellis]cwellis1 at cwellis1:~$
>
>
> [bono]cwbono1 at cwbono1:~$ sudo monit summary
> [sudo] password for cwbono1:
> Monit 5.18.1 uptime: 23m
>  Service Name                     Status
> Type
>  node-cwbono1                     Running
> System
>  restund_process                  Running
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono1 at cwbono1:~$
>
>
>
> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
> [sudo] password for cwsprout1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwsprout1                   Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
>
>
>
>
> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
> [sudo] password for cwhomer1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwhomer1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [homer]cwhomer1 at cwhomer1:~$
>
>
> [dime]cwdime1 at cwdime1:~$ sudo monit summary
> [sudo] password for cwdime1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwdime1                     Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [dime]cwdime1 at cwdime1:~$
>
>
>
> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
> [sudo] password for cwvellum1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwvellum1                   Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [vellum]cwvellum1 at cwvellum1:~$
> 192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono1 at cwbono1:~$
>
>
>
> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
> [sudo] password for cwsprout1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwsprout1                   Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
>
>
>
>
> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
> [sudo] password for cwhomer1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwhomer1                    Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [homer]cwhomer1 at cwhomer1:~$
>
>
> [dime]cwdime1 at cwdime1:~$ sudo monit summary
> [sudo] password for cwdime1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwdime1                     Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [dime]cwdime1 at cwdime1:~$
>
>
>
> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
> [sudo] password for cwvellum1:
> Monit 5.18.1 uptime: 22m
>  Service Name                     Status
> Type
>  node-cwvellum1                   Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
> member 275f7358be9c56b9 is healthy: got healthy result from
> http://192.168.56.107:4000
> member 37a3412cc7457712 is healthy: got healthy result from
> http://192.168.56.109:4000
> member 59ee11d785e88e75 is healthy: got healthy result from
> http://192.168.56.110:4000
> member d0eda43d23f9dc26 is healthy: got healthy result from
> http://192.168.56.106:4000
> member ed17a7546df2174c is healthy: got healthy result from
> http://192.168.56.108:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
> clientURLs=http://192.168.56.109:4000 isLeader=false
> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
> clientURLs=http://192.168.56.110:4000 isLeader=false
> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=false
> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
> clientURLs=http://192.168.56.108:4000 isLeader=true
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [vellum]cwvellum1 at cwvellum1:~$
>
>
> ===================================================
>
> Please let me know what is wrong with the setup?
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/16aaffed/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: vellum-clearwater-etcd.log.1.bz2
Type: application/x-bzip2
Size: 674376 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/16aaffed/attachment.bz2>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sprout-clearwater-etcd.log.1.bz2
Type: application/x-bzip2
Size: 2721265 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/16aaffed/attachment-0001.bz2>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: homer-clearwater-etcd.log.1.bz2
Type: application/x-bzip2
Size: 4250197 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/16aaffed/attachment-0002.bz2>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: eliis-clearwater-etcd.log.1.bz2
Type: application/x-bzip2
Size: 4703213 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/16aaffed/attachment-0003.bz2>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: bono-clearwater-etcd.log.1.bz2
Type: application/x-bzip2
Size: 4406562 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/16aaffed/attachment-0004.bz2>

From hkaranjikar at apm.com  Wed Aug 30 06:25:24 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Wed, 30 Aug 2017 15:55:24 +0530
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <CABmBNEYxGsjFCm2xuFD3ODGv4r86-ugw6tWM0dMNr3B22WBJ_g@mail.gmail.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
	<BLUPR02MB437E06C13B70FCC491E0A27E5B20@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEanzSPTx5=QUczx34_=WLa3b0Ztbt3wZ57-AYx39UN0QQ@mail.gmail.com>
	<BLUPR02MB4379BCC901095CF549CF27FE5B50@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEb+4xu-qBE7Sq_cfO13GMc6WBxJBEpC8YqPrCGj_D=F8Q@mail.gmail.com>
	<BLUPR02MB437E492B6592B3B3EBBEB99E5880@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEaT9TZ1h94v7P5n67qY8ZL3bGyac2Ucr_ALiKp=g55HFg@mail.gmail.com>
	<BY2PR02MB214906F5CE748977C1F924F3F49B0@BY2PR02MB2149.namprd02.prod.outlook.com>
	<CABmBNEZ-g4AtJC5dcVoykVSn__hWj2jgJ1Xsq_4yDH1L_b12YQ@mail.gmail.com>
	<BY2PR02MB214936A7D8033E5BBBA88575F49C0@BY2PR02MB2149.namprd02.prod.outlook.com>
	<CABmBNEYxGsjFCm2xuFD3ODGv4r86-ugw6tWM0dMNr3B22WBJ_g@mail.gmail.com>
Message-ID: <CABmBNEa7-3sZd6tUkAAz0NnntOahyZs3aaWysZ2ndwe1CGUmXg@mail.gmail.com>

PFA remaining logs from dime

Thanks
Hrishikesh

On Wed, Aug 30, 2017 at 3:52 PM, Hrishikesh Karanjikar <hkaranjikar at apm.com>
wrote:

> Please find attached compressed logs of
> vellum, sprout, homer, ellis, bono
>
> On Wed, Aug 30, 2017 at 2:48 PM, Robert Day <Robert.Day at metaswitch.com>
> wrote:
>
>> Hi Hrishikesh,
>>
>>
>>
>> Is it possible for you to compress it and send the compressed version?
>> For example, with `bzip2 /var/log/clearwater-etcd/clearwater-etcd.log.1`?
>>
>>
>>
>> Thanks,
>>
>> Rob
>>
>>
>>
>> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
>> *On Behalf Of *Hrishikesh Karanjikar
>> *Sent:* 29 August 2017 10:25
>> *To:* clearwater at lists.projectclearwater.org
>> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>>
>>
>>
>> Hi Rob,
>>
>> Thanks for your email.
>>
>> File /var/log/clearwater-etcd/clearwater-etcd.log does not show any logs.
>>
>> However I have checked file /var/log/clearwater-etcd/clearwater-etcd.log,1.
>> The size of these files is huge in 100s of Mbs.
>>
>> I won't be able to send you all the logs. Which section of logs will you
>> prefer? The logs immediately after system restart or logs at the end of the
>> file or any particular scenario?
>>
>> Thanks
>>
>> Hrishikesh
>>
>>
>>
>>
>>
>> On Sat, Aug 26, 2017 at 12:47 AM, Robert Day <Robert.Day at metaswitch.com>
>> wrote:
>>
>> Hi Hrishikesh,
>>
>>
>>
>> Could you send /var/log/clearwater-etcd/clearwater-etcd.log (which is
>> different to /var/log/clearwater-etcd/clearwater-etcd-initd.log) from
>> each of your nodes? That should help tell what?s going on here.
>>
>>
>>
>> Thanks,
>>
>> Rob
>>
>>
>>
>> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
>> *On Behalf Of *Hrishikesh Karanjikar
>> *Sent:* 22 August 2017 10:51
>> *To:* clearwater at lists.projectclearwater.org
>> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>>
>>
>>
>> Hi Andrew,
>>
>> Thanks for your constant support. I am facing a weired problem. All of my
>> nodes were working fine. I sent you the logs earlier.
>>
>> I restarted all nodes and suddenly etcd_process on all nodes is failed to
>> execute. I don't know what is the reason. Will you please guide me on this.
>>
>> All nodes are able to ping one another.
>>
>> Here are some logs
>>
>>
>> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
>> cluster may be unhealthy: failed to list members
>> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
>> dial tcp 192.168.56.105:4000: getsockopt: connection refused
>>
>> error #0: dial tcp 192.168.56.105:4000: getsockopt: connection refused
>>
>> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
>> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
>> dial tcp 192.168.56.105:4000: getsockopt: connection refused
>>
>> error #0: dial tcp 192.168.56.105:4000: getsockopt: connection refused
>>
>> 2017-08-22 14:42:44.966660634 Running etcdctl cluster-health
>> cluster may be unhealthy: failed to list members
>> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
>> client: endpoint http://192.168.56.110:4000 exceeded header timeout
>>
>> #cat /var/log/clearwater-etcd/clearwater-etcd-initd.log
>>
>> 2017-08-22 15:17:22.731984254 etcdctl returned 4
>> 2017-08-22 15:17:22.732730248 Not joining an unhealthy cluster
>> 2017-08-22 15:18:02.429508523 Restarting etcd clearwater-etcd
>> 2017-08-22 15:18:02.433629983 Configured ETCDCTL_PEERS:
>> 192.168.56.107:4000,192.168.56.109:4000,192.168.56.110:4000,
>> 192.168.56.108:4000,
>> 2017-08-22 15:18:02.434237379 Check for previous failed startup attempt
>> 2017-08-22 15:18:02.435071900 Running etcdctl member list
>> client: etcd cluster is unavailable or misconfigured; error #0: dial tcp
>> 192.168.56.110:4000: getsockopt: connection refused
>> ; error #1: dial tcp 192.168.56.107:4000: getsockopt: connection refused
>> ; error #2: dial tcp 192.168.56.109:4000: getsockopt: connection refused
>> ; error #3: dial tcp 192.168.56.108:4000: getsockopt: connection refused
>> ; error #4: http: no Host in request URL
>>
>> 2017-08-22 15:18:02.447231505 etcdctl returned 1
>> 2017-08-22 15:18:02.453120361 Joining existing cluster...
>> 2017-08-22 15:18:13.456387174 Configured ETCDCTL_PEERS:
>> 192.168.56.107:4000,192.168.56.109:4000,192.168.56.110:4000,
>> 192.168.56.108:4000,
>> 2017-08-22 15:18:13.457279845 Check cluster is healthy
>> 2017-08-22 15:18:13.458636020 Running etcdctl cluster-health
>> cluster may be unhealthy: failed to list members
>> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
>> http: no Host in request URL
>> ; error #1: dial tcp 192.168.56.109:4000: getsockopt: connection refused
>> ; error #2: dial tcp 192.168.56.108:4000: getsockopt: connection refused
>> ; error #3: dial tcp 192.168.56.110:4000: getsockopt: connection refused
>> ; error #4: dial tcp 192.168.56.107:4000: getsockopt: connection refused
>>
>> error #0: http: no Host in request URL
>> error #1: dial tcp 192.168.56.109:4000: getsockopt: connection refused
>> error #2: dial tcp 192.168.56.108:4000: getsockopt: connection refused
>> error #3: dial tcp 192.168.56.110:4000: getsockopt: connection refused
>> error #4: dial tcp 192.168.56.107:4000: getsockopt: connection refused
>>
>> 2017-08-22 15:18:13.470356301 etcdctl returned 4
>> 2017-08-22 15:18:13.471296869 Not joining an unhealthy cluster
>> 2017-08-22 15:18:43.177856227 Restarting etcd clearwater-etcd
>> 2017-08-22 15:18:43.182076210 Configured ETCDCTL_PEERS:
>> 192.168.56.107:4000,192.168.56.109:4000,192.168.56.110:4000,
>> 192.168.56.108:4000,
>> 2017-08-22 15:18:43.182623728 Check for previous failed startup attempt
>> 2017-08-22 15:18:43.183439470 Running etcdctl member list
>> client: etcd cluster is unavailable or misconfigured; error #0: dial tcp
>> 192.168.56.107:4000: getsockopt: connection refused
>> ; error #1: dial tcp 192.168.56.108:4000: getsockopt: connection refused
>> ; error #2: dial tcp 192.168.56.109:4000: getsockopt: connection refused
>> ; error #3: http: no Host in request URL
>> ; error #4: dial tcp 192.168.56.110:4000: getsockopt: connection refused
>>
>> 2017-08-22 15:18:43.195788447 etcdctl returned 1
>> 2017-08-22 15:18:43.201223078 Joining existing cluster...
>>
>> Thanks
>>
>> Hrishikesh
>>
>>
>>
>>
>>
>> On Thu, Aug 10, 2017 at 6:27 PM, Andrew Edmonds <
>> Andrew.Edmonds at metaswitch.com> wrote:
>>
>> Hi Hrishikesh,
>>
>>
>>
>> The process that Bono uses to resolve a DNS request is to:
>>
>>
>>
>> ?         make a DNS lookup - by default, to dnsmasq running on
>> 127.0.0.1 (local host)
>>
>> ?         dnsmasq reads /etc/hosts
>>
>> ?         dnsmasq responds with an answer
>>
>>
>>
>> The reason why this might be failing but pings are working is that pings
>> may just read /etc/hosts directly rather than using dnsmasq.
>>
>>
>>
>> Can you please try:
>>
>>
>>
>> ?         Running `dig icscf.cwsprout1 @127.0.0.1` - this will show
>> whether dnsmasq is responding with an answer (and therefore whether the
>> problem is in bono or dnsmasq)
>>
>> ?         Running `sudo service dnsmasq restart` - this might trigger it
>> to pick up a change in the /etc/hosts file
>>
>>
>>
>> Thanks,
>>
>>
>>
>> Andrew
>>
>>
>>
>> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
>> *On Behalf Of *Hrishikesh Karanjikar
>> *Sent:* Tuesday, August 8, 2017 10:15 AM
>> *To:* clearwater at lists.projectclearwater.org
>> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>>
>>
>>
>> Hi Andrew,
>>
>> Thanks for your help.
>>
>> I already have configured etc/hosts with the entry. After I added the
>> entry restund process started executing.
>>
>> Bono can ping icscf.cwsprout1
>>
>> [bono]cwbono1 at cwbono1:~$ ping icscf.cwsprout1
>> PING icscf.cwsprout1 (192.168.56.107) 56(84) bytes of data.
>> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=1 ttl=64
>> time=0.218 ms
>> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=2 ttl=64
>> time=0.311 ms
>> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=3 ttl=64
>> time=0.224 ms
>> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=4 ttl=64
>> time=0.188 ms
>> 64 bytes from icscf.cwsprout1 (192.168.56.107): icmp_seq=5 ttl=64
>> time=0.187 ms
>> ^C
>>
>> Thanks
>>
>> Hrishikesh
>>
>>
>>
>>
>>
>> On Mon, Aug 7, 2017 at 4:28 PM, Andrew Edmonds <
>> Andrew.Edmonds at metaswitch.com> wrote:
>>
>> Hi Hrishikesh,
>>
>>
>>
>> Thanks for the updated diags.
>>
>>
>>
>> One thing that can be useful when you are reading through a log file is
>> to search for the pattern ?Error?. This will highlight those logs which
>> indicate an error. If we do that here we see:
>>
>>
>>
>> 03-08-2017 09:24:12.072 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout1 to an IP address - Not found (PJ_ENOTFOUND)
>>
>>
>>
>> This suggests that Bono is unable to resolve the hostname
>> ?icscf.cwsprout1?. We?ve spoken in the past about how you are unable to
>> configure a DNS. Hence I would suggest configuring /etc/hosts to resolve
>> icscf.cwsprout1.
>>
>>
>>
>> Please let me know if this works.
>>
>>
>>
>> Thanks,
>>
>>
>>
>> Andrew
>>
>>
>>
>> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
>> *On Behalf Of *Hrishikesh Karanjikar
>> *Sent:* Thursday, August 3, 2017 10:30 AM
>> *To:* clearwater at lists.projectclearwater.org
>> *Subject:* Re: [Project Clearwater] SIP 408 - Request Timeout
>>
>>
>>
>> Hi Andrew,
>>
>> Thanks a lot for your reply.
>>
>> I did not mention the password but while doing the configuration I am
>> using it.
>>
>> Thanks
>>
>> Hrishikesh
>>
>>
>>
>> On Mon, Jul 31, 2017 at 2:44 PM, Andrew Edmonds <
>> Andrew.Edmonds at metaswitch.com> wrote:
>>
>> Hi Hrishikesh,
>>
>>
>>
>> Thank you for your question.
>>
>>
>>
>> I?m not sure whether this is deliberate but it looks like you haven?t
>> included the password in your Zoiper configuration. You must make sure you
>> use the same password the Ellis client gives you.
>>
>>
>>
>> We have this guide
>> <https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html>
>> that might help you with the configuration you need to use with Zoiper.
>>
>>
>>
>> If this guide goes not help could I please have Bono and Sprout logs
>> during the time of the registration.
>>
>>
>>
>> Thanks,
>>
>>
>>
>> Andrew
>>
>>
>>
>> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
>> *On Behalf Of *Hrishikesh Karanjikar
>> *Sent:* Thursday, July 27, 2017 12:22 PM
>> *To:* clearwater at lists.projectclearwater.org
>> *Subject:* [Project Clearwater] SIP 408 - Request Timeout
>>
>>
>>
>> Hi,
>>
>> I have used manual installation for clearwater.
>>
>> I have created 6 VMs using Virtualbox and using host only network.
>>
>> All 6 nodes are running fine as per monit logs.
>>
>> I have installed Zoiper client and tried to add new account as follows,
>>
>> ===================================================
>> Zoiper Preferences
>>
>> Domain        -    example.com
>> Username    -    6505550708
>> Password    -
>> Auth. Uname    -    6505550708 at example.com
>> Outbound Proxy    -    192.168.56.105:8060
>>
>> Error        -     SIP 408 - Request Timeout
>>
>>
>> Private Identity Generated by Ellis
>>
>> Private Identity:
>>
>> 6505550028 at example.com
>> Password:Na5ZWdQj4
>>
>> ===================================================
>>
>> How ever I am getting error mentioned in subject line.
>>
>> Here are other logs for each node,
>>
>> ===================================================
>>
>> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
>> local_ip=192.168.56.105
>> public_ip=192.168.56.105
>> public_hostname=cwellis1
>> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,1
>> 92.168.56.108,192.168.56.109,192.168.56.110"
>>
>> [ellis]cwellis1 at cwellis1:~$
>> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
>> #####################################################################
>> # No Shared Config has been provided
>> # Replace this file with the Shared Configuration for your deployment
>> #####################################################################
>>
>> home_domain=example.com
>> sprout_hostname=cwsprout1
>> sprout_registration_store=192.168.56.110 #vellum
>> hs_hostname=192.168.56.109:8888 #dime
>> hs_provisioning_hostname=192.168.56.109:8889 #dime
>> ralf_hostname=
>> ralf_session_store=
>> xdms_hostname=192.168.56.108:7888 #homer
>> chronos_hostname=vellum
>> cassandra_hostname=192.168.56.110 #vellum
>>
>> # Email server configuration
>> smtp_smarthost=localhost
>> smtp_username=username
>> smtp_password=password
>> email_recovery_sender=clearwater at example.org
>>
>> # Keys
>> signup_key=secret
>> turn_workaround=secret
>> ellis_api_key=secret
>> ellis_cookie_key=secret
>>
>>
>> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
>> [sudo] password for cwellis1:
>> Monit 5.18.1 uptime: 3h 33m
>>  Service Name                     Status
>> Type
>>  node-cwellis1                    Running
>> System
>>  ntp_process                      Running
>> Process
>>  nginx_process                    Running
>> Process
>>  mysql_process                    Running
>> Process
>>  ellis_process                    Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  nginx_ping                       Status ok
>> Program
>>  nginx_uptime                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  poll_ellis                       Status ok
>> Program
>>  poll_ellis_https                 Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>> [ellis]cwellis1 at cwellis1:~$
>> [ellis]cwellis1 at cwellis1:~$
>> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
>> member 275f7358be9c56b9 is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member 37a3412cc7457712 is healthy: got healthy result from
>> http://192.168.56.109:4000
>> member 59ee11d785e88e75 is healthy: got healthy result from
>> http://192.168.56.110:4000
>> member d0eda43d23f9dc26 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member ed17a7546df2174c is healthy: got healthy result from
>> http://192.168.56.108:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
>> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
>> clientURLs=http://192.168.56.109:4000 isLeader=false
>> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
>> clientURLs=http://192.168.56.110:4000 isLeader=false
>> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=false
>> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
>> clientURLs=http://192.168.56.108:4000 isLeader=true
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>> [ellis]cwellis1 at cwellis1:~$
>>
>>
>> [bono]cwbono1 at cwbono1:~$ sudo monit summary
>> [sudo] password for cwbono1:
>> Monit 5.18.1 uptime: 23m
>>  Service Name                     Status
>> Type
>>  node-cwbono1                     Running
>> System
>>  restund_process                  Running
>> Process
>>  ntp_process                      Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  bono_process                     Running
>> Process
>>  poll_restund                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>>  poll_bono                        Status ok
>> Program
>> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
>> member 275f7358be9c56b9 is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member 37a3412cc7457712 is healthy: got healthy result from
>> http://192.168.56.109:4000
>> member 59ee11d785e88e75 is healthy: got healthy result from
>> http://192.168.56.110:4000
>> member d0eda43d23f9dc26 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member ed17a7546df2174c is healthy: got healthy result from
>> http://192.168.56.108:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
>> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
>> clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat
>> /etc/clearwater/local_config
>> local_ip=192.168.56.105
>> public_ip=192.168.56.105
>> public_hostname=cwellis1
>> etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,1
>> 92.168.56.108,192.168.56.109,192.168.56.110"
>>
>> [ellis]cwellis1 at cwellis1:~$
>> [ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
>> #####################################################################
>> # No Shared Config has been provided
>> # Replace this file with the Shared Configuration for your deployment
>> #####################################################################
>>
>> home_domain=example.com
>> sprout_hostname=cwsprout1
>> sprout_registration_store=192.168.56.110 #vellum
>> hs_hostname=192.168.56.109:8888 #dime
>> hs_provisioning_hostname=192.168.56.109:8889 #dime
>> ralf_hostname=
>> ralf_session_store=
>> xdms_hostname=192.168.56.108:7888 #homer
>> chronos_hostname=vellum
>> cassandra_hostname=192.168.56.110 #vellum
>>
>> # Email server configuration
>> smtp_smarthost=localhost
>> smtp_username=username
>> smtp_password=password
>> email_recovery_sender=clearwater at example.org
>>
>> # Keys
>> signup_key=secret
>> turn_workaround=secret
>> ellis_api_key=secret
>> ellis_cookie_key=secret
>>
>>
>> [ellis]cwellis1 at cwellis1:~$ sudo monit summary
>> [sudo] password for cwellis1:
>> Monit 5.18.1 uptime: 3h 33m
>>  Service Name                     Status
>> Type
>>  node-cwellis1                    Running
>> System
>>  ntp_process                      Running
>> Process
>>  nginx_process                    Running
>> Process
>>  mysql_process                    Running
>> Process
>>  ellis_process                    Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  nginx_ping                       Status ok
>> Program
>>  nginx_uptime                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  poll_ellis                       Status ok
>> Program
>>  poll_ellis_https                 Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>> [ellis]cwellis1 at cwellis1:~$
>> [ellis]cwellis1 at cwellis1:~$
>> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
>> member 275f7358be9c56b9 is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member 37a3412cc7457712 is healthy: got healthy result from
>> http://192.168.56.109:4000
>> member 59ee11d785e88e75 is healthy: got healthy result from
>> http://192.168.56.110:4000
>> member d0eda43d23f9dc26 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member ed17a7546df2174c is healthy: got healthy result from
>> http://192.168.56.108:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
>> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
>> clientURLs=http://192.168.56.109:4000 isLeader=false
>> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
>> clientURLs=http://192.168.56.110:4000 isLeader=false
>> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=false
>> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
>> clientURLs=http://192.168.56.108:4000 isLeader=true
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>> [ellis]cwellis1 at cwellis1:~$
>>
>>
>> [bono]cwbono1 at cwbono1:~$ sudo monit summary
>> [sudo] password for cwbono1:
>> Monit 5.18.1 uptime: 23m
>>  Service Name                     Status
>> Type
>>  node-cwbono1                     Running
>> System
>>  restund_process                  Running
>> Process
>>  ntp_process                      Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  bono_process                     Running
>> Process
>>  poll_restund                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>>  poll_bono                        Status ok
>> Program
>> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
>> member 275f7358be9c56b9 is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member 37a3412cc7457712 is healthy: got healthy result from
>> http://192.168.56.109:4000
>> member 59ee11d785e88e75 is healthy: got healthy result from
>> http://192.168.56.110:4000
>> member d0eda43d23f9dc26 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member ed17a7546df2174c is healthy: got healthy result from
>> http://192.168.56.108:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
>> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
>> clientURLs=http://192.168.56.109:4000 isLeader=false
>> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
>> clientURLs=http://192.168.56.110:4000 isLeader=false
>> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=false
>> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
>> clientURLs=http://192.168.56.108:4000 isLeader=true
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>> [bono]cwbono1 at cwbono1:~$
>>
>>
>>
>> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
>> [sudo] password for cwsprout1:
>> Monit 5.18.1 uptime: 22m
>>  Service Name                     Status
>> Type
>>  node-cwsprout1                   Running
>> System
>>  sprout_process                   Running
>> Process
>>  ntp_process                      Running
>> Process
>>  nginx_process                    Running
>> Process
>>  memento_process                  Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  sprout_uptime                    Status ok
>> Program
>>  poll_sprout_sip                  Status ok
>> Program
>>  poll_sprout_http                 Status ok
>> Program
>>  nginx_ping                       Status ok
>> Program
>>  nginx_uptime                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  memento_uptime                   Status ok
>> Program
>>  poll_memento                     Status ok
>> Program
>>  poll_memento_https               Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
>> member 275f7358be9c56b9 is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member 37a3412cc7457712 is healthy: got healthy result from
>> http://192.168.56.109:4000
>> member 59ee11d785e88e75 is healthy: got healthy result from
>> http://192.168.56.110:4000
>> member d0eda43d23f9dc26 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member ed17a7546df2174c is healthy: got healthy result from
>> http://192.168.56.108:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
>> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
>> clientURLs=http://192.168.56.109:4000 isLeader=false
>> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
>> clientURLs=http://192.168.56.110:4000 isLeader=false
>> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=false
>> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
>> clientURLs=http://192.168.56.108:4000 isLeader=true
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>>
>>
>>
>>
>> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
>> [sudo] password for cwhomer1:
>> Monit 5.18.1 uptime: 22m
>>  Service Name                     Status
>> Type
>>  node-cwhomer1                    Running
>> System
>>  ntp_process                      Running
>> Process
>>  nginx_process                    Running
>> Process
>>  homer_process                    Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  nginx_ping                       Status ok
>> Program
>>  nginx_uptime                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  poll_homer                       Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
>> member 275f7358be9c56b9 is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member 37a3412cc7457712 is healthy: got healthy result from
>> http://192.168.56.109:4000
>> member 59ee11d785e88e75 is healthy: got healthy result from
>> http://192.168.56.110:4000
>> member d0eda43d23f9dc26 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member ed17a7546df2174c is healthy: got healthy result from
>> http://192.168.56.108:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
>> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
>> clientURLs=http://192.168.56.109:4000 isLeader=false
>> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
>> clientURLs=http://192.168.56.110:4000 isLeader=false
>> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=false
>> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
>> clientURLs=http://192.168.56.108:4000 isLeader=true
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>> [homer]cwhomer1 at cwhomer1:~$
>>
>>
>> [dime]cwdime1 at cwdime1:~$ sudo monit summary
>> [sudo] password for cwdime1:
>> Monit 5.18.1 uptime: 22m
>>  Service Name                     Status
>> Type
>>  node-cwdime1                     Running
>> System
>>  snmpd_process                    Running
>> Process
>>  ntp_process                      Running
>> Process
>>  nginx_process                    Running
>> Process
>>  homestead_process                Running
>> Process
>>  homestead-prov_process           Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  nginx_ping                       Status ok
>> Program
>>  nginx_uptime                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  homestead_uptime                 Status ok
>> Program
>>  poll_homestead                   Status ok
>> Program
>>  check_cx_health                  Status ok
>> Program
>>  poll_homestead-prov              Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
>> member 275f7358be9c56b9 is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member 37a3412cc7457712 is healthy: got healthy result from
>> http://192.168.56.109:4000
>> member 59ee11d785e88e75 is healthy: got healthy result from
>> http://192.168.56.110:4000
>> member d0eda43d23f9dc26 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member ed17a7546df2174c is healthy: got healthy result from
>> http://192.168.56.108:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
>> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
>> clientURLs=http://192.168.56.109:4000 isLeader=false
>> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
>> clientURLs=http://192.168.56.110:4000 isLeader=false
>> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=false
>> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
>> clientURLs=http://192.168.56.108:4000 isLeader=true
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>> [dime]cwdime1 at cwdime1:~$
>>
>>
>>
>> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
>> [sudo] password for cwvellum1:
>> Monit 5.18.1 uptime: 22m
>>  Service Name                     Status
>> Type
>>  node-cwvellum1                   Running
>> System
>>  snmpd_process                    Running
>> Process
>>  ntp_process                      Running
>> Process
>>  memcached_process                Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  cassandra_process                Running
>> Process
>>  chronos_process                  Running
>> Process
>>  astaire_process                  Running
>> Process
>>  monit_uptime                     Status ok
>> Program
>>  memcached_uptime                 Status ok
>> Program
>>  poll_memcached                   Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>>  cassandra_uptime                 Status ok
>> Program
>>  poll_cassandra                   Status ok
>> Program
>>  poll_cqlsh                       Status ok
>> Program
>>  chronos_uptime                   Status ok
>> Program
>>  poll_chronos                     Status ok
>> Program
>>  astaire_uptime                   Status ok
>> Program
>> [vellum]cwvellum1 at cwvellum1:~$
>> [vellum]cwvellum1 at cwvellum1:~$
>> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
>> member 275f7358be9c56b9 is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member 37a3412cc7457712 is healthy: got healthy result from
>> http://192.168.56.109:4000
>> member 59ee11d785e88e75 is healthy: got healthy result from
>> http://192.168.56.110:4000
>> member d0eda43d23f9dc26 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member ed17a7546df2174c is healthy: got healthy result from
>> http://192.168.56.108:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
>> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
>> clientURLs=http://192.168.56.109:4000 isLeader=false
>> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
>> clientURLs=http://192.168.56.110:4000 isLeader=false
>> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=false
>> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
>> clientURLs=http://192.168.56.108:4000 isLeader=true
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>> [vellum]cwvellum1 at cwvellum1:~$
>> 192.168.56.109:4000 isLeader=false
>> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
>> clientURLs=http://192.168.56.110:4000 isLeader=false
>> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=false
>> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
>> clientURLs=http://192.168.56.108:4000 isLeader=true
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>> [bono]cwbono1 at cwbono1:~$
>>
>>
>>
>> [sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
>> [sudo] password for cwsprout1:
>> Monit 5.18.1 uptime: 22m
>>  Service Name                     Status
>> Type
>>  node-cwsprout1                   Running
>> System
>>  sprout_process                   Running
>> Process
>>  ntp_process                      Running
>> Process
>>  nginx_process                    Running
>> Process
>>  memento_process                  Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  sprout_uptime                    Status ok
>> Program
>>  poll_sprout_sip                  Status ok
>> Program
>>  poll_sprout_http                 Status ok
>> Program
>>  nginx_ping                       Status ok
>> Program
>>  nginx_uptime                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  memento_uptime                   Status ok
>> Program
>>  poll_memento                     Status ok
>> Program
>>  poll_memento_https               Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
>> member 275f7358be9c56b9 is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member 37a3412cc7457712 is healthy: got healthy result from
>> http://192.168.56.109:4000
>> member 59ee11d785e88e75 is healthy: got healthy result from
>> http://192.168.56.110:4000
>> member d0eda43d23f9dc26 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member ed17a7546df2174c is healthy: got healthy result from
>> http://192.168.56.108:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
>> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
>> clientURLs=http://192.168.56.109:4000 isLeader=false
>> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
>> clientURLs=http://192.168.56.110:4000 isLeader=false
>> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=false
>> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
>> clientURLs=http://192.168.56.108:4000 isLeader=true
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>>
>>
>>
>>
>> [homer]cwhomer1 at cwhomer1:~$ sudo monit summary
>> [sudo] password for cwhomer1:
>> Monit 5.18.1 uptime: 22m
>>  Service Name                     Status
>> Type
>>  node-cwhomer1                    Running
>> System
>>  ntp_process                      Running
>> Process
>>  nginx_process                    Running
>> Process
>>  homer_process                    Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  nginx_ping                       Status ok
>> Program
>>  nginx_uptime                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  poll_homer                       Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
>> member 275f7358be9c56b9 is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member 37a3412cc7457712 is healthy: got healthy result from
>> http://192.168.56.109:4000
>> member 59ee11d785e88e75 is healthy: got healthy result from
>> http://192.168.56.110:4000
>> member d0eda43d23f9dc26 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member ed17a7546df2174c is healthy: got healthy result from
>> http://192.168.56.108:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
>> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
>> clientURLs=http://192.168.56.109:4000 isLeader=false
>> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
>> clientURLs=http://192.168.56.110:4000 isLeader=false
>> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=false
>> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
>> clientURLs=http://192.168.56.108:4000 isLeader=true
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>> [homer]cwhomer1 at cwhomer1:~$
>>
>>
>> [dime]cwdime1 at cwdime1:~$ sudo monit summary
>> [sudo] password for cwdime1:
>> Monit 5.18.1 uptime: 22m
>>  Service Name                     Status
>> Type
>>  node-cwdime1                     Running
>> System
>>  snmpd_process                    Running
>> Process
>>  ntp_process                      Running
>> Process
>>  nginx_process                    Running
>> Process
>>  homestead_process                Running
>> Process
>>  homestead-prov_process           Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  nginx_ping                       Status ok
>> Program
>>  nginx_uptime                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  homestead_uptime                 Status ok
>> Program
>>  poll_homestead                   Status ok
>> Program
>>  check_cx_health                  Status ok
>> Program
>>  poll_homestead-prov              Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
>> member 275f7358be9c56b9 is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member 37a3412cc7457712 is healthy: got healthy result from
>> http://192.168.56.109:4000
>> member 59ee11d785e88e75 is healthy: got healthy result from
>> http://192.168.56.110:4000
>> member d0eda43d23f9dc26 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member ed17a7546df2174c is healthy: got healthy result from
>> http://192.168.56.108:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
>> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
>> clientURLs=http://192.168.56.109:4000 isLeader=false
>> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
>> clientURLs=http://192.168.56.110:4000 isLeader=false
>> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=false
>> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
>> clientURLs=http://192.168.56.108:4000 isLeader=true
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>> [dime]cwdime1 at cwdime1:~$
>>
>>
>>
>> [vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
>> [sudo] password for cwvellum1:
>> Monit 5.18.1 uptime: 22m
>>  Service Name                     Status
>> Type
>>  node-cwvellum1                   Running
>> System
>>  snmpd_process                    Running
>> Process
>>  ntp_process                      Running
>> Process
>>  memcached_process                Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  cassandra_process                Running
>> Process
>>  chronos_process                  Running
>> Process
>>  astaire_process                  Running
>> Process
>>  monit_uptime                     Status ok
>> Program
>>  memcached_uptime                 Status ok
>> Program
>>  poll_memcached                   Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>>  cassandra_uptime                 Status ok
>> Program
>>  poll_cassandra                   Status ok
>> Program
>>  poll_cqlsh                       Status ok
>> Program
>>  chronos_uptime                   Status ok
>> Program
>>  poll_chronos                     Status ok
>> Program
>>  astaire_uptime                   Status ok
>> Program
>> [vellum]cwvellum1 at cwvellum1:~$
>> [vellum]cwvellum1 at cwvellum1:~$
>> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
>> member 275f7358be9c56b9 is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member 37a3412cc7457712 is healthy: got healthy result from
>> http://192.168.56.109:4000
>> member 59ee11d785e88e75 is healthy: got healthy result from
>> http://192.168.56.110:4000
>> member d0eda43d23f9dc26 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member ed17a7546df2174c is healthy: got healthy result from
>> http://192.168.56.108:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
>> 275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> 37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
>> clientURLs=http://192.168.56.109:4000 isLeader=false
>> 59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
>> clientURLs=http://192.168.56.110:4000 isLeader=false
>> d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=false
>> ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
>> clientURLs=http://192.168.56.108:4000 isLeader=true
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>> [vellum]cwvellum1 at cwvellum1:~$
>>
>>
>> ===================================================
>>
>> Please let me know what is wrong with the setup?
>>
>> Thanks
>>
>> Hrishikesh
>>
>>
>>
>>
>>
>>
>> _______________________________________________
>> Clearwater mailing list
>> Clearwater at lists.projectclearwater.org
>> http://lists.projectclearwater.org/mailman/listinfo/
>> clearwater_lists.projectclearwater.org
>>
>>
>>
>>
>> _______________________________________________
>> Clearwater mailing list
>> Clearwater at lists.projectclearwater.org
>> http://lists.projectclearwater.org/mailman/listinfo/
>> clearwater_lists.projectclearwater.org
>>
>>
>>
>>
>> _______________________________________________
>> Clearwater mailing list
>> Clearwater at lists.projectclearwater.org
>> http://lists.projectclearwater.org/mailman/listinfo/
>> clearwater_lists.projectclearwater.org
>>
>>
>>
>>
>> _______________________________________________
>> Clearwater mailing list
>> Clearwater at lists.projectclearwater.org
>> http://lists.projectclearwater.org/mailman/listinfo/
>> clearwater_lists.projectclearwater.org
>>
>>
>>
>> _______________________________________________
>> Clearwater mailing list
>> Clearwater at lists.projectclearwater.org
>> http://lists.projectclearwater.org/mailman/listinfo/
>> clearwater_lists.projectclearwater.org
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/3fc00708/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: dime-clearwater-etcd.log.1.bz2
Type: application/x-bzip2
Size: 15044258 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/3fc00708/attachment.bz2>

From abdulbasit.ta at gmail.com  Wed Aug 30 09:28:05 2017
From: abdulbasit.ta at gmail.com (Abdul Basit Alvi)
Date: Wed, 30 Aug 2017 18:28:05 +0500
Subject: [Project Clearwater] Cassandra and etcd clustering problem
In-Reply-To: <CAEAO5tZ3Bdm+b=BHDkag7dYztDeqJJyoatVMAuwFLrk+x9XA9w@mail.gmail.com>
References: <CAEAO5tZ3Bdm+b=BHDkag7dYztDeqJJyoatVMAuwFLrk+x9XA9w@mail.gmail.com>
Message-ID: <CAEAO5tbL=UssM_fo2qEPX1Hwdc-QPPCPQQdPXeCQv2YvMDZ-EQ@mail.gmail.com>

Ok after stopping monit for Cassandra and restarting the service manually
Cassandra has started. From the system logs I found the log vellum-0
cassandra: Found leaked cassandra 2934 (correct is 3812) - killing 2934.
Cassndra is being killed. Now that i have stoped monit and restarted it
manually it seems to be running fine. However homestead still cannot
connect to Cassandra and I cannot run cqlsh either.

>From netstat I can see cassandra listening on ports 9160 and 9042. However
it is listening tcp6 as shown:

root at 0:/home/ubuntu# ip netns exec signaling netstat -tulnap
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address
State       PID/Program name
tcp        0      0 192.168.1.7:11211       0.0.0.0:*
LISTEN      12491/memcached
tcp        0      0 127.0.0.1:7253          0.0.0.0:*
LISTEN      24241/chronos
tcp        0      0 192.168.1.7:7253        0.0.0.0:*
LISTEN      24241/chronos
tcp        0      0 192.168.1.7:37240       192.168.1.7:11211
TIME_WAIT   -
tcp        0      0 192.168.1.7:37221       192.168.1.7:11211
TIME_WAIT   -
tcp        0      0 192.168.1.7:37279       192.168.1.7:11211
TIME_WAIT   -
tcp        0      0 192.168.1.7:37251       192.168.1.7:11211
TIME_WAIT   -
tcp        0      0 127.0.0.1:36932         127.0.0.1:7253
TIME_WAIT   -
tcp        0      0 192.168.1.7:37256       192.168.1.7:11211
TIME_WAIT   -
tcp        0      0 192.168.1.7:37286       192.168.1.7:11211
TIME_WAIT   -
tcp        0      0 192.168.1.7:37261       192.168.1.7:11211
TIME_WAIT   -
*tcp6       0      0 :::9160                 :::*
LISTEN      29208/java      *
tcp6       0      0 :::11311                :::*
LISTEN      2114/astaire
*tcp6       0      0 :::9042                 :::*
LISTEN      29208/java      *
tcp6       0      0 192.168.1.7:7000        :::*
LISTEN      29208/java
tcp6       0      0 127.0.0.1:39805         :::*
LISTEN      29208/java
tcp6       0      0 127.0.0.1:7199          :::*
LISTEN      29208/java
tcp6       0      0 127.0.0.1:54410         127.0.0.1:39805
ESTABLISHED 29208/java
tcp6       0      0 127.0.0.1:39805         127.0.0.1:54410
ESTABLISHED 29208/java

Running cqlsh I encounter the following error:
root at 0:/home/ubuntu# ip netns exec signaling cqlsh ::1 9042
Connection error: ('Unable to connect to any servers', {'::1':
ReadTimeout(u'code=1200 [Coordinator node timed out waiting for replica
nodes\' responses] message="Operation timed out - received only 0
responses." info={\'received_responses\': 0, \'required_responses\': 1,
\'consistency\': \'ONE\'}',)})

Does cassandra listen on tcp6 natively?

Also in the dime node homestead error logs show no more errors however in
the homestead log I can see the following logs?

30-08-2017 12:45:58.760 UTC Status alarm.cpp:244: Reraising all alarms with
a known state
30-08-2017 12:45:58.763 UTC Status a_record_resolver.cpp:29: Created
ARecordResolver
30-08-2017 12:45:58.763 UTC Status a_record_resolver.cpp:29: Created
ARecordResolver
30-08-2017 12:45:58.766 UTC Status cassandra_store.cpp:266: Configuring
store connection
30-08-2017 12:45:58.766 UTC Status cassandra_store.cpp:267:   Hostname:
vellum.test.com
30-08-2017 12:45:58.766 UTC Status cassandra_store.cpp:268:   Port:
9160
30-08-2017 12:45:58.766 UTC Status cassandra_store.cpp:296: Configuring
store worker pool
30-08-2017 12:45:58.766 UTC Status cassandra_store.cpp:297:   Threads:   10
30-08-2017 12:45:58.766 UTC Status cassandra_store.cpp:298:   Max Queue: 0
30-08-2017 12:45:58.895 UTC Error main.cpp:752: Failed to initialize the
Cassandra cache with error code 1.
30-08-2017 12:45:58.896 UTC Status main.cpp:753: Homestead is shutting down

In homestead netstat logs I cannot see any logs of homestead trying to
connect to ports 9160
Proto Recv-Q Send-Q Local Address           Foreign Address
State
tcp        0      0 localhost:49146         localhost:8000
TIME_WAIT
tcp        0      0 192.168.0.4:45972       192.168.0.6:4000
ESTABLISHED
tcp       42      0 192.168.0.4:50479       192.168.0.7:2380
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.5:34559
ESTABLISHED
tcp        0      0 192.168.0.4:36437       192.168.0.4:4000
ESTABLISHED
tcp        0      0 192.168.0.4:39848       192.168.0.4:4000
ESTABLISHED
tcp        0      0 192.168.0.4:39863       192.168.0.4:4000
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.7:40964
TIME_WAIT
tcp        0      0 192.168.0.4:40269       192.168.0.4:4000
ESTABLISHED
tcp        0    103 192.168.0.4:47648       192.168.0.6:4000
ESTABLISHED
tcp        0      0 192.168.0.4:45663       192.168.0.6:4000
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.7:37374
ESTABLISHED
tcp        0      0 192.168.0.4:42474       192.168.0.6:4000
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.6:57924
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.5:35015
TIME_WAIT
tcp        0      0 192.168.0.4:2380        192.168.0.7:40812
TIME_WAIT
tcp        0      0 192.168.0.4:2380        192.168.0.6:53967
ESTABLISHED
tcp        0      0 192.168.0.4:40268       192.168.0.4:4000
TIME_WAIT
tcp        0      0 192.168.0.4:53551       192.168.0.5:2380
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.6:57810
TIME_WAIT
tcp        0      0 192.168.0.4:37873       192.168.0.4:4000
ESTABLISHED
tcp        0    103 192.168.0.4:46510       192.168.0.6:4000
ESTABLISHED
tcp        0      0 192.168.0.4:39859       192.168.0.4:4000
TIME_WAIT
tcp        0      0 192.168.0.4:2380        192.168.0.6:57904
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.6:57890
TIME_WAIT
tcp        0      0 192.168.0.4:60183       192.168.0.7:2380
ESTABLISHED
tcp        0      0 192.168.0.4:38186       192.168.0.4:4000
ESTABLISHED
tcp        0      0 192.168.0.4:37317       192.168.0.4:4000
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.5:58490
ESTABLISHED
tcp        0      0 192.168.0.4:44222       192.168.0.6:4000
ESTABLISHED
tcp        0      0 192.168.0.4:32981       192.168.0.7:2380
ESTABLISHED
tcp        0      0 192.168.0.4:38725       192.168.0.4:4000
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.7:40407
TIME_WAIT
tcp        0      0 192.168.0.4:35533       192.168.0.4:4000
ESTABLISHED
tcp        0      0 192.168.0.4:34686       192.168.0.4:4000
ESTABLISHED
tcp        0      0 192.168.0.4:43318       192.168.0.6:4000
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.6:57784
TIME_WAIT
tcp        0      0 192.168.0.4:54539       192.168.0.5:2380
ESTABLISHED
tcp        0      0 192.168.0.4:40293       192.168.0.4:4000
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.7:41168
ESTABLISHED
tcp        0      0 192.168.0.4:48057       192.168.0.6:2380
ESTABLISHED
tcp        0      0 192.168.0.4:47629       192.168.0.6:2380
ESTABLISHED
tcp        0      0 192.168.0.4:47663       192.168.0.6:2380
ESTABLISHED
tcp        0      0 192.168.0.4:45102       192.168.0.6:4000
ESTABLISHED
tcp        0      0 192.168.0.4:45341       192.168.0.5:2380
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.5:58487
ESTABLISHED
tcp        0    116 192.168.0.4:ssh         10.1.10.112:49603
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.7:38517
ESTABLISHED
tcp        0      0 192.168.0.4:2380        192.168.0.7:38517
ESTABLISHED
tcp6       0      0 192.168.0.4:4000        192.168.0.4:38725
ESTABLISHED
tcp6       0      0 ip6-localhost:50722     ip6-localhost:4000
ESTABLISHED
tcp6       0      0 192.168.0.4:4000        192.168.0.4:39848
ESTABLISHED
tcp6       0      0 ip6-localhost:4000      ip6-localhost:50724
ESTABLISHED
tcp6       0      0 ip6-localhost:50721     ip6-localhost:4000
ESTABLISHED
tcp6       0      0 ip6-localhost:50720     ip6-localhost:4000
ESTABLISHED
tcp6       0      0 192.168.0.4:4000        192.168.0.4:37873
ESTABLISHED
tcp6       0      0 192.168.0.4:4000        192.168.0.4:34686
ESTABLISHED
tcp6       0      0 ip6-localhost:4000      ip6-localhost:50717
ESTABLISHED
tcp6       0      0 192.168.0.4:4000        192.168.0.4:35533
ESTABLISHED
tcp6       0      0 ip6-localhost:4000      ip6-localhost:50721
ESTABLISHED
tcp6       0      0 192.168.0.4:4000        192.168.0.4:38186
ESTABLISHED
tcp6       0      0 ip6-localhost:4000      ip6-localhost:50723
ESTABLISHED
tcp6       0      0 ip6-localhost:50723     ip6-localhost:4000
ESTABLISHED
tcp6       0      0 ip6-localhost:50724     ip6-localhost:4000
ESTABLISHED
tcp6       0      0 ip6-localhost:50717     ip6-localhost:4000
ESTABLISHED
tcp6       0      0 ip6-localhost:4000      ip6-localhost:50722
ESTABLISHED
tcp6       0      0 192.168.0.4:4000        192.168.0.4:40294
ESTABLISHED
tcp6       0      0 192.168.0.4:4000        192.168.0.4:36437
ESTABLISHED
tcp6       0      0 192.168.0.4:4000        192.168.0.4:39863
ESTABLISHED
tcp6       0      0 192.168.0.4:4000        192.168.0.4:40293
ESTABLISHED
tcp6       0      0 192.168.0.4:4000        192.168.0.4:37317
ESTABLISHED
tcp6       0      0 ip6-localhost:4000      ip6-localhost:50720
ESTABLISHED


Kindly please guide me on what to do.

Regards,

Abdul Basit Alvi




On Sun, Aug 27, 2017 at 10:24 AM, Abdul Basit Alvi <abdulbasit.ta at gmail.com>
wrote:

> Hi,
>
> I have been trying to make the IMS work via manual install. I have
> followed all the instructions to the dot and have tried starting from
> scratch multiple times, but somehow I cant figure out why Cassandra and
> etcd clustering are not working properly.
>
> In the Dime node homestead process is not running, this I know is because
> it cant connect to the vellum node cassandra via the thrift port 9160.
>
> Next in the Vellum node the cassandra process is running but not working
> at all. I have attached the system log files as well as cassandra.yaml and
> the cassandra-env.sh file. For test purposes I have allowed all incomming
> TCP/UDP traffic to and from all nodes.
>
> Can you kindly look at the logs and outputs and point out what am I doing
> wrong?
>
> *[Monit summary of dime]*
> root at dime-0:/home/ubuntu# monit summary
> Monit 5.18.1 uptime: 6h 27m
> * Service Name                     Status                      Type   *
>
>  node-dime-0.test.com             Running
> System
>  snmpd_process                    Running
> Process
>  ralf_process                     Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Does not exist
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  ralf_uptime                      Status ok
> Program
>  poll_ralf                        Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Wait parent
> Program
>  poll_homestead                   Wait parent
> Program
>  check_cx_health                  Wait parent
> Program
>  poll_homestead-prov              Status failed
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status failed
> Program
>  poll_etcd                        Status ok                   Program
> [Dime Local config]
>
> *[etcd cluster health dime]*
> root at dime-0:/home/ubuntu# clearwater-etcdctl cluster-health
> member 5cd7042180fbb2a is unhealthy: got unhealthy result from
> http://192.168.0.6:4000
> failed to check the health of member 208dd0fbcefb149c on
> http://192.168.0.8:4000: Get http://192.168.0.8:4000/health: dial tcp
> 192.168.0.8:4000: getsockopt: connection refused
> member 208dd0fbcefb149c is unreachable: [http://192.168.0.8:4000] are all
> unreachable
> member 2457d2c8a20fc738 is unhealthy: got unhealthy result from
> http://192.168.0.5:4000
> member 48fa49be2b2ae2c2 is unhealthy: got unhealthy result from
> http://192.168.0.3:4000
> failed to check the health of member a48134f48b185ad9 on
> http://192.168.0.7:4000: Get http://192.168.0.7:4000/health: dial tcp
> 192.168.0.7:4000: getsockopt: connection refused
> member a48134f48b185ad9 is unreachable: [http://192.168.0.7:4000] are all
> unreachable
> member e7db4eebbdb94a11 is unhealthy: got unhealthy result from
> http://192.168.0.4:4000
> cluster is unhealthy
>
> *[local config file dime]*
> root at dime-0:/home/ubuntu# cat /etc/clearwater/local_config
> local_ip=192.168.0.4
> public_ip=10.1.10.192
> public_hostname=dime-0.test.com
> etcd_cluster=192.168.0.3,192.168.0.4,192.168.0.5,192.168.0.
> 6,192.168.0.7,192.168.0.8
>
> *[shared config file dime]*
> root at vellum-0:/home/ubuntu# cat /etc/clearwater/shared_config
> # Deployment definitions
> home_domain=test.com
> sprout_hostname=sprout.test.com
> hs_hostname=hs.test.com:8888
> hs_provisioning_hostname=hs-prov.test.com:8889
> dime_hostname=dime.test.com:10888
> xdms_hostname=homer.test.com:7888
> sprout_impi_store=vellum.test.com
> sprout_registration_store=vellum.test.com
> cassandra_hostname=vellum.test.com
> chronos_hostname=vellum.test.com
> dime_session_store=vellum.test.com
>
> upstream_port=0
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
> *[Error Log Homestead]*
> Thrift: Sun Aug 27 05:04:01 2017 TSocket::open() error on socket (after
> THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
> Thrift: Sun Aug 27 05:04:02 2017 TSocket::open() error on socket (after
> THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
> Thrift: Sun Aug 27 05:04:44 2017 TSocket::open() error on socket (after
> THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
> Thrift: Sun Aug 27 05:04:44 2017 TSocket::open() error on socket (after
> THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
> Thrift: Sun Aug 27 05:05:08 2017 TSocket::open() error on socket (after
> THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
> Thrift: Sun Aug 27 05:05:08 2017 TSocket::open() error on socket (after
> THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
> Thrift: Sun Aug 27 05:05:30 2017 TSocket::open() error on socket (after
> THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
> Thrift: Sun Aug 27 05:05:30 2017 TSocket::open() error on socket (after
> THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
> Thrift: Sun Aug 27 05:05:38 2017 TSocket::open() error on socket (after
> THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
> Thrift: Sun Aug 27 05:05:38 2017 TSocket::open() error on socket (after
> THRIFT_POLL) <Host: 192.168.0.8 Port: 9160>Connection refused
>
> *[Running cqlsh on vellum]*
> root at vellum-0:/home/ubuntu# cqlsh
> Connection error: ('Unable to connect to any servers', {'127.0.0.1':
> error(111, "Tried connecting to [('127.0.0.1', 9042)]. Last error:
> Connection refused")}
>
> *[Monit summary of vellum]*
> root at vellum-0:/home/ubuntu# monit summary
> Monit 5.18.1 uptime: 6h 39m
>  Service Name                     Status
> Type
>  node-vellum-0.test.com           Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Execution failed | Does...
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Wait parent
> Program
>  poll_etcd_cluster                Wait parent
> Program
>  poll_etcd                        Wait parent
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status failed
> Program
>  astaire_uptime                   Status ok
> Program
>
> *[local config file vellum]*
> root at vellum-0:/home/ubuntu# cat /etc/clearwater/local_config
> local_ip=192.168.0.8
> public_ip=10.1.10.204
> public_hostname=vellum-0.test.com
> etcd_cluster=192.168.0.3,192.168.0.4,192.168.0.5,192.168.0.
> 6,192.168.0.7,192.168.0.8
>
>
> *[shared config file vellum]*
> root at vellum-0:/home/ubuntu# cat /etc/clearwater/shared_config
> # Deployment definitions
> home_domain=test.com
> sprout_hostname=sprout.test.com
> hs_hostname=hs.test.com:8888
> hs_provisioning_hostname=hs-prov.test.com:8889
> dime_hostname=dime.test.com:10888
> xdms_hostname=homer.test.com:7888
> sprout_impi_store=vellum.test.com
> sprout_registration_store=vellum.test.com
> cassandra_hostname=vellum.test.com
> chronos_hostname=vellum.test.com
> dime_session_store=vellum.test.com
>
> upstream_port=0
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
> *[netstat on vellum]*
> root at vellum-0:/home/ubuntu# netstat -tulnap
> Active Internet connections (servers and established)
> Proto Recv-Q Send-Q Local Address           Foreign Address
> State       PID/Program name
> tcp        0      0 192.168.0.8:11211       0.0.0.0:*
> LISTEN      27005/memcached
> tcp        0      0 127.0.0.1:7253          0.0.0.0:*
> LISTEN      27610/chronos
> tcp        0      0 255.255.255.255:7253    0.0.0.0:*
> LISTEN      27610/chronos
> tcp        0      0 127.0.0.1:53            0.0.0.0:*
> LISTEN      7718/dnsmasq
> tcp        0      0 0.0.0.0:22              0.0.0.0:*
> LISTEN      1189/sshd
> tcp        0      0 127.0.0.1:2812          0.0.0.0:*
> LISTEN      7833/monit
> tcp        0      0 192.168.0.8:44035       192.168.0.8:11211
> TIME_WAIT   -
> tcp        0      0 127.0.0.1:54026         127.0.0.1:7253
> TIME_WAIT   -
> tcp        0      0 192.168.0.8:44064       192.168.0.8:11211
> TIME_WAIT   -
> tcp        0      0 192.168.0.8:44081       192.168.0.8:11211
> TIME_WAIT   -
> tcp        0      0 192.168.0.8:44053       192.168.0.8:11211
> TIME_WAIT   -
> tcp        0      0 192.168.0.8:44054       192.168.0.8:11211
> TIME_WAIT   -
> tcp        0      0 192.168.0.8:44048       192.168.0.8:11211
> TIME_WAIT   -
> tcp        0      0 192.168.0.8:22          10.1.10.112:51998
> ESTABLISHED 26454/sshd: ubuntu
> tcp        0    268 192.168.0.8:22          10.1.10.112:51933
> ESTABLISHED 22779/sshd: ubuntu
> tcp6       0      0 :::11311                :::*
> LISTEN      26657/astaire
> tcp6       0      0 ::1:53                  :::*
> LISTEN      7718/dnsmasq
> tcp6       0      0 :::22                   :::*
> LISTEN      1189/sshd
> udp        0      0 127.0.0.1:53            0.0.0.0:*
> 7718/dnsmasq
> udp        0      0 0.0.0.0:68              0.0.0.0:*
> 601/dhclient
> udp        0      0 192.168.0.8:123         0.0.0.0:*
> 7362/ntpd
> udp        0      0 127.0.0.1:123           0.0.0.0:*
> 7362/ntpd
> udp        0      0 0.0.0.0:123             0.0.0.0:*
> 7362/ntpd
> udp        0      0 0.0.0.0:161             0.0.0.0:*
> 7472/snmpd
> udp        0      0 0.0.0.0:55423           0.0.0.0:*
> 601/dhclient
> udp6       0      0 :::23767                :::*
> 601/dhclient
> udp6       0      0 ::1:53                  :::*
> 7718/dnsmasq
> udp6       0      0 ::1:123                 :::*
> 7362/ntpd
> udp6       0      0 fe80::f816:3eff:fe3:123 :::*
> 7362/ntpd
> udp6       0      0 :::123                  :::*
> 7362/ntpd
> udp6       0      0 :::161                  :::*
> 7472/snmpd
>
> *[Ping results]*
> root at vellum-0:/home/ubuntu# ping hs-prov.test.com
> PING hs-prov.test.com (192.168.0.4) 56(84) bytes of data.
> 64 bytes from 192.168.0.4: icmp_seq=1 ttl=64 time=10.3 ms
> 64 bytes from 192.168.0.4: icmp_seq=2 ttl=64 time=12.1 ms
> 64 bytes from 192.168.0.4: icmp_seq=3 ttl=64 time=9.06 ms
> ^C
> --- hs-prov.test.com ping statistics ---
> 3 packets transmitted, 3 received, 0% packet loss, time 2032ms
> rtt min/avg/max/mdev = 9.063/10.529/12.151/1.270 ms
> root at vellum-0:/home/ubuntu#
> root at vellum-0:/home/ubuntu# ping hs.test.com
> PING hs.test.com (192.168.0.4) 56(84) bytes of data.
> 64 bytes from 192.168.0.4: icmp_seq=1 ttl=64 time=21.3 ms
> 64 bytes from 192.168.0.4: icmp_seq=2 ttl=64 time=4.58 ms
> 64 bytes from 192.168.0.4: icmp_seq=3 ttl=64 time=20.6 ms
> ^C
> --- hs.test.com ping statistics ---
> 3 packets transmitted, 3 received, 0% packet loss, time 2070ms
> rtt min/avg/max/mdev = 4.584/15.523/21.363/7.741 ms
> root at vellum-0:/home/ubuntu# ping dime-0.test.com
> PING dime-0.test.com (10.1.10.192) 56(84) bytes of data.
> 64 bytes from 10.1.10.192: icmp_seq=1 ttl=63 time=25.9 ms
> 64 bytes from 10.1.10.192: icmp_seq=2 ttl=63 time=58.9 ms
> ^C
> --- dime-0.test.com ping statistics ---
> 2 packets transmitted, 2 received, 0% packet loss, time 1029ms
> rtt min/avg/max/mdev = 25.954/42.459/58.964/16.505 ms
>
> root at bono-0:/home/ubuntu# ping vellum-0.test.com
> PING vellum-0.test.com (10.1.10.204) 56(84) bytes of data.
> 64 bytes from 10.1.10.204: icmp_seq=1 ttl=63 time=36.7 ms
> 64 bytes from 10.1.10.204: icmp_seq=2 ttl=63 time=25.2 ms
> ^C
> --- vellum-0.test.com ping statistics ---
> 2 packets transmitted, 2 received, 0% packet loss, time 1002ms
> rtt min/avg/max/mdev = 25.240/30.985/36.730/5.745 ms
> root at bono-0:/home/ubuntu# ping vellum.test.com
> PING vellum.test.com (192.168.0.8) 56(84) bytes of data.
> 64 bytes from 192.168.0.8: icmp_seq=1 ttl=64 time=51.4 ms
> 64 bytes from 192.168.0.8: icmp_seq=2 ttl=64 time=3.93 ms
> 64 bytes from 192.168.0.8: icmp_seq=3 ttl=64 time=4.22 ms
>
> Regards,
>
> Abdul Basit Alvi
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/81bdd9d9/attachment.html>

From basit.alvi at telenor.com.pk  Wed Aug 30 09:43:11 2017
From: basit.alvi at telenor.com.pk (Abdul Basit Alvi)
Date: Wed, 30 Aug 2017 18:43:11 +0500
Subject: [Project Clearwater] Homestead cant connect to cassadra
Message-ID: <13EC7EF33A933C42A118CB8FB501FBC1C082325BFB@ISB-EXMB-CLUS.telenor.com.pk>

At first Cassandra was not working. After some debugging and stopping monit for Cassandra and restarting the service manually Cassandra has started. From the system logs I found the log vellum-0 cassandra: Found leaked cassandra 2934 (correct is 3812) - killing 2934. Cassndra is being killed. Now that i have stoped monit and restarted it manually it seems to be running fine. However homestead still cannot connect to Cassandra and I cannot run cqlsh either.
>From netstat I can see cassandra listening on ports 9160 and 9042. However it is listening tcp6 as shown:

root at 0:/home/ubuntu# ip netns exec signaling netstat -tulnap
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 192.168.1.7:11211<http://192.168.1.7:11211>       0.0.0.0:*               LISTEN      12491/memcached
tcp        0      0 127.0.0.1:7253<http://127.0.0.1:7253>          0.0.0.0:*               LISTEN      24241/chronos
tcp        0      0 192.168.1.7:7253<http://192.168.1.7:7253>        0.0.0.0:*               LISTEN      24241/chronos
tcp        0      0 192.168.1.7:37240<http://192.168.1.7:37240>       192.168.1.7:11211<http://192.168.1.7:11211>       TIME_WAIT   -
tcp        0      0 192.168.1.7:37221<http://192.168.1.7:37221>       192.168.1.7:11211<http://192.168.1.7:11211>       TIME_WAIT   -
tcp        0      0 192.168.1.7:37279<http://192.168.1.7:37279>       192.168.1.7:11211<http://192.168.1.7:11211>       TIME_WAIT   -
tcp        0      0 192.168.1.7:37251<http://192.168.1.7:37251>       192.168.1.7:11211<http://192.168.1.7:11211>       TIME_WAIT   -
tcp        0      0 127.0.0.1:36932<http://127.0.0.1:36932>         127.0.0.1:7253<http://127.0.0.1:7253>          TIME_WAIT   -
tcp        0      0 192.168.1.7:37256<http://192.168.1.7:37256>       192.168.1.7:11211<http://192.168.1.7:11211>       TIME_WAIT   -
tcp        0      0 192.168.1.7:37286<http://192.168.1.7:37286>       192.168.1.7:11211<http://192.168.1.7:11211>       TIME_WAIT   -
tcp        0      0 192.168.1.7:37261<http://192.168.1.7:37261>       192.168.1.7:11211<http://192.168.1.7:11211>       TIME_WAIT   -
tcp6       0      0 :::9160                 :::*                    LISTEN      29208/java
tcp6       0      0 :::11311                :::*                    LISTEN      2114/astaire
tcp6       0      0 :::9042                 :::*                    LISTEN      29208/java
tcp6       0      0 192.168.1.7:7000<http://192.168.1.7:7000>        :::*                    LISTEN      29208/java
tcp6       0      0 127.0.0.1:39805<http://127.0.0.1:39805>         :::*                    LISTEN      29208/java
tcp6       0      0 127.0.0.1:7199<http://127.0.0.1:7199>          :::*                    LISTEN      29208/java
tcp6       0      0 127.0.0.1:54410<http://127.0.0.1:54410>         127.0.0.1:39805<http://127.0.0.1:39805>         ESTABLISHED 29208/java
tcp6       0      0 127.0.0.1:39805<http://127.0.0.1:39805>         127.0.0.1:54410<http://127.0.0.1:54410>         ESTABLISHED 29208/java

Running cqlsh I encounter the following error:
root at 0:/home/ubuntu# ip netns exec signaling cqlsh ::1 9042
Connection error: ('Unable to connect to any servers', {'::1': ReadTimeout(u'code=1200 [Coordinator node timed out waiting for replica nodes\' responses] message="Operation timed out - received only 0 responses." info={\'received_responses\': 0, \'required_responses\': 1, \'consistency\': \'ONE\'}',)})

Does cassandra listen on tcp6 natively?
Also in the dime node homestead error logs show no more errors however in the homestead log I can see the following logs?

30-08-2017 12:45:58.760 UTC Status alarm.cpp:244: Reraising all alarms with a known state
30-08-2017 12:45:58.763 UTC Status a_record_resolver.cpp:29: Created ARecordResolver
30-08-2017 12:45:58.763 UTC Status a_record_resolver.cpp:29: Created ARecordResolver
30-08-2017 12:45:58.766 UTC Status cassandra_store.cpp:266: Configuring store connection
30-08-2017 12:45:58.766 UTC Status cassandra_store.cpp:267:   Hostname:  vellum.test.com<http://vellum.test.com>
30-08-2017 12:45:58.766 UTC Status cassandra_store.cpp:268:   Port:      9160
30-08-2017 12:45:58.766 UTC Status cassandra_store.cpp:296: Configuring store worker pool
30-08-2017 12:45:58.766 UTC Status cassandra_store.cpp:297:   Threads:   10
30-08-2017 12:45:58.766 UTC Status cassandra_store.cpp:298:   Max Queue: 0
30-08-2017 12:45:58.895 UTC Error main.cpp:752: Failed to initialize the Cassandra cache with error code 1.
30-08-2017 12:45:58.896 UTC Status main.cpp:753: Homestead is shutting down
In homestead netstat logs I cannot see any logs of homestead trying to connect to ports 9160
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 localhost:49146         localhost:8000          TIME_WAIT
tcp        0      0 192.168.0.4:45972<http://192.168.0.4:45972>       192.168.0.6:4000<http://192.168.0.6:4000>        ESTABLISHED
tcp       42      0 192.168.0.4:50479<http://192.168.0.4:50479>       192.168.0.7:2380<http://192.168.0.7:2380>        ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.5:34559<http://192.168.0.5:34559>       ESTABLISHED
tcp        0      0 192.168.0.4:36437<http://192.168.0.4:36437>       192.168.0.4:4000<http://192.168.0.4:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:39848<http://192.168.0.4:39848>       192.168.0.4:4000<http://192.168.0.4:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:39863<http://192.168.0.4:39863>       192.168.0.4:4000<http://192.168.0.4:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.7:40964<http://192.168.0.7:40964>       TIME_WAIT
tcp        0      0 192.168.0.4:40269<http://192.168.0.4:40269>       192.168.0.4:4000<http://192.168.0.4:4000>        ESTABLISHED
tcp        0    103 192.168.0.4:47648<http://192.168.0.4:47648>       192.168.0.6:4000<http://192.168.0.6:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:45663<http://192.168.0.4:45663>       192.168.0.6:4000<http://192.168.0.6:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.7:37374<http://192.168.0.7:37374>       ESTABLISHED
tcp        0      0 192.168.0.4:42474<http://192.168.0.4:42474>       192.168.0.6:4000<http://192.168.0.6:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.6:57924<http://192.168.0.6:57924>       ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.5:35015<http://192.168.0.5:35015>       TIME_WAIT
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.7:40812<http://192.168.0.7:40812>       TIME_WAIT
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.6:53967<http://192.168.0.6:53967>       ESTABLISHED
tcp        0      0 192.168.0.4:40268<http://192.168.0.4:40268>       192.168.0.4:4000<http://192.168.0.4:4000>        TIME_WAIT
tcp        0      0 192.168.0.4:53551<http://192.168.0.4:53551>       192.168.0.5:2380<http://192.168.0.5:2380>        ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.6:57810<http://192.168.0.6:57810>       TIME_WAIT
tcp        0      0 192.168.0.4:37873<http://192.168.0.4:37873>       192.168.0.4:4000<http://192.168.0.4:4000>        ESTABLISHED
tcp        0    103 192.168.0.4:46510<http://192.168.0.4:46510>       192.168.0.6:4000<http://192.168.0.6:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:39859<http://192.168.0.4:39859>       192.168.0.4:4000<http://192.168.0.4:4000>        TIME_WAIT
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.6:57904<http://192.168.0.6:57904>       ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.6:57890<http://192.168.0.6:57890>       TIME_WAIT
tcp        0      0 192.168.0.4:60183<http://192.168.0.4:60183>       192.168.0.7:2380<http://192.168.0.7:2380>        ESTABLISHED
tcp        0      0 192.168.0.4:38186<http://192.168.0.4:38186>       192.168.0.4:4000<http://192.168.0.4:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:37317<http://192.168.0.4:37317>       192.168.0.4:4000<http://192.168.0.4:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.5:58490<http://192.168.0.5:58490>       ESTABLISHED
tcp        0      0 192.168.0.4:44222<http://192.168.0.4:44222>       192.168.0.6:4000<http://192.168.0.6:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:32981<http://192.168.0.4:32981>       192.168.0.7:2380<http://192.168.0.7:2380>        ESTABLISHED
tcp        0      0 192.168.0.4:38725<http://192.168.0.4:38725>       192.168.0.4:4000<http://192.168.0.4:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.7:40407<http://192.168.0.7:40407>       TIME_WAIT
tcp        0      0 192.168.0.4:35533<http://192.168.0.4:35533>       192.168.0.4:4000<http://192.168.0.4:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:34686<http://192.168.0.4:34686>       192.168.0.4:4000<http://192.168.0.4:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:43318<http://192.168.0.4:43318>       192.168.0.6:4000<http://192.168.0.6:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.6:57784<http://192.168.0.6:57784>       TIME_WAIT
tcp        0      0 192.168.0.4:54539<http://192.168.0.4:54539>       192.168.0.5:2380<http://192.168.0.5:2380>        ESTABLISHED
tcp        0      0 192.168.0.4:40293<http://192.168.0.4:40293>       192.168.0.4:4000<http://192.168.0.4:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.7:41168<http://192.168.0.7:41168>       ESTABLISHED
tcp        0      0 192.168.0.4:48057<http://192.168.0.4:48057>       192.168.0.6:2380<http://192.168.0.6:2380>        ESTABLISHED
tcp        0      0 192.168.0.4:47629<http://192.168.0.4:47629>       192.168.0.6:2380<http://192.168.0.6:2380>        ESTABLISHED
tcp        0      0 192.168.0.4:47663<http://192.168.0.4:47663>       192.168.0.6:2380<http://192.168.0.6:2380>        ESTABLISHED
tcp        0      0 192.168.0.4:45102<http://192.168.0.4:45102>       192.168.0.6:4000<http://192.168.0.6:4000>        ESTABLISHED
tcp        0      0 192.168.0.4:45341<http://192.168.0.4:45341>       192.168.0.5:2380<http://192.168.0.5:2380>        ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.5:58487<http://192.168.0.5:58487>       ESTABLISHED
tcp        0    116 192.168.0.4:ssh         10.1.10.112:49603<http://10.1.10.112:49603>       ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.7:38517<http://192.168.0.7:38517>       ESTABLISHED
tcp        0      0 192.168.0.4:2380<http://192.168.0.4:2380>        192.168.0.7:38517<http://192.168.0.7:38517>       ESTABLISHED
tcp6       0      0 192.168.0.4:4000<http://192.168.0.4:4000>        192.168.0.4:38725<http://192.168.0.4:38725>       ESTABLISHED
tcp6       0      0 ip6-localhost:50722     ip6-localhost:4000      ESTABLISHED
tcp6       0      0 192.168.0.4:4000<http://192.168.0.4:4000>        192.168.0.4:39848<http://192.168.0.4:39848>       ESTABLISHED
tcp6       0      0 ip6-localhost:4000      ip6-localhost:50724     ESTABLISHED
tcp6       0      0 ip6-localhost:50721     ip6-localhost:4000      ESTABLISHED
tcp6       0      0 ip6-localhost:50720     ip6-localhost:4000      ESTABLISHED
tcp6       0      0 192.168.0.4:4000<http://192.168.0.4:4000>        192.168.0.4:37873<http://192.168.0.4:37873>       ESTABLISHED
tcp6       0      0 192.168.0.4:4000<http://192.168.0.4:4000>        192.168.0.4:34686<http://192.168.0.4:34686>       ESTABLISHED
tcp6       0      0 ip6-localhost:4000      ip6-localhost:50717     ESTABLISHED
tcp6       0      0 192.168.0.4:4000<http://192.168.0.4:4000>        192.168.0.4:35533<http://192.168.0.4:35533>       ESTABLISHED
tcp6       0      0 ip6-localhost:4000      ip6-localhost:50721     ESTABLISHED
tcp6       0      0 192.168.0.4:4000<http://192.168.0.4:4000>        192.168.0.4:38186<http://192.168.0.4:38186>       ESTABLISHED
tcp6       0      0 ip6-localhost:4000      ip6-localhost:50723     ESTABLISHED
tcp6       0      0 ip6-localhost:50723     ip6-localhost:4000      ESTABLISHED
tcp6       0      0 ip6-localhost:50724     ip6-localhost:4000      ESTABLISHED
tcp6       0      0 ip6-localhost:50717     ip6-localhost:4000      ESTABLISHED
tcp6       0      0 ip6-localhost:4000      ip6-localhost:50722     ESTABLISHED
tcp6       0      0 192.168.0.4:4000<http://192.168.0.4:4000>        192.168.0.4:40294<http://192.168.0.4:40294>       ESTABLISHED
tcp6       0      0 192.168.0.4:4000<http://192.168.0.4:4000>        192.168.0.4:36437<http://192.168.0.4:36437>       ESTABLISHED
tcp6       0      0 192.168.0.4:4000<http://192.168.0.4:4000>        192.168.0.4:39863<http://192.168.0.4:39863>       ESTABLISHED
tcp6       0      0 192.168.0.4:4000<http://192.168.0.4:4000>        192.168.0.4:40293<http://192.168.0.4:40293>       ESTABLISHED
tcp6       0      0 192.168.0.4:4000<http://192.168.0.4:4000>        192.168.0.4:37317<http://192.168.0.4:37317>       ESTABLISHED
tcp6       0      0 ip6-localhost:4000      ip6-localhost:50720     ESTABLISHED

Kindly please guide me on what to do.
Abdul Basit Alvi | Virtualized Data Network
[cid:image001.png at 01D321BF.D4C951F0]



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/166cdac4/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 98049 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170830/166cdac4/attachment.png>

