From marco at opencloud.com  Mon Feb  1 17:31:34 2016
From: marco at opencloud.com (Marco Costantini)
Date: Tue, 2 Feb 2016 11:31:34 +1300
Subject: [Clearwater] All-In-One installation and IPv6
Message-ID: <CAMVsSNT3TP0dGtvK9BN9S5C2ebBd+6=cVyUst_d8LRjy+_DXiA@mail.gmail.com>

We have successfully performed an All-In-One installation on a machine with
public IPv4 and IPv6 addresses. Using the 'force_ipv6' file, we are able to
switch between IPv4 and IPv6 Clearwater configurations. Also, we are
testing Clearwater with the use of two PJSUA (pjsip) clients.

When Clearwater is configured for IPv4, our tests succeed (registration,
session established and media transmitted).

When on IPv6, our test fails. We believe we are running the test properly
(we simply switched the IPv4 proxy URI to a valid IPv6 one). However, we
get a 403 FORBIDDEN when trying to register.

Looking into the sprout logs, this entry seems to be the most relevant.

`01-02-2016 22:05:07.272 UTC Error dnscachedresolver.cpp:567: Failed to
retrieve record for [2400:6180:0:e0::18b:2001]: Domain name not found`
(we have changed the IP address here)

Our `/etc/hosts` file has entries for our home-domain -> IPv6 address of
our clearwater install.

Question: What is the best way to debug this? We are happy to provide any
additional logs.

Thank you,
Marco and Amauri.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160202/381e86dc/attachment.html>

From peter.skrzynski at nec.co.nz  Mon Feb  1 21:13:18 2016
From: peter.skrzynski at nec.co.nz (Peter Skrzynski)
Date: Tue, 2 Feb 2016 02:13:18 +0000
Subject: [Clearwater] Clarification of SNMP statistics
In-Reply-To: <CY1PR0201MB15635B6D0709290D4D7499C490DB0@CY1PR0201MB1563.namprd02.prod.outlook.com>
References: <663823bd3b394bb6ab98ab04c2659fb9@ex15w.necnz.internal>
	<CY1PR0201MB15635B6D0709290D4D7499C490DB0@CY1PR0201MB1563.namprd02.prod.outlook.com>
Message-ID: <867d0354306f4f1e86b64bc24f249e37@ex15w.necnz.internal>

Yes, installing clearwater-snmpd worked fine, and can now read statistics on all nodes. Thanks.

From: Matt Williams [mailto:Matt.Williams at metaswitch.com]
Sent: Saturday, 30 January 2016 1:42 a.m.
To: Peter Skrzynski
Cc: clearwater at lists.projectclearwater.org
Subject: RE: Clarification of SNMP statistics

Peter,

Thanks for raising this.

I think the documentation has an omission - you must install clearwater-snmpd on all nodes.  (I suspect Sprout is working because Chronos, on which it depends, in turn depends on clearwater-snmpd, but Homestead and Bono do not depend on Chronos.)

Note that we use clearwater-snmpd (a Clearwater fork of snmpd) rather than vanilla snmpd because this includes a fix that has yet to be accepted upstream.

Installing clearwater-snmpd should resolve the logs you're seeing - Bono and Homestead are trying to connect to snmpd, but it's not installed.

Incidentally, you shouldn't need any "clearwater-snmp-handler-*" packages apart from for Astaire - we've switched most components to use a different infrastructure that doesn't require these handlers, but Astaire is still pending this change.

Please let me know if installing clearwater-snmpd resolves your problems and we'll get the docs updated (or investigate more if not).

Thanks,

Matt

--

Matt Williams
Lead Architect, Project Clearwater
+44 (0) 20 8366 1177

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Peter Skrzynski
Sent: 29 January 2016 02:02
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Clarification of SNMP statistics

Hi,
I wanted to clarify the requirements for obtaining SNMP statistics from IMS nodes.
I am running a manual install of bono/sprout/homestead/ibcf/dns with release 89 ?Vicomte de Bragelonne?.
As per the instructions in ?http://clearwater.readthedocs.org/en/latest/Clearwater_SNMP_Statistics/index.html?, I have run ?sudo apt-get install clearwater-snmp-handler-astaire? on the Sprout node only.

Specifically, the instructions state ?These SNMP statistics require: the clearwater-snmp-handler-astaire packages to be installed for Sprout and Ralf nodes?.

But what about the other IMS nodes?

Is there no other action required to activate snmp statistics for bono and homestead nodes?

Surely they require a snmp daemon to be running?

There is no snmpd entry in ?ps ?ef?, and no /etc/snmp directory, on my bono or homestead nodes, but there is on the sprout node.

Also, the log files for bono and homestead show continuous entries of?

UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

Does that mean that snmp is not set up correctly in those nodes?

Is the documentation incorrect when it says to install the packages only on the sprout node?

Or, in my case, do I need to do ?sudo apt-get install snmpd? on my bono and homestead nodes?

Should the documentation state that?

Any guidance would be most appreciated.

Regards,

Peter.





Peter Skrzynski
Technical Lead
NEC New Zealand Limited
NEC House, Level 6, 40 Taranaki Street, PO Box 1936, Wellington 6011, New Zealand
T: 043816257, M: 0274849530, F: +6443811110
peter.skrzynski at nec.co.nz<mailto:peter.skrzynski at nec.co.nz>
nz.nec.com<http://nz.nec.com>

[cid:image001.jpg at 01D15DCC.3E4759B0]

Please consider the environment before printing this email

Attention:
The information contained in this message and or attachments is intended only for the person or entity to which it is addressed and may contain confidential and/or privileged material.  Any review, retransmission, dissemination, copying or other use of, or taking of any action in reliance upon, this information by persons or entities other than the intended recipient is prohibited. If you received this in error, please contact the sender and delete the material from any system and destroy any copies. NEC has no liability for any act or omission in reliance on the email or any attachment.  Before opening this email or any attachment(s), please check them for viruses. NEC is not responsible for any viruses in this email or any attachment(s); any changes made to this  email or any attachment(s) after they are sent; or any effects this email or any attachment(s) have on your network or computer system.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160202/ffa2aa2f/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 8340 bytes
Desc: image001.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160202/ffa2aa2f/attachment.jpg>

From marco at opencloud.com  Mon Feb  1 21:37:28 2016
From: marco at opencloud.com (Marco Costantini)
Date: Tue, 2 Feb 2016 15:37:28 +1300
Subject: [Clearwater] All-In-One installation and IPv6
In-Reply-To: <CAMVsSNT3TP0dGtvK9BN9S5C2ebBd+6=cVyUst_d8LRjy+_DXiA@mail.gmail.com>
References: <CAMVsSNT3TP0dGtvK9BN9S5C2ebBd+6=cVyUst_d8LRjy+_DXiA@mail.gmail.com>
Message-ID: <CAMVsSNS+_APbbiSkw+vtU2O764U4q2ZAJdY9csmK6zgrEtZBkQ@mail.gmail.com>

To offer further findings on this,

We turned the Sprout tracing on "Debug" and found some interesting things.
Please look at the following lines:

02-02-2016 02:04:54.978 UTC Debug baseresolver.cpp:513: Attempt to parse
[2400:6180:0:e0::18b:2001] as IP address
02-02-2016 02:04:54.978 UTC Verbose dnscachedresolver.cpp:240: Check cache
for [2400:6180:0:e0::18b:2001] type 28

The baseresolver file calls "inet_pton" to parse a given IP address. After
reading the docs (http://man7.org/linux/man-pages/man3/inet_pton.3.html),
it seems that this method does not support parsing IPv6 addresses with
square brackets (e.g. [2400:6180:0:e0::18b:2001]). However, this format is
required in SIP where a port is given along-side the IPv6 address.

The second log line shows that the string was NOT found to be an IP
address, was found to be a hostname, and triggered a dns lookup.

Please advise if you have any ideas on the best solution for this. Is this
a bug? Perhaps, the baseresolver code can strip the square brackets from
the IP string before calling 'inet_pton'.

Thank you,
Amauri and Marco.
Amaurco.

On Tue, Feb 2, 2016 at 11:31 AM, Marco Costantini <marco at opencloud.com>
wrote:

> We have successfully performed an All-In-One installation on a machine
> with public IPv4 and IPv6 addresses. Using the 'force_ipv6' file, we are
> able to switch between IPv4 and IPv6 Clearwater configurations. Also, we
> are testing Clearwater with the use of two PJSUA (pjsip) clients.
>
> When Clearwater is configured for IPv4, our tests succeed (registration,
> session established and media transmitted).
>
> When on IPv6, our test fails. We believe we are running the test properly
> (we simply switched the IPv4 proxy URI to a valid IPv6 one). However, we
> get a 403 FORBIDDEN when trying to register.
>
> Looking into the sprout logs, this entry seems to be the most relevant.
>
> `01-02-2016 22:05:07.272 UTC Error dnscachedresolver.cpp:567: Failed to
> retrieve record for [2400:6180:0:e0::18b:2001]: Domain name not found`
> (we have changed the IP address here)
>
> Our `/etc/hosts` file has entries for our home-domain -> IPv6 address of
> our clearwater install.
>
> Question: What is the best way to debug this? We are happy to provide any
> additional logs.
>
> Thank you,
> Marco and Amauri.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160202/2ea2d3d4/attachment.html>

From shivacharanms at gmail.com  Tue Feb  2 06:50:59 2016
From: shivacharanms at gmail.com (Shiva Charan)
Date: Tue, 2 Feb 2016 03:50:59 -0800
Subject: [Clearwater] Homestead conf file missing.
In-Reply-To: <BLUPR0201MB1556254E10D02B3A9676A04F90DB0@BLUPR0201MB1556.namprd02.prod.outlook.com>
References: <CAJAf6NxiL_60RjM+wLk-=60POy2SiksPsLghC+eJGYFUyM1faA@mail.gmail.com>
	<CY1PR0201MB15630224570CE51F4910B26C90C30@CY1PR0201MB1563.namprd02.prod.outlook.com>
	<CAJAf6NyULNUReE8G5-j-7km3X1NdKgRXOybLwriQQfb0S7-udA@mail.gmail.com>
	<CY1PR0201MB1563197232A39DB6717EEFBB90C30@CY1PR0201MB1563.namprd02.prod.outlook.com>
	<CAJAf6NzjOrU9_Vf46EWWs0J--gYjK5AOZuKor6wLJM2J4F4=LA@mail.gmail.com>
	<CAJAf6NzoonOmEby4quYJtRsT3Lk6BCkXzQjhr8_wEEkpLGgB9g@mail.gmail.com>
	<CY1PR0201MB15637946ED02908A3D179E0990C40@CY1PR0201MB1563.namprd02.prod.outlook.com>
	<CAJAf6NzV08Bc8WhCYG72hM92FUg5cVECA_1x_M=PCJqOHJaM1g@mail.gmail.com>
	<CY1PR0201MB1563FE76A8C424670D6DC39B90C40@CY1PR0201MB1563.namprd02.prod.outlook.com>
	<CAJAf6Nzcf6vLNNGrKVEQ6TpoWpQ=HBz85xZ+kmk4UCZ0YsMA_w@mail.gmail.com>
	<CY1PR0201MB1563975B1F6F0CA0C91DC8E990C40@CY1PR0201MB1563.namprd02.prod.outlook.com>
	<CAJAf6NyKLNKs3mNb5wHtVmQMwsWiA9mfjMjTJ-+gRLW=8ff6JQ@mail.gmail.com>
	<CY1PR0201MB156383DDAD803FBCEA57D64790C70@CY1PR0201MB1563.namprd02.prod.outlook.com>
	<CAJAf6NwnR1tUCrgh-q0zv__d6Pk0f8UD8MbB6xcopoN3bSVEhw@mail.gmail.com>
	<CY1PR0201MB1563BFDDD833EF2E99C12D6E90C70@CY1PR0201MB1563.namprd02.prod.outlook.com>
	<CAJAf6NzuqFqyABitYffzTSqy+BPV0P=WBDbtEfs44Et4M7+zqw@mail.gmail.com>
	<CY1PR0201MB1563A302E15F42A9E4BBD46390DA0@CY1PR0201MB1563.namprd02.prod.outlook.com>
	<CAJAf6NyfQ6nkPVdH0eHDQwfsWx1Eo55e1sPrg6LQkUaUU1cP1A@mail.gmail.com>
	<BLUPR0201MB1556254E10D02B3A9676A04F90DB0@BLUPR0201MB1556.namprd02.prod.outlook.com>
Message-ID: <CAJAf6Nxa4pmqw_3S_Bbo=7rzAmVST8NPEyoF4EpyAPCfS4OF-w@mail.gmail.com>

Hi Matt,

I am trying to perform the stress test on my clearwater environment with
the following link.

http://clearwater.readthedocs.org/en/stable/Clearwater_stress_testing/index.html

I have attached the output file 'sip-stress.1.out' file, are the SIPp calls
going through? and how do i increase
enough load to test the scaling functionality?.

Thanks,
Shiva?
 sip-stress.1.out
<https://drive.google.com/file/d/0B56nF-qdQk6XZEVqczFDUzdyN1k/view?usp=drive_web>
?


On Fri, Jan 29, 2016 at 4:07 AM, Matt Williams <Matt.Williams at metaswitch.com
> wrote:

> Great news - thanks for letting me know!
>
>
>
> Matt
>
>
>
> --
>
>
>
> Matt Williams
> Lead Architect, Project Clearwater
>
> +44 (0) 20 8366 1177
>
>
>
> *From:* Shiva Charan [mailto:shivacharanms at gmail.com]
> *Sent:* 29 January 2016 10:42
>
> *To:* Matt Williams <Matt.Williams at metaswitch.com>
> *Cc:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Clearwater] Homestead conf file missing.
>
>
>
> Hi Matt,
>
>
>
> I followed your steps, the calls are going thourgh now.
>
> Thanks a ton for your help.
>
>
>
> Thanks,
>
> Shiva
>
>
>
> On Thu, Jan 28, 2016 at 5:44 PM, Matt Williams <
> Matt.Williams at metaswitch.com> wrote:
>
> Shiva,
>
>
>
> Thanks!  You need to
>
> ?         delete any subscribers you've created already (as they have the
> wrong home domain)
>
> ?         configure your DNS server to have the DNS records described in
> http://clearwater.readthedocs.org/en/stable/Clearwater_DNS_Usage/index.html#dns-records
> - you may be able to adapt the example zone file at
> http://clearwater.readthedocs.org/en/stable/Clearwater_DNS_Usage/index.html#configuring-zone
> to do this
>
> ?         make sure that your Clearwater VMs are configured to use the
> DNS server - if you haven't done this through DHCP, you can see
> http://clearwater.readthedocs.org/en/stable/Clearwater_DNS_Usage/index.html#client-configuration
> for more details on how to do this
>
> ?         configure your Clearwater VMs with domain names rather than IP
> addresses - see
> http://clearwater.readthedocs.org/en/stable/Manual_Install/index.html#create-the-per-node-configuration
> for per-node configuration and
> http://clearwater.readthedocs.org/en/stable/Manual_Install/index.html#provide-shared-configuration
> for information that is shared across the cluster
>
> ?         delete and recreate the list of available numbers in Ellis (as
> these have the wrong home domain)
>
> o   to delete them, on the Ellis node, run 'echo "DELETE FROM
> ellis.numbers WHERE owner_id IS NULL ;" | sudo mysql'
>
> o   to create them, follow
> http://clearwater.readthedocs.org/en/stable/Manual_Install/index.html#provision-telephone-numbers-in-ellis
> again
>
> ?         create new subscribers.
>
>
>
> Does that make sense?
>
>
>
> Matt
>
>
>
> --
>
>
>
> Matt Williams
> Lead Architect, Project Clearwater
>
> +44 (0) 20 8366 1177
>
>
>
> *From:* Shiva Charan [mailto:shivacharanms at gmail.com]
> *Sent:* 28 January 2016 07:44
> *To:* Matt Williams <Matt.Williams at metaswitch.com>
> *Cc:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Clearwater] Homestead conf file missing.
>
>
>
> HI Matt,
>
>
>
> Ive installed a DNS server in my environment, what all changes do i need
> to do?
>
>
>
> Thanks,
>
> Shiva
>
>
>
> On Mon, Jan 25, 2016 at 4:35 PM, Matt Williams <
> Matt.Williams at metaswitch.com> wrote:
>
> Shiva,
>
>
>
> Yes, in this trace, we do see the INVITE being sent to Bono.  In future,
> you can check this in the bono*.txt logs (as with debug logging enabled,
> all SIP messages are traced there), or with network capture.
>
>
>
> The trace shows that the bono process is being restarted regularly.  I
> suspect this is because the monit process (which monitors and restarts
> processes if they fail) is automatically polling bono with SIP OPTIONS
> messages and these requests are failing.  We see these OPTIONS messages in
> the bono logs, along with the INVITE.  The reason the OPTIONS messages are
> failing is that Bono's local_ip matches home_domain (both in
> /etc/clearwater/local_config), so the bono process doesn't know whether to
> forward these requests on to Sprout (where they fail, as expected) or to
> reply to them itself.
>
>
>
> This is a configuration error.  As I asked you below:
>
> Even when we deploy without DNS, we still configure the home domain to be
> a real domain name (even though it won't resolve in DNS).
>
> Please can you fix this configuration, and confirm that you've done so?
> When you've done this, you'll also need to recreate your subscribers on
> Ellis, as the old ones will have the wrong home domain.
>
>
>
> As below, I'd strongly recommend deploying DNS - while we may be able to
> make a non-DNS deployment work, it's going to take a lot longer, and
> require you to understand a lot more about the solution and do a lot more
> troubleshooting.
>
>
>
> The remote_cluster_settings file is used for geographic redundant
> deployments.  See
> http://clearwater.readthedocs.org/en/latest/Geographic_redundancy/ for
> more details.  You won't need this because you're not running a geographic
> redundant deployment.
>
>
>
> Matt
>
>
>
> --
>
>
>
> Matt Williams
> Lead Architect, Project Clearwater
>
> +44 (0) 20 8366 1177
>
>
>
> *From:* Shiva Charan [mailto:shivacharanms at gmail.com]
> *Sent:* 25 January 2016 10:01
>
>
> *To:* Matt Williams <Matt.Williams at metaswitch.com>
> *Cc:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Clearwater] Homestead conf file missing.
>
>
>
> Hi Matt,
>
>
>
> yes i've given bono node details ( registration is successful ) please
> find the latest logs attached after I tried a call request from zoiper
> client.
>
> Is it still not connecting to bono? what should be the contents of
> "remote_cluster_settings" file in sprout?
>
>
>
> Thanks,
>
> Shiva
>
>
>
>
>
> On Mon, Jan 25, 2016 at 12:06 AM, Matt Williams <
> Matt.Williams at metaswitch.com> wrote:
>
> Shiva,
>
>
>
> The bono_current.txt file you sent contains no record of an INVITE request
> (or a 408 response).  Please can you check that the INVITE is actually
> being sent to Bono and, if it is, gather a log from Bono from this period?
>
>
>
> Also, it may well be a configuration issue on Sprout, so please gather
> Sprout's logs at the same time.
>
>
>
> Thanks,
>
>
>
> Matt
>
>
>
> --
>
>
>
> Matt Williams
> Lead Architect, Project Clearwater
>
> +44 (0) 20 8366 1177
>
>
>
> *From:* Shiva Charan [mailto:shivacharanms at gmail.com]
> *Sent:* 24 January 2016 11:52
>
>
> *To:* Matt Williams <Matt.Williams at metaswitch.com>
> *Cc:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Clearwater] Homestead conf file missing.
>
>
>
> Hi Matt,
>
>
>
> The registrations were successful but when it try to make the call, i get
> a 408 request time out from zoiper client.
>
>
>
> Thanks,
>
> Shiva
>
>
>
> On Sat, Jan 23, 2016 at 12:56 AM, Matt Williams <
> Matt.Williams at metaswitch.com> wrote:
>
> Shiva,
>
>
>
> The Bono and Sprout traces seem to be from different times - the Bono
> traces are from 18:13 and the Sprout traces are from 18:52.  Do you have
> NTP configured?
>
>
>
> The Bono trace seems to show
>
> ?         a Zoiper softphone sending a REGISTER for
> sip:6505550008 at 10.222.5.159
>
> ?         Bono forwarding this on to Sprout
>
> ?         Sprout replying with a 401 authentication challenge
>
> ?         Bono forwarding this back to the Zoiper
>
> ?         a retry from the Zoiper, but no indication of it attempting to
> respond to the authentication challenge.
>
> (The Sprout trace doesn't show anything as it's from a different time.)
>
>
>
> When the Zoiper receives the 401 challenge, it should respond with a new
> REGISTER that responds to this challenge.  It's possible that the trace
> file you've shared is truncated before this happens, or maybe the Zoiper is
> not responding to the challenge for some reason.
>
>
>
> Please can you check your complete Bono logs?
>
> ?         If you see a REGISTER request containing an Authorization
> header, please can you share those logs and we can investigate if/why
> that's failing.
>
> ?         If you don't see any Authorization headers in your REGISTER
> requests, this sounds like a problem with your Zoiper configuration - it
> should be responding to the challenge.
>
>
>
> Thanks,
>
>
>
> Matt
>
>
>
> --
>
>
>
> Matt Williams
> Lead Architect, Project Clearwater
>
> +44 (0) 20 8366 1177
>
>
>
> *From:* Shiva Charan [mailto:shivacharanms at gmail.com]
> *Sent:* 22 January 2016 19:06
>
>
> *To:* Matt Williams <Matt.Williams at metaswitch.com>
> *Cc:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Clearwater] Homestead conf file missing.
>
>
>
> Hi Matt,
>
>
>
> Yes I understand your recommendation, but for now there is no dns server
> in my design.
>
> Will have a segregated setup for that.
>
>
>
> For now please find the bono and sproute logs attached to the mail.
>
>
>
> how do i make this work without the dns?
>
>
>
> Thanks,
>
> Shiva
>
>
>
> On Fri, Jan 22, 2016 at 7:52 PM, Matt Williams <
> Matt.Williams at metaswitch.com> wrote:
>
> Shiva,
>
>
>
> If you're intending to implement DNS, I'd suggest doing it now rather than
> later - you're going down paths that are not well-trodden, and I'm sure
> we're going to hit lots of configuration issues.  It will be *much*
> quicker to deploy DNS now.
>
>
>
> My guess in this case is that, because the subscriber's "domain" parameter
> is an IP address, Bono is just routing the INVITE to that IP address (which
> is itself, so it fails).  Even when we deploy without DNS, we still
> configure the domain to be a real domain name (even though it won't resolve
> in DNS).
>
>
>
> If this doesn't resolve the problem, I'd suggest turning debug logging on
> in Bono and Sprout (as described at
> http://clearwater.readthedocs.org/en/latest/Troubleshooting_and_Recovery/index.html#bono
> and
> http://clearwater.readthedocs.org/en/latest/Troubleshooting_and_Recovery/index.html#sprout)
> - this will give you more detailed diagnostics about what's going on in
> Bono and Sprout.
>
>
>
> Matt
>
>
>
> --
>
>
>
> Matt Williams
> Lead Architect, Project Clearwater
>
> +44 (0) 20 8366 1177
>
>
>
> *From:* Shiva Charan [mailto:shivacharanms at gmail.com]
> *Sent:* 22 January 2016 13:16
>
>
> *To:* Matt Williams <Matt.Williams at metaswitch.com>
> *Cc:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Clearwater] Homestead conf file missing.
>
>
>
> Hi Matt,
>
>
>
> Thanks for the recommendation, will implement DNS later on :) .
>
>
>
> And you were rite I had missed to remove a dns name for sprout, its now
> fixed i am able to open the UI and create numbers.
>
>
>
> But when trying to make a sip call via zoiper client application, the
> registration is not success full.
>
> The screen and log file is as below.
>
>
>
> [image: Inline image 2]
>
>
>
> Any idea?
>
>
>
> Thanks,
>
> Shiva
>
>
>
> On Fri, Jan 22, 2016 at 4:13 PM, Matt Williams <
> Matt.Williams at metaswitch.com> wrote:
>
> Shiva,
>
>
>
> Clearwater is very oriented around DNS.
>
> ?         DNS is used for load-balancing when you have more than 1 node
> in a cluster - in particular, following RFC 3263 for SIP.
>
> ?         DNS is used (following the rules in RFC 6733) for resolving
> Diameter realms to IP addresses.
>
>
>
> It is possible to get Clearwater running without a DNS server, but I would
> strongly recommend that you deploy a DNS server - in particular, if you
> event intend to scale up to a fault-tolerant or higher-capacity deployment,
> it is mandatory.
>
>
>
> The instructions for configuring DNS (and installing a DNS server if you
> don't already have one) are at
> http://clearwater.readthedocs.org/en/latest/Clearwater_DNS_Usage/index.html
> .
>
>
>
> Please let me know if you do definitely need to run without DNS, and we
> can look at how to set this up - but note the restrictions above on
> scalability and fault-tolerance.
>
>
>
> On the issues you're seeing below, I think the problem is that there is
> still configuration entries such as "sprout_hostname" below that contain
> both a hostname prefix and then an IP address - moving to DNS is one way to
> resolve all this.
>
>
>
> Thanks,
>
>
>
> Matt
>
>
>
> --
>
>
>
> Matt Williams
> Lead Architect, Project Clearwater
>
> +44 (0) 20 8366 1177
>
>
>
> *From:* Shiva Charan [mailto:shivacharanms at gmail.com]
> *Sent:* 22 January 2016 06:02
>
>
> *To:* Matt Williams <Matt.Williams at metaswitch.com>
> *Cc:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Clearwater] Homestead conf file missing.
>
>
>
> Hi Matt,
>
>
>
> An update, ive changed the share_config file as follows and also I dont
> have a dns server in my environment( I assume its not mandatory).
>
> Also find the logs below.
>
>
>
> *shared_config *
>
> # Deployment definitions
>
> home_domain=10.222.5.159
>
> sprout_hostname=sprout.10.222.5.172
>
> hs_hostname=10.222.5.173:8888
>
> hs_provisioning_hostname=10.222.5.173:8889
>
> ralf_hostname=10.222.5.163:10888
>
> xdms_hostname=0.222.5.179:7888
>
>
>
> # Email server configuration
>
> smtp_smarthost=example.com
>
> smtp_username=username
>
> smtp_password=password
>
> email_recovery_sender=clearwater at example.org
>
>
>
> # Keys
>
> signup_key=secret
>
> turn_workaround=secret
>
> ellis_api_key=secret
>
> ellis_cookie_key=secret
>
>
>
> *ellis error log*
>
> 22-01-2016 05:51:32.515 UTC INFO homestead.py:62: Pinged Homestead OK
>
> 22-01-2016 05:52:03.235 UTC INFO main.py:115: Process 3 starting up
>
> 22-01-2016 05:52:03.278 UTC WARNING homestead.py:309: Passing SIP password
> in the clear over http
>
> 22-01-2016 05:52:03.295 UTC INFO homestead.py:62: Pinged Homestead OK
>
> 22-01-2016 05:52:34.010 UTC INFO main.py:115: Process 3 starting up
>
> 22-01-2016 05:52:34.055 UTC WARNING homestead.py:309: Passing SIP password
> in the clear over http
>
> 22-01-2016 05:52:34.064 UTC INFO homestead.py:62: Pinged Homestead OK
>
>
>
> *homestead log:*
>
> 22-01-2016 05:53:45.109 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
> 22-01-2016 05:54:00.121 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
> 22-01-2016 05:54:15.128 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
> 22-01-2016 05:54:30.134 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
> 22-01-2016 05:54:45.147 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
> 22-01-2016 05:55:00.161 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
> 22-01-2016 05:55:15.176 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
> 22-01-2016 05:55:30.190 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
> 22-01-2016 05:55:45.204 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
>
>
>
>
> *homer log:*
>
> 2-01-2016 03:18:56.908 UTC INFO main.py:74: Going to listen for HTTP on
> UNIX socket /tmp/.homer-sock-0
>
>
>
> *sprout log:*
>
> 22-01-2016 05:57:49.979 UTC Status load_monitor.cpp:260: Maximum incoming
> request rate/second unchanged - only handled 20 requests in last 142284ms,
> minimum threshold for a change is 35571.000000
>
> 22-01-2016 05:58:08.533 UTC Error dnscachedresolver.cpp:567: Failed to
> retrieve record for hs.10.222.5.173: Could not contact DNS servers
>
> 22-01-2016 05:58:08.533 UTC Error httpconnection.cpp:771: cURL failure
> with cURL error code 6 (see man 3 libcurl-errors) and HTTP error code 404
>
> 22-01-2016 05:58:08.533 UTC Error hssconnection.cpp:615: Could not get
> subscriber data from HSS
>
> 22-01-2016 05:58:38.720 UTC Error httpconnection.cpp:771: cURL failure
> with cURL error code 6 (see man 3 libcurl-errors) and HTTP error code 404
>
> 22-01-2016 05:58:38.720 UTC Error hssconnection.cpp:615: Could not get
> subscriber data from HSS
>
> 22-01-2016 05:58:48.752 UTC Error dnscachedresolver.cpp:567: Failed to
> retrieve record for hs.10.222.5.173: Could not contact DNS servers
>
> 22-01-2016 05:58:48.752 UTC Error httpconnection.cpp:771: cURL failure
> with cURL error code 6 (see man 3 libcurl-errors) and HTTP error code 404
>
> 22-01-2016 05:58:48.752 UTC Error hssconnection.cpp:615: Could not get
> subscriber data from HSS
>
> 22-01-2016 05:59:08.926 UTC Error httpconnection.cpp:771: cURL failure
> with cURL error code 6 (see man 3 libcurl-errors) and HTTP error code 404
>
> 22-01-2016 05:59:08.926 UTC Error hssconnection.cpp:615: Could not get
> subscriber data from HSS
>
>
>
>
>
> bono and ralf log files are attached to the mail
>
>
>
>
>
>
>
> On Fri, Jan 22, 2016 at 8:55 AM, Shiva Charan <shivacharanms at gmail.com>
> wrote:
>
> Hi Matt,
>
>
>
> Seems the shared config file wasn't applied properly to the home stead
> node, i copied the config file to /etc/clearwater/ and ran
>
> "sudo service clearwater-infrastructure restart"  now almost all the
> services are running on all nodes are running, except for
>
> Program 'poll_bono'  Status failed
>
> Program 'poll_memento_https'        Status failed
>
>
>
> And Ellis UI still show the nginx welcome page.
>
>
>
> Please find the shared_config file below.
>
>
>
> [homestead]ubuntu at homestead:/etc/clearwater$ cat shared_config
>
> # Deployment definitions
>
> home_domain=10.222.5.159
>
> sprout_hostname=sprout.10.222.5.172
>
> hs_hostname=hs.10.222.5.173:8888
>
> hs_provisioning_hostname=hs.10.222.5.173:8889
>
> ralf_hostname=ralf.10.222.5.163:10888
>
> xdms_hostname=homer.10.222.5.179:7888
>
>
>
> # Email server configuration
>
> smtp_smarthost=example.com
>
> smtp_username=username
>
> smtp_password=password
>
> email_recovery_sender=clearwater at example.org
>
>
>
> # Keys
>
> signup_key=secret
>
> turn_workaround=secret
>
> ellis_api_key=secret
>
> ellis_cookie_key=secret
>
>
>
> Shiva
>
>
>
>
>
> On Fri, Jan 22, 2016 at 12:45 AM, Matt Williams <
> Matt.Williams at metaswitch.com> wrote:
>
> Shiva,
>
>
>
> Thanks for this.  I notice that the URL that Ellis is trying to ping is "
> http://hs.10.222.5.173:8889/ping".  Please can you check/share your
> /etc/clearwater/shared_config file?
>
>
>
> Matt
>
>
>
> --
>
>
>
> Matt Williams
> Lead Architect, Project Clearwater
>
> +44 (0) 20 8366 1177
>
>
>
> *From:* Shiva Charan [mailto:shivacharanms at gmail.com]
> *Sent:* 21 January 2016 17:44
> *To:* Matt Williams <Matt.Williams at metaswitch.com>
> *Cc:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Clearwater] Homestead conf file missing.
>
>
>
> Hi Matt,
>
>
>
> Thank you for the reply.
>
>
>
> About homestead, yes I tried running "sudo service
> clearwater-infrastructure restart" did not return any error
>
>
>
> [homestead]ubuntu at homestead:/var/log/homestead$ sudo service
> clearwater-infrastructure restart
>
>  * Restarting clearwater-infrastructure clearwater-infrastructure
>
> Configuring monit for only localhost access
>
>  [ OK ]
>
> [homestead]ubuntu at homestead:/var/log/homestead$
>
>
>
> On Ellis node the process seems to run, but its not stable process
> restarts after a while.
>
>
>
> [ellis]ubuntu at ellis:/etc/clearwater$ sudo monit summary
>
> The Monit daemon 5.8.1 uptime: 9h 50m
>
>
>
> Process 'ntp_process'               Running
>
> System 'node-ellis'                 Running
>
> Process 'nginx_process'             Running
>
> Process 'mysql_process'             Running
>
> Process 'ellis_process'             Running
>
> Program 'poll_ellis'                Status ok
>
> Program 'poll_ellis_https'          Status ok
>
> Process 'clearwater_queue_manager'  Running
>
> Process 'etcd_process'              Running
>
> Program 'poll_etcd_cluster'         Waiting
>
> Program 'poll_etcd'                 Status ok
>
> Process 'clearwater_diags_monitor_process' Running
>
> Process 'clearwater_config_manager' Running
>
> Process 'clearwater_cluster_manager' Running
>
>
>
> And the ellis error log file says it failed to ping homestead.
>
> "UTC ERROR homestead.py:68: Failed to ping Homestead at
> http://hs.10.222.5.173:8889/ping. Have you configured your HOMESTEAD_URL?"
>
> netstat on homestead shows no processon 8889 port.
>
>
>
>
>
> Shiva
>
>
>
>
>
>
>
>
>
>
>
> On Thu, Jan 21, 2016 at 6:59 PM, Matt Williams <
> Matt.Williams at metaswitch.com> wrote:
>
> Shiva,
>
>
>
> It's good to hear from you.
>
>
>
> /var/lib/homestead/homestead.conf should be built by the
> clearwater-infrastructure script that is run on package installation and
> boot.  You can rerun it by typing "sudo service clearwater-infrastructure
> restart".  Do you get any errors out when you do so?
>
>
>
> You mention that ellis is displaying the nginx page.  Is the ellis process
> running?  Run "sudo monit summary" on the ellis node to find out.  If it
> isn't running, please check /var/log/ellis/ for details and, if that
> doesn't help, let me know what you see.
>
>
>
> Incidentally, please can you subscribe to the clearwater mailing list, so
> that your posts are automatically approved and you can see other people's
> questions and comments - see
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
> for details.
>
>
>
> Thanks,
>
>
>
> Matt
>
>
>
> --
>
>
>
> Matt Williams
> Lead Architect, Project Clearwater
>
> +44 (0) 20 8366 1177
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Shiva Charan
> *Sent:* 21 January 2016 12:38
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Clearwater] Homestead conf file missing.
>
>
>
> Hi,
>
>
>
> I have a new clearwater manual install setup, homestead process has a
> "Execution failed" status.
>
>
>
> Also when i open the ellis UI with the ellis IP it goes to nginx page, how
> do i get the clearwater UI?
>
>
>
> /etc/log/homestead/homestead_current.txt says the /var/lib/homestead.conf
> file is missing. How do I get this file?
>
>
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> NET-SNMP-EXTEND-MIB: nsExtendResult ::= { nsExtendOutput1Entry 4 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> NET-SNMP-EXTEND-MIB: nsExtendOutNumLines ::= { nsExtendOutput1Entry 3 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> NET-SNMP-EXTEND-MIB: nsExtendOutputFull ::= { nsExtendOutput1Entry 2 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> NET-SNMP-EXTEND-MIB: nsExtendOutput1Line ::= { nsExtendOutput1Entry 1 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> NET-SNMP-EXTEND-MIB: nsExtendOutLine ::= { nsExtendOutput2Entry 2 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> NET-SNMP-EXTEND-MIB: nsExtendLineIndex ::= { nsExtendOutput2Entry 1 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> NET-SNMP-AGENT-MIB: nsNotifyStart ::= { netSnmpNotifications 1 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> NET-SNMP-AGENT-MIB: nsNotifyShutdown ::= { netSnmpNotifications 2 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> NET-SNMP-AGENT-MIB: nsNotifyRestart ::= { netSnmpNotifications 3 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> UCD-SNMP-MIB: laErrMessage ::= { laEntry 101 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> UCD-SNMP-MIB: laErrorFlag ::= { laEntry 100 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> UCD-SNMP-MIB: laLoadFloat ::= { laEntry 6 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> UCD-SNMP-MIB: laLoadInt ::= { laEntry 5 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> UCD-SNMP-MIB: laConfig ::= { laEntry 4 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> UCD-SNMP-MIB: laLoad ::= { laEntry 3 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> UCD-SNMP-MIB: laNames ::= { laEntry 2 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Cannot adopt OID in
> UCD-SNMP-MIB: laIndex ::= { laEntry 1 }
>
> 21-01-2016 12:33:09.196 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
> 21-01-2016 12:33:09.197 UTC Status load_monitor.cpp:105: Constructing
> LoadMonitor
>
> 21-01-2016 12:33:09.197 UTC Status load_monitor.cpp:106:    Target latency
> (usecs)   : 100000
>
> 21-01-2016 12:33:09.197 UTC Status load_monitor.cpp:107:    Max bucket
> size          : 20
>
> 21-01-2016 12:33:09.197 UTC Status load_monitor.cpp:108:    Initial token
> fill rate/s: 100.000000
>
> 21-01-2016 12:33:09.197 UTC Status load_monitor.
>
> ...
>
> [Message clipped]
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160202/61107969/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 86154 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160202/61107969/attachment.png>

From shivacharanms at gmail.com  Tue Feb  2 11:22:13 2016
From: shivacharanms at gmail.com (Shiva Charan)
Date: Tue, 2 Feb 2016 08:22:13 -0800
Subject: [Clearwater] Stress test
Message-ID: <CAJAf6NyF3Sdhjcxncr89ciPcCBt2+Xx14mWwVUA6vBoHRuy_xw@mail.gmail.com>

Hi All,

Currently I have multi vm manual install clearwater setup up and running.
I am trying to perform the stress test on my clearwater environment with
the following link.

http://clearwater.readthedocs.org/en/stable/Clearwater_stress_testing/index.html

I have attached the output file 'sip-stress.1.out' file, are the SIPp calls
going through? and how do i increase enough load to test the scaling
functionality?.

?
 sip-stress.1.out
<https://drive.google.com/file/d/0B56nF-qdQk6XVzZyRkVqMGFiaFE/view?usp=drive_web>
?

Thanks,
Shiva?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160202/b9756c70/attachment.html>

From shivacharanms at gmail.com  Tue Feb  2 21:50:26 2016
From: shivacharanms at gmail.com (Shiva Charan)
Date: Tue, 2 Feb 2016 18:50:26 -0800
Subject: [Clearwater] Stress Test
Message-ID: <CAJAf6NySYvEhCQ3d9WM7U8J-L85kuMq7aqG5D23XEiahG3QHVA@mail.gmail.com>

Hi All,

Currently I have multi vm manual install clearwater setup up and running.
I am trying to perform the stress test on my clearwater environment with
the following link.

http://clearwater.readthedocs.org/en/stable/Clearwater_stress_testing/index.html

I have attached the output file 'sip-stress.1.out' file, are the SIPp calls
going through? and how do i increase enough load to test the scaling
functionality? Please suggest.

?
 sip-stress.1.out
<https://drive.google.com/file/d/0B56nF-qdQk6XcHRoaGoxdzQtbmc/view?usp=drive_web>
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160202/b18ab4dd/attachment.html>

From carlasauvanaud at wanadoo.fr  Wed Feb  3 05:24:00 2016
From: carlasauvanaud at wanadoo.fr (carlasauvanaud at wanadoo.fr)
Date: Wed, 3 Feb 2016 11:24:00 +0100 (CET)
Subject: [Clearwater] SIP stress call rate
Message-ID: <128792603.7850.1454495040599.JavaMail.www@wwinf1m14>

Hi list,

I am trying to test my manual installation of Clearwater using sip-stress.
When running the default scenario with the default sipp command in '/usr/share/clearwater/bin/sip-stress' every thing seems OK.
However, each time I remove the '-users $(...)' option and add a "-r $call_rate" (where I defined before callrate=10) in the sipp command, the default scenario fails at receiving the first 401 response of the first REGISTER with a timeout (no matters the recv_timeout).
(In the sipp.out log, I see that the call rate is however well defined.)

Do you have any idea what I am doing wrong ?

Yours sincerely,
Carla
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160203/e62a4788/attachment.html>

From Eleanor.Merry at metaswitch.com  Wed Feb  3 09:22:04 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Wed, 3 Feb 2016 14:22:04 +0000
Subject: [Clearwater] Clarification of SNMP statistics
In-Reply-To: <867d0354306f4f1e86b64bc24f249e37@ex15w.necnz.internal>
References: <663823bd3b394bb6ab98ab04c2659fb9@ex15w.necnz.internal>
	<CY1PR0201MB15635B6D0709290D4D7499C490DB0@CY1PR0201MB1563.namprd02.prod.outlook.com>
	<867d0354306f4f1e86b64bc24f249e37@ex15w.necnz.internal>
Message-ID: <BN3PR02MB1255D92C1028C7CB925D4F239BD00@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Peter,

Thanks for letting us know. I?ve now updated the documentation to include the clearwater-snmpd dependency.

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Peter Skrzynski
Sent: 02 February 2016 02:13
To: Matt Williams
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] Clarification of SNMP statistics

Yes, installing clearwater-snmpd worked fine, and can now read statistics on all nodes. Thanks.

From: Matt Williams [mailto:Matt.Williams at metaswitch.com]
Sent: Saturday, 30 January 2016 1:42 a.m.
To: Peter Skrzynski
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Clarification of SNMP statistics

Peter,

Thanks for raising this.

I think the documentation has an omission - you must install clearwater-snmpd on all nodes.  (I suspect Sprout is working because Chronos, on which it depends, in turn depends on clearwater-snmpd, but Homestead and Bono do not depend on Chronos.)

Note that we use clearwater-snmpd (a Clearwater fork of snmpd) rather than vanilla snmpd because this includes a fix that has yet to be accepted upstream.

Installing clearwater-snmpd should resolve the logs you're seeing - Bono and Homestead are trying to connect to snmpd, but it's not installed.

Incidentally, you shouldn't need any "clearwater-snmp-handler-*" packages apart from for Astaire - we've switched most components to use a different infrastructure that doesn't require these handlers, but Astaire is still pending this change.

Please let me know if installing clearwater-snmpd resolves your problems and we'll get the docs updated (or investigate more if not).

Thanks,

Matt

--

Matt Williams
Lead Architect, Project Clearwater
+44 (0) 20 8366 1177

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Peter Skrzynski
Sent: 29 January 2016 02:02
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Clarification of SNMP statistics

Hi,
I wanted to clarify the requirements for obtaining SNMP statistics from IMS nodes.
I am running a manual install of bono/sprout/homestead/ibcf/dns with release 89 ?Vicomte de Bragelonne?.
As per the instructions in ?http://clearwater.readthedocs.org/en/latest/Clearwater_SNMP_Statistics/index.html?, I have run ?sudo apt-get install clearwater-snmp-handler-astaire? on the Sprout node only.

Specifically, the instructions state ?These SNMP statistics require: the clearwater-snmp-handler-astaire packages to be installed for Sprout and Ralf nodes?.

But what about the other IMS nodes?

Is there no other action required to activate snmp statistics for bono and homestead nodes?

Surely they require a snmp daemon to be running?

There is no snmpd entry in ?ps ?ef?, and no /etc/snmp directory, on my bono or homestead nodes, but there is on the sprout node.

Also, the log files for bono and homestead show continuous entries of?

UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

Does that mean that snmp is not set up correctly in those nodes?

Is the documentation incorrect when it says to install the packages only on the sprout node?

Or, in my case, do I need to do ?sudo apt-get install snmpd? on my bono and homestead nodes?

Should the documentation state that?

Any guidance would be most appreciated.

Regards,

Peter.





Peter Skrzynski
Technical Lead
NEC New Zealand Limited
NEC House, Level 6, 40 Taranaki Street, PO Box 1936, Wellington 6011, New Zealand
T: 043816257, M: 0274849530, F: +6443811110
peter.skrzynski at nec.co.nz<mailto:peter.skrzynski at nec.co.nz>
nz.nec.com<http://nz.nec.com>

[cid:image001.jpg at 01D15E8E.3F4D42D0]

Please consider the environment before printing this email

Attention:
The information contained in this message and or attachments is intended only for the person or entity to which it is addressed and may contain confidential and/or privileged material.  Any review, retransmission, dissemination, copying or other use of, or taking of any action in reliance upon, this information by persons or entities other than the intended recipient is prohibited. If you received this in error, please contact the sender and delete the material from any system and destroy any copies. NEC has no liability for any act or omission in reliance on the email or any attachment.  Before opening this email or any attachment(s), please check them for viruses. NEC is not responsible for any viruses in this email or any attachment(s); any changes made to this  email or any attachment(s) after they are sent; or any effects this email or any attachment(s) have on your network or computer system.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160203/6200c984/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 8340 bytes
Desc: image001.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160203/6200c984/attachment.jpg>

From Eleanor.Merry at metaswitch.com  Wed Feb  3 10:25:40 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Wed, 3 Feb 2016 15:25:40 +0000
Subject: [Clearwater] All-In-One installation and IPv6
In-Reply-To: <CAMVsSNS+_APbbiSkw+vtU2O764U4q2ZAJdY9csmK6zgrEtZBkQ@mail.gmail.com>
References: <CAMVsSNT3TP0dGtvK9BN9S5C2ebBd+6=cVyUst_d8LRjy+_DXiA@mail.gmail.com>
	<CAMVsSNS+_APbbiSkw+vtU2O764U4q2ZAJdY9csmK6zgrEtZBkQ@mail.gmail.com>
Message-ID: <BN3PR02MB1255FA4EF3CC49634E249A8E9BD00@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi,

Thanks for raising this. I?ve raised a github issue to cover the baseresolver behaviour ? you can track this at: https://github.com/Metaswitch/sprout/issues/1317.

In the meantime, can you try creating an AAAA record for your IPv6 address, and using this record in place of the IP address? You?ll need to change the configuration files in /etc/clearwater, run service clearwater-infrastructure restart, then restart all the Clearwater services.

Hope this helps,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Marco Costantini
Sent: 02 February 2016 02:37
To: clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] All-In-One installation and IPv6

To offer further findings on this,
We turned the Sprout tracing on "Debug" and found some interesting things. Please look at the following lines:

02-02-2016 02:04:54.978 UTC Debug baseresolver.cpp:513: Attempt to parse [2400:6180:0:e0::18b:2001] as IP address
02-02-2016 02:04:54.978 UTC Verbose dnscachedresolver.cpp:240: Check cache for [2400:6180:0:e0::18b:2001] type 28
The baseresolver file calls "inet_pton" to parse a given IP address. After reading the docs (http://man7.org/linux/man-pages/man3/inet_pton.3.html), it seems that this method does not support parsing IPv6 addresses with square brackets (e.g. [2400:6180:0:e0::18b:2001]). However, this format is required in SIP where a port is given along-side the IPv6 address.
The second log line shows that the string was NOT found to be an IP address, was found to be a hostname, and triggered a dns lookup.
Please advise if you have any ideas on the best solution for this. Is this a bug? Perhaps, the baseresolver code can strip the square brackets from the IP string before calling 'inet_pton'.
Thank you,
Amauri and Marco.
Amaurco.

On Tue, Feb 2, 2016 at 11:31 AM, Marco Costantini <marco at opencloud.com<mailto:marco at opencloud.com>> wrote:
We have successfully performed an All-In-One installation on a machine with public IPv4 and IPv6 addresses. Using the 'force_ipv6' file, we are able to switch between IPv4 and IPv6 Clearwater configurations. Also, we are testing Clearwater with the use of two PJSUA (pjsip) clients.
When Clearwater is configured for IPv4, our tests succeed (registration, session established and media transmitted).
When on IPv6, our test fails. We believe we are running the test properly (we simply switched the IPv4 proxy URI to a valid IPv6 one). However, we get a 403 FORBIDDEN when trying to register.
Looking into the sprout logs, this entry seems to be the most relevant.

`01-02-2016 22:05:07.272 UTC Error dnscachedresolver.cpp:567: Failed to retrieve record for [2400:6180:0:e0::18b:2001]: Domain name not found`
(we have changed the IP address here)
Our `/etc/hosts` file has entries for our home-domain -> IPv6 address of our clearwater install.
Question: What is the best way to debug this? We are happy to provide any additional logs.
Thank you,
Marco and Amauri.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160203/05ddb496/attachment.html>

From Eleanor.Merry at metaswitch.com  Thu Feb  4 05:36:00 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Thu, 4 Feb 2016 10:36:00 +0000
Subject: [Clearwater] [Sprout] Memcache Error : Fail to read/write
In-Reply-To: <672cbbc5c38c422f99e31691f5bce7c1@XCH-RTP-004.cisco.com>
References: <CAN2RUgodSqC8phJrqYEbF8x+d6VCVBLt+13cCqHTC9+Cy-6MHg@mail.gmail.com>
	<71ED8D8361F0EF45BB6B949BFDCD46710107D37A6D@ENFICSMBX1.datcon.co.uk>
	<007301d15a06$c3fce710$4bf6b530$@tropo.com>
	<BLUPR0201MB1556280FDDBD23DFF1D32A5A90DB0@BLUPR0201MB1556.namprd02.prod.outlook.com>
	<672cbbc5c38c422f99e31691f5bce7c1@XCH-RTP-004.cisco.com>
Message-ID: <BY1PR02MB1260544F1C9953E8FE88C8E99BD10@BY1PR02MB1260.namprd02.prod.outlook.com>

Hi Kevin,

That's really strange. I've got a couple of questions:

Has your Sprout node ever had the IP address of the Homer node, or had its local_ip value in /etc/clearwater/local_config set to Homer's address.

What's the output of running "sudo clearwater-etcdctl get clearwater/<local site name - value of local_site_name in /etc/clearwater/local_config, or defaults to site1>/sprout/clustering/memcached" on your node? I'm expecting something looking like {"IP address": "normal"} - is the IP address Homer's address or Sprout's?

Thanks,

Ellie

-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Kevin High (khigh)
Sent: 29 January 2016 17:32
To: Matt Williams; Tropo - Kevin High
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] [Sprout] Memcache Error : Fail to read/write

The IP address in the cluster_settings file that was created during install is the IP address of the Homer node and not the Sprout node.  I had to manually change the IP address in that file in both installations in spite of the comments at the top of the file saying not to modify manually.

Thanks
Kevin


 

-----Original Message-----
From: Matt Williams [mailto:Matt.Williams at metaswitch.com]
Sent: Friday, January 29, 2016 2:07 AM
To: Tropo - Kevin High <khigh at tropo.com>
Cc: clearwater at lists.projectclearwater.org
Subject: RE: [Clearwater] [Sprout] Memcache Error : Fail to read/write

Kevin,

Yes, assuming you're using clearwater_cluster_manager (and we recommend that you do), it manages /etc/clearwater/cluster_settings - it uses etcd to synchronize its configuration.

What do you see in /etc/clearwater/cluster_settings that is wrong?  It's possible there's a bug in clearwater_cluster_manager that is generating the wrong configuration.

Please let me know.

Thanks,

Matt

--
?
Matt Williams
Lead Architect, Project Clearwater
+44 (0) 20 8366 1177


-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Kevin Heath High
Sent: 28 January 2016 20:02
To: Eleanor Merry <Eleanor.Merry at metaswitch.com>; clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] [Sprout] Memcache Error : Fail to read/write

I have run into this issue recently in two different installed (one I did, one somebody else did).  I both situations these were single sprout manual installs and both were resolved by modifying the cluster_settings file to point back to sprout.  According to the cluster_settings file it is getting created by clearwater_etcd.  Would be curious how this file is getting generated and if there is something I am doing wrong in the installation that is generating this issue.

Thanks
Kevin

-----Original Message-----
From: clearwater-bounces at lists.projectclearwater.org
[mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Eleanor Merry
Sent: Tuesday, January 27, 2015 10:30 AM
To: Sushant Hiray <hiraysushant at gmail.com>; clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] [Sprout] Memcache Error : Fail to read/write

Hi Sushant, 

Do you see this error on every register attempt, or is it intermittent? How many Sprouts do you have in your deployment?

Also, can you check that the /etc/clearwater/cluster_settings file on each Sprout has the correct values - this should be a single line of the form 'memcached_servers=<sprout 1 IP address>:11211,<sprout 2 IP
address>:11211,...', and the order of the IP addresses must be identical 
address>on
each node. 

Ellie

-----Original Message-----
From: clearwater-bounces at lists.projectclearwater.org
[mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sushant Hiray
Sent: 27 January 2015 09:35
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] [Sprout] Memcache Error : Fail to read/write

Hello,

We recently shifted our clearwater infrastructure to Open Stack.

We are facing an issue with the registration of a user.
The following is the log at Sprout with log_level=5

I've highlighted the relevant error zones:

REGISTER sip:ims.hom SIP/2.0
Via: SIP/2.0/TCP 192.168.1.41:54658
;rport;branch=z9hG4bKPjQhihJs.dF7ZqIk473L9ayKtDZZo-H3kw
Path: <sip:Y77aQms8YX at 192.168.1.41:5058;transport=TCP;lr;ob>
Via: SIP/2.0/UDP 192.168.1.39:5060
;rport=5060;received=192.168.1.39;branch=z9hG4bK107315516
From: <sip:user10 at ims.hom>;tag=1642292484
To: <sip:user10 at ims.hom>
Call-ID: 1374014318
CSeq: 1 REGISTER
Contact: <sip:user10 at 192.168.1.39:5060
;line=9002fc03804a36b>;+sip.instance="<urn:uuid:43be9c7e-a58e-11e4-955c-6ddd
4f8d4915>"
Authorization: Digest username="user10 at ims.hom", realm="ims.hom", nonce="
", uri="sip:ims.hom", response=" ",integrity-protected=ip-assoc-pending
Max-Forwards: 70
User-Agent: eXosip/3.1.0
Expires: 600000
Supported: path, gruu
P-Visited-Network-ID: ims.hom
Session-Expires: 600
Route: <sip:sprout.ims.hom:5054;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
26-01-2015 19:05:38.589 UTC Debug stack.cpp:470: Queuing cloned received message 0x7f8734097078 for worker threads
26-01-2015 19:05:38.589 UTC Debug stack.cpp:206: Worker thread dequeue message 0x7f8734097078
26-01-2015 19:05:38.589 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg REGISTER/cseq=1 (rdata0x7f8734097078)
26-01-2015 19:05:38.589 UTC Debug authentication.cpp:486: Authorization header in request
26-01-2015 19:05:38.589 UTC Debug memcachedstore.cpp:174: Key av\\user10 at ims.hom\  hashes to vbucket 41 via hash 0xbdd4da29
26-01-2015 19:05:38.589 UTC Debug memcachedstore.cpp:373: 1 read replicas for key av\\user10 at ims.hom\



*26-01-2015 19:05:38.589 UTC Debug memcachedstore.cpp:406: Attempt to read from replica 0 (connection 0x7f870c056860)26-01-2015 19:05:38.589 UTC Debug
memcachedstore.cpp:449: Read for av\\user10 at ims.hom\  on replica 0 returned error 47 (SERVER HAS FAILED AND IS DISABLED UNTIL TIMED RETRY)26-01-2015
19:05:38.589 UTC Error memcachedstore.cpp:519: Failed to read data for av\\user10 at ims.hom\  from 1 replicas*26-01-2015 19:05:38.589 UTC Debug
authentication.cpp:539: Verify authentication information in request
26-01-2015 19:05:38.589 UTC Warning authentication.cpp:204: Received an authentication request for user10 at ims.hom with nonce  , but no matching AV found
26-01-2015 19:05:38.589 UTC Debug acr.cpp:48: Created ACR (0x7f870c1aa2b0)
26-01-2015 19:05:38.589 UTC Debug authentication.cpp:644: No authentication information in request or stale nonce, so reject with challenge
26-01-2015 19:05:38.589 UTC Debug pjsip:       endpoint Response msg
401/REGISTER/cseq=1 (tdta0x7f870c1b6510) created
26-01-2015 19:05:38.589 UTC Debug pjutils.cpp:461: Private identity from authorization header = user10 at ims.hom
26-01-2015 19:05:38.589 UTC Debug httpresolver.cpp:70:
HttpResolver::resolve for host hs.ims.hom, port 8888, family 2
26-01-2015 19:05:38.589 UTC Debug baseresolver.cpp:501: Attempt to parse hs.ims.hom as IP address
26-01-2015 19:05:38.589 UTC Debug dnscachedresolver.cpp:179: Pulling 1 records from cache for hs.ims.hom A
26-01-2015 19:05:38.589 UTC Debug baseresolver.cpp:349: Found 1 A/AAAA records, randomizing
26-01-2015 19:05:38.589 UTC Debug baseresolver.cpp:491: 192.168.1.43:8888 transport 6 is not blacklisted
26-01-2015 19:05:38.589 UTC Debug baseresolver.cpp:370: Added a server, now have 1 of 5
26-01-2015 19:05:38.589 UTC Debug baseresolver.cpp:408: Adding 0 servers from blacklist
26-01-2015 19:05:38.589 UTC Debug baseresolver.cpp:501: Attempt to parse
192.168.1.43 as IP address
26-01-2015 19:05:38.589 UTC Debug httpconnection.cpp:553: Sending HTTP request :
http://hs.ims.hom:8888/impi/user10%40ims.hom/av?impu=sip%3Auser10%40ims.hom
(trying 192.168.1.43)
26-01-2015 19:05:38.591 UTC Debug httpconnection.cpp:568: Received HTTP
response: status=200,
doc={"digest":{"ha1":"4a4a6a1e2aad77dcb54cb43acd17c59d","realm":"ims.hom","q
op":"auth"}}
26-01-2015 19:05:38.591 UTC Debug authentication.cpp:146: Digest specified
26-01-2015 19:05:38.591 UTC Debug authentication.cpp:309: Valid AV - generate challenge
26-01-2015 19:05:38.591 UTC Debug authentication.cpp:315: Create WWW-Authenticate header
26-01-2015 19:05:38.591 UTC Debug authentication.cpp:359: Add Digest information
26-01-2015 19:05:38.591 UTC Debug authentication.cpp:386: Write AV to store
26-01-2015 19:05:38.591 UTC Debug avstore.cpp:66: Set AV for user10 at ims.hom \2ebbd3a10aba9cfa
{"branch":"z9hG4bKPjQhihJs.dF7ZqIk473L9ayKtDZZo-H3kw","digest":{"ha1":"4a4a6
a1e2aad77dcb54cb43acd17c59d","qop":"auth","realm":"ims.hom"}}

26-01-2015 19:05:38.591 UTC Debug memcachedstore.cpp:540: Writing 138 bytes to table av key user10 at ims.hom\2ebbd3a10aba9cfa, CAS = 0, expiry = 40
26-01-2015 19:05:38.591 UTC Debug memcachedstore.cpp:174: Key av\\user10 at ims.hom\2ebbd3a10aba9cfa hashes to vbucket 34 via hash 0x325860a2
26-01-2015 19:05:38.591 UTC Debug memcachedstore.cpp:560: 1 write replicas for key av\\user10 at ims.hom\2ebbd3a10aba9cfa
26-01-2015 19:05:38.591 UTC Debug memcachedstore.cpp:609: Attempt conditional write to replica 0 (connection 0x7f870c056860), CAS = 0




*26-01-2015 19:05:38.591 UTC Debug memcachedstore.cpp:652: memcached_add command for av\\user10 at ims.hom\2ebbd3a10aba9cfa failed on replica 0, rc =
47 (SERVER HAS FAILED AND IS DISABLED UNTIL TIMED RETRY), expiry =
40(140217998993504) SERVER HAS FAILED AND IS DISABLED UNTIL TIMED RETRY,
host: 10.129.2.112:11211 <http://10.129.2.112:11211> ->
libmemcached/connect.cc:63326-01-2015 19:05:38.591 UTC Error
memcachedstore.cpp:713: Failed to write data for av\\user10 at ims.hom\2ebbd3a10aba9cfa to 1 replicas26-01-2015 19:05:38.591 UTC Error avstore.cpp:71: Failed to write Authentication Vector for private_id
user10 at ims.hom*26-01-2015 19:05:38.591 UTC Verbose
stack.cpp:259: TX 545 bytes Response msg 401/REGISTER/cseq=1
(tdta0x7f870c1b6510) to TCP 192.168.1.41:54658:
--start msg--

SIP/2.0 401 Unauthorized
Via: SIP/2.0/TCP 192.168.1.41:54658
;rport=54658;received=192.168.1.41;branch=z9hG4bKPjQhihJs.dF7ZqIk473L9ayKtDZ
Zo-H3kw
Via: SIP/2.0/UDP 192.168.1.39:5060
;rport=5060;received=192.168.1.39;branch=z9hG4bK107315516
Call-ID: 1374014318
From: <sip:user10 at ims.hom>;tag=1642292484
To: <sip:user10 at ims.hom>;tag=z9hG4bKPjQhihJs.dF7ZqIk473L9ayKtDZZo-H3kw
CSeq: 1 REGISTER
WWW-Authenticate: Digest
realm="ims.hom",nonce="2ebbd3a10aba9cfa",opaque="1fafe52f54443250",stale=tru
e,algorithm=MD5,qop="auth"
Content-Length:  0


--end msg--
26-01-2015 19:05:38.591 UTC Debug pjsip: tdta0x7f870c1b Destroying txdata Response msg 401/REGISTER/cseq=1 (tdta0x7f870c1b6510)
26-01-2015 19:05:38.591 UTC Debug acr.cpp:82: Sending Null ACR
(0x7f870c1aa2b0)
26-01-2015 19:05:38.591 UTC Debug acr.cpp:53: Destroyed ACR (0x7f870c1aa2b0)
26-01-2015 19:05:38.591 UTC Debug stack.cpp:208: Worker thread completed processing message 0x7f8734097078
26-01-2015 19:05:38.591 UTC Debug stack.cpp:214: Request latency = 1927us
26-01-2015 19:05:38.593 UTC Debug pjsip: sip_endpoint.c Processing incoming
message: Request msg REGISTER/cseq=1 (rdata0x7f873400d328)
26-01-2015 19:05:38.593 UTC Verbose stack.cpp:243: RX 848 bytes Request msg
REGISTER/cseq=1 (rdata0x7f873400d328) from TCP 192.168.1.41:42193:
--start msg--

REGISTER sip:ims.hom SIP/2.0
Via: SIP/2.0/TCP 192.168.1.41:42193
;rport;branch=z9hG4bKPjqvLAEWzN3k7PI663pnNLz6zgU9Sk5XJ6
Path: <sip:Y77aQms8YX at 192.168.1.41:5058;transport=TCP;lr;ob>
Via: SIP/2.0/UDP 192.168.1.39:5060
;rport=5060;received=192.168.1.39;branch=z9hG4bK1731639894
From: <sip:user10 at ims.hom>;tag=1334365898
To: <sip:user10 at ims.hom>
Call-ID: 1090655798
CSeq: 1 REGISTER
Contact: <sip:user10 at 192.168.1.39:5060;line=9002fc03804a36b>
Authorization: Digest username="user10 at ims.hom", realm="ims.hom", nonce="2ebbd3a10aba9cfa", uri="sip:ims.hom", response="6da3503d6706fab2f9250a96ef9136b8",
algorithm=MD5,integrity-protected=ip-assoc-pending
Max-Forwards: 70
User-Agent: eXosip/3.1.0
Expires: 600000
P-Visited-Network-ID: ims.hom
Session-Expires: 600
Route: <sip:sprout.ims.hom:5054;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
26-01-2015 19:05:38.593 UTC Debug stack.cpp:470: Queuing cloned received message 0x7f8734097078 for worker threads
26-01-2015 19:05:38.593 UTC Debug stack.cpp:206: Worker thread dequeue message 0x7f8734097078
26-01-2015 19:05:38.593 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg REGISTER/cseq=1 (rdata0x7f8734097078)
26-01-2015 19:05:38.593 UTC Debug authentication.cpp:486: Authorization header in request
26-01-2015 19:05:38.593 UTC Debug memcachedstore.cpp:174: Key av\\user10 at ims.hom\2ebbd3a10aba9cfa hashes to vbucket 34 via hash 0x325860a2
26-01-2015 19:05:38.593 UTC Debug memcachedstore.cpp:373: 1 read replicas for key av\\user10 at ims.hom\2ebbd3a10aba9cfa
26-01-2015 19:05:38.593 UTC Debug memcachedstore.cpp:406: Attempt to read from replica 0 (connection 0x7f87800cea30)


*26-01-2015 19:05:38.593 UTC Debug memcachedstore.cpp:449: Read for av\\user10 at ims.hom\2ebbd3a10aba9cfa on replica 0 returned error 47 (SERVER HAS FAILED AND IS DISABLED UNTIL TIMED RETRY)26-01-2015 19:05:38.593 UTC Error memcachedstore.cpp:519: Failed to read data for av\\user10 at ims.hom\2ebbd3a10aba9cfa from 1 replicas*26-01-2015 19:05:38.593 UTC Debug authentication.cpp:539: Verify authentication information in request
26-01-2015 19:05:38.593 UTC Warning authentication.cpp:204: Received an authentication request for user10 at ims.hom with nonce 2ebbd3a10aba9cfa, but no matching AV found
26-01-2015 19:05:38.593 UTC Debug acr.cpp:48: Created ACR (0x7f878014fea0)
26-01-2015 19:05:38.593 UTC Debug authentication.cpp:644: No authentication information in request or stale nonce, so reject with challenge
26-01-2015 19:05:38.593 UTC Debug pjsip:       endpoint Response msg
401/REGISTER/cseq=1 (tdta0x7f87801a2070) created
26-01-2015 19:05:38.593 UTC Debug pjutils.cpp:461: Private identity from authorization header = user10 at ims.hom
26-01-2015 19:05:38.593 UTC Debug httpresolver.cpp:70:
HttpResolver::resolve for host hs.ims.hom, port 8888, family 2
26-01-2015 19:05:38.593 UTC Debug baseresolver.cpp:501: Attempt to parse hs.ims.hom as IP address
26-01-2015 19:05:38.593 UTC Debug dnscachedresolver.cpp:179: Pulling 1 records from cache for hs.ims.hom A
26-01-2015 19:05:38.593 UTC Debug baseresolver.cpp:349: Found 1 A/AAAA records, randomizing
26-01-2015 19:05:38.593 UTC Debug baseresolver.cpp:491: 192.168.1.43:8888 transport 6 is not blacklisted
26-01-2015 19:05:38.593 UTC Debug baseresolver.cpp:370: Added a server, now have 1 of 5
26-01-2015 19:05:38.593 UTC Debug baseresolver.cpp:408: Adding 0 servers from blacklist
26-01-2015 19:05:38.593 UTC Debug baseresolver.cpp:501: Attempt to parse
192.168.1.43 as IP address
26-01-2015 19:05:38.593 UTC Debug httpconnection.cpp:553: Sending HTTP request :
http://hs.ims.hom:8888/impi/user10%40ims.hom/av?impu=sip%3Auser10%40ims.hom
(trying 192.168.1.43)
26-01-2015 19:05:38.594 UTC Debug httpconnection.cpp:568: Received HTTP
response: status=200,
doc={"digest":{"ha1":"4a4a6a1e2aad77dcb54cb43acd17c59d","realm":"ims.hom","q
op":"auth"}}
26-01-2015 19:05:38.594 UTC Debug authentication.cpp:146: Digest specified
26-01-2015 19:05:38.594 UTC Debug authentication.cpp:309: Valid AV - generate challenge
26-01-2015 19:05:38.594 UTC Debug authentication.cpp:315: Create WWW-Authenticate header
26-01-2015 19:05:38.594 UTC Debug authentication.cpp:359: Add Digest information
26-01-2015 19:05:38.595 UTC Debug authentication.cpp:386: Write AV to store
26-01-2015 19:05:38.595 UTC Debug avstore.cpp:66: Set AV for user10 at ims.hom
\256f5de87094a4d0
{"branch":"z9hG4bKPjqvLAEWzN3k7PI663pnNLz6zgU9Sk5XJ6","digest":{"ha1":"4a4a6
a1e2aad77dcb54cb43acd17c59d","qop":"auth","realm":"ims.hom"}}

26-01-2015 19:05:38.595 UTC Debug memcachedstore.cpp:540: Writing 138 bytes to table av key user10 at ims.hom\256f5de87094a4d0, CAS = 0, expiry = 40
26-01-2015 19:05:38.595 UTC Debug memcachedstore.cpp:174: Key
av\\user10 at ims.hom\256f5de87094a4d0 hashes to vbucket 127 via hash 0x8c22357f
26-01-2015 19:05:38.595 UTC Debug memcachedstore.cpp:560: 1 write replicas for key av\\user10 at ims.hom\256f5de87094a4d0
26-01-2015 19:05:38.595 UTC Debug memcachedstore.cpp:609: Attempt conditional write to replica 0 (connection 0x7f87800cea30), CAS = 0





*26-01-2015 19:05:38.595 UTC Debug memcachedstore.cpp:652: memcached_add command for av\\user10 at ims.hom\256f5de87094a4d0 failed on replica 0, rc =
47 (SERVER HAS FAILED AND IS DISABLED UNTIL TIMED RETRY), expiry =
40(140219945642544) SERVER HAS FAILED AND IS DISABLED UNTIL TIMED RETRY,
host: 10.129.2.112:11211 <http://10.129.2.112:11211> ->
libmemcached/connect.cc:63326-01-2015 19:05:38.595 UTC Error
memcachedstore.cpp:713: Failed to write data for
av\\user10 at ims.hom\256f5de87094a4d0 to 1 replicas26-01-2015 19:05:38.595 UTC Error avstore.cpp:71: Failed to write Authentication Vector for private_id
user10 at ims.hom26-01-2015 19:05:38.595 UTC Verbose stack.cpp:259:
TX 546 bytes Response msg 401/REGISTER/cseq=1 (tdta0x7f87801a2070) to TCP
192.168.1.41:42193 <http://192.168.1.41:42193>:*

The Memcached server is unable to read/write the data and hence we are facing the problem.
Can you please suggest some steps so that we can curb this issue.

Thanks,
Sushant Hiray,
Senior Undergrad CSE,
IIT Bombay
_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/listinfo/clearwater
_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/listinfo/clearwater


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



From carlasauvanaud at wanadoo.fr  Thu Feb  4 09:04:10 2016
From: carlasauvanaud at wanadoo.fr (carlasauvanaud at wanadoo.fr)
Date: Thu, 4 Feb 2016 15:04:10 +0100 (CET)
Subject: [Clearwater] SIP stress call rate
In-Reply-To: <128792603.7850.1454495040599.JavaMail.www@wwinf1m14>
References: <128792603.7850.1454495040599.JavaMail.www@wwinf1m14>
Message-ID: <1528428447.15149.1454594650115.JavaMail.www@wwinf1c12>

Hi all,
?
I resolved my problem by setting the first line of the generated (by /usr/share/clearwater/infrastructure/scripts/sip-stress) user file as "RANDOM".
Otherwise the SIP messages was sended we a null $my_dn variable, I do not know why though.
?
Carla
?
?
?
?
> Message du 03/02/16 11:24
> De : "carlasauvanaud at wanadoo.fr" 
> A : clearwater at lists.projectclearwater.org
> Copie ? : 
> Objet : [Clearwater] SIP stress call rate
> 
>
> Hi list,

I am trying to test my manual installation of Clearwater using sip-stress.
When running the default scenario with the default sipp command in '/usr/share/clearwater/bin/sip-stress' every thing seems OK.
However, each time I remove the '-users $(...)' option and add a "-r $call_rate" (where I defined before callrate=10) in the sipp command, the default scenario fails at receiving the first 401 response of the first REGISTER with a timeout (no matters the recv_timeout).
> (In the sipp.out log, I see that the call rate is however well defined.)

Do you have any idea what I am doing wrong ?

Yours sincerely,
Carla



_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160204/3bb2613c/attachment.html>

From marco at opencloud.com  Thu Feb  4 16:38:14 2016
From: marco at opencloud.com (Marco Costantini)
Date: Fri, 5 Feb 2016 10:38:14 +1300
Subject: [Clearwater] Redeploy Sprout in All-In-One install
Message-ID: <CAMVsSNQaRTdcugDS+NxxEdBDwgBb6xHkVMAN11CL0GTtNFvBLA@mail.gmail.com>

Hi there,
I want to test a local change to Sprout's code. It pertains to an issue
which I think is now tracked by MS.

Question: How deploy a locally built Sprout onto a local All-in-one install?

I'd prefer to continue using the All-In-One install if that's at all
possible.

Please and Thanks,
Marco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160205/56982945/attachment.html>

From rodrigo.moreira206 at gmail.com  Fri Feb  5 09:57:25 2016
From: rodrigo.moreira206 at gmail.com (Rodrigo Moreira)
Date: Fri, 5 Feb 2016 12:57:25 -0200
Subject: [Clearwater] [Error installing clearwater-managemen]
Message-ID: <CAGRP3yNoQS+o4Pnuc_03_8y1isJdNLFXHyetvwvdFSWp1sZV9Q@mail.gmail.com>

Can anyone help me with this error when installing the module Ralf. When I
install via apt-get clearwater-management it returns the error message. I'm
grateful.



Extracting pyzmq-15.2.0-py2.7-linux-x86_64.egg to
/usr/share/clearwater/clearwater-queue-manager/env/lib/python2.7/site-packages
  File
"/usr/share/clearwater/clearwater-queue-manager/env/lib/python2.7/site-packages/pyzmq-15.2.0-py2.7-linux-x86_64.egg/zmq/asyncio.py",
line 79
    "{!r}".format(fileobj)) from None



I'm grateful.
-- 
Rodrigo M.
(37) 9132-4539
(34) 9889-3069
rodrigo.moreira2007
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160205/57ddc891/attachment.html>

From Eleanor.Merry at metaswitch.com  Mon Feb  8 11:59:20 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Mon, 8 Feb 2016 16:59:20 +0000
Subject: [Clearwater] [Error installing clearwater-managemen]
In-Reply-To: <CAGRP3yNoQS+o4Pnuc_03_8y1isJdNLFXHyetvwvdFSWp1sZV9Q@mail.gmail.com>
References: <CAGRP3yNoQS+o4Pnuc_03_8y1isJdNLFXHyetvwvdFSWp1sZV9Q@mail.gmail.com>
Message-ID: <BN3PR02MB1255A8560A8E6DCCCDDCBB059BD50@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Rodrigo,

I think this is benign. It looks like https://github.com/Metaswitch/clearwater-etcd/issues/258 (which is due to pyzmq warning about some of its files not being valid in python 2.7, but these files aren?t used in our install).

Are you seeing any issues with the install? Did it complete successfully?

Thanks,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Rodrigo Moreira
Sent: 05 February 2016 14:57
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] [Error installing clearwater-managemen]

Can anyone help me with this error when installing the module Ralf. When I install via apt-get clearwater-management it returns the error message. I'm grateful.



Extracting pyzmq-15.2.0-py2.7-linux-x86_64.egg to /usr/share/clearwater/clearwater-queue-manager/env/lib/python2.7/site-packages
  File "/usr/share/clearwater/clearwater-queue-manager/env/lib/python2.7/site-packages/pyzmq-15.2.0-py2.7-linux-x86_64.egg/zmq/asyncio.py", line 79
    "{!r}".format(fileobj)) from None

I'm grateful.
--
Rodrigo M.
(37) 9132-4539
(34) 9889-3069
[Image removed by sender.]rodrigo.moreira2007


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160208/0b805dc3/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 344 bytes
Desc: image001.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160208/0b805dc3/attachment.jpg>

From Eleanor.Merry at metaswitch.com  Mon Feb  8 12:07:34 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Mon, 8 Feb 2016 17:07:34 +0000
Subject: [Clearwater] Redeploy Sprout in All-In-One install
In-Reply-To: <CAMVsSNQaRTdcugDS+NxxEdBDwgBb6xHkVMAN11CL0GTtNFvBLA@mail.gmail.com>
References: <CAMVsSNQaRTdcugDS+NxxEdBDwgBb6xHkVMAN11CL0GTtNFvBLA@mail.gmail.com>
Message-ID: <BN3PR02MB1255011BCC3FD8380191E98E9BD50@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Marco,

The easiest way is to build a debian package, and then install that on your all-in-one node.

You can build a debian package by running ?make deb? in your top level Sprout directory. The built debian packages end up in the parent directory of your top level sprout directory (please see https://github.com/Metaswitch/sprout/blob/dev/docs/Development.md#building-debian-packages for more information).

Hope this helps,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Marco Costantini
Sent: 04 February 2016 21:38
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] Redeploy Sprout in All-In-One install

Hi there,
I want to test a local change to Sprout's code. It pertains to an issue which I think is now tracked by MS.
Question: How deploy a locally built Sprout onto a local All-in-one install?
I'd prefer to continue using the All-In-One install if that's at all possible.
Please and Thanks,
Marco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160208/71ea1345/attachment.html>

From Chris.Elford at metaswitch.com  Wed Feb 10 07:02:46 2016
From: Chris.Elford at metaswitch.com (Chris Elford)
Date: Wed, 10 Feb 2016 12:02:46 +0000
Subject: [Clearwater] FW: Wuthering Heights release note for review
In-Reply-To: <BLUPR0201MB15082FD7E75BD46E9DEF3E6EE2D60@BLUPR0201MB1508.namprd02.prod.outlook.com>
References: <BLUPR0201MB1843155D897D382197A34BAEE1D60@BLUPR0201MB1843.namprd02.prod.outlook.com>
	<BLUPR0201MB15082FD7E75BD46E9DEF3E6EE2D60@BLUPR0201MB1508.namprd02.prod.outlook.com>
Message-ID: <SN1PR0201MB185627272806D4DAF3C86CB7E1D70@SN1PR0201MB1856.namprd02.prod.outlook.com>

The release for Project Clearwater sprint "X Stands for Unknown" has been cut. The code for this release is tagged as release-91 in GitHub.

This release includes the following bug fixes:

*         init.d script doesn't set PID file directory ownership if the directory already exists (https://github.com/Metaswitch/sprout/issues/1304)

*         No cached av found in Database impi  (https://github.com/Metaswitch/homestead/issues/298)

*         init.d script doesn't set PID file directory ownership if the directory already exists (https://github.com/Metaswitch/homestead/issues/296, https://github.com/Metaswitch/ralf/issues/197)

*         If opening a log file fails, we might not retry for a long time (https://github.com/Metaswitch/cpp-common/issues/383)

*         Memcached TCP connections still try to be used after they've closed (https://github.com/Metaswitch/cpp-common/issues/286)

*         Cassandra failed plugin doesn't work for multiple networks (https://github.com/Metaswitch/clearwater-cassandra/issues/69)

*         Diags-gathering scripts should redirect stderr (https://github.com/Metaswitch/clearwater-infrastructure/issues/251)

*         SNMP alarm agent permanently stopped on install (https://github.com/Metaswitch/clearwater-snmp-handlers/issues/118)

*         Tests in clearwarer-snmp-handlers and clearwater-fvtest create a SNMP agent on the same address. (https://github.com/Metaswitch/clearwater-snmp-handlers/issues/116)

*         missing command (https://github.com/Metaswitch/clearwater-readthedocs/issues/141)

*         SNMP docs confuse 'current' and 'previous' (https://github.com/Metaswitch/clearwater-readthedocs/issues/140)

*         init.d script doesn't set PID file directory ownership if the directory already exists (https://github.com/Metaswitch/astaire/issues/45)

*         clearwater-etcd.log gets deleted by the clearwater-etcd-log-cleanup script (https://github.com/Metaswitch/clearwater-etcd/issues/253)

*         clearwater-queue-manager.monit uses the wrong alarm (https://github.com/Metaswitch/clearwater-etcd/issues/251)

*         Queue Manager - Homer restart loop (https://github.com/Metaswitch/clearwater-etcd/issues/249)

*         Uninstalling leaves clearwater-etcd user and group (https://github.com/Metaswitch/clearwater-etcd/issues/247)

*         init.d script malfunctions if etcd_cluster variable contains spaces (https://github.com/Metaswitch/clearwater-etcd/issues/237)

*         It would be handy if check_config_state had an option to show diffs between local config files and those in etcd (https://github.com/Metaswitch/clearwater-etcd/issues/170)

*         Cassandra replication strategy for GR deployments is SimpleStrategy (https://github.com/Metaswitch/clearwater-etcd/issues/114)

*         On install, one node must run `upload_shared_config` and `apply_shared_config` (https://github.com/Metaswitch/clearwater-etcd/issues/62)


For upgrading to this release, follow the instructions at http://clearwater.readthedocs.org/en/latest/Upgrading_a_Clearwater_deployment/index.html. If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.



Thanks,
Chris





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160210/0aac5ffc/attachment.html>

From marco at opencloud.com  Wed Feb 10 16:17:28 2016
From: marco at opencloud.com (Marco Costantini)
Date: Thu, 11 Feb 2016 10:17:28 +1300
Subject: [Clearwater] Sprout eats INVITE
Message-ID: <CAMVsSNS5UQ-30m5gMRfUbhHBV0Wu0UbEDYqyQB4Ng2VyD27KZg@mail.gmail.com>

Hello,

We are testing IPv6 calls and have run into a problem. I'll describe now.
Any ideas for a solution would be appreciated.

The environment is as such:
- one machine has an All-in-one install (with a redeployed sprout-base
package). this is configured for IPv6
- two other machines each are running PJSUA sip client and, here, are
trying to call each other through the proxy on the clearwater machine.

I've attached a log from sprout, starting from just before the registration
of the two endpoints. My problem is that the logging doesn't really explain
why its not forwarding the message. I've done packet captures on all
machines and it seems to me that sprout is the last stop of the incoming
invite, and no outgoing invite exists on the wire.

Please and thank you,
Marco.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160211/534e506e/attachment.html>
-------------- next part --------------
tail -f /var/log/sprout/*current.txt
==> /var/log/sprout/access_current.txt <==
10-02-2016 21:05:46.186 UTC 200 GET /ping 0.000000 seconds
10-02-2016 21:05:56.251 UTC 200 GET /ping 0.000000 seconds
10-02-2016 21:06:06.392 UTC 200 GET /ping 0.000000 seconds
10-02-2016 21:06:16.351 UTC 200 GET /ping 0.000000 seconds
10-02-2016 21:06:26.458 UTC 200 GET /ping 0.000000 seconds
10-02-2016 21:06:36.611 UTC 200 GET /ping 0.000000 seconds
10-02-2016 21:06:46.549 UTC 200 GET /ping 0.000000 seconds
10-02-2016 21:06:56.761 UTC 200 GET /ping 0.000000 seconds
10-02-2016 21:07:06.690 UTC 200 GET /ping 0.000000 seconds
10-02-2016 21:07:16.742 UTC 200 GET /ping 0.000000 seconds

==> /var/log/sprout/log_current.txt <==
10-02-2016 04:03:40.736 UTC Registration: USER_URI=sip:6505550098 at example.com BINDING_ID=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob CONTACT_URI=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob EXPIRES=300
10-02-2016 04:08:28.703 UTC Registration: USER_URI=sip:6505550097 at example.com BINDING_ID=sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob CONTACT_URI=sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob EXPIRES=300
10-02-2016 04:08:35.771 UTC Registration: USER_URI=sip:6505550098 at example.com BINDING_ID=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob CONTACT_URI=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob EXPIRES=300
10-02-2016 04:13:30.811 UTC Registration: USER_URI=sip:6505550098 at example.com BINDING_ID=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob CONTACT_URI=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob EXPIRES=300
10-02-2016 04:14:24.280 UTC Registration: USER_URI=sip:6505550097 at example.com BINDING_ID=sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob CONTACT_URI=sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob EXPIRES=300
10-02-2016 04:15:09.524 UTC Registration: USER_URI=sip:6505550098 at example.com BINDING_ID=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob CONTACT_URI=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob EXPIRES=300
10-02-2016 04:19:19.330 UTC Registration: USER_URI=sip:6505550097 at example.com BINDING_ID=sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob CONTACT_URI=sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob EXPIRES=300
10-02-2016 04:20:04.583 UTC Registration: USER_URI=sip:6505550098 at example.com BINDING_ID=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob CONTACT_URI=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob EXPIRES=300
10-02-2016 04:24:14.374 UTC Registration: USER_URI=sip:6505550097 at example.com BINDING_ID=sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob CONTACT_URI=sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob EXPIRES=300
10-02-2016 04:24:59.613 UTC Registration: USER_URI=sip:6505550098 at example.com BINDING_ID=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob CONTACT_URI=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob EXPIRES=300

==> /var/log/sprout/sprout_current.txt <==
--end msg--
10-02-2016 21:07:16.701 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
10-02-2016 21:07:16.701 UTC Debug pjsip: tdta0x7f07b000 Destroying txdata Response msg 200/OPTIONS/cseq=1856 (tdta0x7f07b0009fd0)
10-02-2016 21:07:16.701 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b80b5058
10-02-2016 21:07:16.701 UTC Debug thread_dispatcher.cpp:199: Request latency = 290us
10-02-2016 21:07:16.742 UTC Verbose httpstack.cpp:286: Process request for URL /ping, args (null)
10-02-2016 21:07:16.742 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
10-02-2016 21:07:17.701 UTC Verbose pjsip: tcps0x7f07b80b TCP connection closed
10-02-2016 21:07:17.701 UTC Debug connection_tracker.cpp:91: Connection 0x7f07b80b27b8 has been destroyed
10-02-2016 21:07:17.701 UTC Verbose pjsip: tcps0x7f07b80b TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
10-02-2016 21:07:20.684 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg REGISTER/cseq=14995 (rdata0x7f07b80f35e0)
10-02-2016 21:07:20.684 UTC Verbose common_sip_processing.cpp:120: RX 909 bytes Request msg REGISTER/cseq=14995 (rdata0x7f07b80f35e0) from TCP 2400:6180:0:d0::18b:3001:38920:
--start msg--

REGISTER sip:example.com SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001]:38920;rport;branch=z9hG4bKPjjroq9YI-eUVuYSb0le9cNDFAuRIBJ0c1
Path: <sip:jOkcJw+mvT@[2400:6180:0:d0::18b:3001]:5058;transport=TCP;lr;ob>
Via: SIP/2.0/UDP [2400:6180:0:d0::160:c001]:5070;rport=5070;received=2400:6180:0:d0::160:c001;branch=z9hG4bKPjVZWvSTGX0sLQG9ITOrWb5ZOOSbaHLRvD
Max-Forwards: 70
From: <sip:6505550098 at example.com>;tag=qcXJhCkJRbQCI9LAn6Dn2GYlCnKaH-vr
To: <sip:6505550098 at example.com>
Call-ID: bycgxu2h8r0Lt282qrPmn7.zbVUHGWJg
CSeq: 14995 REGISTER
User-Agent: PJSUA v2.4.5 Linux-3.13.0.71/x86_64/glibc-2.19
Contact: <sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob>
Expires: 300
Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, INFO, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
P-Visited-Network-ID: example.com
Route: <sip:[2400:6180:0:d0::18b:3001]:5054;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
10-02-2016 21:07:20.684 UTC Debug pjutils.cpp:1648: Logging SAS Call-ID marker, Call-ID bycgxu2h8r0Lt282qrPmn7.zbVUHGWJg
10-02-2016 21:07:20.684 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b80b5838 for worker threads
10-02-2016 21:07:20.684 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b80b5838
10-02-2016 21:07:20.684 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg REGISTER/cseq=14995 (rdata0x7f07b80b5838)
10-02-2016 21:07:20.685 UTC Debug uri_classifier.cpp:167: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:20.685 UTC Debug uri_classifier.cpp:197: Classified URI as 4
10-02-2016 21:07:20.685 UTC Debug authentication.cpp:673: Authentication module invoked
10-02-2016 21:07:20.685 UTC Debug authentication.cpp:687: Request needs authentication
10-02-2016 21:07:20.685 UTC Debug acr.cpp:49: Created ACR (0x7f07b80da9a0)
10-02-2016 21:07:20.685 UTC Debug authentication.cpp:889: No authentication information in request or stale nonce, so reject with challenge
10-02-2016 21:07:20.685 UTC Debug pjsip:       endpoint Response msg 401/REGISTER/cseq=14995 (tdta0x7f07b811c5d0) created
10-02-2016 21:07:20.685 UTC Debug pjutils.cpp:423: Private identity defaulted from public identity = 6505550098 at example.com
10-02-2016 21:07:20.686 UTC Debug httpconnection.cpp:183: Allocated CURL handle 0x7f07b811d5e0
10-02-2016 21:07:20.687 UTC Debug httpresolver.cpp:71: HttpResolver::resolve for host [2400:6180:0:d0::18b:3001], port 8888, family 10
10-02-2016 21:07:20.687 UTC Debug baseresolver.cpp:513: Attempt to parse [2400:6180:0:d0::18b:3001] as IP address
10-02-2016 21:07:20.687 UTC Debug httpresolver.cpp:79: Target is an IP address
10-02-2016 21:07:20.687 UTC Debug httpconnection.cpp:623: Sending HTTP request : http://[2400:6180:0:d0::18b:3001]:8888/impi/6505550098%40example.com/av?impu=sip%3A6505550098%40example.com (trying 2400:6180:0:d0::18b:3001) on new connection
10-02-2016 21:07:20.765 UTC Debug httpconnection.cpp:638: Received HTTP response: status=200, doc={"digest":{"ha1":"ed7e862749772ad47596b5bd3c0f2e43","realm":"example.com","qop":"auth"}}
10-02-2016 21:07:20.765 UTC Debug communicationmonitor.cpp:82: Checking communication changes - successful attempts 1, failures 0
10-02-2016 21:07:20.765 UTC Debug authentication.cpp:159: Verifying AV: {"digest":{"ha1":"ed7e862749772ad47596b5bd3c0f2e43","realm":"example.com","qop":"auth"}}
10-02-2016 21:07:20.765 UTC Debug authentication.cpp:186: Digest specified
10-02-2016 21:07:20.765 UTC Debug authentication.cpp:374: Valid AV - generate challenge
10-02-2016 21:07:20.765 UTC Debug authentication.cpp:383: Create WWW-Authenticate header
10-02-2016 21:07:20.765 UTC Debug authentication.cpp:451: Add Digest information
10-02-2016 21:07:20.765 UTC Debug authentication.cpp:493: Write AV to store
10-02-2016 21:07:20.765 UTC Debug avstore.cpp:72: Set AV for 6505550098 at example.com\131ff1d565852607
{"digest":{"ha1":"ed7e862749772ad47596b5bd3c0f2e43","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjjroq9YI-eUVuYSb0le9cNDFAuRIBJ0c1"}
10-02-2016 21:07:20.765 UTC Debug memcachedstore.cpp:542: Writing 141 bytes to table av key 6505550098 at example.com\131ff1d565852607, CAS = 0, expiry = 40
10-02-2016 21:07:20.765 UTC Debug memcachedstore.cpp:195: Key av\\6505550098 at example.com\131ff1d565852607 hashes to vbucket 12 via hash 0x1f60a18c
10-02-2016 21:07:20.765 UTC Debug memcachedstore.cpp:236: Set up new view 1 for thread
10-02-2016 21:07:20.765 UTC Debug memcachedstore.cpp:243: Setting up server 0 for connection 0x7f07b8169990 (--CONNECT-TIMEOUT=10 --SUPPORT-CAS --POLL-TIMEOUT=250 --BINARY-PROTOCOL)
10-02-2016 21:07:20.766 UTC Debug memcachedstore.cpp:245: Set up connection 0x7f07b8169a00 to server [2400:6180:0:d0::18b:3001]:11211
10-02-2016 21:07:20.766 UTC Debug memcachedstore.cpp:256: Setting server to IP address 2400:6180:0:d0::18b:3001 port 11211
10-02-2016 21:07:20.766 UTC Debug memcachedstore.cpp:562: 1 write replicas for key av\\6505550098 at example.com\131ff1d565852607
10-02-2016 21:07:20.766 UTC Debug memcachedstore.cpp:616: Attempt conditional write to vbucket 12 on replica 0 (connection 0x7f07b8169a00), CAS = 0, expiry = 40
10-02-2016 21:07:20.766 UTC Debug memcachedstore.cpp:816: Attempting to add data for key av\\6505550098 at example.com\131ff1d565852607
10-02-2016 21:07:20.766 UTC Debug memcachedstore.cpp:826: Attempting memcached ADD command
10-02-2016 21:07:20.767 UTC Debug memcachedstore.cpp:916: ADD/CAS returned rc = 0 (SUCCESS)
SUCCESS
10-02-2016 21:07:20.767 UTC Debug memcachedstore.cpp:657: Conditional write succeeded to replica 0
10-02-2016 21:07:20.767 UTC Debug communicationmonitor.cpp:82: Checking communication changes - successful attempts 1, failures 0
10-02-2016 21:07:20.767 UTC Debug authentication.cpp:503: Sending {"impi": "6505550098 at example.com", "impu": "sip:6505550098 at example.com", "nonce": "131ff1d565852607"} to Chronos to set AV timer
10-02-2016 21:07:20.768 UTC Debug httpconnection.cpp:183: Allocated CURL handle 0x7f07b8171b70
10-02-2016 21:07:20.769 UTC Debug httpresolver.cpp:71: HttpResolver::resolve for host 127.0.0.1, port 7253, family 10
10-02-2016 21:07:20.769 UTC Debug baseresolver.cpp:513: Attempt to parse 127.0.0.1 as IP address
10-02-2016 21:07:20.769 UTC Debug httpresolver.cpp:79: Target is an IP address
10-02-2016 21:07:20.769 UTC Debug httpconnection.cpp:623: Sending HTTP request : http://127.0.0.1:7253/timers (trying 127.0.0.1) on new connection
10-02-2016 21:07:20.771 UTC Debug httpconnection.cpp:915: Received header http/1.1200ok with value
10-02-2016 21:07:20.771 UTC Debug httpconnection.cpp:915: Received header location with value /timers/000718a1800000000040001000104104
10-02-2016 21:07:20.771 UTC Debug httpconnection.cpp:915: Received header content-length with value 0
10-02-2016 21:07:20.771 UTC Debug httpconnection.cpp:915: Received header  with value
10-02-2016 21:07:20.771 UTC Debug httpconnection.cpp:638: Received HTTP response: status=200, doc=
10-02-2016 21:07:20.771 UTC Debug communicationmonitor.cpp:82: Checking communication changes - successful attempts 1, failures 0
10-02-2016 21:07:20.771 UTC Debug pjsip: tsx0x7f07b8159 Transaction created for Request msg REGISTER/cseq=14995 (rdata0x7f07b80b5838)
10-02-2016 21:07:20.771 UTC Debug pjsip: tsx0x7f07b8159 Incoming Request msg REGISTER/cseq=14995 (rdata0x7f07b80b5838) in state Null
10-02-2016 21:07:20.771 UTC Debug pjsip: tsx0x7f07b8159 State changed from Null to Trying, event=RX_MSG
10-02-2016 21:07:20.772 UTC Debug pjsip: tsx0x7f07b8159 Sending Response msg 401/REGISTER/cseq=14995 (tdta0x7f07b811c5d0) in state Trying
10-02-2016 21:07:20.772 UTC Verbose common_sip_processing.cpp:136: TX 679 bytes Response msg 401/REGISTER/cseq=14995 (tdta0x7f07b811c5d0) to TCP 2400:6180:0:d0::18b:3001:38920:
--start msg--

SIP/2.0 401 Unauthorized
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001]:38920;rport=38920;received=2400:6180:0:d0::18b:3001;branch=z9hG4bKPjjroq9YI-eUVuYSb0le9cNDFAuRIBJ0c1
Via: SIP/2.0/UDP [2400:6180:0:d0::160:c001]:5070;rport=5070;received=2400:6180:0:d0::160:c001;branch=z9hG4bKPjVZWvSTGX0sLQG9ITOrWb5ZOOSbaHLRvD
Call-ID: bycgxu2h8r0Lt282qrPmn7.zbVUHGWJg
From: <sip:6505550098 at example.com>;tag=qcXJhCkJRbQCI9LAn6Dn2GYlCnKaH-vr
To: <sip:6505550098 at example.com>;tag=z9hG4bKPjjroq9YI-eUVuYSb0le9cNDFAuRIBJ0c1
CSeq: 14995 REGISTER
WWW-Authenticate: Digest  realm="example.com",nonce="131ff1d565852607",opaque="74562d5b3aeb4f90",algorithm=MD5,qop="auth"
Content-Length:  0


--end msg--
10-02-2016 21:07:20.772 UTC Debug pjsip: tsx0x7f07b8159 State changed from Trying to Completed, event=TX_MSG
10-02-2016 21:07:20.772 UTC Debug acr.cpp:83: Sending Null ACR (0x7f07b80da9a0)
10-02-2016 21:07:20.772 UTC Debug acr.cpp:54: Destroyed ACR (0x7f07b80da9a0)
10-02-2016 21:07:20.772 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b80b5838
10-02-2016 21:07:20.772 UTC Debug thread_dispatcher.cpp:199: Request latency = 87700us
10-02-2016 21:07:20.775 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg REGISTER/cseq=14996 (rdata0x7f07b8103bb0)
10-02-2016 21:07:20.775 UTC Verbose common_sip_processing.cpp:120: RX 1226 bytes Request msg REGISTER/cseq=14996 (rdata0x7f07b8103bb0) from TCP 2400:6180:0:d0::18b:3001:33221:
--start msg--

REGISTER sip:example.com SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001]:33221;rport;branch=z9hG4bKPjZqsE4Q3qexAQ9pFtEFYUgj64B0MC3yu0
Path: <sip:jOkcJw+mvT@[2400:6180:0:d0::18b:3001]:5058;transport=TCP;lr;ob>
Via: SIP/2.0/UDP [2400:6180:0:d0::160:c001]:5070;rport=5070;received=2400:6180:0:d0::160:c001;branch=z9hG4bKPjmVym-dqb.PeI6ZXKa6kogLtT7YajvFG3
Max-Forwards: 70
From: <sip:6505550098 at example.com>;tag=qcXJhCkJRbQCI9LAn6Dn2GYlCnKaH-vr
To: <sip:6505550098 at example.com>
Call-ID: bycgxu2h8r0Lt282qrPmn7.zbVUHGWJg
CSeq: 14996 REGISTER
User-Agent: PJSUA v2.4.5 Linux-3.13.0.71/x86_64/glibc-2.19
Contact: <sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob>
Expires: 300
Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, INFO, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
Authorization: Digest response="d8e7c37eb388ea8769ef559a2d32b12b", username="6505550098 at example.com", realm="example.com", nonce="131ff1d565852607", uri="sip:example.com", algorithm=MD5, cnonce="VYG9iiS2Q4JV1EEUJaoli8X1BBm9FZPx", opaque="74562d5b3aeb4f90", qop=auth, nc=00000001,integrity-protected=ip-assoc-pending
P-Visited-Network-ID: example.com
Route: <sip:[2400:6180:0:d0::18b:3001]:5054;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
10-02-2016 21:07:20.775 UTC Debug pjutils.cpp:1648: Logging SAS Call-ID marker, Call-ID bycgxu2h8r0Lt282qrPmn7.zbVUHGWJg
10-02-2016 21:07:20.776 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b80b67e8 for worker threads
10-02-2016 21:07:20.776 UTC Debug pjsip: tsx0x7f07b8159 Timeout timer event
10-02-2016 21:07:20.776 UTC Debug pjsip: tsx0x7f07b8159 State changed from Completed to Terminated, event=TIMER
10-02-2016 21:07:20.776 UTC Debug pjsip: tsx0x7f07b8159 Timeout timer event
10-02-2016 21:07:20.776 UTC Debug pjsip: tsx0x7f07b8159 State changed from Terminated to Destroyed, event=TIMER
10-02-2016 21:07:20.776 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b80b67e8
10-02-2016 21:07:20.776 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg REGISTER/cseq=14996 (rdata0x7f07b80b67e8)
10-02-2016 21:07:20.776 UTC Debug uri_classifier.cpp:167: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:20.776 UTC Debug uri_classifier.cpp:197: Classified URI as 4
10-02-2016 21:07:20.776 UTC Debug authentication.cpp:673: Authentication module invoked
10-02-2016 21:07:20.776 UTC Debug authentication.cpp:581: Authorization header in request
10-02-2016 21:07:20.776 UTC Debug authentication.cpp:687: Request needs authentication
10-02-2016 21:07:20.776 UTC Debug memcachedstore.cpp:195: Key av\\6505550098 at example.com\131ff1d565852607 hashes to vbucket 12 via hash 0x1f60a18c
10-02-2016 21:07:20.776 UTC Debug memcachedstore.cpp:236: Set up new view 1 for thread
10-02-2016 21:07:20.776 UTC Debug memcachedstore.cpp:243: Setting up server 0 for connection 0x7f07c000b4c0 (--CONNECT-TIMEOUT=10 --SUPPORT-CAS --POLL-TIMEOUT=250 --BINARY-PROTOCOL)
10-02-2016 21:07:20.776 UTC Debug memcachedstore.cpp:245: Set up connection 0x7f07c000b570 to server [2400:6180:0:d0::18b:3001]:11211
10-02-2016 21:07:20.776 UTC Debug memcachedstore.cpp:256: Setting server to IP address 2400:6180:0:d0::18b:3001 port 11211
10-02-2016 21:07:20.776 UTC Debug memcachedstore.cpp:367: 1 read replicas for key av\\6505550098 at example.com\131ff1d565852607
10-02-2016 21:07:20.776 UTC Debug memcachedstore.cpp:402: Attempt to read from replica 0 (connection 0x7f07c000b570)
10-02-2016 21:07:20.776 UTC Debug memcachedstore.cpp:780: Fetch result
10-02-2016 21:07:20.776 UTC Debug pjsip: tdta0x7f07b811 Destroying txdata Response msg 401/REGISTER/cseq=14995 (tdta0x7f07b811c5d0)
10-02-2016 21:07:20.776 UTC Debug pjsip: tsx0x7f07b8159 Transaction destroyed!
10-02-2016 21:07:20.777 UTC Debug memcachedstore.cpp:788: Found record on replica
10-02-2016 21:07:20.777 UTC Debug memcachedstore.cpp:410: Read for av\\6505550098 at example.com\131ff1d565852607 on replica 0 returned SUCCESS
10-02-2016 21:07:20.777 UTC Debug memcachedstore.cpp:453: Read 141 bytes from table av key 6505550098 at example.com\131ff1d565852607, CAS = 1
10-02-2016 21:07:20.777 UTC Debug avstore.cpp:106: Retrieved AV for 6505550098 at example.com\131ff1d565852607
{"digest":{"ha1":"ed7e862749772ad47596b5bd3c0f2e43","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjjroq9YI-eUVuYSb0le9cNDFAuRIBJ0c1"}
10-02-2016 21:07:20.777 UTC Debug authentication.cpp:743: Verify authentication information in request
10-02-2016 21:07:20.777 UTC Debug authentication.cpp:159: Verifying AV: {"digest":{"ha1":"ed7e862749772ad47596b5bd3c0f2e43","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjjroq9YI-eUVuYSb0le9cNDFAuRIBJ0c1"}
10-02-2016 21:07:20.777 UTC Debug authentication.cpp:186: Digest specified
10-02-2016 21:07:20.777 UTC Debug authentication.cpp:311: Found Digest HA1 = ed7e862749772ad47596b5bd3c0f2e43
10-02-2016 21:07:20.777 UTC Debug authentication.cpp:749: Request authenticated successfully
10-02-2016 21:07:20.777 UTC Debug avstore.cpp:72: Set AV for 6505550098 at example.com\131ff1d565852607
{"digest":{"ha1":"ed7e862749772ad47596b5bd3c0f2e43","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjjroq9YI-eUVuYSb0le9cNDFAuRIBJ0c1","tombstone":true}
10-02-2016 21:07:20.777 UTC Debug memcachedstore.cpp:542: Writing 158 bytes to table av key 6505550098 at example.com\131ff1d565852607, CAS = 1, expiry = 40
10-02-2016 21:07:20.777 UTC Debug memcachedstore.cpp:195: Key av\\6505550098 at example.com\131ff1d565852607 hashes to vbucket 12 via hash 0x1f60a18c
10-02-2016 21:07:20.777 UTC Debug memcachedstore.cpp:562: 1 write replicas for key av\\6505550098 at example.com\131ff1d565852607
10-02-2016 21:07:20.777 UTC Debug memcachedstore.cpp:616: Attempt conditional write to vbucket 12 on replica 0 (connection 0x7f07c000b570), CAS = 1, expiry = 40
10-02-2016 21:07:20.778 UTC Debug memcachedstore.cpp:657: Conditional write succeeded to replica 0
10-02-2016 21:07:20.778 UTC Debug uri_classifier.cpp:167: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:20.778 UTC Debug uri_classifier.cpp:197: Classified URI as 4
10-02-2016 21:07:20.778 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:20.778 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:07:20.778 UTC Debug acr.cpp:49: Created ACR (0x7f07c0000c40)
10-02-2016 21:07:20.778 UTC Debug registrar.cpp:541: Process REGISTER for public ID sip:6505550098 at example.com
10-02-2016 21:07:20.778 UTC Debug registrar.cpp:549: Report SAS start marker - trail (b7)
10-02-2016 21:07:20.778 UTC Debug hssconnection.cpp:585: Making Homestead request for /impu/sip%3A6505550098%40example.com/reg-data?private_id=6505550098%40example.com
10-02-2016 21:07:20.778 UTC Debug httpconnection.cpp:183: Allocated CURL handle 0x7f07c0014b30
10-02-2016 21:07:20.779 UTC Debug httpresolver.cpp:71: HttpResolver::resolve for host [2400:6180:0:d0::18b:3001], port 8888, family 10
10-02-2016 21:07:20.779 UTC Debug baseresolver.cpp:513: Attempt to parse [2400:6180:0:d0::18b:3001] as IP address
10-02-2016 21:07:20.779 UTC Debug httpresolver.cpp:79: Target is an IP address
10-02-2016 21:07:20.779 UTC Debug httpconnection.cpp:623: Sending HTTP request : http://[2400:6180:0:d0::18b:3001]:8888/impu/sip%3A6505550098%40example.com/reg-data?private_id=6505550098%40example.com (trying 2400:6180:0:d0::18b:3001) on new connection
10-02-2016 21:07:20.840 UTC Debug httpconnection.cpp:915: Received header http/1.1200ok with value
10-02-2016 21:07:20.840 UTC Debug httpconnection.cpp:915: Received header content-length with value 813
10-02-2016 21:07:20.840 UTC Debug httpconnection.cpp:915: Received header content-type with value text/plain
10-02-2016 21:07:20.840 UTC Debug httpconnection.cpp:915: Received header  with value
10-02-2016 21:07:20.840 UTC Debug httpconnection.cpp:638: Received HTTP response: status=200, doc=<ClearwaterRegData>
        <RegistrationState>REGISTERED</RegistrationState>
        <IMSSubscription xsi="http://www.w3.org/2001/XMLSchema-instance" noNamespaceSchemaLocation="CxDataType.xsd">
                <PrivateID>Unspecified</PrivateID>
                <ServiceProfile>
                        <InitialFilterCriteria>
                                <TriggerPoint>
                                        <ConditionTypeCNF>0</ConditionTypeCNF>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNegated>
                                                <Group>0</Group>
                                                <Method>INVITE</Method>
                                                <Extension/>
                                        </SPT>
                                </TriggerPoint>
                                <ApplicationServer>
                                        <ServerName>sip:mmtel.example.com</ServerName>
                                        <DefaultHandling>0</DefaultHandling>
                                </ApplicationServer>
                        </InitialFilterCriteria>
                        <PublicIdentity>
                                <Identity>sip:6505550098 at example.com</Identity>
                        </PublicIdentity>
                </ServiceProfile>
        </IMSSubscription>
</ClearwaterRegData>


10-02-2016 21:07:20.840 UTC Debug hssconnection.cpp:366: Processing Identity node from HSS XML - sip:6505550098 at example.com

10-02-2016 21:07:20.840 UTC Debug registrar.cpp:651: REGISTER for public ID sip:6505550098 at example.com uses AOR sip:6505550098 at example.com
10-02-2016 21:07:20.840 UTC Debug subscriber_data_manager.cpp:366: Get AoR data for sip:6505550098 at example.com
10-02-2016 21:07:20.840 UTC Debug memcachedstore.cpp:195: Key reg\\sip:6505550098 at example.com hashes to vbucket 0 via hash 0xa5f8800
10-02-2016 21:07:20.840 UTC Debug memcachedstore.cpp:367: 1 read replicas for key reg\\sip:6505550098 at example.com
10-02-2016 21:07:20.840 UTC Debug memcachedstore.cpp:402: Attempt to read from replica 0 (connection 0x7f07c000b570)
10-02-2016 21:07:20.840 UTC Debug memcachedstore.cpp:780: Fetch result
10-02-2016 21:07:20.840 UTC Debug memcachedstore.cpp:418: Read for reg\\sip:6505550098 at example.com on replica 0 returned NOTFOUND
10-02-2016 21:07:20.840 UTC Debug memcachedstore.cpp:492: At least one replica returned not found, so return NOT_FOUND
10-02-2016 21:07:20.840 UTC Debug subscriber_data_manager.cpp:407: Data store returned not found, so create new record, CAS = 0
10-02-2016 21:07:20.840 UTC Debug registrar.cpp:249: Retrieved AoR data 0x7f07c00307e0
10-02-2016 21:07:20.840 UTC Debug registrar.cpp:342: Binding identifier for contact = sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob
10-02-2016 21:07:20.840 UTC Debug registrar.cpp:369: Path header sip:jOkcJw+mvT@[2400:6180:0:d0::18b:3001]:5058;transport=TCP;lr;ob
10-02-2016 21:07:20.841 UTC Debug subscriber_data_manager.cpp:196: Set AoR data for sip:6505550098 at example.com, CAS=0, expiry = 1455138750
10-02-2016 21:07:20.841 UTC Debug httpconnection.cpp:183: Allocated CURL handle 0x7f07c0062de0
10-02-2016 21:07:20.842 UTC Debug httpresolver.cpp:71: HttpResolver::resolve for host 127.0.0.1, port 7253, family 10
10-02-2016 21:07:20.842 UTC Debug baseresolver.cpp:513: Attempt to parse 127.0.0.1 as IP address
10-02-2016 21:07:20.842 UTC Debug httpresolver.cpp:79: Target is an IP address
10-02-2016 21:07:20.842 UTC Debug httpconnection.cpp:623: Sending HTTP request : http://127.0.0.1:7253/timers (trying 127.0.0.1) on new connection
10-02-2016 21:07:20.843 UTC Debug httpconnection.cpp:915: Received header http/1.1200ok with value
10-02-2016 21:07:20.844 UTC Debug httpconnection.cpp:915: Received header location with value /timers/000718b3c00000010040001000104104
10-02-2016 21:07:20.844 UTC Debug httpconnection.cpp:915: Received header content-length with value 0
10-02-2016 21:07:20.844 UTC Debug httpconnection.cpp:915: Received header  with value
10-02-2016 21:07:20.844 UTC Debug httpconnection.cpp:638: Received HTTP response: status=200, doc=
10-02-2016 21:07:20.844 UTC Debug memcachedstore.cpp:542: Writing 446 bytes to table reg key sip:6505550098 at example.com, CAS = 0, expiry = 310
10-02-2016 21:07:20.844 UTC Debug memcachedstore.cpp:195: Key reg\\sip:6505550098 at example.com hashes to vbucket 0 via hash 0xa5f8800
10-02-2016 21:07:20.844 UTC Debug memcachedstore.cpp:562: 1 write replicas for key reg\\sip:6505550098 at example.com
10-02-2016 21:07:20.844 UTC Debug memcachedstore.cpp:616: Attempt conditional write to vbucket 0 on replica 0 (connection 0x7f07c000b570), CAS = 0, expiry = 310
10-02-2016 21:07:20.844 UTC Debug memcachedstore.cpp:816: Attempting to add data for key reg\\sip:6505550098 at example.com
10-02-2016 21:07:20.844 UTC Debug memcachedstore.cpp:826: Attempting memcached ADD command
10-02-2016 21:07:20.844 UTC Debug memcachedstore.cpp:916: ADD/CAS returned rc = 0 (SUCCESS)
SUCCESS
10-02-2016 21:07:20.844 UTC Debug memcachedstore.cpp:657: Conditional write succeeded to replica 0
10-02-2016 21:07:20.844 UTC Debug subscriber_data_manager.cpp:438: Data store set_data returned 1
10-02-2016 21:07:20.844 UTC Debug registrar.cpp:116: Bindings for sip:6505550098 at example.com
10-02-2016 21:07:20.844 UTC Debug registrar.cpp:130:   sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob URI=sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob expires=1455138740 q=0 from=bycgxu2h8r0Lt282qrPmn7.zbVUHGWJg cseq=14996 timer=000718b3c00000010040001000104104 private_id=6505550098 at example.com emergency_registration=false
10-02-2016 21:07:20.845 UTC Debug pjsip:       endpoint Response msg 200/REGISTER/cseq=14996 (tdta0x7f07c009a760) created
10-02-2016 21:07:20.845 UTC Verbose common_sip_processing.cpp:136: TX 860 bytes Response msg 200/REGISTER/cseq=14996 (tdta0x7f07c009a760) to TCP 2400:6180:0:d0::18b:3001:33221:
--start msg--

SIP/2.0 200 OK
Service-Route: <sip:[2400:6180:0:d0::18b:3001]:5054;transport=TCP;lr;orig>
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001]:33221;rport=33221;received=2400:6180:0:d0::18b:3001;branch=z9hG4bKPjZqsE4Q3qexAQ9pFtEFYUgj64B0MC3yu0
Via: SIP/2.0/UDP [2400:6180:0:d0::160:c001]:5070;rport=5070;received=2400:6180:0:d0::160:c001;branch=z9hG4bKPjmVym-dqb.PeI6ZXKa6kogLtT7YajvFG3
Call-ID: bycgxu2h8r0Lt282qrPmn7.zbVUHGWJg
From: <sip:6505550098 at example.com>;tag=qcXJhCkJRbQCI9LAn6Dn2GYlCnKaH-vr
To: <sip:6505550098 at example.com>;tag=z9hG4bKPjZqsE4Q3qexAQ9pFtEFYUgj64B0MC3yu0
CSeq: 14996 REGISTER
Supported: outbound
Contact: <sip:6505550098@[2400:6180:0:d0::160:c001]:5070;ob>;expires=300
Require: outbound
Path: <sip:jOkcJw+mvT@[2400:6180:0:d0::18b:3001]:5058;transport=TCP;lr;ob>
P-Associated-URI: <sip:6505550098 at example.com>
Content-Length:  0


--end msg--
10-02-2016 21:07:20.845 UTC Debug acr.cpp:83: Sending Null ACR (0x7f07c0000c40)
10-02-2016 21:07:20.845 UTC Debug acr.cpp:54: Destroyed ACR (0x7f07c0000c40)
10-02-2016 21:07:20.846 UTC Debug ifchandler.cpp:763: Interpreting orig IFC information
10-02-2016 21:07:20.846 UTC Debug ifchandler.cpp:437: SPT class Method: result false
10-02-2016 21:07:20.846 UTC Debug ifchandler.cpp:541: Add to group 0 val false
10-02-2016 21:07:20.846 UTC Debug ifchandler.cpp:559: Result group 0 val false
10-02-2016 21:07:20.846 UTC Debug ifchandler.cpp:572: iFC does not match
10-02-2016 21:07:20.846 UTC Info registration_utils.cpp:187: Found 0 Application Servers
10-02-2016 21:07:20.846 UTC Debug pjsip: tdta0x7f07c009 Destroying txdata Response msg 200/REGISTER/cseq=14996 (tdta0x7f07c009a760)
10-02-2016 21:07:20.846 UTC Debug registrar.cpp:1036: Report SAS end marker - trail (b7)
10-02-2016 21:07:20.846 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b80b67e8
10-02-2016 21:07:20.846 UTC Debug thread_dispatcher.cpp:199: Request latency = 70230us
10-02-2016 21:07:24.036 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg REGISTER/cseq=39544 (rdata0x7f07b80f35e0)
10-02-2016 21:07:24.036 UTC Verbose common_sip_processing.cpp:120: RX 909 bytes Request msg REGISTER/cseq=39544 (rdata0x7f07b80f35e0) from TCP 2400:6180:0:d0::18b:3001:38920:
--start msg--

REGISTER sip:example.com SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001]:38920;rport;branch=z9hG4bKPjF6CDH5bJ8L0DTuwzm0pjifV3rL3z2eTQ
Path: <sip:JWSsk2IumG@[2400:6180:0:d0::18b:3001]:5058;transport=TCP;lr;ob>
Via: SIP/2.0/UDP [2400:6180:0:d0::12b:6001]:5070;rport=5070;received=2400:6180:0:d0::12b:6001;branch=z9hG4bKPjgdYG5cxssPjSsMzhjad7j8ugUXWbKnUX
Max-Forwards: 70
From: <sip:6505550097 at example.com>;tag=tTGWc.BJFJzyCRIodO6GiWzrEAkAbKqH
To: <sip:6505550097 at example.com>
Call-ID: rpPSADxNjjEHLOh3WJoNliu-mE8QeGcx
CSeq: 39544 REGISTER
User-Agent: PJSUA v2.4.5 Linux-3.13.0.71/x86_64/glibc-2.19
Contact: <sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob>
Expires: 300
Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, INFO, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
P-Visited-Network-ID: example.com
Route: <sip:[2400:6180:0:d0::18b:3001]:5054;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
10-02-2016 21:07:24.036 UTC Debug pjutils.cpp:1648: Logging SAS Call-ID marker, Call-ID rpPSADxNjjEHLOh3WJoNliu-mE8QeGcx
10-02-2016 21:07:24.036 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b80b5838 for worker threads
10-02-2016 21:07:24.037 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b80b5838
10-02-2016 21:07:24.037 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg REGISTER/cseq=39544 (rdata0x7f07b80b5838)
10-02-2016 21:07:24.037 UTC Debug uri_classifier.cpp:167: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:24.037 UTC Debug uri_classifier.cpp:197: Classified URI as 4
10-02-2016 21:07:24.037 UTC Debug authentication.cpp:673: Authentication module invoked
10-02-2016 21:07:24.037 UTC Debug authentication.cpp:687: Request needs authentication
10-02-2016 21:07:24.037 UTC Debug acr.cpp:49: Created ACR (0x7f07c80040d0)
10-02-2016 21:07:24.037 UTC Debug authentication.cpp:889: No authentication information in request or stale nonce, so reject with challenge
10-02-2016 21:07:24.037 UTC Debug pjsip:       endpoint Response msg 401/REGISTER/cseq=39544 (tdta0x7f07c80137f0) created
10-02-2016 21:07:24.037 UTC Debug pjutils.cpp:423: Private identity defaulted from public identity = 6505550097 at example.com
10-02-2016 21:07:24.039 UTC Debug httpconnection.cpp:183: Allocated CURL handle 0x7f07c8014aa0
10-02-2016 21:07:24.042 UTC Debug httpresolver.cpp:71: HttpResolver::resolve for host [2400:6180:0:d0::18b:3001], port 8888, family 10
10-02-2016 21:07:24.042 UTC Debug baseresolver.cpp:513: Attempt to parse [2400:6180:0:d0::18b:3001] as IP address
10-02-2016 21:07:24.042 UTC Debug httpresolver.cpp:79: Target is an IP address
10-02-2016 21:07:24.042 UTC Debug httpconnection.cpp:623: Sending HTTP request : http://[2400:6180:0:d0::18b:3001]:8888/impi/6505550097%40example.com/av?impu=sip%3A6505550097%40example.com (trying 2400:6180:0:d0::18b:3001) on new connection
10-02-2016 21:07:24.059 UTC Debug httpconnection.cpp:638: Received HTTP response: status=200, doc={"digest":{"ha1":"38a357edc0327687dd326d7416c3de69","realm":"example.com","qop":"auth"}}
10-02-2016 21:07:24.059 UTC Debug authentication.cpp:159: Verifying AV: {"digest":{"ha1":"38a357edc0327687dd326d7416c3de69","realm":"example.com","qop":"auth"}}
10-02-2016 21:07:24.059 UTC Debug authentication.cpp:186: Digest specified
10-02-2016 21:07:24.059 UTC Debug authentication.cpp:374: Valid AV - generate challenge
10-02-2016 21:07:24.059 UTC Debug authentication.cpp:383: Create WWW-Authenticate header
10-02-2016 21:07:24.059 UTC Debug authentication.cpp:451: Add Digest information
10-02-2016 21:07:24.059 UTC Debug authentication.cpp:493: Write AV to store
10-02-2016 21:07:24.059 UTC Debug avstore.cpp:72: Set AV for 6505550097 at example.com\6cb85d3b61725048
{"digest":{"ha1":"38a357edc0327687dd326d7416c3de69","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjF6CDH5bJ8L0DTuwzm0pjifV3rL3z2eTQ"}
10-02-2016 21:07:24.059 UTC Debug memcachedstore.cpp:542: Writing 141 bytes to table av key 6505550097 at example.com\6cb85d3b61725048, CAS = 0, expiry = 40
10-02-2016 21:07:24.059 UTC Debug memcachedstore.cpp:195: Key av\\6505550097 at example.com\6cb85d3b61725048 hashes to vbucket 14 via hash 0xe64d760e
10-02-2016 21:07:24.059 UTC Debug memcachedstore.cpp:236: Set up new view 1 for thread
10-02-2016 21:07:24.059 UTC Debug memcachedstore.cpp:243: Setting up server 0 for connection 0x7f07c804ca70 (--CONNECT-TIMEOUT=10 --SUPPORT-CAS --POLL-TIMEOUT=250 --BINARY-PROTOCOL)
10-02-2016 21:07:24.059 UTC Debug memcachedstore.cpp:245: Set up connection 0x7f07c8062270 to server [2400:6180:0:d0::18b:3001]:11211
10-02-2016 21:07:24.059 UTC Debug memcachedstore.cpp:256: Setting server to IP address 2400:6180:0:d0::18b:3001 port 11211
10-02-2016 21:07:24.059 UTC Debug memcachedstore.cpp:562: 1 write replicas for key av\\6505550097 at example.com\6cb85d3b61725048
10-02-2016 21:07:24.059 UTC Debug memcachedstore.cpp:616: Attempt conditional write to vbucket 14 on replica 0 (connection 0x7f07c8062270), CAS = 0, expiry = 40
10-02-2016 21:07:24.059 UTC Debug memcachedstore.cpp:816: Attempting to add data for key av\\6505550097 at example.com\6cb85d3b61725048
10-02-2016 21:07:24.059 UTC Debug memcachedstore.cpp:826: Attempting memcached ADD command
10-02-2016 21:07:24.060 UTC Debug memcachedstore.cpp:916: ADD/CAS returned rc = 0 (SUCCESS)
SUCCESS
10-02-2016 21:07:24.060 UTC Debug memcachedstore.cpp:657: Conditional write succeeded to replica 0
10-02-2016 21:07:24.060 UTC Debug authentication.cpp:503: Sending {"impi": "6505550097 at example.com", "impu": "sip:6505550097 at example.com", "nonce": "6cb85d3b61725048"} to Chronos to set AV timer
10-02-2016 21:07:24.061 UTC Debug httpconnection.cpp:183: Allocated CURL handle 0x7f07c806a630
10-02-2016 21:07:24.063 UTC Debug httpresolver.cpp:71: HttpResolver::resolve for host 127.0.0.1, port 7253, family 10
10-02-2016 21:07:24.063 UTC Debug baseresolver.cpp:513: Attempt to parse 127.0.0.1 as IP address
10-02-2016 21:07:24.063 UTC Debug httpresolver.cpp:79: Target is an IP address
10-02-2016 21:07:24.063 UTC Debug httpconnection.cpp:623: Sending HTTP request : http://127.0.0.1:7253/timers (trying 127.0.0.1) on new connection
10-02-2016 21:07:24.065 UTC Debug httpconnection.cpp:915: Received header http/1.1200ok with value
10-02-2016 21:07:24.065 UTC Debug httpconnection.cpp:915: Received header location with value /timers/00071bd9000000020040001000104104
10-02-2016 21:07:24.065 UTC Debug httpconnection.cpp:915: Received header content-length with value 0
10-02-2016 21:07:24.065 UTC Debug httpconnection.cpp:915: Received header  with value
10-02-2016 21:07:24.065 UTC Debug httpconnection.cpp:638: Received HTTP response: status=200, doc=
10-02-2016 21:07:24.065 UTC Debug pjsip: tsx0x7f07c8052 Transaction created for Request msg REGISTER/cseq=39544 (rdata0x7f07b80b5838)
10-02-2016 21:07:24.065 UTC Debug pjsip: tsx0x7f07c8052 Incoming Request msg REGISTER/cseq=39544 (rdata0x7f07b80b5838) in state Null
10-02-2016 21:07:24.065 UTC Debug pjsip: tsx0x7f07c8052 State changed from Null to Trying, event=RX_MSG
10-02-2016 21:07:24.065 UTC Debug pjsip: tsx0x7f07c8052 Sending Response msg 401/REGISTER/cseq=39544 (tdta0x7f07c80137f0) in state Trying
10-02-2016 21:07:24.066 UTC Verbose common_sip_processing.cpp:136: TX 679 bytes Response msg 401/REGISTER/cseq=39544 (tdta0x7f07c80137f0) to TCP 2400:6180:0:d0::18b:3001:38920:
--start msg--

SIP/2.0 401 Unauthorized
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001]:38920;rport=38920;received=2400:6180:0:d0::18b:3001;branch=z9hG4bKPjF6CDH5bJ8L0DTuwzm0pjifV3rL3z2eTQ
Via: SIP/2.0/UDP [2400:6180:0:d0::12b:6001]:5070;rport=5070;received=2400:6180:0:d0::12b:6001;branch=z9hG4bKPjgdYG5cxssPjSsMzhjad7j8ugUXWbKnUX
Call-ID: rpPSADxNjjEHLOh3WJoNliu-mE8QeGcx
From: <sip:6505550097 at example.com>;tag=tTGWc.BJFJzyCRIodO6GiWzrEAkAbKqH
To: <sip:6505550097 at example.com>;tag=z9hG4bKPjF6CDH5bJ8L0DTuwzm0pjifV3rL3z2eTQ
CSeq: 39544 REGISTER
WWW-Authenticate: Digest  realm="example.com",nonce="6cb85d3b61725048",opaque="789163944cf2d5bd",algorithm=MD5,qop="auth"
Content-Length:  0


--end msg--
10-02-2016 21:07:24.066 UTC Debug pjsip: tsx0x7f07c8052 State changed from Trying to Completed, event=TX_MSG
10-02-2016 21:07:24.066 UTC Debug acr.cpp:83: Sending Null ACR (0x7f07c80040d0)
10-02-2016 21:07:24.066 UTC Debug acr.cpp:54: Destroyed ACR (0x7f07c80040d0)
10-02-2016 21:07:24.066 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b80b5838
10-02-2016 21:07:24.066 UTC Debug thread_dispatcher.cpp:199: Request latency = 29267us
10-02-2016 21:07:24.068 UTC Debug pjsip: tsx0x7f07c8052 Timeout timer event
10-02-2016 21:07:24.068 UTC Debug pjsip: tsx0x7f07c8052 State changed from Completed to Terminated, event=TIMER
10-02-2016 21:07:24.068 UTC Debug pjsip: tsx0x7f07c8052 Timeout timer event
10-02-2016 21:07:24.068 UTC Debug pjsip: tsx0x7f07c8052 State changed from Terminated to Destroyed, event=TIMER
10-02-2016 21:07:24.068 UTC Debug pjsip: tdta0x7f07c801 Destroying txdata Response msg 401/REGISTER/cseq=39544 (tdta0x7f07c80137f0)
10-02-2016 21:07:24.068 UTC Debug pjsip: tsx0x7f07c8052 Transaction destroyed!
10-02-2016 21:07:24.069 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg REGISTER/cseq=39545 (rdata0x7f07b80f0790)
10-02-2016 21:07:24.069 UTC Verbose common_sip_processing.cpp:120: RX 1226 bytes Request msg REGISTER/cseq=39545 (rdata0x7f07b80f0790) from TCP 2400:6180:0:d0::18b:3001:52176:
--start msg--

REGISTER sip:example.com SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001]:52176;rport;branch=z9hG4bKPjkCkaKE0ron-ujXbJM-3GxJ9kNmmBWE-a
Path: <sip:JWSsk2IumG@[2400:6180:0:d0::18b:3001]:5058;transport=TCP;lr;ob>
Via: SIP/2.0/UDP [2400:6180:0:d0::12b:6001]:5070;rport=5070;received=2400:6180:0:d0::12b:6001;branch=z9hG4bKPj50j2Ay3PdKjSNIc1RcAfmwqOD2ivk.mb
Max-Forwards: 70
From: <sip:6505550097 at example.com>;tag=tTGWc.BJFJzyCRIodO6GiWzrEAkAbKqH
To: <sip:6505550097 at example.com>
Call-ID: rpPSADxNjjEHLOh3WJoNliu-mE8QeGcx
CSeq: 39545 REGISTER
User-Agent: PJSUA v2.4.5 Linux-3.13.0.71/x86_64/glibc-2.19
Contact: <sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob>
Expires: 300
Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, INFO, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
Authorization: Digest response="eb6f9789e1d15fb903e9c0ffe0f837ab", username="6505550097 at example.com", realm="example.com", nonce="6cb85d3b61725048", uri="sip:example.com", algorithm=MD5, cnonce="Ra3PDVd4ixYT-q1wBtZnpn8PqEQNh1w9", opaque="789163944cf2d5bd", qop=auth, nc=00000001,integrity-protected=ip-assoc-pending
P-Visited-Network-ID: example.com
Route: <sip:[2400:6180:0:d0::18b:3001]:5054;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
10-02-2016 21:07:24.069 UTC Debug pjutils.cpp:1648: Logging SAS Call-ID marker, Call-ID rpPSADxNjjEHLOh3WJoNliu-mE8QeGcx
10-02-2016 21:07:24.069 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b80b5838 for worker threads
10-02-2016 21:07:24.070 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b80b5838
10-02-2016 21:07:24.070 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg REGISTER/cseq=39545 (rdata0x7f07b80b5838)
10-02-2016 21:07:24.070 UTC Debug uri_classifier.cpp:167: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:24.070 UTC Debug uri_classifier.cpp:197: Classified URI as 4
10-02-2016 21:07:24.070 UTC Debug authentication.cpp:673: Authentication module invoked
10-02-2016 21:07:24.070 UTC Debug authentication.cpp:581: Authorization header in request
10-02-2016 21:07:24.070 UTC Debug authentication.cpp:687: Request needs authentication
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:195: Key av\\6505550097 at example.com\6cb85d3b61725048 hashes to vbucket 14 via hash 0xe64d760e
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:236: Set up new view 1 for thread
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:243: Setting up server 0 for connection 0x12307b0 (--CONNECT-TIMEOUT=10 --SUPPORT-CAS --POLL-TIMEOUT=250 --BINARY-PROTOCOL)
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:245: Set up connection 0x1265b40 to server [2400:6180:0:d0::18b:3001]:11211
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:256: Setting server to IP address 2400:6180:0:d0::18b:3001 port 11211
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:367: 1 read replicas for key av\\6505550097 at example.com\6cb85d3b61725048
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:402: Attempt to read from replica 0 (connection 0x1265b40)
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:780: Fetch result
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:788: Found record on replica
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:410: Read for av\\6505550097 at example.com\6cb85d3b61725048 on replica 0 returned SUCCESS
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:453: Read 141 bytes from table av key 6505550097 at example.com\6cb85d3b61725048, CAS = 4
10-02-2016 21:07:24.070 UTC Debug avstore.cpp:106: Retrieved AV for 6505550097 at example.com\6cb85d3b61725048
{"digest":{"ha1":"38a357edc0327687dd326d7416c3de69","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjF6CDH5bJ8L0DTuwzm0pjifV3rL3z2eTQ"}
10-02-2016 21:07:24.070 UTC Debug authentication.cpp:743: Verify authentication information in request
10-02-2016 21:07:24.070 UTC Debug authentication.cpp:159: Verifying AV: {"digest":{"ha1":"38a357edc0327687dd326d7416c3de69","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjF6CDH5bJ8L0DTuwzm0pjifV3rL3z2eTQ"}
10-02-2016 21:07:24.070 UTC Debug authentication.cpp:186: Digest specified
10-02-2016 21:07:24.070 UTC Debug authentication.cpp:311: Found Digest HA1 = 38a357edc0327687dd326d7416c3de69
10-02-2016 21:07:24.070 UTC Debug authentication.cpp:749: Request authenticated successfully
10-02-2016 21:07:24.070 UTC Debug avstore.cpp:72: Set AV for 6505550097 at example.com\6cb85d3b61725048
{"digest":{"ha1":"38a357edc0327687dd326d7416c3de69","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjF6CDH5bJ8L0DTuwzm0pjifV3rL3z2eTQ","tombstone":true}
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:542: Writing 158 bytes to table av key 6505550097 at example.com\6cb85d3b61725048, CAS = 4, expiry = 40
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:195: Key av\\6505550097 at example.com\6cb85d3b61725048 hashes to vbucket 14 via hash 0xe64d760e
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:562: 1 write replicas for key av\\6505550097 at example.com\6cb85d3b61725048
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:616: Attempt conditional write to vbucket 14 on replica 0 (connection 0x1265b40), CAS = 4, expiry = 40
10-02-2016 21:07:24.070 UTC Debug memcachedstore.cpp:657: Conditional write succeeded to replica 0
10-02-2016 21:07:24.070 UTC Debug uri_classifier.cpp:167: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:24.070 UTC Debug uri_classifier.cpp:197: Classified URI as 4
10-02-2016 21:07:24.070 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:24.070 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:07:24.070 UTC Debug acr.cpp:49: Created ACR (0x12455b0)
10-02-2016 21:07:24.070 UTC Debug registrar.cpp:541: Process REGISTER for public ID sip:6505550097 at example.com
10-02-2016 21:07:24.070 UTC Debug registrar.cpp:549: Report SAS start marker - trail (b9)
10-02-2016 21:07:24.071 UTC Debug hssconnection.cpp:585: Making Homestead request for /impu/sip%3A6505550097%40example.com/reg-data?private_id=6505550097%40example.com
10-02-2016 21:07:24.071 UTC Debug httpconnection.cpp:183: Allocated CURL handle 0x126ece0
10-02-2016 21:07:24.073 UTC Debug httpresolver.cpp:71: HttpResolver::resolve for host [2400:6180:0:d0::18b:3001], port 8888, family 10
10-02-2016 21:07:24.073 UTC Debug baseresolver.cpp:513: Attempt to parse [2400:6180:0:d0::18b:3001] as IP address
10-02-2016 21:07:24.073 UTC Debug httpresolver.cpp:79: Target is an IP address
10-02-2016 21:07:24.073 UTC Debug httpconnection.cpp:623: Sending HTTP request : http://[2400:6180:0:d0::18b:3001]:8888/impu/sip%3A6505550097%40example.com/reg-data?private_id=6505550097%40example.com (trying 2400:6180:0:d0::18b:3001) on new connection
10-02-2016 21:07:24.084 UTC Debug httpconnection.cpp:915: Received header http/1.1200ok with value
10-02-2016 21:07:24.084 UTC Debug httpconnection.cpp:915: Received header content-length with value 813
10-02-2016 21:07:24.084 UTC Debug httpconnection.cpp:915: Received header content-type with value text/plain
10-02-2016 21:07:24.084 UTC Debug httpconnection.cpp:915: Received header  with value
10-02-2016 21:07:24.084 UTC Debug httpconnection.cpp:638: Received HTTP response: status=200, doc=<ClearwaterRegData>
        <RegistrationState>REGISTERED</RegistrationState>
        <IMSSubscription xsi="http://www.w3.org/2001/XMLSchema-instance" noNamespaceSchemaLocation="CxDataType.xsd">
                <PrivateID>Unspecified</PrivateID>
                <ServiceProfile>
                        <InitialFilterCriteria>
                                <TriggerPoint>
                                        <ConditionTypeCNF>0</ConditionTypeCNF>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNegated>
                                                <Group>0</Group>
                                                <Method>INVITE</Method>
                                                <Extension/>
                                        </SPT>
                                </TriggerPoint>
                                <ApplicationServer>
                                        <ServerName>sip:mmtel.example.com</ServerName>
                                        <DefaultHandling>0</DefaultHandling>
                                </ApplicationServer>
                        </InitialFilterCriteria>
                        <PublicIdentity>
                                <Identity>sip:6505550097 at example.com</Identity>
                        </PublicIdentity>
                </ServiceProfile>
        </IMSSubscription>
</ClearwaterRegData>


10-02-2016 21:07:24.085 UTC Debug hssconnection.cpp:366: Processing Identity node from HSS XML - sip:6505550097 at example.com

10-02-2016 21:07:24.085 UTC Debug registrar.cpp:651: REGISTER for public ID sip:6505550097 at example.com uses AOR sip:6505550097 at example.com
10-02-2016 21:07:24.085 UTC Debug subscriber_data_manager.cpp:366: Get AoR data for sip:6505550097 at example.com
10-02-2016 21:07:24.085 UTC Debug memcachedstore.cpp:195: Key reg\\sip:6505550097 at example.com hashes to vbucket 58 via hash 0x60d79c3a
10-02-2016 21:07:24.085 UTC Debug memcachedstore.cpp:367: 1 read replicas for key reg\\sip:6505550097 at example.com
10-02-2016 21:07:24.085 UTC Debug memcachedstore.cpp:402: Attempt to read from replica 0 (connection 0x1265b40)
10-02-2016 21:07:24.085 UTC Debug memcachedstore.cpp:780: Fetch result
10-02-2016 21:07:24.085 UTC Debug memcachedstore.cpp:418: Read for reg\\sip:6505550097 at example.com on replica 0 returned NOTFOUND
10-02-2016 21:07:24.085 UTC Debug memcachedstore.cpp:492: At least one replica returned not found, so return NOT_FOUND
10-02-2016 21:07:24.085 UTC Debug subscriber_data_manager.cpp:407: Data store returned not found, so create new record, CAS = 0
10-02-2016 21:07:24.085 UTC Debug registrar.cpp:249: Retrieved AoR data 0x128aa10
10-02-2016 21:07:24.085 UTC Debug registrar.cpp:342: Binding identifier for contact = sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob
10-02-2016 21:07:24.085 UTC Debug registrar.cpp:369: Path header sip:JWSsk2IumG@[2400:6180:0:d0::18b:3001]:5058;transport=TCP;lr;ob
10-02-2016 21:07:24.085 UTC Debug subscriber_data_manager.cpp:196: Set AoR data for sip:6505550097 at example.com, CAS=0, expiry = 1455138754
10-02-2016 21:07:24.086 UTC Debug httpconnection.cpp:183: Allocated CURL handle 0x12bcf40
10-02-2016 21:07:24.087 UTC Debug httpresolver.cpp:71: HttpResolver::resolve for host 127.0.0.1, port 7253, family 10
10-02-2016 21:07:24.087 UTC Debug baseresolver.cpp:513: Attempt to parse 127.0.0.1 as IP address
10-02-2016 21:07:24.087 UTC Debug httpresolver.cpp:79: Target is an IP address
10-02-2016 21:07:24.087 UTC Debug httpconnection.cpp:623: Sending HTTP request : http://127.0.0.1:7253/timers (trying 127.0.0.1) on new connection
10-02-2016 21:07:24.088 UTC Debug httpconnection.cpp:915: Received header http/1.1200ok with value
10-02-2016 21:07:24.088 UTC Debug httpconnection.cpp:915: Received header location with value /timers/00071bdec00000030040001000104104
10-02-2016 21:07:24.088 UTC Debug httpconnection.cpp:915: Received header content-length with value 0
10-02-2016 21:07:24.088 UTC Debug httpconnection.cpp:915: Received header  with value
10-02-2016 21:07:24.088 UTC Debug httpconnection.cpp:638: Received HTTP response: status=200, doc=
10-02-2016 21:07:24.088 UTC Debug memcachedstore.cpp:542: Writing 446 bytes to table reg key sip:6505550097 at example.com, CAS = 0, expiry = 310
10-02-2016 21:07:24.088 UTC Debug memcachedstore.cpp:195: Key reg\\sip:6505550097 at example.com hashes to vbucket 58 via hash 0x60d79c3a
10-02-2016 21:07:24.088 UTC Debug memcachedstore.cpp:562: 1 write replicas for key reg\\sip:6505550097 at example.com
10-02-2016 21:07:24.088 UTC Debug memcachedstore.cpp:616: Attempt conditional write to vbucket 58 on replica 0 (connection 0x1265b40), CAS = 0, expiry = 310
10-02-2016 21:07:24.088 UTC Debug memcachedstore.cpp:816: Attempting to add data for key reg\\sip:6505550097 at example.com
10-02-2016 21:07:24.088 UTC Debug memcachedstore.cpp:826: Attempting memcached ADD command
10-02-2016 21:07:24.088 UTC Debug memcachedstore.cpp:916: ADD/CAS returned rc = 0 (SUCCESS)
SUCCESS
10-02-2016 21:07:24.088 UTC Debug memcachedstore.cpp:657: Conditional write succeeded to replica 0
10-02-2016 21:07:24.088 UTC Debug subscriber_data_manager.cpp:438: Data store set_data returned 1
10-02-2016 21:07:24.088 UTC Debug registrar.cpp:116: Bindings for sip:6505550097 at example.com
10-02-2016 21:07:24.088 UTC Debug registrar.cpp:130:   sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob URI=sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob expires=1455138744 q=0 from=rpPSADxNjjEHLOh3WJoNliu-mE8QeGcx cseq=39545 timer=00071bdec00000030040001000104104 private_id=6505550097 at example.com emergency_registration=false
10-02-2016 21:07:24.088 UTC Debug pjsip:       endpoint Response msg 200/REGISTER/cseq=39545 (tdta0x12f4940) created
10-02-2016 21:07:24.088 UTC Verbose common_sip_processing.cpp:136: TX 860 bytes Response msg 200/REGISTER/cseq=39545 (tdta0x12f4940) to TCP 2400:6180:0:d0::18b:3001:52176:
--start msg--

SIP/2.0 200 OK
Service-Route: <sip:[2400:6180:0:d0::18b:3001]:5054;transport=TCP;lr;orig>
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001]:52176;rport=52176;received=2400:6180:0:d0::18b:3001;branch=z9hG4bKPjkCkaKE0ron-ujXbJM-3GxJ9kNmmBWE-a
Via: SIP/2.0/UDP [2400:6180:0:d0::12b:6001]:5070;rport=5070;received=2400:6180:0:d0::12b:6001;branch=z9hG4bKPj50j2Ay3PdKjSNIc1RcAfmwqOD2ivk.mb
Call-ID: rpPSADxNjjEHLOh3WJoNliu-mE8QeGcx
From: <sip:6505550097 at example.com>;tag=tTGWc.BJFJzyCRIodO6GiWzrEAkAbKqH
To: <sip:6505550097 at example.com>;tag=z9hG4bKPjkCkaKE0ron-ujXbJM-3GxJ9kNmmBWE-a
CSeq: 39545 REGISTER
Supported: outbound
Contact: <sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob>;expires=300
Require: outbound
Path: <sip:JWSsk2IumG@[2400:6180:0:d0::18b:3001]:5058;transport=TCP;lr;ob>
P-Associated-URI: <sip:6505550097 at example.com>
Content-Length:  0


--end msg--
10-02-2016 21:07:24.088 UTC Debug acr.cpp:83: Sending Null ACR (0x12455b0)
10-02-2016 21:07:24.088 UTC Debug acr.cpp:54: Destroyed ACR (0x12455b0)
10-02-2016 21:07:24.088 UTC Debug ifchandler.cpp:763: Interpreting orig IFC information
10-02-2016 21:07:24.088 UTC Debug ifchandler.cpp:437: SPT class Method: result false
10-02-2016 21:07:24.088 UTC Debug ifchandler.cpp:541: Add to group 0 val false
10-02-2016 21:07:24.088 UTC Debug ifchandler.cpp:559: Result group 0 val false
10-02-2016 21:07:24.088 UTC Debug ifchandler.cpp:572: iFC does not match
10-02-2016 21:07:24.088 UTC Info registration_utils.cpp:187: Found 0 Application Servers
10-02-2016 21:07:24.088 UTC Debug pjsip:  tdta0x12f4940 Destroying txdata Response msg 200/REGISTER/cseq=39545 (tdta0x12f4940)
10-02-2016 21:07:24.088 UTC Debug registrar.cpp:1036: Report SAS end marker - trail (b9)
10-02-2016 21:07:24.088 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b80b5838
10-02-2016 21:07:24.088 UTC Debug thread_dispatcher.cpp:199: Request latency = 18847us
10-02-2016 21:07:26.788 UTC Verbose pjsip:    tcplis:5054 TCP listener 2400:6180:0:d0::18b:3001:5054: got incoming TCP connection from [2400:6180:0:d0::18b:3001]:35196, sock=749
10-02-2016 21:07:26.788 UTC Verbose pjsip: tcps0x7f07b80b TCP server transport created
10-02-2016 21:07:26.789 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1866 (rdata0x7f07b80b5af0)
10-02-2016 21:07:26.789 UTC Verbose common_sip_processing.cpp:120: RX 413 bytes Request msg OPTIONS/cseq=1866 (rdata0x7f07b80b5af0) from TCP 2400:6180:0:d0::18b:3001:35196:
--start msg--

OPTIONS sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054 SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport;branch=z9hG4bK-1866
Max-Forwards: 2
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054>
From: poll-sip <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1866
Call-ID: poll-sip-1866
CSeq: 1866 OPTIONS
Contact: <sip:[2400:6180:0:d0::18b:3001]>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
10-02-2016 21:07:26.789 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:26.789 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:07:26.789 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
10-02-2016 21:07:26.789 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b815a818 for worker threads
10-02-2016 21:07:26.789 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b815a818
10-02-2016 21:07:26.789 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1866 (rdata0x7f07b815a818)
10-02-2016 21:07:26.789 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:26.789 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:07:26.789 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1866 (tdta0x7f074c00df50) created
10-02-2016 21:07:26.789 UTC Verbose common_sip_processing.cpp:136: TX 326 bytes Response msg 200/OPTIONS/cseq=1866 (tdta0x7f074c00df50) to TCP 2400:6180:0:d0::18b:3001:35196:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport=35196;received=2400:6180:0:d0::18b:3001;branch=z9hG4bK-1866
Call-ID: poll-sip-1866
From: "poll-sip" <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1866
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=z9hG4bK-1866
CSeq: 1866 OPTIONS
Content-Length:  0


--end msg--
10-02-2016 21:07:26.789 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
10-02-2016 21:07:26.789 UTC Debug pjsip: tdta0x7f074c00 Destroying txdata Response msg 200/OPTIONS/cseq=1866 (tdta0x7f074c00df50)
10-02-2016 21:07:26.789 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b815a818
10-02-2016 21:07:26.789 UTC Debug thread_dispatcher.cpp:199: Request latency = 304us
10-02-2016 21:07:26.791 UTC Verbose httpstack.cpp:286: Process request for URL /ping, args (null)
10-02-2016 21:07:26.791 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)

==> /var/log/sprout/access_current.txt <==
10-02-2016 21:07:26.791 UTC 200 GET /ping 0.000000 seconds

==> /var/log/sprout/sprout_current.txt <==
10-02-2016 21:07:27.790 UTC Verbose pjsip: tcps0x7f07b80b TCP connection closed
10-02-2016 21:07:27.790 UTC Debug connection_tracker.cpp:91: Connection 0x7f07b80b57b8 has been destroyed
10-02-2016 21:07:27.790 UTC Verbose pjsip: tcps0x7f07b80b TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
10-02-2016 21:07:36.968 UTC Verbose pjsip:    tcplis:5054 TCP listener 2400:6180:0:d0::18b:3001:5054: got incoming TCP connection from [2400:6180:0:d0::18b:3001]:35208, sock=749
10-02-2016 21:07:36.968 UTC Verbose pjsip: tcps0x7f07b80b TCP server transport created
10-02-2016 21:07:36.969 UTC Verbose httpstack.cpp:286: Process request for URL /ping, args (null)
10-02-2016 21:07:36.969 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)

==> /var/log/sprout/access_current.txt <==
10-02-2016 21:07:36.969 UTC 200 GET /ping 0.000000 seconds

==> /var/log/sprout/sprout_current.txt <==
10-02-2016 21:07:36.975 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1876 (rdata0x7f07b80b5af0)
10-02-2016 21:07:36.975 UTC Verbose common_sip_processing.cpp:120: RX 413 bytes Request msg OPTIONS/cseq=1876 (rdata0x7f07b80b5af0) from TCP 2400:6180:0:d0::18b:3001:35208:
--start msg--

OPTIONS sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054 SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport;branch=z9hG4bK-1876
Max-Forwards: 2
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054>
From: poll-sip <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1876
Call-ID: poll-sip-1876
CSeq: 1876 OPTIONS
Contact: <sip:[2400:6180:0:d0::18b:3001]>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
10-02-2016 21:07:36.975 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:36.975 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:07:36.975 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
10-02-2016 21:07:36.975 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b815a818 for worker threads
10-02-2016 21:07:36.975 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b815a818
10-02-2016 21:07:36.975 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1876 (rdata0x7f07b815a818)
10-02-2016 21:07:36.975 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:36.975 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:07:36.975 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1876 (tdta0x7f075800df50) created
10-02-2016 21:07:36.975 UTC Verbose common_sip_processing.cpp:136: TX 326 bytes Response msg 200/OPTIONS/cseq=1876 (tdta0x7f075800df50) to TCP 2400:6180:0:d0::18b:3001:35208:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport=35208;received=2400:6180:0:d0::18b:3001;branch=z9hG4bK-1876
Call-ID: poll-sip-1876
From: "poll-sip" <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1876
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=z9hG4bK-1876
CSeq: 1876 OPTIONS
Content-Length:  0


--end msg--
10-02-2016 21:07:36.975 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
10-02-2016 21:07:36.975 UTC Debug pjsip: tdta0x7f075800 Destroying txdata Response msg 200/OPTIONS/cseq=1876 (tdta0x7f075800df50)
10-02-2016 21:07:36.975 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b815a818
10-02-2016 21:07:36.975 UTC Debug thread_dispatcher.cpp:199: Request latency = 587us
10-02-2016 21:07:37.975 UTC Verbose pjsip: tcps0x7f07b80b TCP connection closed
10-02-2016 21:07:37.975 UTC Debug connection_tracker.cpp:91: Connection 0x7f07b80b57b8 has been destroyed
10-02-2016 21:07:37.975 UTC Verbose pjsip: tcps0x7f07b80b TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
10-02-2016 21:07:42.922 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg INVITE/cseq=12984 (rdata0x7f07b809e4b0)
10-02-2016 21:07:42.922 UTC Verbose common_sip_processing.cpp:120: RX 1591 bytes Request msg INVITE/cseq=12984 (rdata0x7f07b809e4b0) from TCP 2400:6180:0:d0::18b:3001:50065:
--start msg--

INVITE sip:6505550098 at example.com SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001]:50065;rport;branch=z9hG4bKPjcXdmum6JycTAGVJe9RH6b26I3hz3R5LU
Record-Route: <sip:[2400:6180:0:d0::18b:3001]:5058;transport=TCP;lr>
Record-Route: <sip:JWSsk2IumG@[2400:6180:0:d0::18b:3001]:5060;transport=UDP;lr>
Via: SIP/2.0/UDP [2400:6180:0:d0::12b:6001]:5070;rport=5070;received=2400:6180:0:d0::12b:6001;branch=z9hG4bKPjTsG.fg54OPAbuOUZmERX-Di0xs8qkyUk
Max-Forwards: 70
From: <sip:6505550097 at example.com>;tag=9YeJjsfl3nzIgQd15EJ9RWO1SR7AaEXb
To: <sip:6505550098 at example.com>
Contact: <sip:6505550097@[2400:6180:0:d0::12b:6001]:5070;ob>
Call-ID: .bMySS7Bo0w.L2cm0TL140Hbex6OoDxt
CSeq: 12984 INVITE
Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, INFO, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
Supported: replaces, 100rel, timer, norefersub
Session-Expires: 600
Min-SE: 90
User-Agent: PJSUA v2.4.5 Linux-3.13.0.71/x86_64/glibc-2.19
P-Asserted-Identity: <sip:6505550097 at example.com>
Route: <sip:[2400:6180:0:d0::18b:3001]:5054;transport=TCP;lr;orig>
Content-Type: application/sdp
Content-Length:   482

v=0
o=- 3664127262 3664127262 IN IP4 128.199.195.208
s=pjmedia
b=AS:84
t=0 0
a=X-nat:0
m=audio 4000 RTP/AVP 98 97 99 104 3 0 8 9 96
c=IN IP4 128.199.195.208
b=TIAS:64000
a=rtcp:4001 IN IP4 128.199.195.208
a=sendrecv
a=rtpmap:98 speex/16000
a=rtpmap:97 speex/8000
a=rtpmap:99 speex/32000
a=rtpmap:104 iLBC/8000
a=fmtp:104 mode=30
a=rtpmap:3 GSM/8000
a=rtpmap:0 PCMU/8000
a=rtpmap:8 PCMA/8000
a=rtpmap:9 G722/8000
a=rtpmap:96 telephone-event/8000
a=fmtp:96 0-16

--end msg--
10-02-2016 21:07:42.923 UTC Debug pjutils.cpp:1648: Logging SAS Call-ID marker, Call-ID .bMySS7Bo0w.L2cm0TL140Hbex6OoDxt
10-02-2016 21:07:42.923 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b80b5838 for worker threads
10-02-2016 21:07:42.923 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b80b5838
10-02-2016 21:07:42.923 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg INVITE/cseq=12984 (rdata0x7f07b80b5838)
10-02-2016 21:07:42.923 UTC Debug uri_classifier.cpp:167: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:42.923 UTC Debug uri_classifier.cpp:197: Classified URI as 4
10-02-2016 21:07:42.923 UTC Debug authentication.cpp:673: Authentication module invoked
10-02-2016 21:07:42.923 UTC Debug authentication.cpp:683: Request does not need authentication
10-02-2016 21:07:42.923 UTC Debug uri_classifier.cpp:167: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:42.923 UTC Debug uri_classifier.cpp:197: Classified URI as 4
10-02-2016 21:07:42.923 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b80b5838
10-02-2016 21:07:42.923 UTC Debug thread_dispatcher.cpp:199: Request latency = 402us
10-02-2016 21:07:47.032 UTC Verbose pjsip:    tcplis:5054 TCP listener 2400:6180:0:d0::18b:3001:5054: got incoming TCP connection from [2400:6180:0:d0::18b:3001]:35218, sock=749
10-02-2016 21:07:47.032 UTC Verbose pjsip: tcps0x7f07b80b TCP server transport created
10-02-2016 21:07:47.034 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1886 (rdata0x7f07b80b5af0)
10-02-2016 21:07:47.034 UTC Verbose common_sip_processing.cpp:120: RX 413 bytes Request msg OPTIONS/cseq=1886 (rdata0x7f07b80b5af0) from TCP 2400:6180:0:d0::18b:3001:35218:
--start msg--

OPTIONS sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054 SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport;branch=z9hG4bK-1886
Max-Forwards: 2
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054>
From: poll-sip <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1886
Call-ID: poll-sip-1886
CSeq: 1886 OPTIONS
Contact: <sip:[2400:6180:0:d0::18b:3001]>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
10-02-2016 21:07:47.034 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:47.034 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:07:47.034 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
10-02-2016 21:07:47.035 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b815a818 for worker threads
10-02-2016 21:07:47.035 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b815a818
10-02-2016 21:07:47.035 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1886 (rdata0x7f07b815a818)
10-02-2016 21:07:47.035 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:47.035 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:07:47.035 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1886 (tdta0x7f076000df50) created
10-02-2016 21:07:47.035 UTC Verbose common_sip_processing.cpp:136: TX 326 bytes Response msg 200/OPTIONS/cseq=1886 (tdta0x7f076000df50) to TCP 2400:6180:0:d0::18b:3001:35218:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport=35218;received=2400:6180:0:d0::18b:3001;branch=z9hG4bK-1886
Call-ID: poll-sip-1886
From: "poll-sip" <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1886
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=z9hG4bK-1886
CSeq: 1886 OPTIONS
Content-Length:  0


--end msg--
10-02-2016 21:07:47.035 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
10-02-2016 21:07:47.035 UTC Debug pjsip: tdta0x7f076000 Destroying txdata Response msg 200/OPTIONS/cseq=1886 (tdta0x7f076000df50)
10-02-2016 21:07:47.035 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b815a818
10-02-2016 21:07:47.035 UTC Debug thread_dispatcher.cpp:199: Request latency = 421us
10-02-2016 21:07:47.036 UTC Verbose httpstack.cpp:286: Process request for URL /ping, args (null)
10-02-2016 21:07:47.036 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)

==> /var/log/sprout/access_current.txt <==
10-02-2016 21:07:47.037 UTC 200 GET /ping 0.000000 seconds

==> /var/log/sprout/sprout_current.txt <==
10-02-2016 21:07:48.036 UTC Verbose pjsip: tcps0x7f07b80b TCP connection closed
10-02-2016 21:07:48.036 UTC Debug connection_tracker.cpp:91: Connection 0x7f07b80b57b8 has been destroyed
10-02-2016 21:07:48.036 UTC Verbose pjsip: tcps0x7f07b80b TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
10-02-2016 21:07:50.780 UTC Verbose httpstack.cpp:286: Process request for URL /authentication-timeout, args (null)
10-02-2016 21:07:50.780 UTC Debug memcachedstore.cpp:195: Key av\\6505550098 at example.com\131ff1d565852607 hashes to vbucket 12 via hash 0x1f60a18c
10-02-2016 21:07:50.780 UTC Debug memcachedstore.cpp:236: Set up new view 1 for thread
10-02-2016 21:07:50.781 UTC Debug memcachedstore.cpp:243: Setting up server 0 for connection 0x7f077c034400 (--CONNECT-TIMEOUT=10 --SUPPORT-CAS --POLL-TIMEOUT=250 --BINARY-PROTOCOL)
10-02-2016 21:07:50.781 UTC Debug memcachedstore.cpp:245: Set up connection 0x7f077c034470 to server [2400:6180:0:d0::18b:3001]:11211
10-02-2016 21:07:50.781 UTC Debug memcachedstore.cpp:256: Setting server to IP address 2400:6180:0:d0::18b:3001 port 11211
10-02-2016 21:07:50.781 UTC Debug memcachedstore.cpp:367: 1 read replicas for key av\\6505550098 at example.com\131ff1d565852607
10-02-2016 21:07:50.782 UTC Debug memcachedstore.cpp:402: Attempt to read from replica 0 (connection 0x7f077c034470)
10-02-2016 21:07:50.782 UTC Debug memcachedstore.cpp:780: Fetch result
10-02-2016 21:07:50.782 UTC Debug memcachedstore.cpp:788: Found record on replica
10-02-2016 21:07:50.782 UTC Debug memcachedstore.cpp:410: Read for av\\6505550098 at example.com\131ff1d565852607 on replica 0 returned SUCCESS
10-02-2016 21:07:50.782 UTC Debug memcachedstore.cpp:453: Read 158 bytes from table av key 6505550098 at example.com\131ff1d565852607, CAS = 2
10-02-2016 21:07:50.782 UTC Debug communicationmonitor.cpp:82: Checking communication changes - successful attempts 10, failures 0
10-02-2016 21:07:50.782 UTC Debug avstore.cpp:106: Retrieved AV for 6505550098 at example.com\131ff1d565852607
{"digest":{"ha1":"ed7e862749772ad47596b5bd3c0f2e43","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjjroq9YI-eUVuYSb0le9cNDFAuRIBJ0c1","tombstone":true}
10-02-2016 21:07:50.782 UTC Debug handlers.cpp:651: Tombstone record indicates Authentication Vector has been used successfully - ignoring timer pop
10-02-2016 21:07:50.782 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /authentication-timeout, args (null)

==> /var/log/sprout/access_current.txt <==
10-02-2016 21:07:50.783 UTC 200 POST /authentication-timeout 0.003002 seconds

==> /var/log/sprout/sprout_current.txt <==
10-02-2016 21:07:54.074 UTC Verbose httpstack.cpp:286: Process request for URL /authentication-timeout, args (null)
10-02-2016 21:07:54.075 UTC Debug memcachedstore.cpp:195: Key av\\6505550097 at example.com\6cb85d3b61725048 hashes to vbucket 14 via hash 0xe64d760e
10-02-2016 21:07:54.075 UTC Debug memcachedstore.cpp:236: Set up new view 1 for thread
10-02-2016 21:07:54.075 UTC Debug memcachedstore.cpp:243: Setting up server 0 for connection 0x7f0774023320 (--CONNECT-TIMEOUT=10 --SUPPORT-CAS --POLL-TIMEOUT=250 --BINARY-PROTOCOL)
10-02-2016 21:07:54.076 UTC Debug memcachedstore.cpp:245: Set up connection 0x7f0774023390 to server [2400:6180:0:d0::18b:3001]:11211
10-02-2016 21:07:54.076 UTC Debug memcachedstore.cpp:256: Setting server to IP address 2400:6180:0:d0::18b:3001 port 11211
10-02-2016 21:07:54.077 UTC Debug memcachedstore.cpp:367: 1 read replicas for key av\\6505550097 at example.com\6cb85d3b61725048
10-02-2016 21:07:54.077 UTC Debug memcachedstore.cpp:402: Attempt to read from replica 0 (connection 0x7f0774023390)
10-02-2016 21:07:54.077 UTC Debug memcachedstore.cpp:780: Fetch result
10-02-2016 21:07:54.078 UTC Debug memcachedstore.cpp:788: Found record on replica
10-02-2016 21:07:54.078 UTC Debug memcachedstore.cpp:410: Read for av\\6505550097 at example.com\6cb85d3b61725048 on replica 0 returned SUCCESS
10-02-2016 21:07:54.078 UTC Debug memcachedstore.cpp:453: Read 158 bytes from table av key 6505550097 at example.com\6cb85d3b61725048, CAS = 5
10-02-2016 21:07:54.078 UTC Debug avstore.cpp:106: Retrieved AV for 6505550097 at example.com\6cb85d3b61725048
{"digest":{"ha1":"38a357edc0327687dd326d7416c3de69","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjF6CDH5bJ8L0DTuwzm0pjifV3rL3z2eTQ","tombstone":true}
10-02-2016 21:07:54.079 UTC Debug handlers.cpp:651: Tombstone record indicates Authentication Vector has been used successfully - ignoring timer pop
10-02-2016 21:07:54.079 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /authentication-timeout, args (null)

==> /var/log/sprout/access_current.txt <==
10-02-2016 21:07:54.079 UTC 200 POST /authentication-timeout 0.004859 seconds

==> /var/log/sprout/sprout_current.txt <==
10-02-2016 21:07:57.091 UTC Verbose pjsip:    tcplis:5054 TCP listener 2400:6180:0:d0::18b:3001:5054: got incoming TCP connection from [2400:6180:0:d0::18b:3001]:35233, sock=753
10-02-2016 21:07:57.091 UTC Verbose pjsip: tcps0x7f07b80b TCP server transport created
10-02-2016 21:07:57.101 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1896 (rdata0x7f07b80b5af0)
10-02-2016 21:07:57.101 UTC Verbose common_sip_processing.cpp:120: RX 413 bytes Request msg OPTIONS/cseq=1896 (rdata0x7f07b80b5af0) from TCP 2400:6180:0:d0::18b:3001:35233:
--start msg--

OPTIONS sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054 SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport;branch=z9hG4bK-1896
Max-Forwards: 2
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054>
From: poll-sip <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1896
Call-ID: poll-sip-1896
CSeq: 1896 OPTIONS
Contact: <sip:[2400:6180:0:d0::18b:3001]>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
10-02-2016 21:07:57.101 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:57.101 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:07:57.101 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
10-02-2016 21:07:57.101 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b815a818 for worker threads
10-02-2016 21:07:57.101 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b815a818
10-02-2016 21:07:57.101 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1896 (rdata0x7f07b815a818)
10-02-2016 21:07:57.101 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:07:57.101 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:07:57.101 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1896 (tdta0x7f075c00df50) created
10-02-2016 21:07:57.101 UTC Verbose common_sip_processing.cpp:136: TX 326 bytes Response msg 200/OPTIONS/cseq=1896 (tdta0x7f075c00df50) to TCP 2400:6180:0:d0::18b:3001:35233:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport=35233;received=2400:6180:0:d0::18b:3001;branch=z9hG4bK-1896
Call-ID: poll-sip-1896
From: "poll-sip" <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1896
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=z9hG4bK-1896
CSeq: 1896 OPTIONS
Content-Length:  0


--end msg--
10-02-2016 21:07:57.101 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
10-02-2016 21:07:57.101 UTC Debug pjsip: tdta0x7f075c00 Destroying txdata Response msg 200/OPTIONS/cseq=1896 (tdta0x7f075c00df50)
10-02-2016 21:07:57.101 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b815a818
10-02-2016 21:07:57.101 UTC Debug thread_dispatcher.cpp:199: Request latency = 610us
10-02-2016 21:07:57.129 UTC Verbose httpstack.cpp:286: Process request for URL /ping, args (null)
10-02-2016 21:07:57.129 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)

==> /var/log/sprout/access_current.txt <==
10-02-2016 21:07:57.129 UTC 200 GET /ping 0.000000 seconds

==> /var/log/sprout/sprout_current.txt <==
10-02-2016 21:07:58.102 UTC Verbose pjsip: tcps0x7f07b80b TCP connection closed
10-02-2016 21:07:58.102 UTC Debug connection_tracker.cpp:91: Connection 0x7f07b80b57b8 has been destroyed
10-02-2016 21:07:58.102 UTC Verbose pjsip: tcps0x7f07b80b TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
10-02-2016 21:08:07.055 UTC Verbose pjsip:    tcplis:5054 TCP listener 2400:6180:0:d0::18b:3001:5054: got incoming TCP connection from [2400:6180:0:d0::18b:3001]:35243, sock=753
10-02-2016 21:08:07.055 UTC Verbose pjsip: tcps0x7f07b80b TCP server transport created
10-02-2016 21:08:07.068 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1906 (rdata0x7f07b80b5af0)
10-02-2016 21:08:07.069 UTC Verbose common_sip_processing.cpp:120: RX 413 bytes Request msg OPTIONS/cseq=1906 (rdata0x7f07b80b5af0) from TCP 2400:6180:0:d0::18b:3001:35243:
--start msg--

OPTIONS sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054 SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport;branch=z9hG4bK-1906
Max-Forwards: 2
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054>
From: poll-sip <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1906
Call-ID: poll-sip-1906
CSeq: 1906 OPTIONS
Contact: <sip:[2400:6180:0:d0::18b:3001]>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
10-02-2016 21:08:07.069 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:08:07.069 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:08:07.069 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
10-02-2016 21:08:07.069 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b815a818 for worker threads
10-02-2016 21:08:07.069 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b815a818
10-02-2016 21:08:07.069 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1906 (rdata0x7f07b815a818)
10-02-2016 21:08:07.069 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:08:07.069 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:08:07.069 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1906 (tdta0x7f076800df50) created
10-02-2016 21:08:07.069 UTC Verbose common_sip_processing.cpp:136: TX 326 bytes Response msg 200/OPTIONS/cseq=1906 (tdta0x7f076800df50) to TCP 2400:6180:0:d0::18b:3001:35243:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport=35243;received=2400:6180:0:d0::18b:3001;branch=z9hG4bK-1906
Call-ID: poll-sip-1906
From: "poll-sip" <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1906
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=z9hG4bK-1906
CSeq: 1906 OPTIONS
Content-Length:  0


--end msg--
10-02-2016 21:08:07.069 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
10-02-2016 21:08:07.069 UTC Debug pjsip: tdta0x7f076800 Destroying txdata Response msg 200/OPTIONS/cseq=1906 (tdta0x7f076800df50)
10-02-2016 21:08:07.069 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b815a818
10-02-2016 21:08:07.069 UTC Debug thread_dispatcher.cpp:199: Request latency = 345us
10-02-2016 21:08:07.092 UTC Verbose httpstack.cpp:286: Process request for URL /ping, args (null)
10-02-2016 21:08:07.092 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)

==> /var/log/sprout/access_current.txt <==
10-02-2016 21:08:07.092 UTC 200 GET /ping 0.000000 seconds

==> /var/log/sprout/sprout_current.txt <==
10-02-2016 21:08:08.070 UTC Verbose pjsip: tcps0x7f07b80b TCP connection closed
10-02-2016 21:08:08.070 UTC Debug connection_tracker.cpp:91: Connection 0x7f07b80b57b8 has been destroyed
10-02-2016 21:08:08.070 UTC Verbose pjsip: tcps0x7f07b80b TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
10-02-2016 21:08:17.117 UTC Verbose httpstack.cpp:286: Process request for URL /ping, args (null)
10-02-2016 21:08:17.117 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)

==> /var/log/sprout/access_current.txt <==
10-02-2016 21:08:17.117 UTC 200 GET /ping 0.000000 seconds

==> /var/log/sprout/sprout_current.txt <==
10-02-2016 21:08:17.128 UTC Verbose pjsip:    tcplis:5054 TCP listener 2400:6180:0:d0::18b:3001:5054: got incoming TCP connection from [2400:6180:0:d0::18b:3001]:35258, sock=753
10-02-2016 21:08:17.128 UTC Verbose pjsip: tcps0x7f07b80b TCP server transport created
10-02-2016 21:08:17.129 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1916 (rdata0x7f07b80b5af0)
10-02-2016 21:08:17.129 UTC Verbose common_sip_processing.cpp:120: RX 413 bytes Request msg OPTIONS/cseq=1916 (rdata0x7f07b80b5af0) from TCP 2400:6180:0:d0::18b:3001:35258:
--start msg--

OPTIONS sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054 SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport;branch=z9hG4bK-1916
Max-Forwards: 2
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054>
From: poll-sip <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1916
Call-ID: poll-sip-1916
CSeq: 1916 OPTIONS
Contact: <sip:[2400:6180:0:d0::18b:3001]>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
10-02-2016 21:08:17.129 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:08:17.129 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:08:17.129 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
10-02-2016 21:08:17.129 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b815a818 for worker threads
10-02-2016 21:08:17.129 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b815a818
10-02-2016 21:08:17.129 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1916 (rdata0x7f07b815a818)
10-02-2016 21:08:17.129 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:08:17.129 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:08:17.129 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1916 (tdta0x7f076400df50) created
10-02-2016 21:08:17.129 UTC Verbose common_sip_processing.cpp:136: TX 326 bytes Response msg 200/OPTIONS/cseq=1916 (tdta0x7f076400df50) to TCP 2400:6180:0:d0::18b:3001:35258:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport=35258;received=2400:6180:0:d0::18b:3001;branch=z9hG4bK-1916
Call-ID: poll-sip-1916
From: "poll-sip" <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1916
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=z9hG4bK-1916
CSeq: 1916 OPTIONS
Content-Length:  0


--end msg--
10-02-2016 21:08:17.129 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
10-02-2016 21:08:17.129 UTC Debug pjsip: tdta0x7f076400 Destroying txdata Response msg 200/OPTIONS/cseq=1916 (tdta0x7f076400df50)
10-02-2016 21:08:17.129 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b815a818
10-02-2016 21:08:17.129 UTC Debug thread_dispatcher.cpp:199: Request latency = 293us
10-02-2016 21:08:18.129 UTC Verbose pjsip: tcps0x7f07b80b TCP connection closed
10-02-2016 21:08:18.129 UTC Debug connection_tracker.cpp:91: Connection 0x7f07b80b57b8 has been destroyed
10-02-2016 21:08:18.129 UTC Verbose pjsip: tcps0x7f07b80b TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
10-02-2016 21:08:27.158 UTC Verbose pjsip:    tcplis:5054 TCP listener 2400:6180:0:d0::18b:3001:5054: got incoming TCP connection from [2400:6180:0:d0::18b:3001]:35265, sock=753
10-02-2016 21:08:27.158 UTC Verbose pjsip: tcps0x7f07b80b TCP server transport created
10-02-2016 21:08:27.160 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1926 (rdata0x7f07b80b5af0)
10-02-2016 21:08:27.160 UTC Verbose common_sip_processing.cpp:120: RX 413 bytes Request msg OPTIONS/cseq=1926 (rdata0x7f07b80b5af0) from TCP 2400:6180:0:d0::18b:3001:35265:
--start msg--

OPTIONS sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054 SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport;branch=z9hG4bK-1926
Max-Forwards: 2
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054>
From: poll-sip <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1926
Call-ID: poll-sip-1926
CSeq: 1926 OPTIONS
Contact: <sip:[2400:6180:0:d0::18b:3001]>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
10-02-2016 21:08:27.160 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:08:27.160 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:08:27.160 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
10-02-2016 21:08:27.160 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b815a818 for worker threads
10-02-2016 21:08:27.160 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b815a818
10-02-2016 21:08:27.160 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1926 (rdata0x7f07b815a818)
10-02-2016 21:08:27.160 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:08:27.160 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:08:27.160 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1926 (tdta0x7f076c00df50) created
10-02-2016 21:08:27.160 UTC Verbose common_sip_processing.cpp:136: TX 326 bytes Response msg 200/OPTIONS/cseq=1926 (tdta0x7f076c00df50) to TCP 2400:6180:0:d0::18b:3001:35265:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport=35265;received=2400:6180:0:d0::18b:3001;branch=z9hG4bK-1926
Call-ID: poll-sip-1926
From: "poll-sip" <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1926
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=z9hG4bK-1926
CSeq: 1926 OPTIONS
Content-Length:  0


--end msg--
10-02-2016 21:08:27.160 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
10-02-2016 21:08:27.160 UTC Debug pjsip: tdta0x7f076c00 Destroying txdata Response msg 200/OPTIONS/cseq=1926 (tdta0x7f076c00df50)
10-02-2016 21:08:27.160 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b815a818
10-02-2016 21:08:27.160 UTC Debug thread_dispatcher.cpp:199: Request latency = 354us
10-02-2016 21:08:27.192 UTC Verbose httpstack.cpp:286: Process request for URL /ping, args (null)
10-02-2016 21:08:27.192 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)

==> /var/log/sprout/access_current.txt <==
10-02-2016 21:08:27.192 UTC 200 GET /ping 0.000000 seconds

==> /var/log/sprout/sprout_current.txt <==
10-02-2016 21:08:28.160 UTC Verbose pjsip: tcps0x7f07b80b TCP connection closed
10-02-2016 21:08:28.160 UTC Debug connection_tracker.cpp:91: Connection 0x7f07b80b57b8 has been destroyed
10-02-2016 21:08:28.160 UTC Verbose pjsip: tcps0x7f07b80b TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
10-02-2016 21:08:37.210 UTC Verbose pjsip:    tcplis:5054 TCP listener 2400:6180:0:d0::18b:3001:5054: got incoming TCP connection from [2400:6180:0:d0::18b:3001]:35276, sock=753
10-02-2016 21:08:37.210 UTC Verbose pjsip: tcps0x7f07b80b TCP server transport created
10-02-2016 21:08:37.210 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1936 (rdata0x7f07b80b5af0)
10-02-2016 21:08:37.210 UTC Verbose common_sip_processing.cpp:120: RX 413 bytes Request msg OPTIONS/cseq=1936 (rdata0x7f07b80b5af0) from TCP 2400:6180:0:d0::18b:3001:35276:
--start msg--

OPTIONS sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054 SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport;branch=z9hG4bK-1936
Max-Forwards: 2
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054>
From: poll-sip <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1936
Call-ID: poll-sip-1936
CSeq: 1936 OPTIONS
Contact: <sip:[2400:6180:0:d0::18b:3001]>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
10-02-2016 21:08:37.210 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:08:37.210 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:08:37.210 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
10-02-2016 21:08:37.210 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b815a818 for worker threads
10-02-2016 21:08:37.210 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b815a818
10-02-2016 21:08:37.210 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1936 (rdata0x7f07b815a818)
10-02-2016 21:08:37.210 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:08:37.210 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:08:37.210 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1936 (tdta0x7f0778010330) created
10-02-2016 21:08:37.210 UTC Verbose common_sip_processing.cpp:136: TX 326 bytes Response msg 200/OPTIONS/cseq=1936 (tdta0x7f0778010330) to TCP 2400:6180:0:d0::18b:3001:35276:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport=35276;received=2400:6180:0:d0::18b:3001;branch=z9hG4bK-1936
Call-ID: poll-sip-1936
From: "poll-sip" <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1936
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=z9hG4bK-1936
CSeq: 1936 OPTIONS
Content-Length:  0


--end msg--
10-02-2016 21:08:37.210 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
10-02-2016 21:08:37.210 UTC Debug pjsip: tdta0x7f077801 Destroying txdata Response msg 200/OPTIONS/cseq=1936 (tdta0x7f0778010330)
10-02-2016 21:08:37.210 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b815a818
10-02-2016 21:08:37.211 UTC Debug thread_dispatcher.cpp:199: Request latency = 379us
10-02-2016 21:08:37.238 UTC Verbose httpstack.cpp:286: Process request for URL /ping, args (null)
10-02-2016 21:08:37.238 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)

==> /var/log/sprout/access_current.txt <==
10-02-2016 21:08:37.238 UTC 200 GET /ping 0.000000 seconds

==> /var/log/sprout/sprout_current.txt <==
10-02-2016 21:08:38.210 UTC Verbose pjsip: tcps0x7f07b80b TCP connection closed
10-02-2016 21:08:38.210 UTC Debug connection_tracker.cpp:91: Connection 0x7f07b80b57b8 has been destroyed
10-02-2016 21:08:38.210 UTC Verbose pjsip: tcps0x7f07b80b TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
10-02-2016 21:08:47.239 UTC Verbose pjsip:    tcplis:5054 TCP listener 2400:6180:0:d0::18b:3001:5054: got incoming TCP connection from [2400:6180:0:d0::18b:3001]:35287, sock=753
10-02-2016 21:08:47.240 UTC Verbose pjsip: tcps0x7f07b80b TCP server transport created
10-02-2016 21:08:47.240 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1946 (rdata0x7f07b80b5af0)
10-02-2016 21:08:47.240 UTC Verbose common_sip_processing.cpp:120: RX 413 bytes Request msg OPTIONS/cseq=1946 (rdata0x7f07b80b5af0) from TCP 2400:6180:0:d0::18b:3001:35287:
--start msg--

OPTIONS sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054 SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport;branch=z9hG4bK-1946
Max-Forwards: 2
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054>
From: poll-sip <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1946
Call-ID: poll-sip-1946
CSeq: 1946 OPTIONS
Contact: <sip:[2400:6180:0:d0::18b:3001]>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
10-02-2016 21:08:47.240 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:08:47.240 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:08:47.240 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
10-02-2016 21:08:47.240 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b815a818 for worker threads
10-02-2016 21:08:47.240 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b815a818
10-02-2016 21:08:47.240 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1946 (rdata0x7f07b815a818)
10-02-2016 21:08:47.240 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:08:47.240 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:08:47.240 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1946 (tdta0x7f07740132b0) created
10-02-2016 21:08:47.240 UTC Verbose common_sip_processing.cpp:136: TX 326 bytes Response msg 200/OPTIONS/cseq=1946 (tdta0x7f07740132b0) to TCP 2400:6180:0:d0::18b:3001:35287:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport=35287;received=2400:6180:0:d0::18b:3001;branch=z9hG4bK-1946
Call-ID: poll-sip-1946
From: "poll-sip" <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1946
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=z9hG4bK-1946
CSeq: 1946 OPTIONS
Content-Length:  0


--end msg--
10-02-2016 21:08:47.240 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
10-02-2016 21:08:47.240 UTC Debug pjsip: tdta0x7f077401 Destroying txdata Response msg 200/OPTIONS/cseq=1946 (tdta0x7f07740132b0)
10-02-2016 21:08:47.240 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b815a818
10-02-2016 21:08:47.240 UTC Debug thread_dispatcher.cpp:199: Request latency = 269us
10-02-2016 21:08:47.274 UTC Verbose httpstack.cpp:286: Process request for URL /ping, args (null)
10-02-2016 21:08:47.274 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)

==> /var/log/sprout/access_current.txt <==
10-02-2016 21:08:47.274 UTC 200 GET /ping 0.000000 seconds

==> /var/log/sprout/sprout_current.txt <==
10-02-2016 21:08:48.240 UTC Verbose pjsip: tcps0x7f07b80b TCP connection closed
10-02-2016 21:08:48.241 UTC Debug connection_tracker.cpp:91: Connection 0x7f07b80b57b8 has been destroyed
10-02-2016 21:08:48.241 UTC Verbose pjsip: tcps0x7f07b80b TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
10-02-2016 21:08:57.423 UTC Verbose pjsip:    tcplis:5054 TCP listener 2400:6180:0:d0::18b:3001:5054: got incoming TCP connection from [2400:6180:0:d0::18b:3001]:35299, sock=753
10-02-2016 21:08:57.423 UTC Verbose pjsip: tcps0x7f07b80b TCP server transport created
10-02-2016 21:08:57.424 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1956 (rdata0x7f07b80b5af0)
10-02-2016 21:08:57.424 UTC Verbose common_sip_processing.cpp:120: RX 413 bytes Request msg OPTIONS/cseq=1956 (rdata0x7f07b80b5af0) from TCP 2400:6180:0:d0::18b:3001:35299:
--start msg--

OPTIONS sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054 SIP/2.0
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport;branch=z9hG4bK-1956
Max-Forwards: 2
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]:5054>
From: poll-sip <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1956
Call-ID: poll-sip-1956
CSeq: 1956 OPTIONS
Contact: <sip:[2400:6180:0:d0::18b:3001]>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
10-02-2016 21:08:57.424 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:08:57.424 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:08:57.424 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
10-02-2016 21:08:57.424 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f07b815a818 for worker threads
10-02-2016 21:08:57.424 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f07b815a818
10-02-2016 21:08:57.424 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1956 (rdata0x7f07b815a818)
10-02-2016 21:08:57.424 UTC Debug uri_classifier.cpp:167: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
10-02-2016 21:08:57.424 UTC Debug uri_classifier.cpp:197: Classified URI as 3
10-02-2016 21:08:57.424 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1956 (tdta0x7f077c024390) created
10-02-2016 21:08:57.424 UTC Verbose common_sip_processing.cpp:136: TX 326 bytes Response msg 200/OPTIONS/cseq=1956 (tdta0x7f077c024390) to TCP 2400:6180:0:d0::18b:3001:35299:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP [2400:6180:0:d0::18b:3001];rport=35299;received=2400:6180:0:d0::18b:3001;branch=z9hG4bK-1956
Call-ID: poll-sip-1956
From: "poll-sip" <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=1956
To: <sip:poll-sip@[2400:6180:0:d0::18b:3001]>;tag=z9hG4bK-1956
CSeq: 1956 OPTIONS
Content-Length:  0


--end msg--
10-02-2016 21:08:57.424 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
10-02-2016 21:08:57.424 UTC Debug pjsip: tdta0x7f077c02 Destroying txdata Response msg 200/OPTIONS/cseq=1956 (tdta0x7f077c024390)
10-02-2016 21:08:57.424 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f07b815a818
10-02-2016 21:08:57.424 UTC Debug thread_dispatcher.cpp:199: Request latency = 681us
10-02-2016 21:08:57.466 UTC Verbose httpstack.cpp:286: Process request for URL /ping, args (null)
10-02-2016 21:08:57.466 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)

==> /var/log/sprout/access_current.txt <==
10-02-2016 21:08:57.466 UTC 200 GET /ping 0.000000 seconds

==> /var/log/sprout/sprout_current.txt <==
10-02-2016 21:08:58.424 UTC Verbose pjsip: tcps0x7f07b80b TCP connection closed
10-02-2016 21:08:58.424 UTC Debug connection_tracker.cpp:91: Connection 0x7f07b80b57b8 has been destroyed
10-02-2016 21:08:58.424 UTC Verbose pjsip: tcps0x7f07b80b TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
^C

From marco at opencloud.com  Wed Feb 10 18:14:18 2016
From: marco at opencloud.com (Marco Costantini)
Date: Thu, 11 Feb 2016 12:14:18 +1300
Subject: [Clearwater] Sprout eats INVITE
In-Reply-To: <CAMVsSNS5UQ-30m5gMRfUbhHBV0Wu0UbEDYqyQB4Ng2VyD27KZg@mail.gmail.com>
References: <CAMVsSNS5UQ-30m5gMRfUbhHBV0Wu0UbEDYqyQB4Ng2VyD27KZg@mail.gmail.com>
Message-ID: <CAMVsSNTK7zEB_rYVfLwEvYvm5u50_b5xW4OUHYFc9-8oV7ohbQ@mail.gmail.com>

This turned out to be a contaminated installation cause by my fooling
around with aptitude and self-built sprout packages. A fresh installation
did not show such problems, even with my latest patch applied (with dpkg).

So please ignore. Thank you,
Marco.

On Thu, Feb 11, 2016 at 10:17 AM, Marco Costantini <marco at opencloud.com>
wrote:

> Hello,
>
> We are testing IPv6 calls and have run into a problem. I'll describe now.
> Any ideas for a solution would be appreciated.
>
> The environment is as such:
> - one machine has an All-in-one install (with a redeployed sprout-base
> package). this is configured for IPv6
> - two other machines each are running PJSUA sip client and, here, are
> trying to call each other through the proxy on the clearwater machine.
>
> I've attached a log from sprout, starting from just before the
> registration of the two endpoints. My problem is that the logging doesn't
> really explain why its not forwarding the message. I've done packet
> captures on all machines and it seems to me that sprout is the last stop of
> the incoming invite, and no outgoing invite exists on the wire.
>
> Please and thank you,
> Marco.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160211/0781336e/attachment.html>

From shivacharanms at gmail.com  Mon Feb 15 05:19:30 2016
From: shivacharanms at gmail.com (Shiva Charan)
Date: Mon, 15 Feb 2016 15:49:30 +0530
Subject: [Clearwater] BONO unable to resolve sprout node
Message-ID: <CAJAf6NwGyTLd1oU=n8+jHPoZ+H+98MHhsRuvi455_C+sqLcATQ@mail.gmail.com>

Hi All,

I have a manual install of clearwater setup, all the services are running
except for "poll_memento_https" will this affect the setup?

The main issue is when is see the logs of bono it has the following message.
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to
resolve sprout.cwhpe.com to an IP address - Not found (PJ_ENOTFOUND)

Bono is able to ping the sprout node with the dns name
root at bono:/var/log/bono# ping sprout.cwhpe.com
PING sprout.cwhpe.com (10.222.5.136) 56(84) bytes of data.
64 bytes from 10.222.5.136: icmp_seq=1 ttl=64 time=0.344 ms
64 bytes from 10.222.5.136: icmp_seq=2 ttl=64 time=0.389 ms
64 bytes from 10.222.5.136: icmp_seq=3 ttl=64 time=0.159 ms
64 bytes from 10.222.5.136: icmp_seq=4 ttl=64 time=0.344 ms
^C
--- sprout.cwhpe.com ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 2998ms
rtt min/avg/max/mdev = 0.159/0.309/0.389/0.088 ms
root at bono:/var/log/bono#


and the registration through zoiper client is also not going through.

Please suggest,

Shiva Charan M S
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160215/49871e91/attachment.html>

From nir.henn at amdocs.com  Tue Feb 16 06:21:34 2016
From: nir.henn at amdocs.com (Nir Henn)
Date: Tue, 16 Feb 2016 11:21:34 +0000
Subject: [Clearwater] Clearwater heat error
Message-ID: <A86BD031-515F-46A0-93BE-5E9FA86701DE@amdocs.com>

Hi there

I am having some issues with the heat template deployment of clearwater and I would appreciate your help.
I am deploying an environment for a project I am working on and from some reason it fails.
From the instance boot log I can notice the following:


ellis-0 login: Cloud-init v. 0.7.5 running 'modules:final' at Tue, 16 Feb 2016 07:43:12 +0000. Up 24.08 seconds.
Provision began: 2016-02-16 07:43:12.223433
/var/lib/heat-cfntools/cfn-userdata

+ echo 'deb http://repo.cw-ngv.com/stable binary/'
+ apt-key add -
+ curl -L http://repo.cw-ngv.com/repo_key
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  3149  100  3149    0     0   5552      0 --:--:-- --:--:-- --:--:--  5563
OK
+ apt-get update
Ign http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty InRelease
Get:1 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates InRelease [65.9 kB]
Get:2 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security InRelease [65.9 kB]
Ign http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ InRelease
Get:3 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports InRelease [64.5 kB]
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty Release.gpg
Get:4 http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ Release.gpg [819 B]
Get:5 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/main Sources [104 kB]
Get:6 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/main Sources [259 kB]
Get:7 http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ Release [1219 B]
Get:8 http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ Packages [18.6 kB]
Get:9 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/restricted Sources [5342 B]
Get:10 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/universe Sources [33.0 kB]
Get:11 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/universe Sources [150 kB]
Get:12 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/main i386 Packages [392 kB]
Get:13 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/universe i386 Packages [124 kB]
Ign http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ Translation-en
Get:14 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/main Translation-en [230 kB]
Get:15 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/universe Translation-en [72.5 kB]
Get:16 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/multiverse Sources [5547 B]
Get:17 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/main i386 Packages [683 kB]
Get:18 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/restricted i386 Packages [15.6 kB]
Get:19 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/universe i386 Packages [339 kB]
Get:20 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/multiverse i386 Packages [13.4 kB]
Get:21 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/main Translation-en [355 kB]
Get:22 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/multiverse Translation-en [6947 B]
Get:23 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/restricted Translation-en [3699 B]
Get:24 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/universe Translation-en [180 kB]
Get:25 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/main Sources [8236 B]
Get:26 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/restricted Sources [28 B]
Get:27 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/universe Sources [33.2 kB]
Get:28 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/multiverse Sources [1898 B]
Get:29 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/main i386 Packages [9442 B]
Get:30 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/restricted i386 Packages [28 B]
Get:31 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/universe i386 Packages [39.8 kB]
Get:32 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/multiverse i386 Packages [1552 B]
Get:33 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/main Translation-en [5561 B]
Get:34 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/multiverse Translation-en [1215 B]
Get:35 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/restricted Translation-en [14 B]
Get:36 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/universe Translation-en [34.6 kB]
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty Release
Get:37 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/main Sources [1064 kB]
Get:38 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/restricted Sources [5433 B]
Get:39 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/universe Sources [6399 kB]
Get:40 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/multiverse Sources [174 kB]
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/main i386 Packages
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/restricted i386 Packages
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/universe i386 Packages
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/multiverse i386 Packages
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/main Translation-en
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/multiverse Translation-en
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/restricted Translation-en
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/universe Translation-en
Fetched 11.0 MB in 1min 38s (112 kB/s)
Reading package lists...
+ mkdir -p /etc/clearwater
+ etcd_ip=
+ '[' -n '' ']'
+ etcd_ip=192.168.0.3
+ cat
+ DEBIAN_FRONTEND=noninteractive
+ apt-get install ellis --yes --force-yes -o DPkg::options::=--force-confnew
Reading package lists...
Building dependency tree...
Reading state information...
E: Unable to locate package ellis
+ DEBIAN_FRONTEND=noninteractive
+ apt-get install clearwater-config-manager --yes --force-yes
Reading package lists...
Building dependency tree...
Reading state information...
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 clearwater-config-manager : Depends: clearwater-etcd but it is not going to be installed
                             Depends: clearwater-queue-manager but it is not going to be installed
E: Unable to correct problems, you have held broken packages.


Any ideas?

Nir.

--

Nir Henn
Innovation Coach, AT&T Foundry

Desk   +97297765298
Mobile +972547606867
AMDOCS | EMBRACE CHALLENGE EXPERIENCE SUCCESS
This email was sent using 100% recycled electrons!

STOP texting while driving. Take the pledge at itcanwait.com<http://www.itcanwait.com/>



This message and the information contained herein is proprietary and confidential and subject to the Amdocs policy statement,
you may review at http://www.amdocs.com/email_disclaimer.asp
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160216/de18c7be/attachment.html>

From nir.henn at amdocs.com  Tue Feb 16 07:12:35 2016
From: nir.henn at amdocs.com (Nir Henn)
Date: Tue, 16 Feb 2016 12:12:35 +0000
Subject: [Clearwater] Clearwater heat error
In-Reply-To: <A86BD031-515F-46A0-93BE-5E9FA86701DE@amdocs.com>
References: <A86BD031-515F-46A0-93BE-5E9FA86701DE@amdocs.com>
Message-ID: <69408CA8-DAD5-4C6F-B958-F05BD3F086EC@amdocs.com>

I think I have found the problem, used Ubuntu 32bit instead of 64bit.
Giving it a try now, if no more post on this issue it means problem solved.

Nir.

From: Nir Henn <nir.henn at amdocs.com<mailto:nir.henn at amdocs.com>>
Date: Tuesday, February 16, 2016 at 1:21 PM
To: "clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>" <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>
Subject: Clearwater heat error

Hi there

I am having some issues with the heat template deployment of clearwater and I would appreciate your help.
I am deploying an environment for a project I am working on and from some reason it fails.
From the instance boot log I can notice the following:


ellis-0 login: Cloud-init v. 0.7.5 running 'modules:final' at Tue, 16 Feb 2016 07:43:12 +0000. Up 24.08 seconds.
Provision began: 2016-02-16 07:43:12.223433
/var/lib/heat-cfntools/cfn-userdata

+ echo 'deb http://repo.cw-ngv.com/stable binary/'
+ apt-key add -
+ curl -L http://repo.cw-ngv.com/repo_key
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  3149  100  3149    0     0   5552      0 --:--:-- --:--:-- --:--:--  5563
OK
+ apt-get update
Ign http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty InRelease
Get:1 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates InRelease [65.9 kB]
Get:2 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security InRelease [65.9 kB]
Ign http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ InRelease
Get:3 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports InRelease [64.5 kB]
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty Release.gpg
Get:4 http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ Release.gpg [819 B]
Get:5 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/main Sources [104 kB]
Get:6 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/main Sources [259 kB]
Get:7 http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ Release [1219 B]
Get:8 http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ Packages [18.6 kB]
Get:9 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/restricted Sources [5342 B]
Get:10 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/universe Sources [33.0 kB]
Get:11 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/universe Sources [150 kB]
Get:12 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/main i386 Packages [392 kB]
Get:13 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/universe i386 Packages [124 kB]
Ign http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ Translation-en
Get:14 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/main Translation-en [230 kB]
Get:15 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/universe Translation-en [72.5 kB]
Get:16 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/multiverse Sources [5547 B]
Get:17 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/main i386 Packages [683 kB]
Get:18 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/restricted i386 Packages [15.6 kB]
Get:19 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/universe i386 Packages [339 kB]
Get:20 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/multiverse i386 Packages [13.4 kB]
Get:21 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/main Translation-en [355 kB]
Get:22 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/multiverse Translation-en [6947 B]
Get:23 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/restricted Translation-en [3699 B]
Get:24 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/universe Translation-en [180 kB]
Get:25 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/main Sources [8236 B]
Get:26 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/restricted Sources [28 B]
Get:27 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/universe Sources [33.2 kB]
Get:28 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/multiverse Sources [1898 B]
Get:29 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/main i386 Packages [9442 B]
Get:30 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/restricted i386 Packages [28 B]
Get:31 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/universe i386 Packages [39.8 kB]
Get:32 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/multiverse i386 Packages [1552 B]
Get:33 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/main Translation-en [5561 B]
Get:34 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/multiverse Translation-en [1215 B]
Get:35 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/restricted Translation-en [14 B]
Get:36 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/universe Translation-en [34.6 kB]
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty Release
Get:37 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/main Sources [1064 kB]
Get:38 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/restricted Sources [5433 B]
Get:39 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/universe Sources [6399 kB]
Get:40 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/multiverse Sources [174 kB]
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/main i386 Packages
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/restricted i386 Packages
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/universe i386 Packages
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/multiverse i386 Packages
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/main Translation-en
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/multiverse Translation-en
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/restricted Translation-en
Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/universe Translation-en
Fetched 11.0 MB in 1min 38s (112 kB/s)
Reading package lists...
+ mkdir -p /etc/clearwater
+ etcd_ip=
+ '[' -n '' ']'
+ etcd_ip=192.168.0.3
+ cat
+ DEBIAN_FRONTEND=noninteractive
+ apt-get install ellis --yes --force-yes -o DPkg::options::=--force-confnew
Reading package lists...
Building dependency tree...
Reading state information...
E: Unable to locate package ellis
+ DEBIAN_FRONTEND=noninteractive
+ apt-get install clearwater-config-manager --yes --force-yes
Reading package lists...
Building dependency tree...
Reading state information...
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 clearwater-config-manager : Depends: clearwater-etcd but it is not going to be installed
                             Depends: clearwater-queue-manager but it is not going to be installed
E: Unable to correct problems, you have held broken packages.


Any ideas?

Nir.

--

Nir Henn
Innovation Coach, AT&T Foundry

Desk   +97297765298
Mobile +972547606867
AMDOCS | EMBRACE CHALLENGE EXPERIENCE SUCCESS
This email was sent using 100% recycled electrons!

STOP texting while driving. Take the pledge at itcanwait.com<http://www.itcanwait.com/>



This message and the information contained herein is proprietary and confidential and subject to the Amdocs policy statement,
you may review at http://www.amdocs.com/email_disclaimer.asp
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160216/cf8f0cef/attachment.html>

From Eleanor.Merry at metaswitch.com  Tue Feb 16 12:17:11 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Tue, 16 Feb 2016 17:17:11 +0000
Subject: [Clearwater] BONO unable to resolve sprout node
In-Reply-To: <CAJAf6NwGyTLd1oU=n8+jHPoZ+H+98MHhsRuvi455_C+sqLcATQ@mail.gmail.com>
References: <CAJAf6NwGyTLd1oU=n8+jHPoZ+H+98MHhsRuvi455_C+sqLcATQ@mail.gmail.com>
Message-ID: <BN3PR02MB12550B859AA9BC4B831DDE159BAD0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Shiva,

The clearwater processes look in /etc/resolv.conf for DNS configuration (defaulting to localhost). If your nodes don?t use localhost (try running nslookup sprout.cwhpe.com and see if the Server value is 127.0.0.1) you?ll need to update the dnsmasq configuration to point to this server rather than localhost. You can find details of how to do this at: http://clearwater.readthedocs.org/en/stable/Clearwater_DNS_Usage/index.html#configuration

Hope this helps,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shiva Charan
Sent: 15 February 2016 10:20
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] BONO unable to resolve sprout node

Hi All,

I have a manual install of clearwater setup, all the services are running except for "poll_memento_https" will this affect the setup?

The main issue is when is see the logs of bono it has the following message.
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)
15-02-2016 10:16:07.862 UTC Error connection_pool.cpp:214: Failed to resolve sprout.cwhpe.com<http://sprout.cwhpe.com> to an IP address - Not found (PJ_ENOTFOUND)

Bono is able to ping the sprout node with the dns name
root at bono:/var/log/bono# ping sprout.cwhpe.com<http://sprout.cwhpe.com>
PING sprout.cwhpe.com<http://sprout.cwhpe.com> (10.222.5.136) 56(84) bytes of data.
64 bytes from 10.222.5.136<http://10.222.5.136>: icmp_seq=1 ttl=64 time=0.344 ms
64 bytes from 10.222.5.136<http://10.222.5.136>: icmp_seq=2 ttl=64 time=0.389 ms
64 bytes from 10.222.5.136<http://10.222.5.136>: icmp_seq=3 ttl=64 time=0.159 ms
64 bytes from 10.222.5.136<http://10.222.5.136>: icmp_seq=4 ttl=64 time=0.344 ms
^C
--- sprout.cwhpe.com<http://sprout.cwhpe.com> ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 2998ms
rtt min/avg/max/mdev = 0.159/0.309/0.389/0.088 ms
root at bono:/var/log/bono#


and the registration through zoiper client is also not going through.

Please suggest,

Shiva Charan M S
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160216/ec9c704c/attachment.html>

From Eleanor.Merry at metaswitch.com  Tue Feb 16 12:17:15 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Tue, 16 Feb 2016 17:17:15 +0000
Subject: [Clearwater] Clearwater heat error
In-Reply-To: <69408CA8-DAD5-4C6F-B958-F05BD3F086EC@amdocs.com>
References: <A86BD031-515F-46A0-93BE-5E9FA86701DE@amdocs.com>
	<69408CA8-DAD5-4C6F-B958-F05BD3F086EC@amdocs.com>
Message-ID: <BN3PR02MB125582E8ABFCC8E4CBE447669BAD0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Nir,

That?s right, we only support 64bit.

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Nir Henn
Sent: 16 February 2016 12:13
To: clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] Clearwater heat error

I think I have found the problem, used Ubuntu 32bit instead of 64bit.
Giving it a try now, if no more post on this issue it means problem solved.

Nir.

From: Nir Henn <nir.henn at amdocs.com<mailto:nir.henn at amdocs.com>>
Date: Tuesday, February 16, 2016 at 1:21 PM
To: "clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>" <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>
Subject: Clearwater heat error

Hi there

I am having some issues with the heat template deployment of clearwater and I would appreciate your help.
I am deploying an environment for a project I am working on and from some reason it fails.
From the instance boot log I can notice the following:


ellis-0 login: Cloud-init v. 0.7.5 running 'modules:final' at Tue, 16 Feb 2016 07:43:12 +0000. Up 24.08 seconds.

Provision began: 2016-02-16 07:43:12.223433

/var/lib/heat-cfntools/cfn-userdata



+ echo 'deb http://repo.cw-ngv.com/stable binary/'

+ apt-key add -

+ curl -L http://repo.cw-ngv.com/repo_key

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current

                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  3149  100  3149    0     0   5552      0 --:--:-- --:--:-- --:--:--  5563

OK

+ apt-get update

Ign http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty InRelease

Get:1 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates InRelease [65.9 kB]

Get:2 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security InRelease [65.9 kB]

Ign http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ InRelease

Get:3 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports InRelease [64.5 kB]

Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty Release.gpg

Get:4 http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ Release.gpg [819 B]

Get:5 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/main Sources [104 kB]

Get:6 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/main Sources [259 kB]

Get:7 http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ Release [1219 B]

Get:8 http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ Packages [18.6 kB]

Get:9 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/restricted Sources [5342 B]

Get:10 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/universe Sources [33.0 kB]

Get:11 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/universe Sources [150 kB]

Get:12 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/main i386 Packages [392 kB]

Get:13 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/universe i386 Packages [124 kB]

Ign http://repo.cw-ngv.com<http://repo.cw-ngv.com/> binary/ Translation-en

Get:14 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/main Translation-en [230 kB]

Get:15 http://security.ubuntu.com<http://security.ubuntu.com/> trusty-security/universe Translation-en [72.5 kB]

Get:16 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/multiverse Sources [5547 B]

Get:17 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/main i386 Packages [683 kB]

Get:18 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/restricted i386 Packages [15.6 kB]

Get:19 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/universe i386 Packages [339 kB]

Get:20 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/multiverse i386 Packages [13.4 kB]

Get:21 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/main Translation-en [355 kB]

Get:22 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/multiverse Translation-en [6947 B]

Get:23 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/restricted Translation-en [3699 B]

Get:24 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-updates/universe Translation-en [180 kB]

Get:25 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/main Sources [8236 B]

Get:26 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/restricted Sources [28 B]

Get:27 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/universe Sources [33.2 kB]

Get:28 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/multiverse Sources [1898 B]

Get:29 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/main i386 Packages [9442 B]

Get:30 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/restricted i386 Packages [28 B]

Get:31 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/universe i386 Packages [39.8 kB]

Get:32 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/multiverse i386 Packages [1552 B]

Get:33 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/main Translation-en [5561 B]

Get:34 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/multiverse Translation-en [1215 B]

Get:35 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/restricted Translation-en [14 B]

Get:36 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty-backports/universe Translation-en [34.6 kB]

Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty Release

Get:37 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/main Sources [1064 kB]

Get:38 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/restricted Sources [5433 B]

Get:39 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/universe Sources [6399 kB]

Get:40 http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/multiverse Sources [174 kB]

Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/main i386 Packages

Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/restricted i386 Packages

Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/universe i386 Packages

Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/multiverse i386 Packages

Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/main Translation-en

Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/multiverse Translation-en

Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/restricted Translation-en

Hit http://nova.clouds.archive.ubuntu.com<http://nova.clouds.archive.ubuntu.com/> trusty/universe Translation-en

Fetched 11.0 MB in 1min 38s (112 kB/s)

Reading package lists...

+ mkdir -p /etc/clearwater

+ etcd_ip=

+ '[' -n '' ']'

+ etcd_ip=192.168.0.3

+ cat

+ DEBIAN_FRONTEND=noninteractive

+ apt-get install ellis --yes --force-yes -o DPkg::options::=--force-confnew

Reading package lists...

Building dependency tree...

Reading state information...

E: Unable to locate package ellis

+ DEBIAN_FRONTEND=noninteractive

+ apt-get install clearwater-config-manager --yes --force-yes

Reading package lists...

Building dependency tree...

Reading state information...

Some packages could not be installed. This may mean that you have

requested an impossible situation or if you are using the unstable

distribution that some required packages have not yet been created

or been moved out of Incoming.

The following information may help to resolve the situation:



The following packages have unmet dependencies:

 clearwater-config-manager : Depends: clearwater-etcd but it is not going to be installed

                             Depends: clearwater-queue-manager but it is not going to be installed

E: Unable to correct problems, you have held broken packages.



Any ideas?

Nir.

--


Nir Henn
Innovation Coach, AT&T Foundry

Desk   +97297765298
Mobile +972547606867
AMDOCS | EMBRACE CHALLENGE EXPERIENCE SUCCESS
This email was sent using 100% recycled electrons!

STOP texting while driving. Take the pledge at itcanwait.com<http://www.itcanwait.com/>




This message and the information contained herein is proprietary and confidential and subject to the Amdocs policy statement, you may review at http://www.amdocs.com/email_disclaimer.asp
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160216/0153bb8f/attachment.html>

From tahir.masood at xflowresearch.com  Wed Feb 17 08:03:44 2016
From: tahir.masood at xflowresearch.com (Tahir Masood)
Date: Wed, 17 Feb 2016 18:03:44 +0500
Subject: [Clearwater] Call disconnects after 30 seconds
Message-ID: <004901d16983$a523e3c0$ef6bab40$@xflowresearch.com>

Dear all,

I am facing an issue of call disconnection after 30 seconds  have used both
Xlite and Zoiper but the problem persists. I have change the log_level to 5
in sprout and here are the logs. Can you please suggest me a solution  

 

17-02-2016 12:59:42.374 UTC Debug pjsip: sip_endpoint.c Processing incoming
mess                     age: Request msg OPTIONS/cseq=97398
(rdata0x7ff14c06fb50)

17-02-2016 12:59:42.374 UTC Verbose common_sip_processing.cpp:120: RX 342
bytes                      Request msg OPTIONS/cseq=97398
(rdata0x7ff14c06fb50) from TCP 192.168.0.6:50982:

--start msg--

 

OPTIONS sip:poll-sip at 192.168.0.6:5054 SIP/2.0

Via: SIP/2.0/TCP 192.168.0.6;rport;branch=z9hG4bK-97398

Max-Forwards: 2

To: <sip:poll-sip at 192.168.0.6:5054>

From: poll-sip <sip:poll-sip at 192.168.0.6>;tag=97398

Call-ID: poll-sip-97398

CSeq: 97398 OPTIONS

Contact: <sip:192.168.0.6>

Accept: application/sdp

Content-Length: 0

User-Agent: poll-sip

 

 

--end msg--

17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:42.374 UTC Debug common_sip_processing.cpp:212: Skipping
SAS lo                     gging for OPTIONS request

17-02-2016 12:59:42.374 UTC Debug thread_dispatcher.cpp:253: Queuing cloned
rece                     ived message 0x7ff14c01d7c8 for worker threads

17-02-2016 12:59:42.374 UTC Debug thread_dispatcher.cpp:149: Worker thread
deque                     ue message 0x7ff14c01d7c8

17-02-2016 12:59:42.374 UTC Debug pjsip: sip_endpoint.c Distributing rdata
to mo                     dules: Request msg OPTIONS/cseq=97398
(rdata0x7ff14c01d7c8)

17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:42.374 UTC Debug pjsip:       endpoint Response msg
200/OPTIONS                     /cseq=97398 (tdta0x7ff13c408040) created

17-02-2016 12:59:42.374 UTC Verbose common_sip_processing.cpp:136: TX 273
bytes                      Response msg 200/OPTIONS/cseq=97398
(tdta0x7ff13c408040) to TCP 192.168.0.6:5098                     2:

--start msg--

 

SIP/2.0 200 OK

Via: SIP/2.0/TCP
192.168.0.6;rport=50982;received=192.168.0.6;branch=z9hG4bK-973
98

Call-ID: poll-sip-97398

From: "poll-sip" <sip:poll-sip at 192.168.0.6>;tag=97398

To: <sip:poll-sip at 192.168.0.6>;tag=z9hG4bK-97398

CSeq: 97398 OPTIONS

Content-Length:  0

 

 

--end msg--

17-02-2016 12:59:42.374 UTC Debug common_sip_processing.cpp:254: Skipping
SAS lo                     gging for OPTIONS response

17-02-2016 12:59:42.374 UTC Debug pjsip: tdta0x7ff13c40 Destroying txdata
Respon                     se msg 200/OPTIONS/cseq=97398
(tdta0x7ff13c408040)

17-02-2016 12:59:42.375 UTC Debug thread_dispatcher.cpp:193: Worker thread
compl                     eted processing message 0x7ff14c01d7c8

17-02-2016 12:59:42.375 UTC Debug thread_dispatcher.cpp:199: Request latency
= 6                     61us

17-02-2016 12:59:42.385 UTC Verbose httpstack.cpp:286: Process request for
URL /                     ping, args (null)

17-02-2016 12:59:42.385 UTC Verbose httpstack.cpp:69: Sending response 200
to re                     quest for URL /ping, args (null)

17-02-2016 12:59:43.375 UTC Verbose pjsip: tcps0x7ff14c06 TCP connection
closed

17-02-2016 12:59:43.375 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     06f818 has been destroyed

17-02-2016 12:59:43.376 UTC Verbose pjsip: tcps0x7ff14c06 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:43.519 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:34463, sock=1055

17-02-2016 12:59:43.519 UTC Verbose pjsip: tcps0x7ff14c06 TCP server
transport c                     reated

17-02-2016 12:59:43.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP connection
closed

17-02-2016 12:59:43.520 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     114c98 has been destroyed

17-02-2016 12:59:43.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:44.483 UTC Debug pjsip: sip_endpoint.c Processing incoming
mess                     age: Request msg REGISTER/cseq=30
(rdata0x7ff14c04bf60)

17-02-2016 12:59:44.483 UTC Verbose common_sip_processing.cpp:120: RX 1242
bytes                      Request msg REGISTER/cseq=30
(rdata0x7ff14c04bf60) from TCP 192.168.0.4:53691:

--start msg--

 

REGISTER sip:dellnfv.com;transport=UDP SIP/2.0

Via: SIP/2.0/TCP
192.168.0.4:53691;rport;branch=z9hG4bKPjcGZ7eF5y6OvGqDehOqxpk4M
EbqS4Ag4l

Path: <sip:rP6Yw8PkfR at 192.168.0.4:5058;transport=TCP;lr;ob>

Via: SIP/2.0/UDP
192.168.1.8:64835;rport=64835;received=192.168.1.8;branch=z9hG4
bK-524287-1---1c645317b5676c22

Max-Forwards: 70

Contact:
<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68
ac1>

To: <sip:2010000007 at dellnfv.com>

From: <sip:2010000007 at dellnfv.com>;tag=698b4951

Call-ID: Pp7e4umpNXHLsITvW6y7RA..

CSeq: 30 REGISTER

Expires: 60

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO,
SUBSCRIB                     E

Supported: replaces, norefersub, extended-refer, timer, outbound, path,
X-cisco-                     serviceuri

User-Agent: Zoiper r35079

Authorization: Digest response="49157d3d9e710e40b06a6d4a6c9af137",
username="201                     0000007 at dellnfv.com", realm="dellnfv.com",
nonce="0d8f08d16e444374", uri="sip:de
llnfv.com;transport=UDP", algorithm=MD5,
cnonce="450410c6c32d2ff38597f3468504591                     d",
opaque="2975ef5f2182abff", qop=auth, nc=0000001d,integrity-protected=ip-asso
c-yes

Allow-Events: presence, kpml

P-Visited-Network-ID: dellnfv.com

Route: <sip:sprout.dellnfv.com:5054;transport=TCP;lr;orig>

Content-Length:  0

 

 

--end msg--

17-02-2016 12:59:44.484 UTC Debug pjutils.cpp:1648: Logging SAS Call-ID
marker,                      Call-ID Pp7e4umpNXHLsITvW6y7RA..

17-02-2016 12:59:44.484 UTC Debug thread_dispatcher.cpp:253: Queuing cloned
rece                     ived message 0x7ff14c114d18 for worker threads

17-02-2016 12:59:44.484 UTC Debug thread_dispatcher.cpp:149: Worker thread
deque                     ue message 0x7ff14c114d18

17-02-2016 12:59:44.484 UTC Debug pjsip: sip_endpoint.c Distributing rdata
to mo                     dules: Request msg REGISTER/cseq=30
(rdata0x7ff14c114d18)

17-02-2016 12:59:44.484 UTC Debug uri_classifier.cpp:167: home domain: true,
loc                     al_to_node: false, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as
4

17-02-2016 12:59:44.485 UTC Debug authentication.cpp:673: Authentication
module                      invoked

17-02-2016 12:59:44.485 UTC Debug authentication.cpp:581: Authorization
header i                     n request

17-02-2016 12:59:44.485 UTC Info authentication.cpp:595: SIP Digest
authenticate                     d request integrity protected by edge proxy

17-02-2016 12:59:44.485 UTC Debug authentication.cpp:683: Request does not
need                      authentication

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:167: home domain: true,
loc                     al_to_node: false, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as
4

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:44.485 UTC Debug acr.cpp:1763: Create RalfACR for node type
S-C                     SCF with role Originating

17-02-2016 12:59:44.485 UTC Debug acr.cpp:49: Created ACR (0x7ff144148500)

17-02-2016 12:59:44.485 UTC Debug acr.cpp:175: Created S-CSCF Ralf ACR

17-02-2016 12:59:44.485 UTC Debug acr.cpp:214: Set record type for P/S-CSCF

17-02-2016 12:59:44.485 UTC Debug acr.cpp:222: Non-dialog message =>
EVENT_RECOR                     D

17-02-2016 12:59:44.485 UTC Debug acr.cpp:1491: Stored 0 subscription
identifier                     s

17-02-2016 12:59:44.485 UTC Debug registrar.cpp:541: Process REGISTER for
public                      ID sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.485 UTC Debug registrar.cpp:549: Report SAS start marker
- t                     rail (1e3)

17-02-2016 12:59:44.485 UTC Debug hssconnection.cpp:585: Making Homestead
reques                     t for
/impu/sip%3A2010000007%40dellnfv.com/reg-data?private_id=2010000007%40dell
nfv.com

17-02-2016 12:59:44.485 UTC Debug httpresolver.cpp:71: HttpResolver::resolve
for                      host hs.dellnfv.com, port 8888, family 2

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:513: Attempt to parse
hs.dell                     nfv.com as IP address

17-02-2016 12:59:44.485 UTC Debug dnscachedresolver.cpp:667: Removing record
for                      hs.dellnfv.com (type 1, expiry time 1455713976)
from the expiry list

17-02-2016 12:59:44.485 UTC Verbose dnscachedresolver.cpp:240: Check cache
for h                     s.dellnfv.com type 1

17-02-2016 12:59:44.485 UTC Debug dnscachedresolver.cpp:326: Pulling 1
records f                     rom cache for hs.dellnfv.com A

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:361: Found 1 A/AAAA
records,                      randomizing

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:503: 192.168.0.8:8888
transpo                     rt 6 is not blacklisted

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:382: Added a server, now
have                      1 of 5

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:420: Adding 0 servers
from bl                     acklist

17-02-2016 12:59:44.485 UTC Debug httpconnection.cpp:623: Sending HTTP
request :
http://hs.dellnfv.com:8888/impu/sip%3A2010000007%40dellnfv.com/reg-data?priv
ate                     _id=2010000007%40dellnfv.com (trying 192.168.0.8) on
new connection

17-02-2016 12:59:44.494 UTC Debug httpconnection.cpp:915: Received header
http/1                     .1200ok with value

17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header
conten                     t-length with value 869

17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header
conten                     t-type with value text/plain

17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header
with                      value

17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:638: Received HTTP
response                     : status=200, doc=<ClearwaterRegData>

        <RegistrationState>REGISTERED</RegistrationState>

        <IMSSubscription xsi="http://www.w3.org/2001/XMLSchema-instance"
noNames                     paceSchemaLocation="CxDataType.xsd">

                <PrivateID>2010000007 at dellnfv.com</PrivateID>

                <ServiceProfile>

                        <InitialFilterCriteria>

                                <TriggerPoint>

 
<ConditionTypeCNF>0</ConditionTypeCNF>

                                        <SPT>

 
<ConditionNegated>0</ConditionNe                     gated>

                                                <Group>0</Group>

                                                <Method>INVITE</Method>

                                                <Extension/>

                                        </SPT>

                                </TriggerPoint>

                                <ApplicationServer>

 
<ServerName>sip:mmtel.dellnfv.com</Serve                     rName>

                                        <DefaultHandling>0</DefaultHandling>

                                </ApplicationServer>

                        </InitialFilterCriteria>

                        <PublicIdentity>

                                <BarringIndication>1</BarringIndication>

 
<Identity>sip:2010000007 at dellnfv.com</Identity>

                        </PublicIdentity>

                </ServiceProfile>

        </IMSSubscription>

</ClearwaterRegData>

 

 

17-02-2016 12:59:44.495 UTC Debug communicationmonitor.cpp:82: Checking
communic                     ation changes - successful attempts 1, failures
0

17-02-2016 12:59:44.495 UTC Debug hssconnection.cpp:366: Processing Identity
nod                     e from HSS XML - sip:2010000007 at dellnfv.com

 

17-02-2016 12:59:44.495 UTC Debug registrar.cpp:651: REGISTER for public ID
sip:                     2010000007 at dellnfv.com uses AOR
sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.495 UTC Debug subscriber_data_manager.cpp:366: Get AoR
data                      for sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:195: Key
reg\\sip:201000000                     7 at dellnfv.com hashes to vbucket 19
via hash 0x9f135593

17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:367: 1 read replicas
for ke                     y reg\\sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:402: Attempt to read
from r                     eplica 0 (connection 0x7ff1440ae680)

17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:780: Fetch result

17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:788: Found record on
replic                     a

17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:410: Read for
reg\\sip:2010                     000007 at dellnfv.com on replica 0 returned
SUCCESS

17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:453: Read 469 bytes
from ta                     ble reg key sip:2010000007 at dellnfv.com, CAS =
1537

17-02-2016 12:59:44.496 UTC Debug communicationmonitor.cpp:82: Checking
communic                     ation changes - successful attempts 4, failures
0

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:376: Data
store re                     turned a record, CAS = 1537

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:477: Try to
deseri                     alize record for sip:2010000007 at dellnfv.com with
'JSON' deserializer

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:969:
Deserialize J                     SON document:
{"bindings":{"sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinst
ance=907814e95ea68ac1":{"uri":"sip:2010000007 at 192.168.1.8:64835;transport=UD
P;ri
nstance=907814e95ea68ac1","cid":"Pp7e4umpNXHLsITvW6y7RA..","cseq":29,"expire
s":1
455713990,"priority":0,"params":{},"paths":["sip:rP6Yw8PkfR at 192.168.0.4:5058
;tra
nsport=TCP;lr;ob"],"timer_id":"016dc709400001210040001000104104","private_id
":"2
010000007 at dellnfv.com","emergency_reg":false}},"subscriptions":{},"notify_cs
eq":                     29}

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:994:
Binding: si
p:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68ac1

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:482:
Deserializati                     on suceeded

17-02-2016 12:59:44.496 UTC Debug registrar.cpp:249: Retrieved AoR data
0x7ff144                     23a0a0

17-02-2016 12:59:44.496 UTC Debug registrar.cpp:342: Binding identifier for
cont                     act =
sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68ac1

17-02-2016 12:59:44.496 UTC Debug registrar.cpp:369: Path header
sip:rP6Yw8PkfR@                     192.168.0.4:5058;transport=TCP;lr;ob

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:196: Set AoR
data                      for sip:2010000007 at dellnfv.com, CAS=1537, expiry =
1455714054

17-02-2016 12:59:44.496 UTC Debug httpresolver.cpp:71: HttpResolver::resolve
for                      host 127.0.0.1, port 7253, family 2

17-02-2016 12:59:44.496 UTC Debug baseresolver.cpp:513: Attempt to parse
127.0.0                     .1 as IP address

17-02-2016 12:59:44.496 UTC Debug httpresolver.cpp:79: Target is an IP
address

17-02-2016 12:59:44.496 UTC Debug httpconnection.cpp:623: Sending HTTP
request :
http://127.0.0.1:7253/timers/016dc709400001210040001000104104 (trying
127.0.0.1                     ) on new connection

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header
http/1                     .1200ok with value

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header
locati                     on with value
/timers/016dc709400001210040001000104104

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header
conten                     t-length with value 0

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header
with                      value

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:638: Received HTTP
response                     : status=200, doc=

17-02-2016 12:59:44.498 UTC Debug communicationmonitor.cpp:82: Checking
communic                     ation changes - successful attempts 1, failures
0

17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:542: Writing 469 bytes
to t                     able reg key sip:2010000007 at dellnfv.com, CAS =
1537, expiry = 70

17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:195: Key
reg\\sip:201000000                     7 at dellnfv.com hashes to vbucket 19
via hash 0x9f135593

17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:562: 1 write replicas
for k                     ey reg\\sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:616: Attempt
conditional wr                     ite to vbucket 19 on replica 0
(connection 0x7ff1440ae680), CAS = 1537, expiry =                      70

17-02-2016 12:59:44.499 UTC Debug memcachedstore.cpp:657: Conditional write
succ                     eeded to replica 0

17-02-2016 12:59:44.499 UTC Debug subscriber_data_manager.cpp:438: Data
store se                     t_data returned 1

17-02-2016 12:59:44.499 UTC Debug registrar.cpp:116: Bindings for
sip:2010000007                     @dellnfv.com

17-02-2016 12:59:44.499 UTC Debug registrar.cpp:130:
sip:2010000007 at 192.168.1.
8:64835;transport=UDP;rinstance=907814e95ea68ac1
URI=sip:2010000007 at 192.168.1.8:
64835;transport=UDP;rinstance=907814e95ea68ac1 expires=1455714044 q=0
from=Pp7e4                     umpNXHLsITvW6y7RA.. cseq=30
timer=016dc709400001210040001000104104 private_id=20
10000007 at dellnfv.com emergency_registration=false

17-02-2016 12:59:44.499 UTC Debug pjsip:       endpoint Response msg
200/REGISTE                     R/cseq=30 (tdta0x7ff14423a790) created

17-02-2016 12:59:44.499 UTC Debug acr.cpp:1550: Store associated URIs

17-02-2016 12:59:44.499 UTC Verbose common_sip_processing.cpp:136: TX 765
bytes                      Response msg 200/REGISTER/cseq=30
(tdta0x7ff14423a790) to TCP 192.168.0.4:53691:

--start msg--

 

SIP/2.0 200 OK

Service-Route: <sip:sprout.dellnfv.com:5054;transport=TCP;lr;orig>

Via: SIP/2.0/TCP
192.168.0.4:53691;rport=53691;received=192.168.0.4;branch=z9hG4
bKPjcGZ7eF5y6OvGqDehOqxpk4MEbqS4Ag4l

Via: SIP/2.0/UDP
192.168.1.8:64835;rport=64835;received=192.168.1.8;branch=z9hG4
bK-524287-1---1c645317b5676c22

Call-ID: Pp7e4umpNXHLsITvW6y7RA..

From: <sip:2010000007 at dellnfv.com>;tag=698b4951

To:
<sip:2010000007 at dellnfv.com>;tag=z9hG4bKPjcGZ7eF5y6OvGqDehOqxpk4MEbqS4Ag4l

CSeq: 30 REGISTER

Supported: outbound

Contact:
<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68
ac1>;expires=60

Require: outbound

Path: <sip:rP6Yw8PkfR at 192.168.0.4:5058;transport=TCP;lr;ob>

P-Associated-URI: <sip:2010000007 at dellnfv.com>

Content-Length:  0

 

 

--end msg--

17-02-2016 12:59:44.499 UTC Info acr.cpp:658: No CCF or ECF to send ACR for
sess                     ion Pp7e4umpNXHLsITvW6y7RA.. to - dropping!

17-02-2016 12:59:44.499 UTC Debug acr.cpp:54: Destroyed ACR (0x7ff144148500)

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:763: Interpreting orig IFC
info                     rmation

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:437: SPT class Method:
result f                     alse

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:541: Add to group 0 val
false

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:559: Result group 0 val
false

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:572: iFC does not match

17-02-2016 12:59:44.499 UTC Info registration_utils.cpp:187: Found 0
Application                      Servers

17-02-2016 12:59:44.499 UTC Debug pjsip: tdta0x7ff14423 Destroying txdata
Respon                     se msg 200/REGISTER/cseq=30 (tdta0x7ff14423a790)

17-02-2016 12:59:44.499 UTC Debug registrar.cpp:1036: Report SAS end marker
- tr                     ail (1e3)

17-02-2016 12:59:44.499 UTC Debug thread_dispatcher.cpp:193: Worker thread
compl                     eted processing message 0x7ff14c114d18

17-02-2016 12:59:44.499 UTC Debug thread_dispatcher.cpp:199: Request latency
= 1                     5474us

17-02-2016 12:59:46.381 UTC Verbose pjsip: tcps0x7ff14c15 TCP transport
destroye                     d normally

17-02-2016 12:59:46.520 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:35812, sock=408

17-02-2016 12:59:46.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP server
transport c                     reated

17-02-2016 12:59:47.520 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:41045, sock=938

17-02-2016 12:59:47.520 UTC Verbose pjsip: tcps0x7ff14c15 TCP server
transport c                     reated

17-02-2016 12:59:47.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP connection
closed

17-02-2016 12:59:47.521 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     05d228 has been destroyed

17-02-2016 12:59:47.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:49.521 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:51223, sock=429

17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP server
transport c                     reated

17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c07 TCP connection
closed

17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c07 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:52.359 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.6:51013, sock=970

17-02-2016 12:59:52.359 UTC Verbose pjsip: tcps0x7ff14c07 TCP server
transport c                     reated

17-02-2016 12:59:52.359 UTC Debug pjsip: sip_endpoint.c Processing incoming
mess                     age: Request msg OPTIONS/cseq=97408
(rdata0x7ff14c0747e0)

17-02-2016 12:59:52.359 UTC Verbose common_sip_processing.cpp:120: RX 342
bytes                      Request msg OPTIONS/cseq=97408
(rdata0x7ff14c0747e0) from TCP 192.168.0.6:51013:

--start msg--

 

OPTIONS sip:poll-sip at 192.168.0.6:5054 SIP/2.0

Via: SIP/2.0/TCP 192.168.0.6;rport;branch=z9hG4bK-97408

Max-Forwards: 2

To: <sip:poll-sip at 192.168.0.6:5054>

From: poll-sip <sip:poll-sip at 192.168.0.6>;tag=97408

Call-ID: poll-sip-97408

CSeq: 97408 OPTIONS

Contact: <sip:192.168.0.6>

Accept: application/sdp

Content-Length: 0

User-Agent: poll-sip

 

 

--end msg--

17-02-2016 12:59:52.359 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:52.359 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:52.359 UTC Debug common_sip_processing.cpp:212: Skipping
SAS lo                     gging for OPTIONS request

17-02-2016 12:59:52.359 UTC Debug thread_dispatcher.cpp:253: Queuing cloned
rece                     ived message 0x7ff14c01d7c8 for worker threads

17-02-2016 12:59:52.359 UTC Debug thread_dispatcher.cpp:149: Worker thread
deque                     ue message 0x7ff14c01d7c8

17-02-2016 12:59:52.360 UTC Debug pjsip: sip_endpoint.c Distributing rdata
to mo                     dules: Request msg OPTIONS/cseq=97408
(rdata0x7ff14c01d7c8)

17-02-2016 12:59:52.360 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:52.360 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:52.360 UTC Debug pjsip:       endpoint Response msg
200/OPTIONS                     /cseq=97408 (tdta0x7ff14c298780) created

17-02-2016 12:59:52.360 UTC Verbose common_sip_processing.cpp:136: TX 273
bytes                      Response msg 200/OPTIONS/cseq=97408
(tdta0x7ff14c298780) to TCP 192.168.0.6:5101                     3:

--start msg--

 

SIP/2.0 200 OK

Via: SIP/2.0/TCP
192.168.0.6;rport=51013;received=192.168.0.6;branch=z9hG4bK-974
08

Call-ID: poll-sip-97408

From: "poll-sip" <sip:poll-sip at 192.168.0.6>;tag=97408

To: <sip:poll-sip at 192.168.0.6>;tag=z9hG4bK-97408

CSeq: 97408 OPTIONS

Content-Length:  0

 

 

--end msg--

17-02-2016 12:59:52.360 UTC Debug common_sip_processing.cpp:254: Skipping
SAS lo                     gging for OPTIONS response

17-02-2016 12:59:52.360 UTC Debug pjsip: tdta0x7ff14c29 Destroying txdata
Respon                     se msg 200/OPTIONS/cseq=97408
(tdta0x7ff14c298780)

17-02-2016 12:59:52.360 UTC Debug thread_dispatcher.cpp:193: Worker thread
compl                     eted processing message 0x7ff14c01d7c8

17-02-2016 12:59:52.360 UTC Debug thread_dispatcher.cpp:199: Request latency
= 2                     42us

17-02-2016 12:59:52.360 UTC Info load_monitor.cpp:212: Accepted 100.000000%
of r                     equests, latency error = -0.953790, overload
responses = 0

17-02-2016 12:59:52.360 UTC Status load_monitor.cpp:260: Maximum incoming
reques                     t rate/second unchanged - only handled 20
requests in last 73256ms, minimum thre                     shold for a
change is 18314.000000

17-02-2016 12:59:52.360 UTC Debug snmp_continuous_accumulator_table.cpp:108:
Acc                     umulating sample 500ui into continuous accumulator
statistic

17-02-2016 12:59:52.360 UTC Debug snmp_continuous_accumulator_table.cpp:108:
Acc                     umulating sample 500ui into continuous accumulator
statistic

17-02-2016 12:59:52.361 UTC Verbose httpstack.cpp:286: Process request for
URL /                     ping, args (null)

17-02-2016 12:59:52.361 UTC Verbose httpstack.cpp:69: Sending response 200
to re                     quest for URL /ping, args (null)

17-02-2016 12:59:53.362 UTC Verbose pjsip: tcps0x7ff14c07 TCP connection
closed

17-02-2016 12:59:53.362 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     0744a8 has been destroyed

17-02-2016 12:59:53.362 UTC Verbose pjsip: tcps0x7ff14c07 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:53.522 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:49292, sock=970

17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c07 TCP server
transport c                     reated

17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c08 TCP connection
closed

17-02-2016 12:59:53.522 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     085a08 has been destroyed

17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c08 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

 

 

Regards,

 

Tahir Masood

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160217/03e52e4a/attachment.html>

From Eleanor.Merry at metaswitch.com  Wed Feb 17 09:32:15 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Wed, 17 Feb 2016 14:32:15 +0000
Subject: [Clearwater] Call disconnects after 30 seconds
In-Reply-To: <004901d16983$a523e3c0$ef6bab40$@xflowresearch.com>
References: <004901d16983$a523e3c0$ef6bab40$@xflowresearch.com>
Message-ID: <BN3PR02MB12550A399B914CF838CC4EDF9BAE0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Tahir,

How have you set up your deployment? Is it a manual install/all-in-one install/etc..?

Also, can you please send me over the full debug logs from Sprout and Bono (the logs you've pasted in below don't have an INVITE in them)?

Thanks,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Tahir Masood
Sent: 17 February 2016 13:04
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] Call disconnects after 30 seconds

Dear all,
I am facing an issue of call disconnection after 30 seconds  have used both Xlite and Zoiper but the problem persists. I have change the log_level to 5 in sprout and here are the logs. Can you please suggest me a solution

17-02-2016 12:59:42.374 UTC Debug pjsip: sip_endpoint.c Processing incoming mess                     age: Request msg OPTIONS/cseq=97398 (rdata0x7ff14c06fb50)
17-02-2016 12:59:42.374 UTC Verbose common_sip_processing.cpp:120: RX 342 bytes                      Request msg OPTIONS/cseq=97398 (rdata0x7ff14c06fb50) from TCP 192.168.0.6:50982:
--start msg--

OPTIONS sip:poll-sip at 192.168.0.6:5054 SIP/2.0
Via: SIP/2.0/TCP 192.168.0.6;rport;branch=z9hG4bK-97398
Max-Forwards: 2
To: <sip:poll-sip at 192.168.0.6:5054>
From: poll-sip <sip:poll-sip at 192.168.0.6>;tag=97398
Call-ID: poll-sip-97398
CSeq: 97398 OPTIONS
Contact: <sip:192.168.0.6>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:167: home domain: false, lo                     cal_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:197: Classified URI as 3
17-02-2016 12:59:42.374 UTC Debug common_sip_processing.cpp:212: Skipping SAS lo                     gging for OPTIONS request
17-02-2016 12:59:42.374 UTC Debug thread_dispatcher.cpp:253: Queuing cloned rece                     ived message 0x7ff14c01d7c8 for worker threads
17-02-2016 12:59:42.374 UTC Debug thread_dispatcher.cpp:149: Worker thread deque                     ue message 0x7ff14c01d7c8
17-02-2016 12:59:42.374 UTC Debug pjsip: sip_endpoint.c Distributing rdata to mo                     dules: Request msg OPTIONS/cseq=97398 (rdata0x7ff14c01d7c8)
17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:167: home domain: false, lo                     cal_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:197: Classified URI as 3
17-02-2016 12:59:42.374 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS                     /cseq=97398 (tdta0x7ff13c408040) created
17-02-2016 12:59:42.374 UTC Verbose common_sip_processing.cpp:136: TX 273 bytes                      Response msg 200/OPTIONS/cseq=97398 (tdta0x7ff13c408040) to TCP 192.168.0.6:5098                     2:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 192.168.0.6;rport=50982;received=192.168.0.6;branch=z9hG4bK-973                     98
Call-ID: poll-sip-97398
From: "poll-sip" <sip:poll-sip at 192.168.0.6>;tag=97398
To: <sip:poll-sip at 192.168.0.6>;tag=z9hG4bK-97398
CSeq: 97398 OPTIONS
Content-Length:  0


--end msg--
17-02-2016 12:59:42.374 UTC Debug common_sip_processing.cpp:254: Skipping SAS lo                     gging for OPTIONS response
17-02-2016 12:59:42.374 UTC Debug pjsip: tdta0x7ff13c40 Destroying txdata Respon                     se msg 200/OPTIONS/cseq=97398 (tdta0x7ff13c408040)
17-02-2016 12:59:42.375 UTC Debug thread_dispatcher.cpp:193: Worker thread compl                     eted processing message 0x7ff14c01d7c8
17-02-2016 12:59:42.375 UTC Debug thread_dispatcher.cpp:199: Request latency = 6                     61us
17-02-2016 12:59:42.385 UTC Verbose httpstack.cpp:286: Process request for URL /                     ping, args (null)
17-02-2016 12:59:42.385 UTC Verbose httpstack.cpp:69: Sending response 200 to re                     quest for URL /ping, args (null)
17-02-2016 12:59:43.375 UTC Verbose pjsip: tcps0x7ff14c06 TCP connection closed
17-02-2016 12:59:43.375 UTC Debug connection_tracker.cpp:91: Connection 0x7ff14c                     06f818 has been destroyed
17-02-2016 12:59:43.376 UTC Verbose pjsip: tcps0x7ff14c06 TCP transport destroye                     d with reason 70016: End of file (PJ_EEOF)
17-02-2016 12:59:43.519 UTC Verbose pjsip:    tcplis:5054 TCP listener 192.168.0                     .6:5054: got incoming TCP connection from 192.168.0.4:34463, sock=1055
17-02-2016 12:59:43.519 UTC Verbose pjsip: tcps0x7ff14c06 TCP server transport c                     reated
17-02-2016 12:59:43.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP connection closed
17-02-2016 12:59:43.520 UTC Debug connection_tracker.cpp:91: Connection 0x7ff14c                     114c98 has been destroyed
17-02-2016 12:59:43.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP transport destroye                     d with reason 70016: End of file (PJ_EEOF)
17-02-2016 12:59:44.483 UTC Debug pjsip: sip_endpoint.c Processing incoming mess                     age: Request msg REGISTER/cseq=30 (rdata0x7ff14c04bf60)
17-02-2016 12:59:44.483 UTC Verbose common_sip_processing.cpp:120: RX 1242 bytes                      Request msg REGISTER/cseq=30 (rdata0x7ff14c04bf60) from TCP 192.168.0.4:53691:
--start msg--

REGISTER sip:dellnfv.com;transport=UDP SIP/2.0
Via: SIP/2.0/TCP 192.168.0.4:53691;rport;branch=z9hG4bKPjcGZ7eF5y6OvGqDehOqxpk4M                     EbqS4Ag4l
Path: <sip:rP6Yw8PkfR at 192.168.0.4:5058;transport=TCP;lr;ob>
Via: SIP/2.0/UDP 192.168.1.8:64835;rport=64835;received=192.168.1.8;branch=z9hG4                     bK-524287-1---1c645317b5676c22
Max-Forwards: 70
Contact: <sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68                     ac1<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68????????????????????%20ac1>>
To: <sip:2010000007 at dellnfv.com>
From: <sip:2010000007 at dellnfv.com>;tag=698b4951
Call-ID: Pp7e4umpNXHLsITvW6y7RA..
CSeq: 30 REGISTER
Expires: 60
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIB                     E
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-                     serviceuri
User-Agent: Zoiper r35079
Authorization: Digest response="49157d3d9e710e40b06a6d4a6c9af137", username="201                     0000007 at dellnfv.com<mailto:0000007 at dellnfv.com>", realm="dellnfv.com", nonce="0d8f08d16e444374", uri="sip:de                     llnfv.com;transport=UDP<sip:de????????????????????%20llnfv.com;transport=UDP>", algorithm=MD5, cnonce="450410c6c32d2ff38597f3468504591                     d", opaque="2975ef5f2182abff", qop=auth, nc=0000001d,integrity-protected=ip-asso                     c-yes
Allow-Events: presence, kpml
P-Visited-Network-ID: dellnfv.com
Route: <sip:sprout.dellnfv.com:5054;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
17-02-2016 12:59:44.484 UTC Debug pjutils.cpp:1648: Logging SAS Call-ID marker,                      Call-ID Pp7e4umpNXHLsITvW6y7RA..
17-02-2016 12:59:44.484 UTC Debug thread_dispatcher.cpp:253: Queuing cloned rece                     ived message 0x7ff14c114d18 for worker threads
17-02-2016 12:59:44.484 UTC Debug thread_dispatcher.cpp:149: Worker thread deque                     ue message 0x7ff14c114d18
17-02-2016 12:59:44.484 UTC Debug pjsip: sip_endpoint.c Distributing rdata to mo                     dules: Request msg REGISTER/cseq=30 (rdata0x7ff14c114d18)
17-02-2016 12:59:44.484 UTC Debug uri_classifier.cpp:167: home domain: true, loc                     al_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as 4
17-02-2016 12:59:44.485 UTC Debug authentication.cpp:673: Authentication module                      invoked
17-02-2016 12:59:44.485 UTC Debug authentication.cpp:581: Authorization header i                     n request
17-02-2016 12:59:44.485 UTC Info authentication.cpp:595: SIP Digest authenticate                     d request integrity protected by edge proxy
17-02-2016 12:59:44.485 UTC Debug authentication.cpp:683: Request does not need                      authentication
17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:167: home domain: true, loc                     al_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as 4
17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:167: home domain: false, lo                     cal_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as 3
17-02-2016 12:59:44.485 UTC Debug acr.cpp:1763: Create RalfACR for node type S-C                     SCF with role Originating
17-02-2016 12:59:44.485 UTC Debug acr.cpp:49: Created ACR (0x7ff144148500)
17-02-2016 12:59:44.485 UTC Debug acr.cpp:175: Created S-CSCF Ralf ACR
17-02-2016 12:59:44.485 UTC Debug acr.cpp:214: Set record type for P/S-CSCF
17-02-2016 12:59:44.485 UTC Debug acr.cpp:222: Non-dialog message => EVENT_RECOR                     D
17-02-2016 12:59:44.485 UTC Debug acr.cpp:1491: Stored 0 subscription identifier                     s
17-02-2016 12:59:44.485 UTC Debug registrar.cpp:541: Process REGISTER for public                      ID sip:2010000007 at dellnfv.com
17-02-2016 12:59:44.485 UTC Debug registrar.cpp:549: Report SAS start marker - t                     rail (1e3)
17-02-2016 12:59:44.485 UTC Debug hssconnection.cpp:585: Making Homestead reques                     t for /impu/sip%3A2010000007%40dellnfv.com/reg-data?private_id=2010000007%40dell                     nfv.com
17-02-2016 12:59:44.485 UTC Debug httpresolver.cpp:71: HttpResolver::resolve for                      host hs.dellnfv.com, port 8888, family 2
17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:513: Attempt to parse hs.dell                     nfv.com as IP address
17-02-2016 12:59:44.485 UTC Debug dnscachedresolver.cpp:667: Removing record for                      hs.dellnfv.com (type 1, expiry time 1455713976) from the expiry list
17-02-2016 12:59:44.485 UTC Verbose dnscachedresolver.cpp:240: Check cache for h                     s.dellnfv.com type 1
17-02-2016 12:59:44.485 UTC Debug dnscachedresolver.cpp:326: Pulling 1 records f                     rom cache for hs.dellnfv.com A
17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:361: Found 1 A/AAAA records,                      randomizing
17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:503: 192.168.0.8:8888 transpo                     rt 6 is not blacklisted
17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:382: Added a server, now have                      1 of 5
17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:420: Adding 0 servers from bl                     acklist
17-02-2016 12:59:44.485 UTC Debug httpconnection.cpp:623: Sending HTTP request :                      http://hs.dellnfv.com:8888/impu/sip%3A2010000007%40dellnfv.com/reg-data?private                     _id=2010000007%40dellnfv.com (trying 192.168.0.8) on new connection
17-02-2016 12:59:44.494 UTC Debug httpconnection.cpp:915: Received header http/1                     .1200ok with value
17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header conten                     t-length with value 869
17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header conten                     t-type with value text/plain
17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header  with                      value
17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:638: Received HTTP response                     : status=200, doc=<ClearwaterRegData>
        <RegistrationState>REGISTERED</RegistrationState>
        <IMSSubscription xsi="http://www.w3.org/2001/XMLSchema-instance" noNames                     paceSchemaLocation="CxDataType.xsd">
                <PrivateID>2010000007 at dellnfv.com</PrivateID<mailto:2010000007 at dellnfv.com%3c/PrivateID>>
                <ServiceProfile>
                        <InitialFilterCriteria>
                                <TriggerPoint>
                                        <ConditionTypeCNF>0</ConditionTypeCNF>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNe                     gated>
                                                <Group>0</Group>
                                                <Method>INVITE</Method>
                                                <Extension/>
                                        </SPT>
                                </TriggerPoint>
                                <ApplicationServer>
                                        <ServerName>sip:mmtel.dellnfv.com</Serve<sip:mmtel.dellnfv.com%3c/Serve>                     rName>
                                        <DefaultHandling>0</DefaultHandling>
                                </ApplicationServer>
                        </InitialFilterCriteria>
                        <PublicIdentity>
                                <BarringIndication>1</BarringIndication>
                                <Identity>sip:2010000007 at dellnfv.com</Identity<sip:2010000007 at dellnfv.com%3c/Identity>>
                        </PublicIdentity>
                </ServiceProfile>
        </IMSSubscription>
</ClearwaterRegData>


17-02-2016 12:59:44.495 UTC Debug communicationmonitor.cpp:82: Checking communic                     ation changes - successful attempts 1, failures 0
17-02-2016 12:59:44.495 UTC Debug hssconnection.cpp:366: Processing Identity nod                     e from HSS XML - sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.495 UTC Debug registrar.cpp:651: REGISTER for public ID sip:                     2010000007 at dellnfv.com<mailto:2010000007 at dellnfv.com> uses AOR sip:2010000007 at dellnfv.com
17-02-2016 12:59:44.495 UTC Debug subscriber_data_manager.cpp:366: Get AoR data                      for sip:2010000007 at dellnfv.com
17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:195: Key reg\\sip:201000000                     7 at dellnfv.com<mailto:7 at dellnfv.com> hashes to vbucket 19 via hash 0x9f135593
17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:367: 1 read replicas for ke                     y reg\\sip:2010000007 at dellnfv.com
17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:402: Attempt to read from r                     eplica 0 (connection 0x7ff1440ae680)
17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:780: Fetch result
17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:788: Found record on replic                     a
17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:410: Read for reg\\sip:2010                     000007 at dellnfv.com<mailto:000007 at dellnfv.com> on replica 0 returned SUCCESS
17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:453: Read 469 bytes from ta                     ble reg key sip:2010000007 at dellnfv.com, CAS = 1537
17-02-2016 12:59:44.496 UTC Debug communicationmonitor.cpp:82: Checking communic                     ation changes - successful attempts 4, failures 0
17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:376: Data store re                     turned a record, CAS = 1537
17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:477: Try to deseri                     alize record for sip:2010000007 at dellnfv.com with 'JSON' deserializer
17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:969: Deserialize J                     SON document: {"bindings":{"sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinst                     ance=907814e95ea68ac1<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinst????????????????????%20ance=907814e95ea68ac1>":{"uri":"sip:2010000007 at 192.168.1.8:64835;transport=UDP;ri                     nstance=907814e95ea68ac1<sip:2010000007 at 192.168.1.8:64835;transport=UDP;ri????????????????????%20nstance=907814e95ea68ac1>","cid":"Pp7e4umpNXHLsITvW6y7RA..","cseq":29,"expires":1                     455713990,"priority":0,"params":{},"paths":["sip:rP6Yw8PkfR at 192.168.0.4:5058;tra                     nsport=TCP;lr;ob<sip:rP6Yw8PkfR at 192.168.0.4:5058;tra????????????????????%20nsport=TCP;lr;ob>"],"timer_id":"016dc709400001210040001000104104","private_id":"2                     010000007 at dellnfv.com","emergency_reg":false}},"subscriptions":{},"notify_cseq<mailto:010000007 at dellnfv.com%22,%22emergency_reg%22:false%7d%7d,%22subscriptions%22:%7b%7d,%22notify_cseq>":                     29}
17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:994:   Binding: si                     p:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68ac1
17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:482: Deserializati                     on suceeded
17-02-2016 12:59:44.496 UTC Debug registrar.cpp:249: Retrieved AoR data 0x7ff144                     23a0a0
17-02-2016 12:59:44.496 UTC Debug registrar.cpp:342: Binding identifier for cont                     act = sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68ac1
17-02-2016 12:59:44.496 UTC Debug registrar.cpp:369: Path header sip:rP6Yw8PkfR@                     192.168.0.4:5058;transport=TCP;lr;ob
17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:196: Set AoR data                      for sip:2010000007 at dellnfv.com, CAS=1537, expiry = 1455714054
17-02-2016 12:59:44.496 UTC Debug httpresolver.cpp:71: HttpResolver::resolve for                      host 127.0.0.1, port 7253, family 2
17-02-2016 12:59:44.496 UTC Debug baseresolver.cpp:513: Attempt to parse 127.0.0                     .1 as IP address
17-02-2016 12:59:44.496 UTC Debug httpresolver.cpp:79: Target is an IP address
17-02-2016 12:59:44.496 UTC Debug httpconnection.cpp:623: Sending HTTP request :                      http://127.0.0.1:7253/timers/016dc709400001210040001000104104 (trying 127.0.0.1                     ) on new connection
17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header http/1                     .1200ok with value
17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header locati                     on with value /timers/016dc709400001210040001000104104
17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header conten                     t-length with value 0
17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header  with                      value
17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:638: Received HTTP response                     : status=200, doc=
17-02-2016 12:59:44.498 UTC Debug communicationmonitor.cpp:82: Checking communic                     ation changes - successful attempts 1, failures 0
17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:542: Writing 469 bytes to t                     able reg key sip:2010000007 at dellnfv.com, CAS = 1537, expiry = 70
17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:195: Key reg\\sip:201000000                     7 at dellnfv.com<mailto:7 at dellnfv.com> hashes to vbucket 19 via hash 0x9f135593
17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:562: 1 write replicas for k                     ey reg\\sip:2010000007 at dellnfv.com
17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:616: Attempt conditional wr                     ite to vbucket 19 on replica 0 (connection 0x7ff1440ae680), CAS = 1537, expiry =                      70
17-02-2016 12:59:44.499 UTC Debug memcachedstore.cpp:657: Conditional write succ                     eeded to replica 0
17-02-2016 12:59:44.499 UTC Debug subscriber_data_manager.cpp:438: Data store se                     t_data returned 1
17-02-2016 12:59:44.499 UTC Debug registrar.cpp:116: Bindings for sip:2010000007                     @dellnfv.com
17-02-2016 12:59:44.499 UTC Debug registrar.cpp:130:   sip:2010000007 at 192.168.1.                     8:64835;transport=UDP;rinstance=907814e95ea68ac1 URI=sip:2010000007 at 192.168.1.8:                     64835;transport=UDP;rinstance=907814e95ea68ac1 expires=1455714044 q=0 from=Pp7e4                     umpNXHLsITvW6y7RA.. cseq=30 timer=016dc709400001210040001000104104 private_id=20                     10000007 at dellnfv.com<mailto:10000007 at dellnfv.com> emergency_registration=false
17-02-2016 12:59:44.499 UTC Debug pjsip:       endpoint Response msg 200/REGISTE                     R/cseq=30 (tdta0x7ff14423a790) created
17-02-2016 12:59:44.499 UTC Debug acr.cpp:1550: Store associated URIs
17-02-2016 12:59:44.499 UTC Verbose common_sip_processing.cpp:136: TX 765 bytes                      Response msg 200/REGISTER/cseq=30 (tdta0x7ff14423a790) to TCP 192.168.0.4:53691:
--start msg--

SIP/2.0 200 OK
Service-Route: <sip:sprout.dellnfv.com:5054;transport=TCP;lr;orig>
Via: SIP/2.0/TCP 192.168.0.4:53691;rport=53691;received=192.168.0.4;branch=z9hG4                     bKPjcGZ7eF5y6OvGqDehOqxpk4MEbqS4Ag4l
Via: SIP/2.0/UDP 192.168.1.8:64835;rport=64835;received=192.168.1.8;branch=z9hG4                     bK-524287-1---1c645317b5676c22
Call-ID: Pp7e4umpNXHLsITvW6y7RA..
From: <sip:2010000007 at dellnfv.com>;tag=698b4951
To: <sip:2010000007 at dellnfv.com>;tag=z9hG4bKPjcGZ7eF5y6OvGqDehOqxpk4MEbqS4Ag4l
CSeq: 30 REGISTER
Supported: outbound
Contact: <sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68                     ac1<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68????????????????????%20ac1>>;expires=60
Require: outbound
Path: <sip:rP6Yw8PkfR at 192.168.0.4:5058;transport=TCP;lr;ob>
P-Associated-URI: <sip:2010000007 at dellnfv.com>
Content-Length:  0


--end msg--
17-02-2016 12:59:44.499 UTC Info acr.cpp:658: No CCF or ECF to send ACR for sess                     ion Pp7e4umpNXHLsITvW6y7RA.. to - dropping!
17-02-2016 12:59:44.499 UTC Debug acr.cpp:54: Destroyed ACR (0x7ff144148500)
17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:763: Interpreting orig IFC info                     rmation
17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:437: SPT class Method: result f                     alse
17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:541: Add to group 0 val false
17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:559: Result group 0 val false
17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:572: iFC does not match
17-02-2016 12:59:44.499 UTC Info registration_utils.cpp:187: Found 0 Application                      Servers
17-02-2016 12:59:44.499 UTC Debug pjsip: tdta0x7ff14423 Destroying txdata Respon                     se msg 200/REGISTER/cseq=30 (tdta0x7ff14423a790)
17-02-2016 12:59:44.499 UTC Debug registrar.cpp:1036: Report SAS end marker - tr                     ail (1e3)
17-02-2016 12:59:44.499 UTC Debug thread_dispatcher.cpp:193: Worker thread compl                     eted processing message 0x7ff14c114d18
17-02-2016 12:59:44.499 UTC Debug thread_dispatcher.cpp:199: Request latency = 1                     5474us
17-02-2016 12:59:46.381 UTC Verbose pjsip: tcps0x7ff14c15 TCP transport destroye                     d normally
17-02-2016 12:59:46.520 UTC Verbose pjsip:    tcplis:5054 TCP listener 192.168.0                     .6:5054: got incoming TCP connection from 192.168.0.4:35812, sock=408
17-02-2016 12:59:46.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP server transport c                     reated
17-02-2016 12:59:47.520 UTC Verbose pjsip:    tcplis:5054 TCP listener 192.168.0                     .6:5054: got incoming TCP connection from 192.168.0.4:41045, sock=938
17-02-2016 12:59:47.520 UTC Verbose pjsip: tcps0x7ff14c15 TCP server transport c                     reated
17-02-2016 12:59:47.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP connection closed
17-02-2016 12:59:47.521 UTC Debug connection_tracker.cpp:91: Connection 0x7ff14c                     05d228 has been destroyed
17-02-2016 12:59:47.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP transport destroye                     d with reason 70016: End of file (PJ_EEOF)
17-02-2016 12:59:49.521 UTC Verbose pjsip:    tcplis:5054 TCP listener 192.168.0                     .6:5054: got incoming TCP connection from 192.168.0.4:51223, sock=429
17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP server transport c                     reated
17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c07 TCP connection closed
17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c07 TCP transport destroye                     d with reason 70016: End of file (PJ_EEOF)
17-02-2016 12:59:52.359 UTC Verbose pjsip:    tcplis:5054 TCP listener 192.168.0                     .6:5054: got incoming TCP connection from 192.168.0.6:51013, sock=970
17-02-2016 12:59:52.359 UTC Verbose pjsip: tcps0x7ff14c07 TCP server transport c                     reated
17-02-2016 12:59:52.359 UTC Debug pjsip: sip_endpoint.c Processing incoming mess                     age: Request msg OPTIONS/cseq=97408 (rdata0x7ff14c0747e0)
17-02-2016 12:59:52.359 UTC Verbose common_sip_processing.cpp:120: RX 342 bytes                      Request msg OPTIONS/cseq=97408 (rdata0x7ff14c0747e0) from TCP 192.168.0.6:51013:
--start msg--

OPTIONS sip:poll-sip at 192.168.0.6:5054 SIP/2.0
Via: SIP/2.0/TCP 192.168.0.6;rport;branch=z9hG4bK-97408
Max-Forwards: 2
To: <sip:poll-sip at 192.168.0.6:5054>
From: poll-sip <sip:poll-sip at 192.168.0.6>;tag=97408
Call-ID: poll-sip-97408
CSeq: 97408 OPTIONS
Contact: <sip:192.168.0.6>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-02-2016 12:59:52.359 UTC Debug uri_classifier.cpp:167: home domain: false, lo                     cal_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:52.359 UTC Debug uri_classifier.cpp:197: Classified URI as 3
17-02-2016 12:59:52.359 UTC Debug common_sip_processing.cpp:212: Skipping SAS lo                     gging for OPTIONS request
17-02-2016 12:59:52.359 UTC Debug thread_dispatcher.cpp:253: Queuing cloned rece                     ived message 0x7ff14c01d7c8 for worker threads
17-02-2016 12:59:52.359 UTC Debug thread_dispatcher.cpp:149: Worker thread deque                     ue message 0x7ff14c01d7c8
17-02-2016 12:59:52.360 UTC Debug pjsip: sip_endpoint.c Distributing rdata to mo                     dules: Request msg OPTIONS/cseq=97408 (rdata0x7ff14c01d7c8)
17-02-2016 12:59:52.360 UTC Debug uri_classifier.cpp:167: home domain: false, lo                     cal_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:52.360 UTC Debug uri_classifier.cpp:197: Classified URI as 3
17-02-2016 12:59:52.360 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS                     /cseq=97408 (tdta0x7ff14c298780) created
17-02-2016 12:59:52.360 UTC Verbose common_sip_processing.cpp:136: TX 273 bytes                      Response msg 200/OPTIONS/cseq=97408 (tdta0x7ff14c298780) to TCP 192.168.0.6:5101                     3:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 192.168.0.6;rport=51013;received=192.168.0.6;branch=z9hG4bK-974                     08
Call-ID: poll-sip-97408
From: "poll-sip" <sip:poll-sip at 192.168.0.6>;tag=97408
To: <sip:poll-sip at 192.168.0.6>;tag=z9hG4bK-97408
CSeq: 97408 OPTIONS
Content-Length:  0


--end msg--
17-02-2016 12:59:52.360 UTC Debug common_sip_processing.cpp:254: Skipping SAS lo                     gging for OPTIONS response
17-02-2016 12:59:52.360 UTC Debug pjsip: tdta0x7ff14c29 Destroying txdata Respon                     se msg 200/OPTIONS/cseq=97408 (tdta0x7ff14c298780)
17-02-2016 12:59:52.360 UTC Debug thread_dispatcher.cpp:193: Worker thread compl                     eted processing message 0x7ff14c01d7c8
17-02-2016 12:59:52.360 UTC Debug thread_dispatcher.cpp:199: Request latency = 2                     42us
17-02-2016 12:59:52.360 UTC Info load_monitor.cpp:212: Accepted 100.000000% of r                     equests, latency error = -0.953790, overload responses = 0
17-02-2016 12:59:52.360 UTC Status load_monitor.cpp:260: Maximum incoming reques                     t rate/second unchanged - only handled 20 requests in last 73256ms, minimum thre                     shold for a change is 18314.000000
17-02-2016 12:59:52.360 UTC Debug snmp_continuous_accumulator_table.cpp:108: Acc                     umulating sample 500ui into continuous accumulator statistic
17-02-2016 12:59:52.360 UTC Debug snmp_continuous_accumulator_table.cpp:108: Acc                     umulating sample 500ui into continuous accumulator statistic
17-02-2016 12:59:52.361 UTC Verbose httpstack.cpp:286: Process request for URL /                     ping, args (null)
17-02-2016 12:59:52.361 UTC Verbose httpstack.cpp:69: Sending response 200 to re                     quest for URL /ping, args (null)
17-02-2016 12:59:53.362 UTC Verbose pjsip: tcps0x7ff14c07 TCP connection closed
17-02-2016 12:59:53.362 UTC Debug connection_tracker.cpp:91: Connection 0x7ff14c                     0744a8 has been destroyed
17-02-2016 12:59:53.362 UTC Verbose pjsip: tcps0x7ff14c07 TCP transport destroye                     d with reason 70016: End of file (PJ_EEOF)
17-02-2016 12:59:53.522 UTC Verbose pjsip:    tcplis:5054 TCP listener 192.168.0                     .6:5054: got incoming TCP connection from 192.168.0.4:49292, sock=970
17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c07 TCP server transport c                     reated
17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c08 TCP connection closed
17-02-2016 12:59:53.522 UTC Debug connection_tracker.cpp:91: Connection 0x7ff14c                     085a08 has been destroyed
17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c08 TCP transport destroye                     d with reason 70016: End of file (PJ_EEOF)


Regards,

Tahir Masood

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160217/20634fe2/attachment.html>

From moreira_r at outlook.com  Wed Feb 17 17:21:53 2016
From: moreira_r at outlook.com (Rodrigo Moreira)
Date: Wed, 17 Feb 2016 20:21:53 -0200
Subject: [Clearwater] [Ellis Error on provisioning numbers]
Message-ID: <BAY181-W29057AE38772088035CE249EAE0@phx.gbl>

   Hi,

 I would like yours help, I have problem in the number of provisioning for the new user. After entering Ellis service on port 80 and request the registration of a new user, I is reported the following error:

 'Failed to update the server (see detailed diagnostics in console developer). Please refresh the page'

 Already deleted the records in the tables Ellis data base.
 Usually this problem is solved that way, it is reasonable to reinstall Ellis?

 Since I am already grateful.

Att.,

Rodrigo M.
+55 (37) 9 99132 - 4539
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160217/df40d41e/attachment.html>

From tahir.masood at xflowresearch.com  Thu Feb 18 00:07:50 2016
From: tahir.masood at xflowresearch.com (Tahir Masood)
Date: Thu, 18 Feb 2016 10:07:50 +0500
Subject: [Clearwater] Call disconnects after 30 seconds
In-Reply-To: <BN3PR02MB12550A399B914CF838CC4EDF9BAE0@BN3PR02MB1255.namprd02.prod.outlook.com>
References: <004901d16983$a523e3c0$ef6bab40$@xflowresearch.com>
	<BN3PR02MB12550A399B914CF838CC4EDF9BAE0@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <001401d16a0a$5496ef10$fdc4cd30$@xflowresearch.com>

Hi Ellie 

Here are the attached sprout logs the deployment method is manual in
OpenStack environment 

 

 

Regards,

 

Tahir Masood

 

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com] 
Sent: Wednesday, February 17, 2016 7:32 PM
To: Tahir Masood; clearwater at lists.projectclearwater.org
Subject: RE: [Clearwater] Call disconnects after 30 seconds

 

Hi Tahir,

 

How have you set up your deployment? Is it a manual install/all-in-one
install/etc..?

 

Also, can you please send me over the full debug logs from Sprout and Bono
(the logs you?ve pasted in below don?t have an INVITE in them)?

 

Thanks,

 

Ellie

 

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On
Behalf Of Tahir Masood
Sent: 17 February 2016 13:04
To: clearwater at lists.projectclearwater.org
<mailto:clearwater at lists.projectclearwater.org> 
Subject: [Clearwater] Call disconnects after 30 seconds

 

Dear all,

I am facing an issue of call disconnection after 30 seconds  have used both
Xlite and Zoiper but the problem persists. I have change the log_level to 5
in sprout and here are the logs. Can you please suggest me a solution  

 

17-02-2016 12:59:42.374 UTC Debug pjsip: sip_endpoint.c Processing incoming
mess                     age: Request msg OPTIONS/cseq=97398
(rdata0x7ff14c06fb50)

17-02-2016 12:59:42.374 UTC Verbose common_sip_processing.cpp:120: RX 342
bytes                      Request msg OPTIONS/cseq=97398
(rdata0x7ff14c06fb50) from TCP 192.168.0.6:50982:

--start msg--

 

OPTIONS sip:poll-sip at 192.168.0.6:5054 SIP/2.0

Via: SIP/2.0/TCP 192.168.0.6;rport;branch=z9hG4bK-97398

Max-Forwards: 2

To: <sip:poll-sip at 192.168.0.6:5054>

From: poll-sip <sip:poll-sip at 192.168.0.6>;tag=97398

Call-ID: poll-sip-97398

CSeq: 97398 OPTIONS

Contact: <sip:192.168.0.6>

Accept: application/sdp

Content-Length: 0

User-Agent: poll-sip

 

 

--end msg--

17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:42.374 UTC Debug common_sip_processing.cpp:212: Skipping
SAS lo                     gging for OPTIONS request

17-02-2016 12:59:42.374 UTC Debug thread_dispatcher.cpp:253: Queuing cloned
rece                     ived message 0x7ff14c01d7c8 for worker threads

17-02-2016 12:59:42.374 UTC Debug thread_dispatcher.cpp:149: Worker thread
deque                     ue message 0x7ff14c01d7c8

17-02-2016 12:59:42.374 UTC Debug pjsip: sip_endpoint.c Distributing rdata
to mo                     dules: Request msg OPTIONS/cseq=97398
(rdata0x7ff14c01d7c8)

17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:42.374 UTC Debug pjsip:       endpoint Response msg
200/OPTIONS                     /cseq=97398 (tdta0x7ff13c408040) created

17-02-2016 12:59:42.374 UTC Verbose common_sip_processing.cpp:136: TX 273
bytes                      Response msg 200/OPTIONS/cseq=97398
(tdta0x7ff13c408040) to TCP 192.168.0.6:5098                     2:

--start msg--

 

SIP/2.0 200 OK

Via: SIP/2.0/TCP
192.168.0.6;rport=50982;received=192.168.0.6;branch=z9hG4bK-973
98

Call-ID: poll-sip-97398

From: "poll-sip" <sip:poll-sip at 192.168.0.6>;tag=97398

To: <sip:poll-sip at 192.168.0.6>;tag=z9hG4bK-97398

CSeq: 97398 OPTIONS

Content-Length:  0

 

 

--end msg--

17-02-2016 12:59:42.374 UTC Debug common_sip_processing.cpp:254: Skipping
SAS lo                     gging for OPTIONS response

17-02-2016 12:59:42.374 UTC Debug pjsip: tdta0x7ff13c40 Destroying txdata
Respon                     se msg 200/OPTIONS/cseq=97398
(tdta0x7ff13c408040)

17-02-2016 12:59:42.375 UTC Debug thread_dispatcher.cpp:193: Worker thread
compl                     eted processing message 0x7ff14c01d7c8

17-02-2016 12:59:42.375 UTC Debug thread_dispatcher.cpp:199: Request latency
= 6                     61us

17-02-2016 12:59:42.385 UTC Verbose httpstack.cpp:286: Process request for
URL /                     ping, args (null)

17-02-2016 12:59:42.385 UTC Verbose httpstack.cpp:69: Sending response 200
to re                     quest for URL /ping, args (null)

17-02-2016 12:59:43.375 UTC Verbose pjsip: tcps0x7ff14c06 TCP connection
closed

17-02-2016 12:59:43.375 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     06f818 has been destroyed

17-02-2016 12:59:43.376 UTC Verbose pjsip: tcps0x7ff14c06 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:43.519 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:34463, sock=1055

17-02-2016 12:59:43.519 UTC Verbose pjsip: tcps0x7ff14c06 TCP server
transport c                     reated

17-02-2016 12:59:43.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP connection
closed

17-02-2016 12:59:43.520 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     114c98 has been destroyed

17-02-2016 12:59:43.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:44.483 UTC Debug pjsip: sip_endpoint.c Processing incoming
mess                     age: Request msg REGISTER/cseq=30
(rdata0x7ff14c04bf60)

17-02-2016 12:59:44.483 UTC Verbose common_sip_processing.cpp:120: RX 1242
bytes                      Request msg REGISTER/cseq=30
(rdata0x7ff14c04bf60) from TCP 192.168.0.4:53691:

--start msg--

 

REGISTER sip:dellnfv.com;transport=UDP SIP/2.0

Via: SIP/2.0/TCP
192.168.0.4:53691;rport;branch=z9hG4bKPjcGZ7eF5y6OvGqDehOqxpk4M
EbqS4Ag4l

Path: <sip:rP6Yw8PkfR at 192.168.0.4:5058;transport=TCP;lr;ob>

Via: SIP/2.0/UDP
192.168.1.8:64835;rport=64835;received=192.168.1.8;branch=z9hG4
bK-524287-1---1c645317b5676c22

Max-Forwards: 70

Contact:
<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68
<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68
%20ac1>                      ac1>

To: <sip:2010000007 at dellnfv.com>

From: <sip:2010000007 at dellnfv.com>;tag=698b4951

Call-ID: Pp7e4umpNXHLsITvW6y7RA..

CSeq: 30 REGISTER

Expires: 60

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO,
SUBSCRIB                     E

Supported: replaces, norefersub, extended-refer, timer, outbound, path,
X-cisco-                     serviceuri

User-Agent: Zoiper r35079

Authorization: Digest response="49157d3d9e710e40b06a6d4a6c9af137",
username="201                     0000007 at dellnfv.com
<mailto:0000007 at dellnfv.com> ", realm="dellnfv.com",
nonce="0d8f08d16e444374", uri="sip:de <sip:de
%20llnfv.com;transport=UDP>                      llnfv.com;transport=UDP",
algorithm=MD5, cnonce="450410c6c32d2ff38597f3468504591
d", opaque="2975ef5f2182abff", qop=auth,
nc=0000001d,integrity-protected=ip-asso                     c-yes

Allow-Events: presence, kpml

P-Visited-Network-ID: dellnfv.com

Route: <sip:sprout.dellnfv.com:5054;transport=TCP;lr;orig>

Content-Length:  0

 

 

--end msg--

17-02-2016 12:59:44.484 UTC Debug pjutils.cpp:1648: Logging SAS Call-ID
marker,                      Call-ID Pp7e4umpNXHLsITvW6y7RA..

17-02-2016 12:59:44.484 UTC Debug thread_dispatcher.cpp:253: Queuing cloned
rece                     ived message 0x7ff14c114d18 for worker threads

17-02-2016 12:59:44.484 UTC Debug thread_dispatcher.cpp:149: Worker thread
deque                     ue message 0x7ff14c114d18

17-02-2016 12:59:44.484 UTC Debug pjsip: sip_endpoint.c Distributing rdata
to mo                     dules: Request msg REGISTER/cseq=30
(rdata0x7ff14c114d18)

17-02-2016 12:59:44.484 UTC Debug uri_classifier.cpp:167: home domain: true,
loc                     al_to_node: false, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as
4

17-02-2016 12:59:44.485 UTC Debug authentication.cpp:673: Authentication
module                      invoked

17-02-2016 12:59:44.485 UTC Debug authentication.cpp:581: Authorization
header i                     n request

17-02-2016 12:59:44.485 UTC Info authentication.cpp:595: SIP Digest
authenticate                     d request integrity protected by edge proxy

17-02-2016 12:59:44.485 UTC Debug authentication.cpp:683: Request does not
need                      authentication

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:167: home domain: true,
loc                     al_to_node: false, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as
4

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:44.485 UTC Debug acr.cpp:1763: Create RalfACR for node type
S-C                     SCF with role Originating

17-02-2016 12:59:44.485 UTC Debug acr.cpp:49: Created ACR (0x7ff144148500)

17-02-2016 12:59:44.485 UTC Debug acr.cpp:175: Created S-CSCF Ralf ACR

17-02-2016 12:59:44.485 UTC Debug acr.cpp:214: Set record type for P/S-CSCF

17-02-2016 12:59:44.485 UTC Debug acr.cpp:222: Non-dialog message =>
EVENT_RECOR                     D

17-02-2016 12:59:44.485 UTC Debug acr.cpp:1491: Stored 0 subscription
identifier                     s

17-02-2016 12:59:44.485 UTC Debug registrar.cpp:541: Process REGISTER for
public                      ID sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.485 UTC Debug registrar.cpp:549: Report SAS start marker
- t                     rail (1e3)

17-02-2016 12:59:44.485 UTC Debug hssconnection.cpp:585: Making Homestead
reques                     t for
/impu/sip%3A2010000007%40dellnfv.com/reg-data?private_id=2010000007%40dell
nfv.com

17-02-2016 12:59:44.485 UTC Debug httpresolver.cpp:71: HttpResolver::resolve
for                      host hs.dellnfv.com, port 8888, family 2

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:513: Attempt to parse
hs.dell                     nfv.com as IP address

17-02-2016 12:59:44.485 UTC Debug dnscachedresolver.cpp:667: Removing record
for                      hs.dellnfv.com (type 1, expiry time 1455713976)
from the expiry list

17-02-2016 12:59:44.485 UTC Verbose dnscachedresolver.cpp:240: Check cache
for h                     s.dellnfv.com type 1

17-02-2016 12:59:44.485 UTC Debug dnscachedresolver.cpp:326: Pulling 1
records f                     rom cache for hs.dellnfv.com A

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:361: Found 1 A/AAAA
records,                      randomizing

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:503: 192.168.0.8:8888
transpo                     rt 6 is not blacklisted

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:382: Added a server, now
have                      1 of 5

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:420: Adding 0 servers
from bl                     acklist

17-02-2016 12:59:44.485 UTC Debug httpconnection.cpp:623: Sending HTTP
request :
http://hs.dellnfv.com:8888/impu/sip%3A2010000007%40dellnfv.com/reg-data?priv
ate                     _id=2010000007%40dellnfv.com (trying 192.168.0.8) on
new connection

17-02-2016 12:59:44.494 UTC Debug httpconnection.cpp:915: Received header
http/1                     .1200ok with value

17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header
conten                     t-length with value 869

17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header
conten                     t-type with value text/plain

17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header
with                      value

17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:638: Received HTTP
response                     : status=200, doc=<ClearwaterRegData>

        <RegistrationState>REGISTERED</RegistrationState>

        <IMSSubscription xsi="http://www.w3.org/2001/XMLSchema-instance"
noNames                     paceSchemaLocation="CxDataType.xsd">

                <PrivateID>2010000007 at dellnfv.com
<mailto:2010000007 at dellnfv.com%3c/PrivateID> </PrivateID>

                <ServiceProfile>

                        <InitialFilterCriteria>

                                <TriggerPoint>

 
<ConditionTypeCNF>0</ConditionTypeCNF>

                                        <SPT>

 
<ConditionNegated>0</ConditionNe                     gated>

                                                <Group>0</Group>

                                                <Method>INVITE</Method>

                                                <Extension/>

                                        </SPT>

                                </TriggerPoint>

                                <ApplicationServer>

                                        <ServerName>sip:mmtel.dellnfv.com
<sip:mmtel.dellnfv.com%3c/Serve> </Serve                     rName>

                                        <DefaultHandling>0</DefaultHandling>

                                </ApplicationServer>

                        </InitialFilterCriteria>

                        <PublicIdentity>

                                <BarringIndication>1</BarringIndication>

                                <Identity>sip:2010000007 at dellnfv.com
<sip:2010000007 at dellnfv.com%3c/Identity> </Identity>

                        </PublicIdentity>

                </ServiceProfile>

        </IMSSubscription>

</ClearwaterRegData>

 

 

17-02-2016 12:59:44.495 UTC Debug communicationmonitor.cpp:82: Checking
communic                     ation changes - successful attempts 1, failures
0

17-02-2016 12:59:44.495 UTC Debug hssconnection.cpp:366: Processing Identity
nod                     e from HSS XML - sip:2010000007 at dellnfv.com

 

17-02-2016 12:59:44.495 UTC Debug registrar.cpp:651: REGISTER for public ID
sip:                     2010000007 at dellnfv.com
<mailto:2010000007 at dellnfv.com>  uses AOR sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.495 UTC Debug subscriber_data_manager.cpp:366: Get AoR
data                      for sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:195: Key
reg\\sip:201000000                     7 at dellnfv.com <mailto:7 at dellnfv.com>
hashes to vbucket 19 via hash 0x9f135593

17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:367: 1 read replicas
for ke                     y reg\\sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:402: Attempt to read
from r                     eplica 0 (connection 0x7ff1440ae680)

17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:780: Fetch result

17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:788: Found record on
replic                     a

17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:410: Read for
reg\\sip:2010                     000007 at dellnfv.com
<mailto:000007 at dellnfv.com>  on replica 0 returned SUCCESS

17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:453: Read 469 bytes
from ta                     ble reg key sip:2010000007 at dellnfv.com, CAS =
1537

17-02-2016 12:59:44.496 UTC Debug communicationmonitor.cpp:82: Checking
communic                     ation changes - successful attempts 4, failures
0

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:376: Data
store re                     turned a record, CAS = 1537

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:477: Try to
deseri                     alize record for sip:2010000007 at dellnfv.com with
'JSON' deserializer

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:969:
Deserialize J                     SON document:
{"bindings":{"sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinst
<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinst
%20ance=907814e95ea68ac1>
ance=907814e95ea68ac1":{"uri":"sip:2010000007 at 192.168.1.8:64835;transport=UD
P;ri <sip:2010000007 at 192.168.1.8:64835;transport=UDP;ri
%20nstance=907814e95ea68ac1>
nstance=907814e95ea68ac1","cid":"Pp7e4umpNXHLsITvW6y7RA..","cseq":29,"expire
s":1
455713990,"priority":0,"params":{},"paths":["sip:rP6Yw8PkfR at 192.168.0.4:5058
;tra <sip:rP6Yw8PkfR at 192.168.0.4:5058;tra
%20nsport=TCP;lr;ob>
nsport=TCP;lr;ob"],"timer_id":"016dc709400001210040001000104104","private_id
":"2                     010000007 at dellnfv.com
<mailto:010000007 at dellnfv.com%22,%22emergency_reg%22:false%7d%7d,%22subscrip
tions%22:%7b%7d,%22notify_cseq>
","emergency_reg":false}},"subscriptions":{},"notify_cseq":
29}

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:994:
Binding: si
p:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68ac1

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:482:
Deserializati                     on suceeded

17-02-2016 12:59:44.496 UTC Debug registrar.cpp:249: Retrieved AoR data
0x7ff144                     23a0a0

17-02-2016 12:59:44.496 UTC Debug registrar.cpp:342: Binding identifier for
cont                     act =
sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68ac1

17-02-2016 12:59:44.496 UTC Debug registrar.cpp:369: Path header
sip:rP6Yw8PkfR@                     192.168.0.4:5058;transport=TCP;lr;ob

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:196: Set AoR
data                      for sip:2010000007 at dellnfv.com, CAS=1537, expiry =
1455714054

17-02-2016 12:59:44.496 UTC Debug httpresolver.cpp:71: HttpResolver::resolve
for                      host 127.0.0.1, port 7253, family 2

17-02-2016 12:59:44.496 UTC Debug baseresolver.cpp:513: Attempt to parse
127.0.0                     .1 as IP address

17-02-2016 12:59:44.496 UTC Debug httpresolver.cpp:79: Target is an IP
address

17-02-2016 12:59:44.496 UTC Debug httpconnection.cpp:623: Sending HTTP
request :
http://127.0.0.1:7253/timers/016dc709400001210040001000104104 (trying
127.0.0.1                     ) on new connection

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header
http/1                     .1200ok with value

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header
locati                     on with value
/timers/016dc709400001210040001000104104

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header
conten                     t-length with value 0

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header
with                      value

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:638: Received HTTP
response                     : status=200, doc=

17-02-2016 12:59:44.498 UTC Debug communicationmonitor.cpp:82: Checking
communic                     ation changes - successful attempts 1, failures
0

17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:542: Writing 469 bytes
to t                     able reg key sip:2010000007 at dellnfv.com, CAS =
1537, expiry = 70

17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:195: Key
reg\\sip:201000000                     7 at dellnfv.com <mailto:7 at dellnfv.com>
hashes to vbucket 19 via hash 0x9f135593

17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:562: 1 write replicas
for k                     ey reg\\sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:616: Attempt
conditional wr                     ite to vbucket 19 on replica 0
(connection 0x7ff1440ae680), CAS = 1537, expiry =                      70

17-02-2016 12:59:44.499 UTC Debug memcachedstore.cpp:657: Conditional write
succ                     eeded to replica 0

17-02-2016 12:59:44.499 UTC Debug subscriber_data_manager.cpp:438: Data
store se                     t_data returned 1

17-02-2016 12:59:44.499 UTC Debug registrar.cpp:116: Bindings for
sip:2010000007                     @dellnfv.com

17-02-2016 12:59:44.499 UTC Debug registrar.cpp:130:
sip:2010000007 at 192.168.1.
8:64835;transport=UDP;rinstance=907814e95ea68ac1
URI=sip:2010000007 at 192.168.1.8:
64835;transport=UDP;rinstance=907814e95ea68ac1 expires=1455714044 q=0
from=Pp7e4                     umpNXHLsITvW6y7RA.. cseq=30
timer=016dc709400001210040001000104104 private_id=20
10000007 at dellnfv.com <mailto:10000007 at dellnfv.com>
emergency_registration=false

17-02-2016 12:59:44.499 UTC Debug pjsip:       endpoint Response msg
200/REGISTE                     R/cseq=30 (tdta0x7ff14423a790) created

17-02-2016 12:59:44.499 UTC Debug acr.cpp:1550: Store associated URIs

17-02-2016 12:59:44.499 UTC Verbose common_sip_processing.cpp:136: TX 765
bytes                      Response msg 200/REGISTER/cseq=30
(tdta0x7ff14423a790) to TCP 192.168.0.4:53691:

--start msg--

 

SIP/2.0 200 OK

Service-Route: <sip:sprout.dellnfv.com:5054;transport=TCP;lr;orig>

Via: SIP/2.0/TCP
192.168.0.4:53691;rport=53691;received=192.168.0.4;branch=z9hG4
bKPjcGZ7eF5y6OvGqDehOqxpk4MEbqS4Ag4l

Via: SIP/2.0/UDP
192.168.1.8:64835;rport=64835;received=192.168.1.8;branch=z9hG4
bK-524287-1---1c645317b5676c22

Call-ID: Pp7e4umpNXHLsITvW6y7RA..

From: <sip:2010000007 at dellnfv.com>;tag=698b4951

To:
<sip:2010000007 at dellnfv.com>;tag=z9hG4bKPjcGZ7eF5y6OvGqDehOqxpk4MEbqS4Ag4l

CSeq: 30 REGISTER

Supported: outbound

Contact:
<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68
<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68
%20ac1>                      ac1>;expires=60

Require: outbound

Path: <sip:rP6Yw8PkfR at 192.168.0.4:5058;transport=TCP;lr;ob>

P-Associated-URI: <sip:2010000007 at dellnfv.com>

Content-Length:  0

 

 

--end msg--

17-02-2016 12:59:44.499 UTC Info acr.cpp:658: No CCF or ECF to send ACR for
sess                     ion Pp7e4umpNXHLsITvW6y7RA.. to - dropping!

17-02-2016 12:59:44.499 UTC Debug acr.cpp:54: Destroyed ACR (0x7ff144148500)

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:763: Interpreting orig IFC
info                     rmation

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:437: SPT class Method:
result f                     alse

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:541: Add to group 0 val
false

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:559: Result group 0 val
false

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:572: iFC does not match

17-02-2016 12:59:44.499 UTC Info registration_utils.cpp:187: Found 0
Application                      Servers

17-02-2016 12:59:44.499 UTC Debug pjsip: tdta0x7ff14423 Destroying txdata
Respon                     se msg 200/REGISTER/cseq=30 (tdta0x7ff14423a790)

17-02-2016 12:59:44.499 UTC Debug registrar.cpp:1036: Report SAS end marker
- tr                     ail (1e3)

17-02-2016 12:59:44.499 UTC Debug thread_dispatcher.cpp:193: Worker thread
compl                     eted processing message 0x7ff14c114d18

17-02-2016 12:59:44.499 UTC Debug thread_dispatcher.cpp:199: Request latency
= 1                     5474us

17-02-2016 12:59:46.381 UTC Verbose pjsip: tcps0x7ff14c15 TCP transport
destroye                     d normally

17-02-2016 12:59:46.520 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:35812, sock=408

17-02-2016 12:59:46.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP server
transport c                     reated

17-02-2016 12:59:47.520 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:41045, sock=938

17-02-2016 12:59:47.520 UTC Verbose pjsip: tcps0x7ff14c15 TCP server
transport c                     reated

17-02-2016 12:59:47.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP connection
closed

17-02-2016 12:59:47.521 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     05d228 has been destroyed

17-02-2016 12:59:47.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:49.521 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:51223, sock=429

17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP server
transport c                     reated

17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c07 TCP connection
closed

17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c07 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:52.359 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.6:51013, sock=970

17-02-2016 12:59:52.359 UTC Verbose pjsip: tcps0x7ff14c07 TCP server
transport c                     reated

17-02-2016 12:59:52.359 UTC Debug pjsip: sip_endpoint.c Processing incoming
mess                     age: Request msg OPTIONS/cseq=97408
(rdata0x7ff14c0747e0)

17-02-2016 12:59:52.359 UTC Verbose common_sip_processing.cpp:120: RX 342
bytes                      Request msg OPTIONS/cseq=97408
(rdata0x7ff14c0747e0) from TCP 192.168.0.6:51013:

--start msg--

 

OPTIONS sip:poll-sip at 192.168.0.6:5054 SIP/2.0

Via: SIP/2.0/TCP 192.168.0.6;rport;branch=z9hG4bK-97408

Max-Forwards: 2

To: <sip:poll-sip at 192.168.0.6:5054>

From: poll-sip <sip:poll-sip at 192.168.0.6>;tag=97408

Call-ID: poll-sip-97408

CSeq: 97408 OPTIONS

Contact: <sip:192.168.0.6>

Accept: application/sdp

Content-Length: 0

User-Agent: poll-sip

 

 

--end msg--

17-02-2016 12:59:52.359 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:52.359 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:52.359 UTC Debug common_sip_processing.cpp:212: Skipping
SAS lo                     gging for OPTIONS request

17-02-2016 12:59:52.359 UTC Debug thread_dispatcher.cpp:253: Queuing cloned
rece                     ived message 0x7ff14c01d7c8 for worker threads

17-02-2016 12:59:52.359 UTC Debug thread_dispatcher.cpp:149: Worker thread
deque                     ue message 0x7ff14c01d7c8

17-02-2016 12:59:52.360 UTC Debug pjsip: sip_endpoint.c Distributing rdata
to mo                     dules: Request msg OPTIONS/cseq=97408
(rdata0x7ff14c01d7c8)

17-02-2016 12:59:52.360 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:52.360 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:52.360 UTC Debug pjsip:       endpoint Response msg
200/OPTIONS                     /cseq=97408 (tdta0x7ff14c298780) created

17-02-2016 12:59:52.360 UTC Verbose common_sip_processing.cpp:136: TX 273
bytes                      Response msg 200/OPTIONS/cseq=97408
(tdta0x7ff14c298780) to TCP 192.168.0.6:5101                     3:

--start msg--

 

SIP/2.0 200 OK

Via: SIP/2.0/TCP
192.168.0.6;rport=51013;received=192.168.0.6;branch=z9hG4bK-974
08

Call-ID: poll-sip-97408

From: "poll-sip" <sip:poll-sip at 192.168.0.6>;tag=97408

To: <sip:poll-sip at 192.168.0.6>;tag=z9hG4bK-97408

CSeq: 97408 OPTIONS

Content-Length:  0

 

 

--end msg--

17-02-2016 12:59:52.360 UTC Debug common_sip_processing.cpp:254: Skipping
SAS lo                     gging for OPTIONS response

17-02-2016 12:59:52.360 UTC Debug pjsip: tdta0x7ff14c29 Destroying txdata
Respon                     se msg 200/OPTIONS/cseq=97408
(tdta0x7ff14c298780)

17-02-2016 12:59:52.360 UTC Debug thread_dispatcher.cpp:193: Worker thread
compl                     eted processing message 0x7ff14c01d7c8

17-02-2016 12:59:52.360 UTC Debug thread_dispatcher.cpp:199: Request latency
= 2                     42us

17-02-2016 12:59:52.360 UTC Info load_monitor.cpp:212: Accepted 100.000000%
of r                     equests, latency error = -0.953790, overload
responses = 0

17-02-2016 12:59:52.360 UTC Status load_monitor.cpp:260: Maximum incoming
reques                     t rate/second unchanged - only handled 20
requests in last 73256ms, minimum thre                     shold for a
change is 18314.000000

17-02-2016 12:59:52.360 UTC Debug snmp_continuous_accumulator_table.cpp:108:
Acc                     umulating sample 500ui into continuous accumulator
statistic

17-02-2016 12:59:52.360 UTC Debug snmp_continuous_accumulator_table.cpp:108:
Acc                     umulating sample 500ui into continuous accumulator
statistic

17-02-2016 12:59:52.361 UTC Verbose httpstack.cpp:286: Process request for
URL /                     ping, args (null)

17-02-2016 12:59:52.361 UTC Verbose httpstack.cpp:69: Sending response 200
to re                     quest for URL /ping, args (null)

17-02-2016 12:59:53.362 UTC Verbose pjsip: tcps0x7ff14c07 TCP connection
closed

17-02-2016 12:59:53.362 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     0744a8 has been destroyed

17-02-2016 12:59:53.362 UTC Verbose pjsip: tcps0x7ff14c07 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:53.522 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:49292, sock=970

17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c07 TCP server
transport c                     reated

17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c08 TCP connection
closed

17-02-2016 12:59:53.522 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     085a08 has been destroyed

17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c08 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

 

 

Regards,

 

Tahir Masood

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160218/e8b6d4ec/attachment.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Sprout logs.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160218/e8b6d4ec/attachment.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Bono logs.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160218/e8b6d4ec/attachment-0001.txt>

From tahir.masood at xflowresearch.com  Thu Feb 18 00:25:12 2016
From: tahir.masood at xflowresearch.com (Tahir Masood)
Date: Thu, 18 Feb 2016 10:25:12 +0500
Subject: [Clearwater] Call disconnects after 30 seconds
References: <004901d16983$a523e3c0$ef6bab40$@xflowresearch.com>
	<BN3PR02MB12550A399B914CF838CC4EDF9BAE0@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <002701d16a0c$c1b80730$45281590$@xflowresearch.com>

Hi Ellie

Detailed Bono logs are also attached 

 

Regards,

 

Tahir Masood

 

From: Tahir Masood [mailto:tahir.masood at xflowresearch.com] 
Sent: Thursday, February 18, 2016 10:08 AM
To: 'Eleanor Merry'; 'clearwater at lists.projectclearwater.org'
Subject: RE: [Clearwater] Call disconnects after 30 seconds

 

Hi Ellie 

Here are the attached sprout logs the deployment method is manual in
OpenStack environment 

 

 

Regards,

 

Tahir Masood

 

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com] 
Sent: Wednesday, February 17, 2016 7:32 PM
To: Tahir Masood; clearwater at lists.projectclearwater.org
<mailto:clearwater at lists.projectclearwater.org> 
Subject: RE: [Clearwater] Call disconnects after 30 seconds

 

Hi Tahir,

 

How have you set up your deployment? Is it a manual install/all-in-one
install/etc..?

 

Also, can you please send me over the full debug logs from Sprout and Bono
(the logs you?ve pasted in below don?t have an INVITE in them)?

 

Thanks,

 

Ellie

 

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On
Behalf Of Tahir Masood
Sent: 17 February 2016 13:04
To: clearwater at lists.projectclearwater.org
<mailto:clearwater at lists.projectclearwater.org> 
Subject: [Clearwater] Call disconnects after 30 seconds

 

Dear all,

I am facing an issue of call disconnection after 30 seconds  have used both
Xlite and Zoiper but the problem persists. I have change the log_level to 5
in sprout and here are the logs. Can you please suggest me a solution  

 

17-02-2016 12:59:42.374 UTC Debug pjsip: sip_endpoint.c Processing incoming
mess                     age: Request msg OPTIONS/cseq=97398
(rdata0x7ff14c06fb50)

17-02-2016 12:59:42.374 UTC Verbose common_sip_processing.cpp:120: RX 342
bytes                      Request msg OPTIONS/cseq=97398
(rdata0x7ff14c06fb50) from TCP 192.168.0.6:50982:

--start msg--

 

OPTIONS sip:poll-sip at 192.168.0.6:5054 SIP/2.0

Via: SIP/2.0/TCP 192.168.0.6;rport;branch=z9hG4bK-97398

Max-Forwards: 2

To: <sip:poll-sip at 192.168.0.6:5054>

From: poll-sip <sip:poll-sip at 192.168.0.6>;tag=97398

Call-ID: poll-sip-97398

CSeq: 97398 OPTIONS

Contact: <sip:192.168.0.6>

Accept: application/sdp

Content-Length: 0

User-Agent: poll-sip

 

 

--end msg--

17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:42.374 UTC Debug common_sip_processing.cpp:212: Skipping
SAS lo                     gging for OPTIONS request

17-02-2016 12:59:42.374 UTC Debug thread_dispatcher.cpp:253: Queuing cloned
rece                     ived message 0x7ff14c01d7c8 for worker threads

17-02-2016 12:59:42.374 UTC Debug thread_dispatcher.cpp:149: Worker thread
deque                     ue message 0x7ff14c01d7c8

17-02-2016 12:59:42.374 UTC Debug pjsip: sip_endpoint.c Distributing rdata
to mo                     dules: Request msg OPTIONS/cseq=97398
(rdata0x7ff14c01d7c8)

17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:42.374 UTC Debug pjsip:       endpoint Response msg
200/OPTIONS                     /cseq=97398 (tdta0x7ff13c408040) created

17-02-2016 12:59:42.374 UTC Verbose common_sip_processing.cpp:136: TX 273
bytes                      Response msg 200/OPTIONS/cseq=97398
(tdta0x7ff13c408040) to TCP 192.168.0.6:5098                     2:

--start msg--

 

SIP/2.0 200 OK

Via: SIP/2.0/TCP
192.168.0.6;rport=50982;received=192.168.0.6;branch=z9hG4bK-973
98

Call-ID: poll-sip-97398

From: "poll-sip" <sip:poll-sip at 192.168.0.6>;tag=97398

To: <sip:poll-sip at 192.168.0.6>;tag=z9hG4bK-97398

CSeq: 97398 OPTIONS

Content-Length:  0

 

 

--end msg--

17-02-2016 12:59:42.374 UTC Debug common_sip_processing.cpp:254: Skipping
SAS lo                     gging for OPTIONS response

17-02-2016 12:59:42.374 UTC Debug pjsip: tdta0x7ff13c40 Destroying txdata
Respon                     se msg 200/OPTIONS/cseq=97398
(tdta0x7ff13c408040)

17-02-2016 12:59:42.375 UTC Debug thread_dispatcher.cpp:193: Worker thread
compl                     eted processing message 0x7ff14c01d7c8

17-02-2016 12:59:42.375 UTC Debug thread_dispatcher.cpp:199: Request latency
= 6                     61us

17-02-2016 12:59:42.385 UTC Verbose httpstack.cpp:286: Process request for
URL /                     ping, args (null)

17-02-2016 12:59:42.385 UTC Verbose httpstack.cpp:69: Sending response 200
to re                     quest for URL /ping, args (null)

17-02-2016 12:59:43.375 UTC Verbose pjsip: tcps0x7ff14c06 TCP connection
closed

17-02-2016 12:59:43.375 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     06f818 has been destroyed

17-02-2016 12:59:43.376 UTC Verbose pjsip: tcps0x7ff14c06 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:43.519 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:34463, sock=1055

17-02-2016 12:59:43.519 UTC Verbose pjsip: tcps0x7ff14c06 TCP server
transport c                     reated

17-02-2016 12:59:43.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP connection
closed

17-02-2016 12:59:43.520 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     114c98 has been destroyed

17-02-2016 12:59:43.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:44.483 UTC Debug pjsip: sip_endpoint.c Processing incoming
mess                     age: Request msg REGISTER/cseq=30
(rdata0x7ff14c04bf60)

17-02-2016 12:59:44.483 UTC Verbose common_sip_processing.cpp:120: RX 1242
bytes                      Request msg REGISTER/cseq=30
(rdata0x7ff14c04bf60) from TCP 192.168.0.4:53691:

--start msg--

 

REGISTER sip:dellnfv.com;transport=UDP SIP/2.0

Via: SIP/2.0/TCP
192.168.0.4:53691;rport;branch=z9hG4bKPjcGZ7eF5y6OvGqDehOqxpk4M
EbqS4Ag4l

Path: <sip:rP6Yw8PkfR at 192.168.0.4:5058;transport=TCP;lr;ob>

Via: SIP/2.0/UDP
192.168.1.8:64835;rport=64835;received=192.168.1.8;branch=z9hG4
bK-524287-1---1c645317b5676c22

Max-Forwards: 70

Contact:
<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68
<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68
%20ac1>                      ac1>

To: <sip:2010000007 at dellnfv.com>

From: <sip:2010000007 at dellnfv.com>;tag=698b4951

Call-ID: Pp7e4umpNXHLsITvW6y7RA..

CSeq: 30 REGISTER

Expires: 60

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO,
SUBSCRIB                     E

Supported: replaces, norefersub, extended-refer, timer, outbound, path,
X-cisco-                     serviceuri

User-Agent: Zoiper r35079

Authorization: Digest response="49157d3d9e710e40b06a6d4a6c9af137",
username="201                     0000007 at dellnfv.com
<mailto:0000007 at dellnfv.com> ", realm="dellnfv.com",
nonce="0d8f08d16e444374", uri="sip:de <sip:de
%20llnfv.com;transport=UDP>                      llnfv.com;transport=UDP",
algorithm=MD5, cnonce="450410c6c32d2ff38597f3468504591
d", opaque="2975ef5f2182abff", qop=auth,
nc=0000001d,integrity-protected=ip-asso                     c-yes

Allow-Events: presence, kpml

P-Visited-Network-ID: dellnfv.com

Route: <sip:sprout.dellnfv.com:5054;transport=TCP;lr;orig>

Content-Length:  0

 

 

--end msg--

17-02-2016 12:59:44.484 UTC Debug pjutils.cpp:1648: Logging SAS Call-ID
marker,                      Call-ID Pp7e4umpNXHLsITvW6y7RA..

17-02-2016 12:59:44.484 UTC Debug thread_dispatcher.cpp:253: Queuing cloned
rece                     ived message 0x7ff14c114d18 for worker threads

17-02-2016 12:59:44.484 UTC Debug thread_dispatcher.cpp:149: Worker thread
deque                     ue message 0x7ff14c114d18

17-02-2016 12:59:44.484 UTC Debug pjsip: sip_endpoint.c Distributing rdata
to mo                     dules: Request msg REGISTER/cseq=30
(rdata0x7ff14c114d18)

17-02-2016 12:59:44.484 UTC Debug uri_classifier.cpp:167: home domain: true,
loc                     al_to_node: false, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as
4

17-02-2016 12:59:44.485 UTC Debug authentication.cpp:673: Authentication
module                      invoked

17-02-2016 12:59:44.485 UTC Debug authentication.cpp:581: Authorization
header i                     n request

17-02-2016 12:59:44.485 UTC Info authentication.cpp:595: SIP Digest
authenticate                     d request integrity protected by edge proxy

17-02-2016 12:59:44.485 UTC Debug authentication.cpp:683: Request does not
need                      authentication

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:167: home domain: true,
loc                     al_to_node: false, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as
4

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:44.485 UTC Debug acr.cpp:1763: Create RalfACR for node type
S-C                     SCF with role Originating

17-02-2016 12:59:44.485 UTC Debug acr.cpp:49: Created ACR (0x7ff144148500)

17-02-2016 12:59:44.485 UTC Debug acr.cpp:175: Created S-CSCF Ralf ACR

17-02-2016 12:59:44.485 UTC Debug acr.cpp:214: Set record type for P/S-CSCF

17-02-2016 12:59:44.485 UTC Debug acr.cpp:222: Non-dialog message =>
EVENT_RECOR                     D

17-02-2016 12:59:44.485 UTC Debug acr.cpp:1491: Stored 0 subscription
identifier                     s

17-02-2016 12:59:44.485 UTC Debug registrar.cpp:541: Process REGISTER for
public                      ID sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.485 UTC Debug registrar.cpp:549: Report SAS start marker
- t                     rail (1e3)

17-02-2016 12:59:44.485 UTC Debug hssconnection.cpp:585: Making Homestead
reques                     t for
/impu/sip%3A2010000007%40dellnfv.com/reg-data?private_id=2010000007%40dell
nfv.com

17-02-2016 12:59:44.485 UTC Debug httpresolver.cpp:71: HttpResolver::resolve
for                      host hs.dellnfv.com, port 8888, family 2

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:513: Attempt to parse
hs.dell                     nfv.com as IP address

17-02-2016 12:59:44.485 UTC Debug dnscachedresolver.cpp:667: Removing record
for                      hs.dellnfv.com (type 1, expiry time 1455713976)
from the expiry list

17-02-2016 12:59:44.485 UTC Verbose dnscachedresolver.cpp:240: Check cache
for h                     s.dellnfv.com type 1

17-02-2016 12:59:44.485 UTC Debug dnscachedresolver.cpp:326: Pulling 1
records f                     rom cache for hs.dellnfv.com A

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:361: Found 1 A/AAAA
records,                      randomizing

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:503: 192.168.0.8:8888
transpo                     rt 6 is not blacklisted

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:382: Added a server, now
have                      1 of 5

17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:420: Adding 0 servers
from bl                     acklist

17-02-2016 12:59:44.485 UTC Debug httpconnection.cpp:623: Sending HTTP
request :
http://hs.dellnfv.com:8888/impu/sip%3A2010000007%40dellnfv.com/reg-data?priv
ate                     _id=2010000007%40dellnfv.com (trying 192.168.0.8) on
new connection

17-02-2016 12:59:44.494 UTC Debug httpconnection.cpp:915: Received header
http/1                     .1200ok with value

17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header
conten                     t-length with value 869

17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header
conten                     t-type with value text/plain

17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header
with                      value

17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:638: Received HTTP
response                     : status=200, doc=<ClearwaterRegData>

        <RegistrationState>REGISTERED</RegistrationState>

        <IMSSubscription xsi="http://www.w3.org/2001/XMLSchema-instance"
noNames                     paceSchemaLocation="CxDataType.xsd">

                <PrivateID>2010000007 at dellnfv.com
<mailto:2010000007 at dellnfv.com%3c/PrivateID> </PrivateID>

                <ServiceProfile>

                        <InitialFilterCriteria>

                                <TriggerPoint>

 
<ConditionTypeCNF>0</ConditionTypeCNF>

                                        <SPT>

 
<ConditionNegated>0</ConditionNe                     gated>

                                                <Group>0</Group>

                                                <Method>INVITE</Method>

                                                <Extension/>

                                        </SPT>

                                </TriggerPoint>

                                <ApplicationServer>

                                        <ServerName>sip:mmtel.dellnfv.com
<sip:mmtel.dellnfv.com%3c/Serve> </Serve                     rName>

                                        <DefaultHandling>0</DefaultHandling>

                                </ApplicationServer>

                        </InitialFilterCriteria>

                        <PublicIdentity>

                                <BarringIndication>1</BarringIndication>

                                <Identity>sip:2010000007 at dellnfv.com
<sip:2010000007 at dellnfv.com%3c/Identity> </Identity>

                        </PublicIdentity>

                </ServiceProfile>

        </IMSSubscription>

</ClearwaterRegData>

 

 

17-02-2016 12:59:44.495 UTC Debug communicationmonitor.cpp:82: Checking
communic                     ation changes - successful attempts 1, failures
0

17-02-2016 12:59:44.495 UTC Debug hssconnection.cpp:366: Processing Identity
nod                     e from HSS XML - sip:2010000007 at dellnfv.com

 

17-02-2016 12:59:44.495 UTC Debug registrar.cpp:651: REGISTER for public ID
sip:                     2010000007 at dellnfv.com
<mailto:2010000007 at dellnfv.com>  uses AOR sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.495 UTC Debug subscriber_data_manager.cpp:366: Get AoR
data                      for sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:195: Key
reg\\sip:201000000                     7 at dellnfv.com <mailto:7 at dellnfv.com>
hashes to vbucket 19 via hash 0x9f135593

17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:367: 1 read replicas
for ke                     y reg\\sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:402: Attempt to read
from r                     eplica 0 (connection 0x7ff1440ae680)

17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:780: Fetch result

17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:788: Found record on
replic                     a

17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:410: Read for
reg\\sip:2010                     000007 at dellnfv.com
<mailto:000007 at dellnfv.com>  on replica 0 returned SUCCESS

17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:453: Read 469 bytes
from ta                     ble reg key sip:2010000007 at dellnfv.com, CAS =
1537

17-02-2016 12:59:44.496 UTC Debug communicationmonitor.cpp:82: Checking
communic                     ation changes - successful attempts 4, failures
0

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:376: Data
store re                     turned a record, CAS = 1537

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:477: Try to
deseri                     alize record for sip:2010000007 at dellnfv.com with
'JSON' deserializer

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:969:
Deserialize J                     SON document:
{"bindings":{"sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinst
<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinst
%20ance=907814e95ea68ac1>
ance=907814e95ea68ac1":{"uri":"sip:2010000007 at 192.168.1.8:64835;transport=UD
P;ri <sip:2010000007 at 192.168.1.8:64835;transport=UDP;ri
%20nstance=907814e95ea68ac1>
nstance=907814e95ea68ac1","cid":"Pp7e4umpNXHLsITvW6y7RA..","cseq":29,"expire
s":1
455713990,"priority":0,"params":{},"paths":["sip:rP6Yw8PkfR at 192.168.0.4:5058
;tra <sip:rP6Yw8PkfR at 192.168.0.4:5058;tra
%20nsport=TCP;lr;ob>
nsport=TCP;lr;ob"],"timer_id":"016dc709400001210040001000104104","private_id
":"2                     010000007 at dellnfv.com
<mailto:010000007 at dellnfv.com%22,%22emergency_reg%22:false%7d%7d,%22subscrip
tions%22:%7b%7d,%22notify_cseq>
","emergency_reg":false}},"subscriptions":{},"notify_cseq":
29}

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:994:
Binding: si
p:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68ac1

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:482:
Deserializati                     on suceeded

17-02-2016 12:59:44.496 UTC Debug registrar.cpp:249: Retrieved AoR data
0x7ff144                     23a0a0

17-02-2016 12:59:44.496 UTC Debug registrar.cpp:342: Binding identifier for
cont                     act =
sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68ac1

17-02-2016 12:59:44.496 UTC Debug registrar.cpp:369: Path header
sip:rP6Yw8PkfR@                     192.168.0.4:5058;transport=TCP;lr;ob

17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:196: Set AoR
data                      for sip:2010000007 at dellnfv.com, CAS=1537, expiry =
1455714054

17-02-2016 12:59:44.496 UTC Debug httpresolver.cpp:71: HttpResolver::resolve
for                      host 127.0.0.1, port 7253, family 2

17-02-2016 12:59:44.496 UTC Debug baseresolver.cpp:513: Attempt to parse
127.0.0                     .1 as IP address

17-02-2016 12:59:44.496 UTC Debug httpresolver.cpp:79: Target is an IP
address

17-02-2016 12:59:44.496 UTC Debug httpconnection.cpp:623: Sending HTTP
request :
http://127.0.0.1:7253/timers/016dc709400001210040001000104104 (trying
127.0.0.1                     ) on new connection

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header
http/1                     .1200ok with value

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header
locati                     on with value
/timers/016dc709400001210040001000104104

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header
conten                     t-length with value 0

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header
with                      value

17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:638: Received HTTP
response                     : status=200, doc=

17-02-2016 12:59:44.498 UTC Debug communicationmonitor.cpp:82: Checking
communic                     ation changes - successful attempts 1, failures
0

17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:542: Writing 469 bytes
to t                     able reg key sip:2010000007 at dellnfv.com, CAS =
1537, expiry = 70

17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:195: Key
reg\\sip:201000000                     7 at dellnfv.com <mailto:7 at dellnfv.com>
hashes to vbucket 19 via hash 0x9f135593

17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:562: 1 write replicas
for k                     ey reg\\sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:616: Attempt
conditional wr                     ite to vbucket 19 on replica 0
(connection 0x7ff1440ae680), CAS = 1537, expiry =                      70

17-02-2016 12:59:44.499 UTC Debug memcachedstore.cpp:657: Conditional write
succ                     eeded to replica 0

17-02-2016 12:59:44.499 UTC Debug subscriber_data_manager.cpp:438: Data
store se                     t_data returned 1

17-02-2016 12:59:44.499 UTC Debug registrar.cpp:116: Bindings for
sip:2010000007                     @dellnfv.com

17-02-2016 12:59:44.499 UTC Debug registrar.cpp:130:
sip:2010000007 at 192.168.1.
8:64835;transport=UDP;rinstance=907814e95ea68ac1
URI=sip:2010000007 at 192.168.1.8:
64835;transport=UDP;rinstance=907814e95ea68ac1 expires=1455714044 q=0
from=Pp7e4                     umpNXHLsITvW6y7RA.. cseq=30
timer=016dc709400001210040001000104104 private_id=20
10000007 at dellnfv.com <mailto:10000007 at dellnfv.com>
emergency_registration=false

17-02-2016 12:59:44.499 UTC Debug pjsip:       endpoint Response msg
200/REGISTE                     R/cseq=30 (tdta0x7ff14423a790) created

17-02-2016 12:59:44.499 UTC Debug acr.cpp:1550: Store associated URIs

17-02-2016 12:59:44.499 UTC Verbose common_sip_processing.cpp:136: TX 765
bytes                      Response msg 200/REGISTER/cseq=30
(tdta0x7ff14423a790) to TCP 192.168.0.4:53691:

--start msg--

 

SIP/2.0 200 OK

Service-Route: <sip:sprout.dellnfv.com:5054;transport=TCP;lr;orig>

Via: SIP/2.0/TCP
192.168.0.4:53691;rport=53691;received=192.168.0.4;branch=z9hG4
bKPjcGZ7eF5y6OvGqDehOqxpk4MEbqS4Ag4l

Via: SIP/2.0/UDP
192.168.1.8:64835;rport=64835;received=192.168.1.8;branch=z9hG4
bK-524287-1---1c645317b5676c22

Call-ID: Pp7e4umpNXHLsITvW6y7RA..

From: <sip:2010000007 at dellnfv.com>;tag=698b4951

To:
<sip:2010000007 at dellnfv.com>;tag=z9hG4bKPjcGZ7eF5y6OvGqDehOqxpk4MEbqS4Ag4l

CSeq: 30 REGISTER

Supported: outbound

Contact:
<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68
<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68
%20ac1>                      ac1>;expires=60

Require: outbound

Path: <sip:rP6Yw8PkfR at 192.168.0.4:5058;transport=TCP;lr;ob>

P-Associated-URI: <sip:2010000007 at dellnfv.com>

Content-Length:  0

 

 

--end msg--

17-02-2016 12:59:44.499 UTC Info acr.cpp:658: No CCF or ECF to send ACR for
sess                     ion Pp7e4umpNXHLsITvW6y7RA.. to - dropping!

17-02-2016 12:59:44.499 UTC Debug acr.cpp:54: Destroyed ACR (0x7ff144148500)

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:763: Interpreting orig IFC
info                     rmation

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:437: SPT class Method:
result f                     alse

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:541: Add to group 0 val
false

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:559: Result group 0 val
false

17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:572: iFC does not match

17-02-2016 12:59:44.499 UTC Info registration_utils.cpp:187: Found 0
Application                      Servers

17-02-2016 12:59:44.499 UTC Debug pjsip: tdta0x7ff14423 Destroying txdata
Respon                     se msg 200/REGISTER/cseq=30 (tdta0x7ff14423a790)

17-02-2016 12:59:44.499 UTC Debug registrar.cpp:1036: Report SAS end marker
- tr                     ail (1e3)

17-02-2016 12:59:44.499 UTC Debug thread_dispatcher.cpp:193: Worker thread
compl                     eted processing message 0x7ff14c114d18

17-02-2016 12:59:44.499 UTC Debug thread_dispatcher.cpp:199: Request latency
= 1                     5474us

17-02-2016 12:59:46.381 UTC Verbose pjsip: tcps0x7ff14c15 TCP transport
destroye                     d normally

17-02-2016 12:59:46.520 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:35812, sock=408

17-02-2016 12:59:46.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP server
transport c                     reated

17-02-2016 12:59:47.520 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:41045, sock=938

17-02-2016 12:59:47.520 UTC Verbose pjsip: tcps0x7ff14c15 TCP server
transport c                     reated

17-02-2016 12:59:47.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP connection
closed

17-02-2016 12:59:47.521 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     05d228 has been destroyed

17-02-2016 12:59:47.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:49.521 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:51223, sock=429

17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP server
transport c                     reated

17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c07 TCP connection
closed

17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c07 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:52.359 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.6:51013, sock=970

17-02-2016 12:59:52.359 UTC Verbose pjsip: tcps0x7ff14c07 TCP server
transport c                     reated

17-02-2016 12:59:52.359 UTC Debug pjsip: sip_endpoint.c Processing incoming
mess                     age: Request msg OPTIONS/cseq=97408
(rdata0x7ff14c0747e0)

17-02-2016 12:59:52.359 UTC Verbose common_sip_processing.cpp:120: RX 342
bytes                      Request msg OPTIONS/cseq=97408
(rdata0x7ff14c0747e0) from TCP 192.168.0.6:51013:

--start msg--

 

OPTIONS sip:poll-sip at 192.168.0.6:5054 SIP/2.0

Via: SIP/2.0/TCP 192.168.0.6;rport;branch=z9hG4bK-97408

Max-Forwards: 2

To: <sip:poll-sip at 192.168.0.6:5054>

From: poll-sip <sip:poll-sip at 192.168.0.6>;tag=97408

Call-ID: poll-sip-97408

CSeq: 97408 OPTIONS

Contact: <sip:192.168.0.6>

Accept: application/sdp

Content-Length: 0

User-Agent: poll-sip

 

 

--end msg--

17-02-2016 12:59:52.359 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:52.359 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:52.359 UTC Debug common_sip_processing.cpp:212: Skipping
SAS lo                     gging for OPTIONS request

17-02-2016 12:59:52.359 UTC Debug thread_dispatcher.cpp:253: Queuing cloned
rece                     ived message 0x7ff14c01d7c8 for worker threads

17-02-2016 12:59:52.359 UTC Debug thread_dispatcher.cpp:149: Worker thread
deque                     ue message 0x7ff14c01d7c8

17-02-2016 12:59:52.360 UTC Debug pjsip: sip_endpoint.c Distributing rdata
to mo                     dules: Request msg OPTIONS/cseq=97408
(rdata0x7ff14c01d7c8)

17-02-2016 12:59:52.360 UTC Debug uri_classifier.cpp:167: home domain:
false, lo                     cal_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true,
treat_number_as_phone: false

17-02-2016 12:59:52.360 UTC Debug uri_classifier.cpp:197: Classified URI as
3

17-02-2016 12:59:52.360 UTC Debug pjsip:       endpoint Response msg
200/OPTIONS                     /cseq=97408 (tdta0x7ff14c298780) created

17-02-2016 12:59:52.360 UTC Verbose common_sip_processing.cpp:136: TX 273
bytes                      Response msg 200/OPTIONS/cseq=97408
(tdta0x7ff14c298780) to TCP 192.168.0.6:5101                     3:

--start msg--

 

SIP/2.0 200 OK

Via: SIP/2.0/TCP
192.168.0.6;rport=51013;received=192.168.0.6;branch=z9hG4bK-974
08

Call-ID: poll-sip-97408

From: "poll-sip" <sip:poll-sip at 192.168.0.6>;tag=97408

To: <sip:poll-sip at 192.168.0.6>;tag=z9hG4bK-97408

CSeq: 97408 OPTIONS

Content-Length:  0

 

 

--end msg--

17-02-2016 12:59:52.360 UTC Debug common_sip_processing.cpp:254: Skipping
SAS lo                     gging for OPTIONS response

17-02-2016 12:59:52.360 UTC Debug pjsip: tdta0x7ff14c29 Destroying txdata
Respon                     se msg 200/OPTIONS/cseq=97408
(tdta0x7ff14c298780)

17-02-2016 12:59:52.360 UTC Debug thread_dispatcher.cpp:193: Worker thread
compl                     eted processing message 0x7ff14c01d7c8

17-02-2016 12:59:52.360 UTC Debug thread_dispatcher.cpp:199: Request latency
= 2                     42us

17-02-2016 12:59:52.360 UTC Info load_monitor.cpp:212: Accepted 100.000000%
of r                     equests, latency error = -0.953790, overload
responses = 0

17-02-2016 12:59:52.360 UTC Status load_monitor.cpp:260: Maximum incoming
reques                     t rate/second unchanged - only handled 20
requests in last 73256ms, minimum thre                     shold for a
change is 18314.000000

17-02-2016 12:59:52.360 UTC Debug snmp_continuous_accumulator_table.cpp:108:
Acc                     umulating sample 500ui into continuous accumulator
statistic

17-02-2016 12:59:52.360 UTC Debug snmp_continuous_accumulator_table.cpp:108:
Acc                     umulating sample 500ui into continuous accumulator
statistic

17-02-2016 12:59:52.361 UTC Verbose httpstack.cpp:286: Process request for
URL /                     ping, args (null)

17-02-2016 12:59:52.361 UTC Verbose httpstack.cpp:69: Sending response 200
to re                     quest for URL /ping, args (null)

17-02-2016 12:59:53.362 UTC Verbose pjsip: tcps0x7ff14c07 TCP connection
closed

17-02-2016 12:59:53.362 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     0744a8 has been destroyed

17-02-2016 12:59:53.362 UTC Verbose pjsip: tcps0x7ff14c07 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

17-02-2016 12:59:53.522 UTC Verbose pjsip:    tcplis:5054 TCP listener
192.168.0                     .6:5054: got incoming TCP connection from
192.168.0.4:49292, sock=970

17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c07 TCP server
transport c                     reated

17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c08 TCP connection
closed

17-02-2016 12:59:53.522 UTC Debug connection_tracker.cpp:91: Connection
0x7ff14c                     085a08 has been destroyed

17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c08 TCP transport
destroye                     d with reason 70016: End of file (PJ_EEOF)

 

 

Regards,

 

Tahir Masood

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160218/9127b6db/attachment.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: bono_logs2.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160218/9127b6db/attachment.txt>

From Eleanor.Merry at metaswitch.com  Thu Feb 18 14:03:17 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Thu, 18 Feb 2016 19:03:17 +0000
Subject: [Clearwater] [Ellis Error on provisioning numbers]
In-Reply-To: <BAY181-W29057AE38772088035CE249EAE0@phx.gbl>
References: <BAY181-W29057AE38772088035CE249EAE0@phx.gbl>
Message-ID: <BN3PR02MB12550A2CD05B116D65EDB89E9BAF0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi,

Can you please send me the Ellis logs? These are in /var/log/ellis/ellis* on your Ellis node.

Thanks,

Ellie

From: Rodrigo Moreira [mailto:moreira_r at outlook.com]
Sent: 17 February 2016 22:22
To: Eleanor Merry
Cc: clearwater at lists.projectclearwater.org
Subject: [Ellis Error on provisioning numbers]

Hi,

I would like yours help, I have problem in the number of provisioning for the new user. After entering Ellis service on port 80 and request the registration of a new user, I is reported the following error:

'Failed to update the server (see detailed diagnostics in console developer). Please refresh the page'

Already deleted the records in the tables Ellis data base.
Usually this problem is solved that way, it is reasonable to reinstall Ellis?

Since I am already grateful.


Att.,

Rodrigo M.
+55 (37) 9 99132 - 4539
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160218/03bba8c1/attachment.html>

From moreira_r at outlook.com  Thu Feb 18 15:11:51 2016
From: moreira_r at outlook.com (Rodrigo Moreira)
Date: Thu, 18 Feb 2016 18:11:51 -0200
Subject: [Clearwater] [Ellis Error on provisioning numbers]
In-Reply-To: <BN3PR02MB12550A2CD05B116D65EDB89E9BAF0@BN3PR02MB1255.namprd02.prod.outlook.com>
References: <BAY181-W29057AE38772088035CE249EAE0@phx.gbl>,
	<BN3PR02MB12550A2CD05B116D65EDB89E9BAF0@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <BAY181-W1ED005BD2692B26356C219EAF0@phx.gbl>

Hi,

Sure.

18-02-2016 12:00:01.655 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
18-02-2016 12:00:03.048 UTC WARNING simple_httpclient.py:325: uncaught exception
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 323, in cleanup
    yield
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 177, in __init__
    0, 0)
gaierror: [Errno -2] Name or service not known
18-02-2016 12:00:03.275 UTC ERROR homestead.py:68: Failed to ping Homestead at http://ellis.nyc3.example.com:8889/ping. Have you configured your HOMESTEAD_URL?
18-02-2016 12:02:26.952 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 29.09ms
18-02-2016 12:03:12.847 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 25.56ms
18-02-2016 12:04:36.777 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.60ms
18-02-2016 12:04:57.355 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 27.60ms
18-02-2016 12:06:19.454 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 21.88ms
18-02-2016 12:07:03.050 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.97ms
18-02-2016 12:07:50.946 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.53ms
18-02-2016 12:08:50.810 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 38.71ms
18-02-2016 12:09:30.680 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.44ms
18-02-2016 12:10:33.273 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.90ms
18-02-2016 12:11:35.405 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 23.22ms
18-02-2016 12:12:36.485 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 38.60ms
18-02-2016 12:13:42.096 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 19.70ms
18-02-2016 12:14:40.905 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 22.42ms
18-02-2016 12:15:24.002 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 19.14ms
18-02-2016 12:16:46.775 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 16.85ms
18-02-2016 12:17:36.025 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.81ms
18-02-2016 12:18:24.653 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 14.56ms
18-02-2016 12:19:41.954 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 50.59ms
18-02-2016 12:20:01.664 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 34.94ms
18-02-2016 12:21:23.588 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 11.19ms
18-02-2016 12:22:26.289 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.54ms
18-02-2016 12:23:11.485 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.89ms
18-02-2016 12:24:15.538 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 27.40ms
18-02-2016 12:25:11.607 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 25.17ms
18-02-2016 12:26:03.068 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 22.01ms
18-02-2016 12:26:44.912 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 23.79ms
18-02-2016 12:28:09.706 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.60ms
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@18-02-2016 12:39:36.430 UTC INFO main.py:115: Pr$
18-02-2016 12:39:37.440 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
18-02-2016 12:39:37.649 UTC WARNING simple_httpclient.py:325: uncaught exception
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 323, in cleanup
    yield
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 177, in __init__


18-02-2016 12:00:01.655 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
18-02-2016 12:00:03.048 UTC WARNING simple_httpclient.py:325: uncaught exception
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 323, in cleanup
    yield
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 177, in __init__
    0, 0)
gaierror: [Errno -2] Name or service not known
18-02-2016 12:00:03.275 UTC ERROR homestead.py:68: Failed to ping Homestead at http://ellis.nyc3.example.com:8889/ping. Have you configured your HOMESTEAD_URL?
18-02-2016 12:02:26.952 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 29.09ms
18-02-2016 12:03:12.847 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 25.56ms
18-02-2016 12:04:36.777 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.60ms
18-02-2016 12:04:57.355 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 27.60ms
18-02-2016 12:06:19.454 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 21.88ms
18-02-2016 12:07:03.050 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.97ms
18-02-2016 12:07:50.946 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.53ms
18-02-2016 12:08:50.810 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 38.71ms
18-02-2016 12:09:30.680 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.44ms
18-02-2016 12:10:33.273 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.90ms
18-02-2016 12:11:35.405 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 23.22ms
18-02-2016 12:12:36.485 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 38.60ms
18-02-2016 12:13:42.096 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 19.70ms
18-02-2016 12:14:40.905 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 22.42ms
18-02-2016 12:15:24.002 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 19.14ms
18-02-2016 12:16:46.775 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 16.85ms
18-02-2016 12:17:36.025 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.81ms
18-02-2016 12:18:24.653 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 14.56ms
18-02-2016 12:19:41.954 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 50.59ms
18-02-2016 12:20:01.664 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 34.94ms
18-02-2016 12:21:23.588 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 11.19ms
18-02-2016 12:22:26.289 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.54ms
18-02-2016 12:23:11.485 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.89ms
18-02-2016 12:24:15.538 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 27.40ms
18-02-2016 12:25:11.607 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 25.17ms
18-02-2016 12:26:03.068 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 22.01ms
18-02-2016 12:26:44.912 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 23.79ms
18-02-2016 12:28:09.706 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.60ms
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@18-02-2016 12:39:36.430 UTC INFO main.py:115: Pr$
18-02-2016 12:39:37.440 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
18-02-2016 12:39:37.649 UTC WARNING simple_httpclient.py:325: uncaught exception
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 323, in cleanup
    yield
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 177, in __init__


I'm grateful!


Att.,
Rodrigo M.
+55 (37) 9 99132 - 4539
From: Eleanor.Merry at metaswitch.com
To: moreira_r at outlook.com
CC: clearwater at lists.projectclearwater.org
Subject: RE: [Ellis Error on provisioning numbers]
Date: Thu, 18 Feb 2016 19:03:17 +0000









Hi,

 
Can you please send me the Ellis logs? These are in /var/log/ellis/ellis* on your Ellis node.
 
Thanks,
 
Ellie
 


From: Rodrigo Moreira [mailto:moreira_r at outlook.com]


Sent: 17 February 2016 22:22

To: Eleanor Merry

Cc: clearwater at lists.projectclearwater.org

Subject: [Ellis Error on provisioning numbers]


 



Hi,



I would like yours help, I have
problem in the number of 
provisioning for the new user. 
After entering Ellis service on port
80 and request 
the registration of a new user, I 
is reported the following error:



'Failed to update 
the server (see 
detailed diagnostics in 
console developer). Please
refresh the page'



Already deleted the 
records in the tables Ellis
data base.

Usually this problem 
is solved that way, 
it is reasonable to reinstall Ellis?



Since I am already 
grateful.






Att.,



Rodrigo M.

+55 (37) 9 99132 - 4539

 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160218/f00e0552/attachment.html>

From Eleanor.Merry at metaswitch.com  Fri Feb 19 09:27:57 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 19 Feb 2016 14:27:57 +0000
Subject: [Clearwater] [Ellis Error on provisioning numbers]
In-Reply-To: <BAY181-W1ED005BD2692B26356C219EAF0@phx.gbl>
References: <BAY181-W29057AE38772088035CE249EAE0@phx.gbl>,
	<BN3PR02MB12550A2CD05B116D65EDB89E9BAF0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<BAY181-W1ED005BD2692B26356C219EAF0@phx.gbl>
Message-ID: <BN3PR02MB12552DE6070505DA4FC8C4139BA00@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi,

I notice that your Homestead URL is ellis.nyc3.example.com. Is this really correct?

Also, can you please turn on debug logging on your Ellis? You can see how to do this at http://clearwater.readthedocs.org/en/latest/Troubleshooting_and_Recovery/index.html#ellis.

Thanks,

Ellie

From: Rodrigo Moreira [mailto:moreira_r at outlook.com]
Sent: 18 February 2016 20:12
To: Eleanor Merry
Cc: clearwater at lists.projectclearwater.org
Subject: RE: [Ellis Error on provisioning numbers]

Hi,

Sure.

18-02-2016 12:00:01.655 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
18-02-2016 12:00:03.048 UTC WARNING simple_httpclient.py:325: uncaught exception
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 323, in cleanup
    yield
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 177, in __init__
    0, 0)
gaierror: [Errno -2] Name or service not known
18-02-2016 12:00:03.275 UTC ERROR homestead.py:68: Failed to ping Homestead at http://ellis.nyc3.example.com:8889/ping. Have you configured your HOMESTEAD_URL?
18-02-2016 12:02:26.952 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 29.09ms
18-02-2016 12:03:12.847 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 25.56ms
18-02-2016 12:04:36.777 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.60ms
18-02-2016 12:04:57.355 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 27.60ms
18-02-2016 12:06:19.454 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 21.88ms
18-02-2016 12:07:03.050 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.97ms
18-02-2016 12:07:50.946 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.53ms
18-02-2016 12:08:50.810 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 38.71ms
18-02-2016 12:09:30.680 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.44ms
18-02-2016 12:10:33.273 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.90ms
18-02-2016 12:11:35.405 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 23.22ms
18-02-2016 12:12:36.485 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 38.60ms
18-02-2016 12:13:42.096 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 19.70ms
18-02-2016 12:14:40.905 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 22.42ms
18-02-2016 12:15:24.002 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 19.14ms
18-02-2016 12:16:46.775 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 16.85ms
18-02-2016 12:17:36.025 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.81ms
18-02-2016 12:18:24.653 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 14.56ms
18-02-2016 12:19:41.954 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 50.59ms
18-02-2016 12:20:01.664 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 34.94ms
18-02-2016 12:21:23.588 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 11.19ms
18-02-2016 12:22:26.289 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.54ms
18-02-2016 12:23:11.485 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.89ms
18-02-2016 12:24:15.538 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 27.40ms
18-02-2016 12:25:11.607 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 25.17ms
18-02-2016 12:26:03.068 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 22.01ms
18-02-2016 12:26:44.912 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 23.79ms
18-02-2016 12:28:09.706 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.60ms
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@18-02-2016 12:39:36.430 UTC INFO main.py:115: Pr$
18-02-2016 12:39:37.440 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
18-02-2016 12:39:37.649 UTC WARNING simple_httpclient.py:325: uncaught exception
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 323, in cleanup
    yield
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 177, in __init__


18-02-2016 12:00:01.655 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
18-02-2016 12:00:03.048 UTC WARNING simple_httpclient.py:325: uncaught exception
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 323, in cleanup
    yield
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 177, in __init__
    0, 0)
gaierror: [Errno -2] Name or service not known
18-02-2016 12:00:03.275 UTC ERROR homestead.py:68: Failed to ping Homestead at http://ellis.nyc3.example.com:8889/ping. Have you configured your HOMESTEAD_URL?
18-02-2016 12:02:26.952 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 29.09ms
18-02-2016 12:03:12.847 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 25.56ms
18-02-2016 12:04:36.777 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.60ms
18-02-2016 12:04:57.355 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 27.60ms
18-02-2016 12:06:19.454 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 21.88ms
18-02-2016 12:07:03.050 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.97ms
18-02-2016 12:07:50.946 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.53ms
18-02-2016 12:08:50.810 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 38.71ms
18-02-2016 12:09:30.680 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.44ms
18-02-2016 12:10:33.273 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.90ms
18-02-2016 12:11:35.405 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 23.22ms
18-02-2016 12:12:36.485 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 38.60ms
18-02-2016 12:13:42.096 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 19.70ms
18-02-2016 12:14:40.905 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 22.42ms
18-02-2016 12:15:24.002 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 19.14ms
18-02-2016 12:16:46.775 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 16.85ms
18-02-2016 12:17:36.025 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.81ms
18-02-2016 12:18:24.653 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 14.56ms
18-02-2016 12:19:41.954 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 50.59ms
18-02-2016 12:20:01.664 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 34.94ms
18-02-2016 12:21:23.588 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 11.19ms
18-02-2016 12:22:26.289 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.54ms
18-02-2016 12:23:11.485 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.89ms
18-02-2016 12:24:15.538 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 27.40ms
18-02-2016 12:25:11.607 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 25.17ms
18-02-2016 12:26:03.068 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 22.01ms
18-02-2016 12:26:44.912 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 23.79ms
18-02-2016 12:28:09.706 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.60ms
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@18-02-2016 12:39:36.430 UTC INFO main.py:115: Pr$
18-02-2016 12:39:37.440 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
18-02-2016 12:39:37.649 UTC WARNING simple_httpclient.py:325: uncaught exception
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 323, in cleanup
    yield
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 177, in __init__


I'm grateful!


Att.,
Rodrigo M.
+55 (37) 9 99132 - 4539

________________________________
From: Eleanor.Merry at metaswitch.com<mailto:Eleanor.Merry at metaswitch.com>
To: moreira_r at outlook.com<mailto:moreira_r at outlook.com>
CC: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Ellis Error on provisioning numbers]
Date: Thu, 18 Feb 2016 19:03:17 +0000
Hi,

Can you please send me the Ellis logs? These are in /var/log/ellis/ellis* on your Ellis node.

Thanks,

Ellie

From: Rodrigo Moreira [mailto:moreira_r at outlook.com]
Sent: 17 February 2016 22:22
To: Eleanor Merry
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Ellis Error on provisioning numbers]

Hi,

I would like yours help, I have problem in the number of provisioning for the new user. After entering Ellis service on port 80 and request the registration of a new user, I is reported the following error:

'Failed to update the server (see detailed diagnostics in console developer). Please refresh the page'

Already deleted the records in the tables Ellis data base.
Usually this problem is solved that way, it is reasonable to reinstall Ellis?

Since I am already grateful.


Att.,

Rodrigo M.
+55 (37) 9 99132 - 4539
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160219/34d148bc/attachment.html>

From vinayak.ram at hpe.com  Fri Feb 19 10:01:20 2016
From: vinayak.ram at hpe.com (Ram, Vinayak (NFV BU))
Date: Fri, 19 Feb 2016 15:01:20 +0000
Subject: [Clearwater] Call drop after 30 seconds
Message-ID: <DF4PR84MB0059649B9A36D637EECEE7B6FBA00@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>


Hi All,

I have a clearwater manual setup up and running and I am using zoiper SIP phone to make the calls, but the call tends to drop after 30 seconds is there a way I can extend this?

Please suggest.

Thanks,
Vinayak

Mail: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160219/2b0a8ecb/attachment.html>

From Eleanor.Merry at metaswitch.com  Fri Feb 19 12:00:32 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 19 Feb 2016 17:00:32 +0000
Subject: [Clearwater] Call disconnects after 30 seconds
In-Reply-To: <002701d16a0c$c1b80730$45281590$@xflowresearch.com>
References: <004901d16983$a523e3c0$ef6bab40$@xflowresearch.com>
	<BN3PR02MB12550A399B914CF838CC4EDF9BAE0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<002701d16a0c$c1b80730$45281590$@xflowresearch.com>
Message-ID: <BN3PR02MB12558371577382286AA5594A9BA00@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Tahir,

This looks like a problem with the ACK not being sent correctly. However, the detailed Bono logs you sent don't cover the whole call, so I can't see from them whether the caller is just not sending an ACK at all, or if Bono is rejecting the ACK.

Can you please check whether the caller is sending ACKs, and also please send debug Bono logs for the whole call?

Ellie

From: Tahir Masood [mailto:tahir.masood at xflowresearch.com]
Sent: 18 February 2016 05:25
To: Eleanor Merry; clearwater at lists.projectclearwater.org
Subject: RE: [Clearwater] Call disconnects after 30 seconds

Hi Ellie
Detailed Bono logs are also attached

Regards,

Tahir Masood

From: Tahir Masood [mailto:tahir.masood at xflowresearch.com]
Sent: Thursday, February 18, 2016 10:08 AM
To: 'Eleanor Merry'; 'clearwater at lists.projectclearwater.org'
Subject: RE: [Clearwater] Call disconnects after 30 seconds

Hi Ellie
Here are the attached sprout logs the deployment method is manual in OpenStack environment


Regards,

Tahir Masood

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Wednesday, February 17, 2016 7:32 PM
To: Tahir Masood; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Clearwater] Call disconnects after 30 seconds

Hi Tahir,

How have you set up your deployment? Is it a manual install/all-in-one install/etc..?

Also, can you please send me over the full debug logs from Sprout and Bono (the logs you've pasted in below don't have an INVITE in them)?

Thanks,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Tahir Masood
Sent: 17 February 2016 13:04
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Call disconnects after 30 seconds

Dear all,
I am facing an issue of call disconnection after 30 seconds  have used both Xlite and Zoiper but the problem persists. I have change the log_level to 5 in sprout and here are the logs. Can you please suggest me a solution

17-02-2016 12:59:42.374 UTC Debug pjsip: sip_endpoint.c Processing incoming mess                     age: Request msg OPTIONS/cseq=97398 (rdata0x7ff14c06fb50)
17-02-2016 12:59:42.374 UTC Verbose common_sip_processing.cpp:120: RX 342 bytes                      Request msg OPTIONS/cseq=97398 (rdata0x7ff14c06fb50) from TCP 192.168.0.6:50982:
--start msg--

OPTIONS sip:poll-sip at 192.168.0.6:5054 SIP/2.0
Via: SIP/2.0/TCP 192.168.0.6;rport;branch=z9hG4bK-97398
Max-Forwards: 2
To: <sip:poll-sip at 192.168.0.6:5054>
From: poll-sip <sip:poll-sip at 192.168.0.6>;tag=97398
Call-ID: poll-sip-97398
CSeq: 97398 OPTIONS
Contact: <sip:192.168.0.6>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:167: home domain: false, lo                     cal_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:197: Classified URI as 3
17-02-2016 12:59:42.374 UTC Debug common_sip_processing.cpp:212: Skipping SAS lo                     gging for OPTIONS request
17-02-2016 12:59:42.374 UTC Debug thread_dispatcher.cpp:253: Queuing cloned rece                     ived message 0x7ff14c01d7c8 for worker threads
17-02-2016 12:59:42.374 UTC Debug thread_dispatcher.cpp:149: Worker thread deque                     ue message 0x7ff14c01d7c8
17-02-2016 12:59:42.374 UTC Debug pjsip: sip_endpoint.c Distributing rdata to mo                     dules: Request msg OPTIONS/cseq=97398 (rdata0x7ff14c01d7c8)
17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:167: home domain: false, lo                     cal_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:42.374 UTC Debug uri_classifier.cpp:197: Classified URI as 3
17-02-2016 12:59:42.374 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS                     /cseq=97398 (tdta0x7ff13c408040) created
17-02-2016 12:59:42.374 UTC Verbose common_sip_processing.cpp:136: TX 273 bytes                      Response msg 200/OPTIONS/cseq=97398 (tdta0x7ff13c408040) to TCP 192.168.0.6:5098                     2:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 192.168.0.6;rport=50982;received=192.168.0.6;branch=z9hG4bK-973                     98
Call-ID: poll-sip-97398
From: "poll-sip" <sip:poll-sip at 192.168.0.6>;tag=97398
To: <sip:poll-sip at 192.168.0.6>;tag=z9hG4bK-97398
CSeq: 97398 OPTIONS
Content-Length:  0


--end msg--
17-02-2016 12:59:42.374 UTC Debug common_sip_processing.cpp:254: Skipping SAS lo                     gging for OPTIONS response
17-02-2016 12:59:42.374 UTC Debug pjsip: tdta0x7ff13c40 Destroying txdata Respon                     se msg 200/OPTIONS/cseq=97398 (tdta0x7ff13c408040)
17-02-2016 12:59:42.375 UTC Debug thread_dispatcher.cpp:193: Worker thread compl                     eted processing message 0x7ff14c01d7c8
17-02-2016 12:59:42.375 UTC Debug thread_dispatcher.cpp:199: Request latency = 6                     61us
17-02-2016 12:59:42.385 UTC Verbose httpstack.cpp:286: Process request for URL /                     ping, args (null)
17-02-2016 12:59:42.385 UTC Verbose httpstack.cpp:69: Sending response 200 to re                     quest for URL /ping, args (null)
17-02-2016 12:59:43.375 UTC Verbose pjsip: tcps0x7ff14c06 TCP connection closed
17-02-2016 12:59:43.375 UTC Debug connection_tracker.cpp:91: Connection 0x7ff14c                     06f818 has been destroyed
17-02-2016 12:59:43.376 UTC Verbose pjsip: tcps0x7ff14c06 TCP transport destroye                     d with reason 70016: End of file (PJ_EEOF)
17-02-2016 12:59:43.519 UTC Verbose pjsip:    tcplis:5054 TCP listener 192.168.0                     .6:5054: got incoming TCP connection from 192.168.0.4:34463, sock=1055
17-02-2016 12:59:43.519 UTC Verbose pjsip: tcps0x7ff14c06 TCP server transport c                     reated
17-02-2016 12:59:43.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP connection closed
17-02-2016 12:59:43.520 UTC Debug connection_tracker.cpp:91: Connection 0x7ff14c                     114c98 has been destroyed
17-02-2016 12:59:43.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP transport destroye                     d with reason 70016: End of file (PJ_EEOF)
17-02-2016 12:59:44.483 UTC Debug pjsip: sip_endpoint.c Processing incoming mess                     age: Request msg REGISTER/cseq=30 (rdata0x7ff14c04bf60)
17-02-2016 12:59:44.483 UTC Verbose common_sip_processing.cpp:120: RX 1242 bytes                      Request msg REGISTER/cseq=30 (rdata0x7ff14c04bf60) from TCP 192.168.0.4:53691:
--start msg--

REGISTER sip:dellnfv.com;transport=UDP SIP/2.0
Via: SIP/2.0/TCP 192.168.0.4:53691;rport;branch=z9hG4bKPjcGZ7eF5y6OvGqDehOqxpk4M                     EbqS4Ag4l
Path: <sip:rP6Yw8PkfR at 192.168.0.4:5058;transport=TCP;lr;ob>
Via: SIP/2.0/UDP 192.168.1.8:64835;rport=64835;received=192.168.1.8;branch=z9hG4                     bK-524287-1---1c645317b5676c22
Max-Forwards: 70
Contact: <sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68                     ac1<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68????????????????????%20ac1>>
To: <sip:2010000007 at dellnfv.com>
From: <sip:2010000007 at dellnfv.com>;tag=698b4951
Call-ID: Pp7e4umpNXHLsITvW6y7RA..
CSeq: 30 REGISTER
Expires: 60
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIB                     E
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-                     serviceuri
User-Agent: Zoiper r35079
Authorization: Digest response="49157d3d9e710e40b06a6d4a6c9af137", username="201                     0000007 at dellnfv.com<mailto:0000007 at dellnfv.com>", realm="dellnfv.com", nonce="0d8f08d16e444374", uri="sip:de                     llnfv.com;transport=UDP<sip:de????????????????????%20llnfv.com;transport=UDP>", algorithm=MD5, cnonce="450410c6c32d2ff38597f3468504591                     d", opaque="2975ef5f2182abff", qop=auth, nc=0000001d,integrity-protected=ip-asso                     c-yes
Allow-Events: presence, kpml
P-Visited-Network-ID: dellnfv.com
Route: <sip:sprout.dellnfv.com:5054;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
17-02-2016 12:59:44.484 UTC Debug pjutils.cpp:1648: Logging SAS Call-ID marker,                      Call-ID Pp7e4umpNXHLsITvW6y7RA..
17-02-2016 12:59:44.484 UTC Debug thread_dispatcher.cpp:253: Queuing cloned rece                     ived message 0x7ff14c114d18 for worker threads
17-02-2016 12:59:44.484 UTC Debug thread_dispatcher.cpp:149: Worker thread deque                     ue message 0x7ff14c114d18
17-02-2016 12:59:44.484 UTC Debug pjsip: sip_endpoint.c Distributing rdata to mo                     dules: Request msg REGISTER/cseq=30 (rdata0x7ff14c114d18)
17-02-2016 12:59:44.484 UTC Debug uri_classifier.cpp:167: home domain: true, loc                     al_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as 4
17-02-2016 12:59:44.485 UTC Debug authentication.cpp:673: Authentication module                      invoked
17-02-2016 12:59:44.485 UTC Debug authentication.cpp:581: Authorization header i                     n request
17-02-2016 12:59:44.485 UTC Info authentication.cpp:595: SIP Digest authenticate                     d request integrity protected by edge proxy
17-02-2016 12:59:44.485 UTC Debug authentication.cpp:683: Request does not need                      authentication
17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:167: home domain: true, loc                     al_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as 4
17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:167: home domain: false, lo                     cal_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:44.485 UTC Debug uri_classifier.cpp:197: Classified URI as 3
17-02-2016 12:59:44.485 UTC Debug acr.cpp:1763: Create RalfACR for node type S-C                     SCF with role Originating
17-02-2016 12:59:44.485 UTC Debug acr.cpp:49: Created ACR (0x7ff144148500)
17-02-2016 12:59:44.485 UTC Debug acr.cpp:175: Created S-CSCF Ralf ACR
17-02-2016 12:59:44.485 UTC Debug acr.cpp:214: Set record type for P/S-CSCF
17-02-2016 12:59:44.485 UTC Debug acr.cpp:222: Non-dialog message => EVENT_RECOR                     D
17-02-2016 12:59:44.485 UTC Debug acr.cpp:1491: Stored 0 subscription identifier                     s
17-02-2016 12:59:44.485 UTC Debug registrar.cpp:541: Process REGISTER for public                      ID sip:2010000007 at dellnfv.com
17-02-2016 12:59:44.485 UTC Debug registrar.cpp:549: Report SAS start marker - t                     rail (1e3)
17-02-2016 12:59:44.485 UTC Debug hssconnection.cpp:585: Making Homestead reques                     t for /impu/sip%3A2010000007%40dellnfv.com/reg-data?private_id=2010000007%40dell                     nfv.com
17-02-2016 12:59:44.485 UTC Debug httpresolver.cpp:71: HttpResolver::resolve for                      host hs.dellnfv.com, port 8888, family 2
17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:513: Attempt to parse hs.dell                     nfv.com as IP address
17-02-2016 12:59:44.485 UTC Debug dnscachedresolver.cpp:667: Removing record for                      hs.dellnfv.com (type 1, expiry time 1455713976) from the expiry list
17-02-2016 12:59:44.485 UTC Verbose dnscachedresolver.cpp:240: Check cache for h                     s.dellnfv.com type 1
17-02-2016 12:59:44.485 UTC Debug dnscachedresolver.cpp:326: Pulling 1 records f                     rom cache for hs.dellnfv.com A
17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:361: Found 1 A/AAAA records,                      randomizing
17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:503: 192.168.0.8:8888 transpo                     rt 6 is not blacklisted
17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:382: Added a server, now have                      1 of 5
17-02-2016 12:59:44.485 UTC Debug baseresolver.cpp:420: Adding 0 servers from bl                     acklist
17-02-2016 12:59:44.485 UTC Debug httpconnection.cpp:623: Sending HTTP request :                      http://hs.dellnfv.com:8888/impu/sip%3A2010000007%40dellnfv.com/reg-data?private                     _id=2010000007%40dellnfv.com (trying 192.168.0.8) on new connection
17-02-2016 12:59:44.494 UTC Debug httpconnection.cpp:915: Received header http/1                     .1200ok with value
17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header conten                     t-length with value 869
17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header conten                     t-type with value text/plain
17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:915: Received header  with                      value
17-02-2016 12:59:44.495 UTC Debug httpconnection.cpp:638: Received HTTP response                     : status=200, doc=<ClearwaterRegData>
        <RegistrationState>REGISTERED</RegistrationState>
        <IMSSubscription xsi="http://www.w3.org/2001/XMLSchema-instance" noNames                     paceSchemaLocation="CxDataType.xsd">
                <PrivateID>2010000007 at dellnfv.com</PrivateID<mailto:2010000007 at dellnfv.com%3c/PrivateID>>
                <ServiceProfile>
                        <InitialFilterCriteria>
                                <TriggerPoint>
                                        <ConditionTypeCNF>0</ConditionTypeCNF>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNe                     gated>
                                                <Group>0</Group>
                                                <Method>INVITE</Method>
                                                <Extension/>
                                        </SPT>
                                </TriggerPoint>
                                <ApplicationServer>
                                        <ServerName>sip:mmtel.dellnfv.com</Serve<sip:mmtel.dellnfv.com%3c/Serve>                     rName>
                                        <DefaultHandling>0</DefaultHandling>
                                </ApplicationServer>
                        </InitialFilterCriteria>
                        <PublicIdentity>
                                <BarringIndication>1</BarringIndication>
                                <Identity>sip:2010000007 at dellnfv.com</Identity<sip:2010000007 at dellnfv.com%3c/Identity>>
                        </PublicIdentity>
                </ServiceProfile>
        </IMSSubscription>
</ClearwaterRegData>


17-02-2016 12:59:44.495 UTC Debug communicationmonitor.cpp:82: Checking communic                     ation changes - successful attempts 1, failures 0
17-02-2016 12:59:44.495 UTC Debug hssconnection.cpp:366: Processing Identity nod                     e from HSS XML - sip:2010000007 at dellnfv.com

17-02-2016 12:59:44.495 UTC Debug registrar.cpp:651: REGISTER for public ID sip:                     2010000007 at dellnfv.com<mailto:2010000007 at dellnfv.com> uses AOR sip:2010000007 at dellnfv.com
17-02-2016 12:59:44.495 UTC Debug subscriber_data_manager.cpp:366: Get AoR data                      for sip:2010000007 at dellnfv.com
17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:195: Key reg\\sip:201000000                     7 at dellnfv.com<mailto:7 at dellnfv.com> hashes to vbucket 19 via hash 0x9f135593
17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:367: 1 read replicas for ke                     y reg\\sip:2010000007 at dellnfv.com
17-02-2016 12:59:44.495 UTC Debug memcachedstore.cpp:402: Attempt to read from r                     eplica 0 (connection 0x7ff1440ae680)
17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:780: Fetch result
17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:788: Found record on replic                     a
17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:410: Read for reg\\sip:2010                     000007 at dellnfv.com<mailto:000007 at dellnfv.com> on replica 0 returned SUCCESS
17-02-2016 12:59:44.496 UTC Debug memcachedstore.cpp:453: Read 469 bytes from ta                     ble reg key sip:2010000007 at dellnfv.com, CAS = 1537
17-02-2016 12:59:44.496 UTC Debug communicationmonitor.cpp:82: Checking communic                     ation changes - successful attempts 4, failures 0
17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:376: Data store re                     turned a record, CAS = 1537
17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:477: Try to deseri                     alize record for sip:2010000007 at dellnfv.com with 'JSON' deserializer
17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:969: Deserialize J                     SON document: {"bindings":{"sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinst                     ance=907814e95ea68ac1<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinst????????????????????%20ance=907814e95ea68ac1>":{"uri":"sip:2010000007 at 192.168.1.8:64835;transport=UDP;ri                     nstance=907814e95ea68ac1<sip:2010000007 at 192.168.1.8:64835;transport=UDP;ri????????????????????%20nstance=907814e95ea68ac1>","cid":"Pp7e4umpNXHLsITvW6y7RA..","cseq":29,"expires":1                     455713990,"priority":0,"params":{},"paths":["sip:rP6Yw8PkfR at 192.168.0.4:5058;tra                     nsport=TCP;lr;ob<sip:rP6Yw8PkfR at 192.168.0.4:5058;tra????????????????????%20nsport=TCP;lr;ob>"],"timer_id":"016dc709400001210040001000104104","private_id":"2                     010000007 at dellnfv.com","emergency_reg":false}},"subscriptions":{},"notify_cseq<mailto:010000007 at dellnfv.com%22,%22emergency_reg%22:false%7d%7d,%22subscriptions%22:%7b%7d,%22notify_cseq>":                     29}
17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:994:   Binding: si                     p:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68ac1
17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:482: Deserializati                     on suceeded
17-02-2016 12:59:44.496 UTC Debug registrar.cpp:249: Retrieved AoR data 0x7ff144                     23a0a0
17-02-2016 12:59:44.496 UTC Debug registrar.cpp:342: Binding identifier for cont                     act = sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68ac1
17-02-2016 12:59:44.496 UTC Debug registrar.cpp:369: Path header sip:rP6Yw8PkfR@                     192.168.0.4:5058;transport=TCP;lr;ob
17-02-2016 12:59:44.496 UTC Debug subscriber_data_manager.cpp:196: Set AoR data                      for sip:2010000007 at dellnfv.com, CAS=1537, expiry = 1455714054
17-02-2016 12:59:44.496 UTC Debug httpresolver.cpp:71: HttpResolver::resolve for                      host 127.0.0.1, port 7253, family 2
17-02-2016 12:59:44.496 UTC Debug baseresolver.cpp:513: Attempt to parse 127.0.0                     .1 as IP address
17-02-2016 12:59:44.496 UTC Debug httpresolver.cpp:79: Target is an IP address
17-02-2016 12:59:44.496 UTC Debug httpconnection.cpp:623: Sending HTTP request :                      http://127.0.0.1:7253/timers/016dc709400001210040001000104104 (trying 127.0.0.1                     ) on new connection
17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header http/1                     .1200ok with value
17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header locati                     on with value /timers/016dc709400001210040001000104104
17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header conten                     t-length with value 0
17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:915: Received header  with                      value
17-02-2016 12:59:44.498 UTC Debug httpconnection.cpp:638: Received HTTP response                     : status=200, doc=
17-02-2016 12:59:44.498 UTC Debug communicationmonitor.cpp:82: Checking communic                     ation changes - successful attempts 1, failures 0
17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:542: Writing 469 bytes to t                     able reg key sip:2010000007 at dellnfv.com, CAS = 1537, expiry = 70
17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:195: Key reg\\sip:201000000                     7 at dellnfv.com<mailto:7 at dellnfv.com> hashes to vbucket 19 via hash 0x9f135593
17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:562: 1 write replicas for k                     ey reg\\sip:2010000007 at dellnfv.com
17-02-2016 12:59:44.498 UTC Debug memcachedstore.cpp:616: Attempt conditional wr                     ite to vbucket 19 on replica 0 (connection 0x7ff1440ae680), CAS = 1537, expiry =                      70
17-02-2016 12:59:44.499 UTC Debug memcachedstore.cpp:657: Conditional write succ                     eeded to replica 0
17-02-2016 12:59:44.499 UTC Debug subscriber_data_manager.cpp:438: Data store se                     t_data returned 1
17-02-2016 12:59:44.499 UTC Debug registrar.cpp:116: Bindings for sip:2010000007                     @dellnfv.com
17-02-2016 12:59:44.499 UTC Debug registrar.cpp:130:   sip:2010000007 at 192.168.1.                     8:64835;transport=UDP;rinstance=907814e95ea68ac1 URI=sip:2010000007 at 192.168.1.8:                     64835;transport=UDP;rinstance=907814e95ea68ac1 expires=1455714044 q=0 from=Pp7e4                     umpNXHLsITvW6y7RA.. cseq=30 timer=016dc709400001210040001000104104 private_id=20                     10000007 at dellnfv.com<mailto:10000007 at dellnfv.com> emergency_registration=false
17-02-2016 12:59:44.499 UTC Debug pjsip:       endpoint Response msg 200/REGISTE                     R/cseq=30 (tdta0x7ff14423a790) created
17-02-2016 12:59:44.499 UTC Debug acr.cpp:1550: Store associated URIs
17-02-2016 12:59:44.499 UTC Verbose common_sip_processing.cpp:136: TX 765 bytes                      Response msg 200/REGISTER/cseq=30 (tdta0x7ff14423a790) to TCP 192.168.0.4:53691:
--start msg--

SIP/2.0 200 OK
Service-Route: <sip:sprout.dellnfv.com:5054;transport=TCP;lr;orig>
Via: SIP/2.0/TCP 192.168.0.4:53691;rport=53691;received=192.168.0.4;branch=z9hG4                     bKPjcGZ7eF5y6OvGqDehOqxpk4MEbqS4Ag4l
Via: SIP/2.0/UDP 192.168.1.8:64835;rport=64835;received=192.168.1.8;branch=z9hG4                     bK-524287-1---1c645317b5676c22
Call-ID: Pp7e4umpNXHLsITvW6y7RA..
From: <sip:2010000007 at dellnfv.com>;tag=698b4951
To: <sip:2010000007 at dellnfv.com>;tag=z9hG4bKPjcGZ7eF5y6OvGqDehOqxpk4MEbqS4Ag4l
CSeq: 30 REGISTER
Supported: outbound
Contact: <sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68                     ac1<sip:2010000007 at 192.168.1.8:64835;transport=UDP;rinstance=907814e95ea68????????????????????%20ac1>>;expires=60
Require: outbound
Path: <sip:rP6Yw8PkfR at 192.168.0.4:5058;transport=TCP;lr;ob>
P-Associated-URI: <sip:2010000007 at dellnfv.com>
Content-Length:  0


--end msg--
17-02-2016 12:59:44.499 UTC Info acr.cpp:658: No CCF or ECF to send ACR for sess                     ion Pp7e4umpNXHLsITvW6y7RA.. to - dropping!
17-02-2016 12:59:44.499 UTC Debug acr.cpp:54: Destroyed ACR (0x7ff144148500)
17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:763: Interpreting orig IFC info                     rmation
17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:437: SPT class Method: result f                     alse
17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:541: Add to group 0 val false
17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:559: Result group 0 val false
17-02-2016 12:59:44.499 UTC Debug ifchandler.cpp:572: iFC does not match
17-02-2016 12:59:44.499 UTC Info registration_utils.cpp:187: Found 0 Application                      Servers
17-02-2016 12:59:44.499 UTC Debug pjsip: tdta0x7ff14423 Destroying txdata Respon                     se msg 200/REGISTER/cseq=30 (tdta0x7ff14423a790)
17-02-2016 12:59:44.499 UTC Debug registrar.cpp:1036: Report SAS end marker - tr                     ail (1e3)
17-02-2016 12:59:44.499 UTC Debug thread_dispatcher.cpp:193: Worker thread compl                     eted processing message 0x7ff14c114d18
17-02-2016 12:59:44.499 UTC Debug thread_dispatcher.cpp:199: Request latency = 1                     5474us
17-02-2016 12:59:46.381 UTC Verbose pjsip: tcps0x7ff14c15 TCP transport destroye                     d normally
17-02-2016 12:59:46.520 UTC Verbose pjsip:    tcplis:5054 TCP listener 192.168.0                     .6:5054: got incoming TCP connection from 192.168.0.4:35812, sock=408
17-02-2016 12:59:46.520 UTC Verbose pjsip: tcps0x7ff14c11 TCP server transport c                     reated
17-02-2016 12:59:47.520 UTC Verbose pjsip:    tcplis:5054 TCP listener 192.168.0                     .6:5054: got incoming TCP connection from 192.168.0.4:41045, sock=938
17-02-2016 12:59:47.520 UTC Verbose pjsip: tcps0x7ff14c15 TCP server transport c                     reated
17-02-2016 12:59:47.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP connection closed
17-02-2016 12:59:47.521 UTC Debug connection_tracker.cpp:91: Connection 0x7ff14c                     05d228 has been destroyed
17-02-2016 12:59:47.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP transport destroye                     d with reason 70016: End of file (PJ_EEOF)
17-02-2016 12:59:49.521 UTC Verbose pjsip:    tcplis:5054 TCP listener 192.168.0                     .6:5054: got incoming TCP connection from 192.168.0.4:51223, sock=429
17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c05 TCP server transport c                     reated
17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c07 TCP connection closed
17-02-2016 12:59:49.521 UTC Verbose pjsip: tcps0x7ff14c07 TCP transport destroye                     d with reason 70016: End of file (PJ_EEOF)
17-02-2016 12:59:52.359 UTC Verbose pjsip:    tcplis:5054 TCP listener 192.168.0                     .6:5054: got incoming TCP connection from 192.168.0.6:51013, sock=970
17-02-2016 12:59:52.359 UTC Verbose pjsip: tcps0x7ff14c07 TCP server transport c                     reated
17-02-2016 12:59:52.359 UTC Debug pjsip: sip_endpoint.c Processing incoming mess                     age: Request msg OPTIONS/cseq=97408 (rdata0x7ff14c0747e0)
17-02-2016 12:59:52.359 UTC Verbose common_sip_processing.cpp:120: RX 342 bytes                      Request msg OPTIONS/cseq=97408 (rdata0x7ff14c0747e0) from TCP 192.168.0.6:51013:
--start msg--

OPTIONS sip:poll-sip at 192.168.0.6:5054 SIP/2.0
Via: SIP/2.0/TCP 192.168.0.6;rport;branch=z9hG4bK-97408
Max-Forwards: 2
To: <sip:poll-sip at 192.168.0.6:5054>
From: poll-sip <sip:poll-sip at 192.168.0.6>;tag=97408
Call-ID: poll-sip-97408
CSeq: 97408 OPTIONS
Contact: <sip:192.168.0.6>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-02-2016 12:59:52.359 UTC Debug uri_classifier.cpp:167: home domain: false, lo                     cal_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:52.359 UTC Debug uri_classifier.cpp:197: Classified URI as 3
17-02-2016 12:59:52.359 UTC Debug common_sip_processing.cpp:212: Skipping SAS lo                     gging for OPTIONS request
17-02-2016 12:59:52.359 UTC Debug thread_dispatcher.cpp:253: Queuing cloned rece                     ived message 0x7ff14c01d7c8 for worker threads
17-02-2016 12:59:52.359 UTC Debug thread_dispatcher.cpp:149: Worker thread deque                     ue message 0x7ff14c01d7c8
17-02-2016 12:59:52.360 UTC Debug pjsip: sip_endpoint.c Distributing rdata to mo                     dules: Request msg OPTIONS/cseq=97408 (rdata0x7ff14c01d7c8)
17-02-2016 12:59:52.360 UTC Debug uri_classifier.cpp:167: home domain: false, lo                     cal_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true,                      treat_number_as_phone: false
17-02-2016 12:59:52.360 UTC Debug uri_classifier.cpp:197: Classified URI as 3
17-02-2016 12:59:52.360 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS                     /cseq=97408 (tdta0x7ff14c298780) created
17-02-2016 12:59:52.360 UTC Verbose common_sip_processing.cpp:136: TX 273 bytes                      Response msg 200/OPTIONS/cseq=97408 (tdta0x7ff14c298780) to TCP 192.168.0.6:5101                     3:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 192.168.0.6;rport=51013;received=192.168.0.6;branch=z9hG4bK-974                     08
Call-ID: poll-sip-97408
From: "poll-sip" <sip:poll-sip at 192.168.0.6>;tag=97408
To: <sip:poll-sip at 192.168.0.6>;tag=z9hG4bK-97408
CSeq: 97408 OPTIONS
Content-Length:  0


--end msg--
17-02-2016 12:59:52.360 UTC Debug common_sip_processing.cpp:254: Skipping SAS lo                     gging for OPTIONS response
17-02-2016 12:59:52.360 UTC Debug pjsip: tdta0x7ff14c29 Destroying txdata Respon                     se msg 200/OPTIONS/cseq=97408 (tdta0x7ff14c298780)
17-02-2016 12:59:52.360 UTC Debug thread_dispatcher.cpp:193: Worker thread compl                     eted processing message 0x7ff14c01d7c8
17-02-2016 12:59:52.360 UTC Debug thread_dispatcher.cpp:199: Request latency = 2                     42us
17-02-2016 12:59:52.360 UTC Info load_monitor.cpp:212: Accepted 100.000000% of r                     equests, latency error = -0.953790, overload responses = 0
17-02-2016 12:59:52.360 UTC Status load_monitor.cpp:260: Maximum incoming reques                     t rate/second unchanged - only handled 20 requests in last 73256ms, minimum thre                     shold for a change is 18314.000000
17-02-2016 12:59:52.360 UTC Debug snmp_continuous_accumulator_table.cpp:108: Acc                     umulating sample 500ui into continuous accumulator statistic
17-02-2016 12:59:52.360 UTC Debug snmp_continuous_accumulator_table.cpp:108: Acc                     umulating sample 500ui into continuous accumulator statistic
17-02-2016 12:59:52.361 UTC Verbose httpstack.cpp:286: Process request for URL /                     ping, args (null)
17-02-2016 12:59:52.361 UTC Verbose httpstack.cpp:69: Sending response 200 to re                     quest for URL /ping, args (null)
17-02-2016 12:59:53.362 UTC Verbose pjsip: tcps0x7ff14c07 TCP connection closed
17-02-2016 12:59:53.362 UTC Debug connection_tracker.cpp:91: Connection 0x7ff14c                     0744a8 has been destroyed
17-02-2016 12:59:53.362 UTC Verbose pjsip: tcps0x7ff14c07 TCP transport destroye                     d with reason 70016: End of file (PJ_EEOF)
17-02-2016 12:59:53.522 UTC Verbose pjsip:    tcplis:5054 TCP listener 192.168.0                     .6:5054: got incoming TCP connection from 192.168.0.4:49292, sock=970
17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c07 TCP server transport c                     reated
17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c08 TCP connection closed
17-02-2016 12:59:53.522 UTC Debug connection_tracker.cpp:91: Connection 0x7ff14c                     085a08 has been destroyed
17-02-2016 12:59:53.522 UTC Verbose pjsip: tcps0x7ff14c08 TCP transport destroye                     d with reason 70016: End of file (PJ_EEOF)


Regards,

Tahir Masood

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160219/1be2ff37/attachment.html>

From Eleanor.Merry at metaswitch.com  Fri Feb 19 12:03:31 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 19 Feb 2016 17:03:31 +0000
Subject: [Clearwater] Call drop after 30 seconds
In-Reply-To: <DF4PR84MB0059649B9A36D637EECEE7B6FBA00@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>
References: <DF4PR84MB0059649B9A36D637EECEE7B6FBA00@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>
Message-ID: <BN3PR02MB12559BB9D4C93481982C6E3C9BA00@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Vinayak,

Calls dropping after 30 seconds probably indicates a problem with the ACK not being sent correctly for the INVITE.

Can you please send me the debug logs from your Sprout and Bono nodes for a call? You can turn on debug logging by creating/editing the file /etc/clearwater/user_settings, add log_level=5 and then restart Sprout/Bono (service <sprout/bono> stop - they're automatically restarted by monit). The logs are output in /var/log/sprout/ and /var/log/bono/.

Ellie


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Ram, Vinayak (NFV BU)
Sent: 19 February 2016 15:01
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] Call drop after 30 seconds


Hi All,

I have a clearwater manual setup up and running and I am using zoiper SIP phone to make the calls, but the call tends to drop after 30 seconds is there a way I can extend this?

Please suggest.

Thanks,
Vinayak

Mail: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160219/2824668d/attachment.html>

From moreira_r at outlook.com  Fri Feb 19 15:04:11 2016
From: moreira_r at outlook.com (Rodrigo Moreira)
Date: Fri, 19 Feb 2016 18:04:11 -0200
Subject: [Clearwater] RES: [Ellis Error on provisioning numbers]
Message-ID: <BAY402-EAS18593668B04A552FDF4744A9EA00@phx.gbl>

Hi,

Just missing a setting in the ?shared_config? file where I reversed the machines, where it was homestead put ellis. I appreciate you promptness.

Thanks.

Att.,
Rodrigo M.
37 - 9 9132-4539
Skype: rodrigo.moreira2007



Sent from Mail for Windows 10

De: Eleanor Merry<mailto:Eleanor.Merry at metaswitch.com>
Enviado:sexta-feira, 19 de fevereiro de 2016 12:28
Para: Rodrigo Moreira<mailto:moreira_r at outlook.com>
Cc:clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Assunto: RE: [Ellis Error on provisioning numbers]

Hi,

I notice that your Homestead URL is ellis.nyc3.example.com. Is this really correct?

Also, can you please turn on debug logging on your Ellis? You can see how to do this at http://clearwater.readthedocs.org/en/latest/Troubleshooting_and_Recovery/index.html#ellis.

Thanks,

Ellie

From: Rodrigo Moreira [mailto:moreira_r at outlook.com]
Sent: 18 February 2016 20:12
To: Eleanor Merry
Cc: clearwater at lists.projectclearwater.org
Subject: RE: [Ellis Error on provisioning numbers]

Hi,

Sure.

18-02-2016 12:00:01.655 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
18-02-2016 12:00:03.048 UTC WARNING simple_httpclient.py:325: uncaught exception
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 323, in cleanup
    yield
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 177, in __init__
    0, 0)
gaierror: [Errno -2] Name or service not known
18-02-2016 12:00:03.275 UTC ERROR homestead.py:68: Failed to ping Homestead at http://ellis.nyc3.example.com:8889/ping. Have you configured your HOMESTEAD_URL?
18-02-2016 12:02:26.952 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 29.09ms
18-02-2016 12:03:12.847 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 25.56ms
18-02-2016 12:04:36.777 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.60ms
18-02-2016 12:04:57.355 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 27.60ms
18-02-2016 12:06:19.454 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 21.88ms
18-02-2016 12:07:03.050 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.97ms
18-02-2016 12:07:50.946 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.53ms
18-02-2016 12:08:50.810 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 38.71ms
18-02-2016 12:09:30.680 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.44ms
18-02-2016 12:10:33.273 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.90ms
18-02-2016 12:11:35.405 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 23.22ms
18-02-2016 12:12:36.485 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 38.60ms
18-02-2016 12:13:42.096 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 19.70ms
18-02-2016 12:14:40.905 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 22.42ms
18-02-2016 12:15:24.002 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 19.14ms
18-02-2016 12:16:46.775 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 16.85ms
18-02-2016 12:17:36.025 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.81ms
18-02-2016 12:18:24.653 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 14.56ms
18-02-2016 12:19:41.954 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 50.59ms
18-02-2016 12:20:01.664 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 34.94ms
18-02-2016 12:21:23.588 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 11.19ms
18-02-2016 12:22:26.289 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.54ms
18-02-2016 12:23:11.485 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.89ms
18-02-2016 12:24:15.538 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 27.40ms
18-02-2016 12:25:11.607 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 25.17ms
18-02-2016 12:26:03.068 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 22.01ms
18-02-2016 12:26:44.912 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 23.79ms
18-02-2016 12:28:09.706 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.60ms
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@18-02-2016 12:39:36.430 UTC INFO main.py:115: Pr$
18-02-2016 12:39:37.440 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
18-02-2016 12:39:37.649 UTC WARNING simple_httpclient.py:325: uncaught exception
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 323, in cleanup
    yield
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 177, in __init__


18-02-2016 12:00:01.655 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
18-02-2016 12:00:03.048 UTC WARNING simple_httpclient.py:325: uncaught exception
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 323, in cleanup
    yield
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 177, in __init__
    0, 0)
gaierror: [Errno -2] Name or service not known
18-02-2016 12:00:03.275 UTC ERROR homestead.py:68: Failed to ping Homestead at http://ellis.nyc3.example.com:8889/ping. Have you configured your HOMESTEAD_URL?
18-02-2016 12:02:26.952 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 29.09ms
18-02-2016 12:03:12.847 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 25.56ms
18-02-2016 12:04:36.777 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.60ms
18-02-2016 12:04:57.355 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 27.60ms
18-02-2016 12:06:19.454 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 21.88ms
18-02-2016 12:07:03.050 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.97ms
18-02-2016 12:07:50.946 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.53ms
18-02-2016 12:08:50.810 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 38.71ms
18-02-2016 12:09:30.680 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.44ms
18-02-2016 12:10:33.273 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.90ms
18-02-2016 12:11:35.405 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 23.22ms
18-02-2016 12:12:36.485 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 38.60ms
18-02-2016 12:13:42.096 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 19.70ms
18-02-2016 12:14:40.905 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 22.42ms
18-02-2016 12:15:24.002 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 19.14ms
18-02-2016 12:16:46.775 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 16.85ms
18-02-2016 12:17:36.025 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.81ms
18-02-2016 12:18:24.653 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 14.56ms
18-02-2016 12:19:41.954 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 50.59ms
18-02-2016 12:20:01.664 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 34.94ms
18-02-2016 12:21:23.588 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 11.19ms
18-02-2016 12:22:26.289 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.54ms
18-02-2016 12:23:11.485 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 18.89ms
18-02-2016 12:24:15.538 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 27.40ms
18-02-2016 12:25:11.607 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 25.17ms
18-02-2016 12:26:03.068 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 22.01ms
18-02-2016 12:26:44.912 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 23.79ms
18-02-2016 12:28:09.706 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 24.60ms
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@18-02-2016 12:39:36.430 UTC INFO main.py:115: Pr$
18-02-2016 12:39:37.440 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
18-02-2016 12:39:37.649 UTC WARNING simple_httpclient.py:325: uncaught exception
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 323, in cleanup
    yield
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/simple_httpclient.py", line 177, in __init__


I'm grateful!


Att.,
Rodrigo M.
+55 (37) 9 99132 - 4539

________________________________
From: Eleanor.Merry at metaswitch.com<mailto:Eleanor.Merry at metaswitch.com>
To: moreira_r at outlook.com<mailto:moreira_r at outlook.com>
CC: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Ellis Error on provisioning numbers]
Date: Thu, 18 Feb 2016 19:03:17 +0000
Hi,

Can you please send me the Ellis logs? These are in /var/log/ellis/ellis* on your Ellis node.

Thanks,

Ellie

From: Rodrigo Moreira [mailto:moreira_r at outlook.com]
Sent: 17 February 2016 22:22
To: Eleanor Merry
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Ellis Error on provisioning numbers]

Hi,

I would like yours help, I have problem in the number of provisioning for the new user. After entering Ellis service on port 80 and request the registration of a new user, I is reported the following error:

'Failed to update the server (see detailed diagnostics in console developer). Please refresh the page'

Already deleted the records in the tables Ellis data base.
Usually this problem is solved that way, it is reasonable to reinstall Ellis?

Since I am already grateful.


Att.,

Rodrigo M.
+55 (37) 9 99132 - 4539
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160219/2cb4b80b/attachment.html>

From moreira_r at outlook.com  Sat Feb 20 14:47:44 2016
From: moreira_r at outlook.com (Rodrigo Moreira)
Date: Sat, 20 Feb 2016 17:47:44 -0200
Subject: [Clearwater] [Error when i try to upload shared_config]
Message-ID: <BAY181-W555F40C53B4376D7CC39199EA10@phx.gbl>

Hi, 

   Could you please tell what happens?
 Follows logs:

BONO:


I can ping sprout.nyc3.example.com on the host bono.nyc3.example.com

Log files:


20-02-2016 13:57:21.194 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Status bono.cpp:3316: Create list of PBXes
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:148: Finished loading plug-ins
20-02-2016 13:57:21.197 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:36.207 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:51.214 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:06.228 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:21.235 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:36.249 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:51.263 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:06.277 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:21.279 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:36.294 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:51.306 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)


SPROUT:

Log files:

20-02-2016 19:19:04.990 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:14.962 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:25.086 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:35.096 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:45.181 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:55.167 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:05.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:15.221 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:25.345 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:35.360 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:45.342 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:55.359 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:05.475 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:15.465 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:25.501 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:35.547 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:45.606 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:55.596 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:05.568 UTC 200 GET /ping 0.000000 seconds


RALF:

Log files:


20-02-2016 19:19:54.153 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:04.177 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:14.175 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:24.216 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:34.266 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:44.287 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:54.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:04.329 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:14.369 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:24.370 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:34.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:44.387 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:54.468 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:04.474 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:14.504 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:24.550 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:34.523 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:44.580 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:54.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:04.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:14.718 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:24.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:34.769 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:44.689 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:54.762 UTC 200 GET /ping 0.000000 seconds


HOMESTEAD:

Log files:

20-02-2016 19:25:29.325 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:25:48.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:11.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:22.522 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:33.317 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:43.202 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:53.594 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:04.931 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:14.657 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:25.383 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:36.587 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:46.124 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:57.708 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:28:08.325 UTC 200 GET /ping 0.000000 seconds


Process 'ntp_process'               Running
System 'node-homestead'             Running
Process 'nginx_process'             Running
Process 'homestead_process'         Running
Program 'poll_homestead'            Status ok
Process 'homestead-prov_process'    Running
Program 'poll_homestead-prov'       Status ok
Process 'clearwater_queue_manager'  Running
Process 'etcd_process'              Running
Program 'poll_etcd_cluster'         Waiting
Program 'poll_etcd'                 Status ok
Process 'clearwater_diags_monitor_process' Running
Process 'clearwater_config_manager' Running
Process 'clearwater_cluster_manager' Running
Process 'cassandra_process'         Running
Program 'poll_cassandra'            Status ok


Ellis:

I can't to upload shared_config when i try to enter line:

/usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config

Some errors:

root at ellis:/home/ubuntu# /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config 
Upload shared configuration failed to http://10.12.0.66:4000/v2/keys/clearwater/site1/configuration/shared_config
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   684  100    35  100   649      5    108  0:00:07  0:00:06  0:00:01     0


What's going on?

Thnaks?


Att.,
Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160220/3733e4e3/attachment.html>

From vinayak.ram at hpe.com  Fri Feb 19 12:10:09 2016
From: vinayak.ram at hpe.com (Ram, Vinayak (NFV BU))
Date: Fri, 19 Feb 2016 17:10:09 +0000
Subject: [Clearwater] Call drop after 30 seconds
In-Reply-To: <BN3PR02MB12559BB9D4C93481982C6E3C9BA00@BN3PR02MB1255.namprd02.prod.outlook.com>
References: <DF4PR84MB0059649B9A36D637EECEE7B6FBA00@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>
	<BN3PR02MB12559BB9D4C93481982C6E3C9BA00@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <DF4PR84MB005908B89DE460D242E7C434FBA00@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>

+ Shiva

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Friday, February 19, 2016 10:34 PM
To: Ram, Vinayak (NFV BU); clearwater at lists.projectclearwater.org
Subject: RE: Call drop after 30 seconds

Hi Vinayak,

Calls dropping after 30 seconds probably indicates a problem with the ACK not being sent correctly for the INVITE.

Can you please send me the debug logs from your Sprout and Bono nodes for a call? You can turn on debug logging by creating/editing the file /etc/clearwater/user_settings, add log_level=5 and then restart Sprout/Bono (service <sprout/bono> stop - they're automatically restarted by monit). The logs are output in /var/log/sprout/ and /var/log/bono/.

Ellie


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Ram, Vinayak (NFV BU)
Sent: 19 February 2016 15:01
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Call drop after 30 seconds


Hi All,

I have a clearwater manual setup up and running and I am using zoiper SIP phone to make the calls, but the call tends to drop after 30 seconds is there a way I can extend this?

Please suggest.

Thanks,
Vinayak

Mail: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160219/fbfc33d2/attachment.html>

From shiva-charan.m-s at hpe.com  Fri Feb 19 14:51:49 2016
From: shiva-charan.m-s at hpe.com (M S, Shiva Charan (NFV BU))
Date: Fri, 19 Feb 2016 19:51:49 +0000
Subject: [Clearwater] Call drop after 30 seconds
In-Reply-To: <DF4PR84MB005908B89DE460D242E7C434FBA00@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>
References: <DF4PR84MB0059649B9A36D637EECEE7B6FBA00@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>
	<BN3PR02MB12559BB9D4C93481982C6E3C9BA00@BN3PR02MB1255.namprd02.prod.outlook.com>
	<DF4PR84MB005908B89DE460D242E7C434FBA00@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>
Message-ID: <08DF8AFC8BDC494F8AF90F9AFBE66B8F12549FBA@G4W3220.americas.hpqcorp.net>

HI,

Please find the logs attached.

Thanks,
Shiva Charan M S

From: Ram, Vinayak (NFV BU)
Sent: Friday, February 19, 2016 10:40 PM
To: Eleanor Merry; clearwater at lists.projectclearwater.org
Cc: M S, Shiva Charan (NFV BU)
Subject: RE: Call drop after 30 seconds

+ Shiva

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Friday, February 19, 2016 10:34 PM
To: Ram, Vinayak (NFV BU); clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Call drop after 30 seconds

Hi Vinayak,

Calls dropping after 30 seconds probably indicates a problem with the ACK not being sent correctly for the INVITE.

Can you please send me the debug logs from your Sprout and Bono nodes for a call? You can turn on debug logging by creating/editing the file /etc/clearwater/user_settings, add log_level=5 and then restart Sprout/Bono (service <sprout/bono> stop - they're automatically restarted by monit). The logs are output in /var/log/sprout/ and /var/log/bono/.

Ellie


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Ram, Vinayak (NFV BU)
Sent: 19 February 2016 15:01
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Call drop after 30 seconds


Hi All,

I have a clearwater manual setup up and running and I am using zoiper SIP phone to make the calls, but the call tends to drop after 30 seconds is there a way I can extend this?

Please suggest.

Thanks,
Vinayak

Mail: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160219/4cb0d887/attachment.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: bono_20160219T190000Z.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160219/4cb0d887/attachment.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sprout_20160219T190000Z.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160219/4cb0d887/attachment-0001.txt>

From vinayak.ram at hpe.com  Fri Feb 19 21:12:50 2016
From: vinayak.ram at hpe.com (Ram, Vinayak (NFV BU))
Date: Sat, 20 Feb 2016 02:12:50 +0000
Subject: [Clearwater] Call drop after 30 seconds
In-Reply-To: <BN3PR02MB12559BB9D4C93481982C6E3C9BA00@BN3PR02MB1255.namprd02.prod.outlook.com>
References: <DF4PR84MB0059649B9A36D637EECEE7B6FBA00@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>
	<BN3PR02MB12559BB9D4C93481982C6E3C9BA00@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <DF4PR84MB0059153465E44E69F02C1773FBA10@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>

Hello Ellie,

Please find attached the requested logs.

Regards,
Vinayak



From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Friday, February 19, 2016 10:34 PM
To: Ram, Vinayak (NFV BU); clearwater at lists.projectclearwater.org
Subject: RE: Call drop after 30 seconds

Hi Vinayak,

Calls dropping after 30 seconds probably indicates a problem with the ACK not being sent correctly for the INVITE.

Can you please send me the debug logs from your Sprout and Bono nodes for a call? You can turn on debug logging by creating/editing the file /etc/clearwater/user_settings, add log_level=5 and then restart Sprout/Bono (service <sprout/bono> stop - they're automatically restarted by monit). The logs are output in /var/log/sprout/ and /var/log/bono/.

Ellie


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Ram, Vinayak (NFV BU)
Sent: 19 February 2016 15:01
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Call drop after 30 seconds


Hi All,

I have a clearwater manual setup up and running and I am using zoiper SIP phone to make the calls, but the call tends to drop after 30 seconds is there a way I can extend this?

Please suggest.

Thanks,
Vinayak

Mail: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160220/d1dcbbe2/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: logs.zip
Type: application/x-zip-compressed
Size: 1132541 bytes
Desc: logs.zip
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160220/d1dcbbe2/attachment.bin>

From Chris.Elford at metaswitch.com  Mon Feb 22 12:24:34 2016
From: Chris.Elford at metaswitch.com (Chris Elford)
Date: Mon, 22 Feb 2016 17:24:34 +0000
Subject: [Clearwater] [Error when i try to upload shared_config]
In-Reply-To: <BAY181-W555F40C53B4376D7CC39199EA10@phx.gbl>
References: <BAY181-W555F40C53B4376D7CC39199EA10@phx.gbl>
Message-ID: <SN1PR0201MB18560AC68412A55E8ABA8B6EE1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>

Hi Rodrigo,

It looks like your Ellis node is having trouble talking to the etcd cluster and uploading its configuration. Can you please send us the etcd logs from your Ellis node? You can find these at /var/log/ellis. It would also help to know when you attempted to upload shared config - do you have a note of the time at which you ran the command?

Yours,

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Rodrigo Moreira
Sent: 20 February 2016 19:48
To: Eleanor Merry <Eleanor.Merry at metaswitch.com>; clearwater at lists.projectclearwater.org
Subject: [Clearwater] [Error when i try to upload shared_config]

Hi,
Could you please tell what happens?
Follows logs:

BONO:



  *   I can ping sprout.nyc3.example.com on the host bono.nyc3.example.com


Log files:


20-02-2016 13:57:21.194 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Status bono.cpp:3316: Create list of PBXes
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:148: Finished loading plug-ins
20-02-2016 13:57:21.197 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:36.207 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:51.214 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:06.228 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:21.235 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:36.249 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:51.263 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:06.277 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:21.279 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:36.294 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:51.306 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)



SPROUT:

Log files:

20-02-2016 19:19:04.990 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:14.962 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:25.086 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:35.096 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:45.181 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:55.167 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:05.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:15.221 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:25.345 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:35.360 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:45.342 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:55.359 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:05.475 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:15.465 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:25.501 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:35.547 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:45.606 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:55.596 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:05.568 UTC 200 GET /ping 0.000000 seconds


RALF:

Log files:


20-02-2016 19:19:54.153 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:04.177 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:14.175 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:24.216 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:34.266 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:44.287 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:54.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:04.329 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:14.369 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:24.370 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:34.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:44.387 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:54.468 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:04.474 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:14.504 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:24.550 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:34.523 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:44.580 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:54.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:04.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:14.718 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:24.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:34.769 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:44.689 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:54.762 UTC 200 GET /ping 0.000000 seconds


HOMESTEAD:

Log files:

20-02-2016 19:25:29.325 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:25:48.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:11.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:22.522 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:33.317 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:43.202 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:53.594 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:04.931 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:14.657 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:25.383 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:36.587 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:46.124 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:57.708 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:28:08.325 UTC 200 GET /ping 0.000000 seconds


Process 'ntp_process'               Running
System 'node-homestead'             Running
Process 'nginx_process'             Running
Process 'homestead_process'         Running
Program 'poll_homestead'            Status ok
Process 'homestead-prov_process'    Running
Program 'poll_homestead-prov'       Status ok
Process 'clearwater_queue_manager'  Running
Process 'etcd_process'              Running
Program 'poll_etcd_cluster'         Waiting
Program 'poll_etcd'                 Status ok
Process 'clearwater_diags_monitor_process' Running
Process 'clearwater_config_manager' Running
Process 'clearwater_cluster_manager' Running
Process 'cassandra_process'         Running
Program 'poll_cassandra'            Status ok


Ellis:

I can't to upload shared_config when i try to enter line:

/usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config

Some errors:

root at ellis:/home/ubuntu# /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config
Upload shared configuration failed to http://10.12.0.66:4000/v2/keys/clearwater/site1/configuration/shared_config
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   684  100    35  100   649      5    108  0:00:07  0:00:06  0:00:01     0

What's going on?

Thnaks?


Att.,
Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160222/c45b22cd/attachment.html>

From Chris.Elford at metaswitch.com  Mon Feb 22 12:29:23 2016
From: Chris.Elford at metaswitch.com (Chris Elford)
Date: Mon, 22 Feb 2016 17:29:23 +0000
Subject: [Clearwater] [Error when i try to upload shared_config]
In-Reply-To: <SN1PR0201MB18560AC68412A55E8ABA8B6EE1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>
References: <BAY181-W555F40C53B4376D7CC39199EA10@phx.gbl>
	<SN1PR0201MB18560AC68412A55E8ABA8B6EE1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>
Message-ID: <SN1PR0201MB1856DD18B48A5E8F85FF5199E1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>

Sorry, I slipped up there.

The etcd logs are in /var/log/clearwater-etcd, /var/log/clearwater-cluster-manager, and /var/log/clearwater-config-manager.

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Chris Elford
Sent: 22 February 2016 17:25
To: Rodrigo Moreira <moreira_r at outlook.com>
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] [Error when i try to upload shared_config]

Hi Rodrigo,

It looks like your Ellis node is having trouble talking to the etcd cluster and uploading its configuration. Can you please send us the etcd logs from your Ellis node? You can find these at /var/log/ellis. It would also help to know when you attempted to upload shared config - do you have a note of the time at which you ran the command?

Yours,

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Rodrigo Moreira
Sent: 20 February 2016 19:48
To: Eleanor Merry <Eleanor.Merry at metaswitch.com<mailto:Eleanor.Merry at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] [Error when i try to upload shared_config]

Hi,
Could you please tell what happens?
Follows logs:

BONO:


  *   I can ping sprout.nyc3.example.com on the host bono.nyc3.example.com

Log files:


20-02-2016 13:57:21.194 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Status bono.cpp:3316: Create list of PBXes
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:148: Finished loading plug-ins
20-02-2016 13:57:21.197 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:36.207 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:51.214 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:06.228 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:21.235 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:36.249 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:51.263 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:06.277 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:21.279 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:36.294 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:51.306 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)


SPROUT:

Log files:

20-02-2016 19:19:04.990 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:14.962 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:25.086 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:35.096 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:45.181 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:55.167 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:05.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:15.221 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:25.345 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:35.360 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:45.342 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:55.359 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:05.475 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:15.465 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:25.501 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:35.547 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:45.606 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:55.596 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:05.568 UTC 200 GET /ping 0.000000 seconds


RALF:

Log files:


20-02-2016 19:19:54.153 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:04.177 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:14.175 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:24.216 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:34.266 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:44.287 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:54.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:04.329 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:14.369 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:24.370 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:34.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:44.387 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:54.468 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:04.474 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:14.504 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:24.550 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:34.523 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:44.580 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:54.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:04.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:14.718 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:24.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:34.769 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:44.689 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:54.762 UTC 200 GET /ping 0.000000 seconds


HOMESTEAD:

Log files:

20-02-2016 19:25:29.325 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:25:48.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:11.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:22.522 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:33.317 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:43.202 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:53.594 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:04.931 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:14.657 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:25.383 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:36.587 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:46.124 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:57.708 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:28:08.325 UTC 200 GET /ping 0.000000 seconds


Process 'ntp_process'               Running
System 'node-homestead'             Running
Process 'nginx_process'             Running
Process 'homestead_process'         Running
Program 'poll_homestead'            Status ok
Process 'homestead-prov_process'    Running
Program 'poll_homestead-prov'       Status ok
Process 'clearwater_queue_manager'  Running
Process 'etcd_process'              Running
Program 'poll_etcd_cluster'         Waiting
Program 'poll_etcd'                 Status ok
Process 'clearwater_diags_monitor_process' Running
Process 'clearwater_config_manager' Running
Process 'clearwater_cluster_manager' Running
Process 'cassandra_process'         Running
Program 'poll_cassandra'            Status ok


Ellis:

I can't to upload shared_config when i try to enter line:

/usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config

Some errors:

root at ellis:/home/ubuntu# /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config
Upload shared configuration failed to http://10.12.0.66:4000/v2/keys/clearwater/site1/configuration/shared_config
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   684  100    35  100   649      5    108  0:00:07  0:00:06  0:00:01     0

What's going on?

Thnaks?


Att.,
Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160222/1f1332be/attachment.html>

From Eleanor.Merry at metaswitch.com  Mon Feb 22 13:44:31 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Mon, 22 Feb 2016 18:44:31 +0000
Subject: [Clearwater] Call drop after 30 seconds
In-Reply-To: <DF4PR84MB0059153465E44E69F02C1773FBA10@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>
References: <DF4PR84MB0059649B9A36D637EECEE7B6FBA00@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>
	<BN3PR02MB12559BB9D4C93481982C6E3C9BA00@BN3PR02MB1255.namprd02.prod.outlook.com>
	<DF4PR84MB0059153465E44E69F02C1773FBA10@DF4PR84MB0059.NAMPRD84.PROD.OUTLOOK.COM>
Message-ID: <BN3PR02MB12559F2E1A4B8EF2C09E030F9BA30@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Vinayak,

It doesn't look like the client ever sends an ACK to the 200 OK. You can see in the Bono logs the INVITE, a 180 Ringing, and many 200 OKs - but no ACKs (we retransmit the 200 OK a lot because we haven't seen the ACK). The client tears the call down after around 30 secs because it hasn't seen the ACK.

It looks like the public_hostname setting in /etc/clearwater/local_config is set to just 'bono' (we add a Record-Route header using that hostname, 'Record-Route: <sip:<flow identifier>@bono:5060;transport=UDP;lr<sip:%3cflow%20identifier%3e at bono:5060;transport=UDP;lr>>'). The client can't resolve this, so it fails to send the ACK.

Can you try setting public_hostname to something that can be resolved by the clients (e.g. an IP address, or bono.cwhpe.com)

Ellie

From: Ram, Vinayak (NFV BU) [mailto:vinayak.ram at hpe.com]
Sent: 20 February 2016 02:13
To: Eleanor Merry; clearwater at lists.projectclearwater.org
Cc: M S, Shiva Charan (NFV BU)
Subject: RE: Call drop after 30 seconds

Hello Ellie,

Please find attached the requested logs.

Regards,
Vinayak



From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Friday, February 19, 2016 10:34 PM
To: Ram, Vinayak (NFV BU); clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Call drop after 30 seconds

Hi Vinayak,

Calls dropping after 30 seconds probably indicates a problem with the ACK not being sent correctly for the INVITE.

Can you please send me the debug logs from your Sprout and Bono nodes for a call? You can turn on debug logging by creating/editing the file /etc/clearwater/user_settings, add log_level=5 and then restart Sprout/Bono (service <sprout/bono> stop - they're automatically restarted by monit). The logs are output in /var/log/sprout/ and /var/log/bono/.

Ellie


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Ram, Vinayak (NFV BU)
Sent: 19 February 2016 15:01
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Call drop after 30 seconds


Hi All,

I have a clearwater manual setup up and running and I am using zoiper SIP phone to make the calls, but the call tends to drop after 30 seconds is there a way I can extend this?

Please suggest.

Thanks,
Vinayak

Mail: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160222/4c4237fa/attachment.html>

From moreira_r at outlook.com  Mon Feb 22 17:17:54 2016
From: moreira_r at outlook.com (Rodrigo Moreira)
Date: Mon, 22 Feb 2016 19:17:54 -0300
Subject: [Clearwater] [Error when i try to upload shared_config]
In-Reply-To: <SN1PR0201MB1856DD18B48A5E8F85FF5199E1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>
References: <BAY181-W555F40C53B4376D7CC39199EA10@phx.gbl>,
	<SN1PR0201MB18560AC68412A55E8ABA8B6EE1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>,
	<SN1PR0201MB1856DD18B48A5E8F85FF5199E1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>
Message-ID: <BAY181-W4014AFE75D1DAD61C4DC059EA30@phx.gbl>

Sure,

The Ellis log:


cluster-maneger:

22-02-2016 22:13:26.514 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-cluster-manager.pid.lockfile
22-02-2016 22:13:26.527 UTC ERROR logging_config.py:101 (thread MainThread): Uncaught exception:
  Exception: OSError
  Detail: [Errno 2] No such file or directory: '/usr/share/clearwater/clearwater-cluster-manager/plugins/'
  Traceback:
    File "/usr/share/clearwater/bin/clearwater-cluster-manager", line 40, in <module>
    main(sys.argv[1:])
  File "build_clustermgr/bdist.linux-x86_64/egg/metaswitch/clearwater/cluster_manager/main.py", line 169, in main
    etcd_cluster_key=etcd_cluster_key))
  File "build_shared/bdist.linux-x86_64/egg/metaswitch/clearwater/etcd_shared/plugin_loader.py", line 47, in load_plugins_in_dir
    files = os.listdir(dir)

config-maneger:

22-02-2016 22:09:21.237 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-config-manager.pid.lockfile
22-02-2016 22:09:21.238 UTC INFO plugin_loader.py:50 (thread MainThread): Inspecting shared_config_plugin.py
22-02-2016 22:09:21.281 UTC INFO plugin_loader.py:58 (thread MainThread): Loading shared_config_plugin.py
22-02-2016 22:09:21.282 UTC INFO plugin_loader.py:60 (thread MainThread): Loaded shared_config_plugin.py successfully
22-02-2016 22:09:21.314 UTC INFO alarms.py:55 (thread MainThread): Imported /usr/share/clearwater/bin/alarms.py
22-02-2016 22:09:23.418 UTC INFO main.py:136 (thread MainThread): Loaded plugin <shared_config_plugin.SharedConfigPlugin object at 0x7f3324bbe850>
22-02-2016 22:09:31.138 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:10:08.493 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:10:44.514 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:11:20.546 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:11:56.578 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:12:32.596 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:13:08.628 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:13:44.654 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:14:20.687 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:14:56.715 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:15:32.740 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $


What's wrong?

Thanks.

Att.,
Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007From: Chris.Elford at metaswitch.com
To: Chris.Elford at metaswitch.com; moreira_r at outlook.com
CC: clearwater at lists.projectclearwater.org
Subject: RE: [Clearwater] [Error when i try to upload shared_config]
Date: Mon, 22 Feb 2016 17:29:23 +0000









Sorry, I slipped up there.

 
The etcd logs are in /var/log/clearwater-etcd, /var/log/clearwater-cluster-manager, and /var/log/clearwater-config-manager.
 
Chris
 


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
On Behalf Of Chris Elford

Sent: 22 February 2016 17:25

To: Rodrigo Moreira <moreira_r at outlook.com>

Cc: clearwater at lists.projectclearwater.org

Subject: Re: [Clearwater] [Error when i try to upload shared_config]


 
Hi Rodrigo,
 
It looks like your Ellis node is having trouble talking to the etcd cluster and uploading its configuration. Can you please send us
 the etcd logs from your Ellis node? You can find these at /var/log/ellis. It would also help to know when you attempted to upload shared config ? do you have a note of the time at which you ran the command?
 
Yours,
 
Chris
 


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
On Behalf Of Rodrigo Moreira

Sent: 20 February 2016 19:48

To: Eleanor Merry <Eleanor.Merry at metaswitch.com>;
clearwater at lists.projectclearwater.org

Subject: [Clearwater] [Error when i try to upload shared_config]


 

Hi,



Could you
please
tell
what happens?

Follows logs:



BONO:






I can ping
sprout.nyc3.example.com on the host bono.nyc3.example.com


Log files:





20-02-2016 13:57:21.194 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Status bono.cpp:3316: Create list of PBXes

20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins

20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:148: Finished loading plug-ins

20-02-2016 13:57:21.197 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:57:36.207 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:57:51.214 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:58:06.228 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:58:21.235 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:58:36.249 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:58:51.263 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:59:06.277 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:59:21.279 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:59:36.294 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:59:51.306 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)








SPROUT:



Log files:



20-02-2016 19:19:04.990 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:19:14.962 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:19:25.086 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:19:35.096 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:19:45.181 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:19:55.167 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:05.169 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:15.221 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:25.345 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:35.360 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:45.342 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:55.359 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:05.475 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:15.465 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:25.501 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:35.547 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:45.606 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:55.596 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:05.568 UTC 200 GET /ping 0.000000 seconds





RALF:



Log files:





20-02-2016 19:19:54.153 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:04.177 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:14.175 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:24.216 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:34.266 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:44.287 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:54.368 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:04.329 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:14.369 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:24.370 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:34.368 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:44.387 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:54.468 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:04.474 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:14.504 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:24.550 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:34.523 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:44.580 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:54.561 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:23:04.648 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:23:14.718 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:23:24.648 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:23:34.769 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:23:44.689 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:23:54.762 UTC 200 GET /ping 0.000000 seconds





HOMESTEAD:



Log files:



20-02-2016 19:25:29.325 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:25:48.561 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:26:11.169 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:26:22.522 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:26:33.317 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:26:43.202 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:26:53.594 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:27:04.931 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:27:14.657 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:27:25.383 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:27:36.587 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:27:46.124 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:27:57.708 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:28:08.325 UTC 200 GET /ping 0.000000 seconds





Process 'ntp_process'               Running

System 'node-homestead'             Running

Process 'nginx_process'             Running

Process 'homestead_process'         Running

Program 'poll_homestead'            Status ok

Process 'homestead-prov_process'    Running

Program 'poll_homestead-prov'       Status ok

Process 'clearwater_queue_manager'  Running

Process 'etcd_process'              Running

Program 'poll_etcd_cluster'         Waiting

Program 'poll_etcd'                 Status ok

Process 'clearwater_diags_monitor_process' Running

Process 'clearwater_config_manager' Running

Process 'clearwater_cluster_manager' Running

Process 'cassandra_process'         Running

Program 'poll_cassandra'            Status ok





Ellis:



I can't to upload shared_config when i try to enter line:
/usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config

Some errors:

root at ellis:/home/ubuntu# /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config 
Upload shared configuration failed to http://10.12.0.66:4000/v2/keys/clearwater/site1/configuration/shared_config
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   684  100    35  100   649      5    108  0:00:07  0:00:06  0:00:01     0


What's going on?



Thnaks?





Att.,

Rodrigo M.

+ 55 37 - 9 9132-4539

Skype: rodrigo.moreira2007 


 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160222/97524a7d/attachment.html>

From Graeme.Robertson at metaswitch.com  Tue Feb 23 09:18:41 2016
From: Graeme.Robertson at metaswitch.com (Graeme Robertson)
Date: Tue, 23 Feb 2016 14:18:41 +0000
Subject: [Clearwater] The Year of the Flood release note
In-Reply-To: <BLUPR0201MB176389B4A6D7745267A50BDDE3A40@BLUPR0201MB1763.namprd02.prod.outlook.com>
References: <BLUPR0201MB176389B4A6D7745267A50BDDE3A40@BLUPR0201MB1763.namprd02.prod.outlook.com>
Message-ID: <BLUPR0201MB1763D10F315D4C306DFFA7DFE3A40@BLUPR0201MB1763.namprd02.prod.outlook.com>

The release for Project Clearwater sprint "The Year of the Flood" has been cut. The code for this release is tagged as release-92 in github.

In this release, we have updated the Chronos API to handle statistics in a more functional and more accurate way.

*         The "statistics" section of a timer request is now built as a set of tag 'type' and 'count' pairs, rather than simply an array of tag names.

*         A timer can now represent more complex objects, such as a whole implicit registration set.

Sprout and Ralf have been updated to use this new structure.

*         Sprout now uses a single timer for a whole registration set, decreasing the load on a deployment.

*         We have also fixed how registrations are counted by our statistics (where we previously were representing the number of bindings), and added statistics for the number of bindings alongside the already present number of subscriptions.

We will be releasing a blog post in the near future providing some more detail on these changes.

In this release, we've also completed development and integration of the clearwater-aio-proxy Juju charm (https://github.com/Metaswitch/clearwater-juju/).  This enables Clearwater to be deployed and managed via the Open-Source Mano (http://osm.etsi.org/) project, which is being demonstrated at Mobile World Congress 2016 this week in Barcelona.

This release also includes the following bug fixes:

*         chown: invalid option -- 'r' error when starting Ralf (https://github.com/Metaswitch/ralf/issues/203)

*         cpp-common files are no longer included in the coverage check (https://github.com/Metaswitch/cpp-common/issues/440)

*         No Search on ReadTheDocs (https://github.com/Metaswitch/clearwater-readthedocs/issues/137)

*         Uninstalling leaves snmp user and group (https://github.com/Metaswitch/clearwater-net-snmp/issues/4)


For upgrading to this release, follow the instructions at http://clearwater.readthedocs.org/en/latest/Upgrading_a_Clearwater_deployment/index.html. If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.



Thanks,
Graeme

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160223/368f6cd6/attachment.html>

From Chris.Elford at metaswitch.com  Tue Feb 23 10:57:19 2016
From: Chris.Elford at metaswitch.com (Chris Elford)
Date: Tue, 23 Feb 2016 15:57:19 +0000
Subject: [Clearwater] [Error when i try to upload shared_config]
In-Reply-To: <BAY181-W4014AFE75D1DAD61C4DC059EA30@phx.gbl>
References: <BAY181-W555F40C53B4376D7CC39199EA10@phx.gbl>,
	<SN1PR0201MB18560AC68412A55E8ABA8B6EE1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>,
	<SN1PR0201MB1856DD18B48A5E8F85FF5199E1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>
	<BAY181-W4014AFE75D1DAD61C4DC059EA30@phx.gbl>
Message-ID: <SN1PR0201MB1856ACF6501A6E11756FABF3E1A40@SN1PR0201MB1856.namprd02.prod.outlook.com>

Hi Rodrigo,

It looks like your system does not have all of the directories that Clearwater expects to exist. In particular, you are missing the folder '/usr/share/clearwater/clearwater-cluster-manager/plugins/', which is normally created when clearwater-cluster-manager is installed.

I suspect that the easiest thing to do to get your Ellis node working might be to save off its configuration (/etc/clearwater/local_config and /etc/clearwater/shared_config), delete the node, and re-install it from scratch. You can then copy on the saved-off config and try uploading it again.

If that does not work (or if you are not able to do that) then we will need more details of how you installed your Ellis node.

Yours,

Chris

From: Rodrigo Moreira [mailto:moreira_r at outlook.com]
Sent: 22 February 2016 22:18
To: Chris Elford <Chris.Elford at metaswitch.com>
Cc: clearwater at lists.projectclearwater.org
Subject: RE: [Clearwater] [Error when i try to upload shared_config]

Sure,

The Ellis log:


cluster-maneger:

22-02-2016 22:13:26.514 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-cluster-manager.pid.lockfile
22-02-2016 22:13:26.527 UTC ERROR logging_config.py:101 (thread MainThread): Uncaught exception:
  Exception: OSError
  Detail: [Errno 2] No such file or directory: '/usr/share/clearwater/clearwater-cluster-manager/plugins/'
  Traceback:
    File "/usr/share/clearwater/bin/clearwater-cluster-manager", line 40, in <module>
    main(sys.argv[1:])
  File "build_clustermgr/bdist.linux-x86_64/egg/metaswitch/clearwater/cluster_manager/main.py", line 169, in main
    etcd_cluster_key=etcd_cluster_key))
  File "build_shared/bdist.linux-x86_64/egg/metaswitch/clearwater/etcd_shared/plugin_loader.py", line 47, in load_plugins_in_dir
    files = os.listdir(dir)

config-maneger:

22-02-2016 22:09:21.237 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-config-manager.pid.lockfile
22-02-2016 22:09:21.238 UTC INFO plugin_loader.py:50 (thread MainThread): Inspecting shared_config_plugin.py
22-02-2016 22:09:21.281 UTC INFO plugin_loader.py:58 (thread MainThread): Loading shared_config_plugin.py
22-02-2016 22:09:21.282 UTC INFO plugin_loader.py:60 (thread MainThread): Loaded shared_config_plugin.py successfully
22-02-2016 22:09:21.314 UTC INFO alarms.py:55 (thread MainThread): Imported /usr/share/clearwater/bin/alarms.py
22-02-2016 22:09:23.418 UTC INFO main.py:136 (thread MainThread): Loaded plugin <shared_config_plugin.SharedConfigPlugin object at 0x7f3324bbe850>
22-02-2016 22:09:31.138 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:10:08.493 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:10:44.514 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:11:20.546 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:11:56.578 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:12:32.596 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:13:08.628 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:13:44.654 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:14:20.687 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:14:56.715 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:15:32.740 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $


What's wrong?

Thanks.

Att.,
Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007
________________________________
From: Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>
To: Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>; moreira_r at outlook.com<mailto:moreira_r at outlook.com>
CC: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Clearwater] [Error when i try to upload shared_config]
Date: Mon, 22 Feb 2016 17:29:23 +0000
Sorry, I slipped up there.

The etcd logs are in /var/log/clearwater-etcd, /var/log/clearwater-cluster-manager, and /var/log/clearwater-config-manager.

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Chris Elford
Sent: 22 February 2016 17:25
To: Rodrigo Moreira <moreira_r at outlook.com<mailto:moreira_r at outlook.com>>
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Clearwater] [Error when i try to upload shared_config]

Hi Rodrigo,

It looks like your Ellis node is having trouble talking to the etcd cluster and uploading its configuration. Can you please send us the etcd logs from your Ellis node? You can find these at /var/log/ellis. It would also help to know when you attempted to upload shared config - do you have a note of the time at which you ran the command?

Yours,

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Rodrigo Moreira
Sent: 20 February 2016 19:48
To: Eleanor Merry <Eleanor.Merry at metaswitch.com<mailto:Eleanor.Merry at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] [Error when i try to upload shared_config]

Hi,
Could you please tell what happens?
Follows logs:

BONO:

  *   I can ping sprout.nyc3.example.com on the host bono.nyc3.example.com

Log files:


20-02-2016 13:57:21.194 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Status bono.cpp:3316: Create list of PBXes
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:148: Finished loading plug-ins
20-02-2016 13:57:21.197 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:36.207 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:51.214 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:06.228 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:21.235 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:36.249 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:51.263 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:06.277 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:21.279 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:36.294 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:51.306 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

SPROUT:

Log files:

20-02-2016 19:19:04.990 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:14.962 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:25.086 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:35.096 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:45.181 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:55.167 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:05.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:15.221 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:25.345 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:35.360 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:45.342 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:55.359 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:05.475 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:15.465 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:25.501 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:35.547 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:45.606 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:55.596 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:05.568 UTC 200 GET /ping 0.000000 seconds


RALF:

Log files:


20-02-2016 19:19:54.153 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:04.177 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:14.175 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:24.216 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:34.266 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:44.287 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:54.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:04.329 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:14.369 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:24.370 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:34.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:44.387 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:54.468 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:04.474 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:14.504 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:24.550 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:34.523 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:44.580 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:54.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:04.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:14.718 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:24.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:34.769 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:44.689 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:54.762 UTC 200 GET /ping 0.000000 seconds


HOMESTEAD:

Log files:

20-02-2016 19:25:29.325 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:25:48.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:11.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:22.522 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:33.317 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:43.202 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:53.594 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:04.931 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:14.657 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:25.383 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:36.587 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:46.124 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:57.708 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:28:08.325 UTC 200 GET /ping 0.000000 seconds


Process 'ntp_process'               Running
System 'node-homestead'             Running
Process 'nginx_process'             Running
Process 'homestead_process'         Running
Program 'poll_homestead'            Status ok
Process 'homestead-prov_process'    Running
Program 'poll_homestead-prov'       Status ok
Process 'clearwater_queue_manager'  Running
Process 'etcd_process'              Running
Program 'poll_etcd_cluster'         Waiting
Program 'poll_etcd'                 Status ok
Process 'clearwater_diags_monitor_process' Running
Process 'clearwater_config_manager' Running
Process 'clearwater_cluster_manager' Running
Process 'cassandra_process'         Running
Program 'poll_cassandra'            Status ok


Ellis:

I can't to upload shared_config when i try to enter line:

/usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config

Some errors:

root at ellis:/home/ubuntu# /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config
Upload shared configuration failed to http://10.12.0.66:4000/v2/keys/clearwater/site1/configuration/shared_config
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   684  100    35  100   649      5    108  0:00:07  0:00:06  0:00:01     0

What's going on?

Thnaks?


Att.,
Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160223/bf876295/attachment.html>

From Eleanor.Merry at metaswitch.com  Tue Feb 23 11:18:34 2016
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Tue, 23 Feb 2016 16:18:34 +0000
Subject: [Clearwater] [Error when i try to upload shared_config]
In-Reply-To: <SN1PR0201MB1856ACF6501A6E11756FABF3E1A40@SN1PR0201MB1856.namprd02.prod.outlook.com>
References: <BAY181-W555F40C53B4376D7CC39199EA10@phx.gbl>,
	<SN1PR0201MB18560AC68412A55E8ABA8B6EE1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>,
	<SN1PR0201MB1856DD18B48A5E8F85FF5199E1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>
	<BAY181-W4014AFE75D1DAD61C4DC059EA30@phx.gbl>
	<SN1PR0201MB1856ACF6501A6E11756FABF3E1A40@SN1PR0201MB1856.namprd02.prod.outlook.com>
Message-ID: <BLUPR02MB12516DE3D0E4A41C24F9C82B9BA40@BLUPR02MB1251.namprd02.prod.outlook.com>

Hi Rodrigo,

We're looking into this at the moment - can you please not delete your Ellis node yet?

Thanks,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Chris Elford
Sent: 23 February 2016 15:57
To: Rodrigo Moreira
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] [Error when i try to upload shared_config]

Hi Rodrigo,

It looks like your system does not have all of the directories that Clearwater expects to exist. In particular, you are missing the folder '/usr/share/clearwater/clearwater-cluster-manager/plugins/', which is normally created when clearwater-cluster-manager is installed.

I suspect that the easiest thing to do to get your Ellis node working might be to save off its configuration (/etc/clearwater/local_config and /etc/clearwater/shared_config), delete the node, and re-install it from scratch. You can then copy on the saved-off config and try uploading it again.

If that does not work (or if you are not able to do that) then we will need more details of how you installed your Ellis node.

Yours,

Chris

From: Rodrigo Moreira [mailto:moreira_r at outlook.com]
Sent: 22 February 2016 22:18
To: Chris Elford <Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>>
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Clearwater] [Error when i try to upload shared_config]

Sure,

The Ellis log:


cluster-maneger:

22-02-2016 22:13:26.514 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-cluster-manager.pid.lockfile
22-02-2016 22:13:26.527 UTC ERROR logging_config.py:101 (thread MainThread): Uncaught exception:
  Exception: OSError
  Detail: [Errno 2] No such file or directory: '/usr/share/clearwater/clearwater-cluster-manager/plugins/'
  Traceback:
    File "/usr/share/clearwater/bin/clearwater-cluster-manager", line 40, in <module>
    main(sys.argv[1:])
  File "build_clustermgr/bdist.linux-x86_64/egg/metaswitch/clearwater/cluster_manager/main.py", line 169, in main
    etcd_cluster_key=etcd_cluster_key))
  File "build_shared/bdist.linux-x86_64/egg/metaswitch/clearwater/etcd_shared/plugin_loader.py", line 47, in load_plugins_in_dir
    files = os.listdir(dir)

config-maneger:

22-02-2016 22:09:21.237 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-config-manager.pid.lockfile
22-02-2016 22:09:21.238 UTC INFO plugin_loader.py:50 (thread MainThread): Inspecting shared_config_plugin.py
22-02-2016 22:09:21.281 UTC INFO plugin_loader.py:58 (thread MainThread): Loading shared_config_plugin.py
22-02-2016 22:09:21.282 UTC INFO plugin_loader.py:60 (thread MainThread): Loaded shared_config_plugin.py successfully
22-02-2016 22:09:21.314 UTC INFO alarms.py:55 (thread MainThread): Imported /usr/share/clearwater/bin/alarms.py
22-02-2016 22:09:23.418 UTC INFO main.py:136 (thread MainThread): Loaded plugin <shared_config_plugin.SharedConfigPlugin object at 0x7f3324bbe850>
22-02-2016 22:09:31.138 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:10:08.493 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:10:44.514 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:11:20.546 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:11:56.578 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:12:32.596 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:13:08.628 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:13:44.654 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:14:20.687 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:14:56.715 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:15:32.740 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $


What's wrong?

Thanks.

Att.,
Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007
________________________________
From: Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>
To: Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>; moreira_r at outlook.com<mailto:moreira_r at outlook.com>
CC: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Clearwater] [Error when i try to upload shared_config]
Date: Mon, 22 Feb 2016 17:29:23 +0000
Sorry, I slipped up there.

The etcd logs are in /var/log/clearwater-etcd, /var/log/clearwater-cluster-manager, and /var/log/clearwater-config-manager.

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Chris Elford
Sent: 22 February 2016 17:25
To: Rodrigo Moreira <moreira_r at outlook.com<mailto:moreira_r at outlook.com>>
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Clearwater] [Error when i try to upload shared_config]

Hi Rodrigo,

It looks like your Ellis node is having trouble talking to the etcd cluster and uploading its configuration. Can you please send us the etcd logs from your Ellis node? You can find these at /var/log/ellis. It would also help to know when you attempted to upload shared config - do you have a note of the time at which you ran the command?

Yours,

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Rodrigo Moreira
Sent: 20 February 2016 19:48
To: Eleanor Merry <Eleanor.Merry at metaswitch.com<mailto:Eleanor.Merry at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] [Error when i try to upload shared_config]

Hi,
Could you please tell what happens?
Follows logs:

BONO:

  *   I can ping sprout.nyc3.example.com on the host bono.nyc3.example.com

Log files:


20-02-2016 13:57:21.194 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Status bono.cpp:3316: Create list of PBXes
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:148: Finished loading plug-ins
20-02-2016 13:57:21.197 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:36.207 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:51.214 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:06.228 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:21.235 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:36.249 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:51.263 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:06.277 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:21.279 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:36.294 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:51.306 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

SPROUT:

Log files:

20-02-2016 19:19:04.990 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:14.962 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:25.086 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:35.096 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:45.181 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:55.167 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:05.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:15.221 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:25.345 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:35.360 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:45.342 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:55.359 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:05.475 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:15.465 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:25.501 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:35.547 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:45.606 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:55.596 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:05.568 UTC 200 GET /ping 0.000000 seconds


RALF:

Log files:


20-02-2016 19:19:54.153 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:04.177 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:14.175 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:24.216 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:34.266 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:44.287 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:54.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:04.329 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:14.369 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:24.370 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:34.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:44.387 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:54.468 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:04.474 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:14.504 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:24.550 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:34.523 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:44.580 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:54.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:04.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:14.718 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:24.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:34.769 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:44.689 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:54.762 UTC 200 GET /ping 0.000000 seconds


HOMESTEAD:

Log files:

20-02-2016 19:25:29.325 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:25:48.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:11.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:22.522 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:33.317 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:43.202 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:53.594 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:04.931 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:14.657 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:25.383 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:36.587 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:46.124 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:57.708 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:28:08.325 UTC 200 GET /ping 0.000000 seconds


Process 'ntp_process'               Running
System 'node-homestead'             Running
Process 'nginx_process'             Running
Process 'homestead_process'         Running
Program 'poll_homestead'            Status ok
Process 'homestead-prov_process'    Running
Program 'poll_homestead-prov'       Status ok
Process 'clearwater_queue_manager'  Running
Process 'etcd_process'              Running
Program 'poll_etcd_cluster'         Waiting
Program 'poll_etcd'                 Status ok
Process 'clearwater_diags_monitor_process' Running
Process 'clearwater_config_manager' Running
Process 'clearwater_cluster_manager' Running
Process 'cassandra_process'         Running
Program 'poll_cassandra'            Status ok


Ellis:

I can't to upload shared_config when i try to enter line:

/usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config

Some errors:

root at ellis:/home/ubuntu# /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config
Upload shared configuration failed to http://10.12.0.66:4000/v2/keys/clearwater/site1/configuration/shared_config
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   684  100    35  100   649      5    108  0:00:07  0:00:06  0:00:01     0

What's going on?

Thnaks?


Att.,
Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160223/72420248/attachment.html>

From Chris.Elford at metaswitch.com  Tue Feb 23 12:39:24 2016
From: Chris.Elford at metaswitch.com (Chris Elford)
Date: Tue, 23 Feb 2016 17:39:24 +0000
Subject: [Clearwater] [Error when i try to upload shared_config]
In-Reply-To: <BLUPR02MB12516DE3D0E4A41C24F9C82B9BA40@BLUPR02MB1251.namprd02.prod.outlook.com>
References: <BAY181-W555F40C53B4376D7CC39199EA10@phx.gbl>,
	<SN1PR0201MB18560AC68412A55E8ABA8B6EE1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>,
	<SN1PR0201MB1856DD18B48A5E8F85FF5199E1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>
	<BAY181-W4014AFE75D1DAD61C4DC059EA30@phx.gbl>
	<SN1PR0201MB1856ACF6501A6E11756FABF3E1A40@SN1PR0201MB1856.namprd02.prod.outlook.com>
	<BLUPR02MB12516DE3D0E4A41C24F9C82B9BA40@BLUPR02MB1251.namprd02.prod.outlook.com>
Message-ID: <SN1PR0201MB18567B8B6AC95395509454ABE1A40@SN1PR0201MB1856.namprd02.prod.outlook.com>

Hi Rodrigo,

It looks like the error message that I saw below is benign. We need to figure out what is wrong with etcd.

Please send over:

*       the logs from /var/log/clearwater-config-manager

*       your local config file (/etc/clearwater/local_config).

We might need some etcd diagnostics. Please send us the results of running the following commands.

*       clearwater-etcdctl cluster-health

*       clearwater-etcdctl member list

Yours,

Chris

From: Eleanor Merry
Sent: 23 February 2016 16:19
To: Chris Elford <Chris.Elford at metaswitch.com>; Rodrigo Moreira <moreira_r at outlook.com>
Cc: clearwater at lists.projectclearwater.org
Subject: RE: [Clearwater] [Error when i try to upload shared_config]

Hi Rodrigo,

We're looking into this at the moment - can you please not delete your Ellis node yet?

Thanks,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Chris Elford
Sent: 23 February 2016 15:57
To: Rodrigo Moreira
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Clearwater] [Error when i try to upload shared_config]

Hi Rodrigo,

It looks like your system does not have all of the directories that Clearwater expects to exist. In particular, you are missing the folder '/usr/share/clearwater/clearwater-cluster-manager/plugins/', which is normally created when clearwater-cluster-manager is installed.

I suspect that the easiest thing to do to get your Ellis node working might be to save off its configuration (/etc/clearwater/local_config and /etc/clearwater/shared_config), delete the node, and re-install it from scratch. You can then copy on the saved-off config and try uploading it again.

If that does not work (or if you are not able to do that) then we will need more details of how you installed your Ellis node.

Yours,

Chris

From: Rodrigo Moreira [mailto:moreira_r at outlook.com]
Sent: 22 February 2016 22:18
To: Chris Elford <Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>>
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Clearwater] [Error when i try to upload shared_config]

Sure,

The Ellis log:


cluster-maneger:

22-02-2016 22:13:26.514 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-cluster-manager.pid.lockfile
22-02-2016 22:13:26.527 UTC ERROR logging_config.py:101 (thread MainThread): Uncaught exception:
  Exception: OSError
  Detail: [Errno 2] No such file or directory: '/usr/share/clearwater/clearwater-cluster-manager/plugins/'
  Traceback:
    File "/usr/share/clearwater/bin/clearwater-cluster-manager", line 40, in <module>
    main(sys.argv[1:])
  File "build_clustermgr/bdist.linux-x86_64/egg/metaswitch/clearwater/cluster_manager/main.py", line 169, in main
    etcd_cluster_key=etcd_cluster_key))
  File "build_shared/bdist.linux-x86_64/egg/metaswitch/clearwater/etcd_shared/plugin_loader.py", line 47, in load_plugins_in_dir
    files = os.listdir(dir)

config-maneger:

22-02-2016 22:09:21.237 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-config-manager.pid.lockfile
22-02-2016 22:09:21.238 UTC INFO plugin_loader.py:50 (thread MainThread): Inspecting shared_config_plugin.py
22-02-2016 22:09:21.281 UTC INFO plugin_loader.py:58 (thread MainThread): Loading shared_config_plugin.py
22-02-2016 22:09:21.282 UTC INFO plugin_loader.py:60 (thread MainThread): Loaded shared_config_plugin.py successfully
22-02-2016 22:09:21.314 UTC INFO alarms.py:55 (thread MainThread): Imported /usr/share/clearwater/bin/alarms.py
22-02-2016 22:09:23.418 UTC INFO main.py:136 (thread MainThread): Loaded plugin <shared_config_plugin.SharedConfigPlugin object at 0x7f3324bbe850>
22-02-2016 22:09:31.138 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:10:08.493 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:10:44.514 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:11:20.546 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:11:56.578 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:12:32.596 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:13:08.628 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:13:44.654 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:14:20.687 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:14:56.715 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:15:32.740 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $


What's wrong?

Thanks.

Att.,
Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007
________________________________
From: Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>
To: Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>; moreira_r at outlook.com<mailto:moreira_r at outlook.com>
CC: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Clearwater] [Error when i try to upload shared_config]
Date: Mon, 22 Feb 2016 17:29:23 +0000
Sorry, I slipped up there.

The etcd logs are in /var/log/clearwater-etcd, /var/log/clearwater-cluster-manager, and /var/log/clearwater-config-manager.

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Chris Elford
Sent: 22 February 2016 17:25
To: Rodrigo Moreira <moreira_r at outlook.com<mailto:moreira_r at outlook.com>>
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Clearwater] [Error when i try to upload shared_config]

Hi Rodrigo,

It looks like your Ellis node is having trouble talking to the etcd cluster and uploading its configuration. Can you please send us the etcd logs from your Ellis node? You can find these at /var/log/ellis. It would also help to know when you attempted to upload shared config - do you have a note of the time at which you ran the command?

Yours,

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Rodrigo Moreira
Sent: 20 February 2016 19:48
To: Eleanor Merry <Eleanor.Merry at metaswitch.com<mailto:Eleanor.Merry at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] [Error when i try to upload shared_config]

Hi,
Could you please tell what happens?
Follows logs:

BONO:

  *   I can ping sprout.nyc3.example.com on the host bono.nyc3.example.com

Log files:


20-02-2016 13:57:21.194 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Status bono.cpp:3316: Create list of PBXes
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:148: Finished loading plug-ins
20-02-2016 13:57:21.197 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:36.207 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:51.214 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:06.228 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:21.235 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:36.249 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:51.263 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:06.277 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:21.279 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:36.294 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:51.306 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

SPROUT:

Log files:

20-02-2016 19:19:04.990 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:14.962 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:25.086 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:35.096 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:45.181 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:55.167 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:05.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:15.221 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:25.345 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:35.360 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:45.342 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:55.359 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:05.475 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:15.465 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:25.501 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:35.547 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:45.606 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:55.596 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:05.568 UTC 200 GET /ping 0.000000 seconds


RALF:

Log files:


20-02-2016 19:19:54.153 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:04.177 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:14.175 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:24.216 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:34.266 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:44.287 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:54.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:04.329 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:14.369 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:24.370 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:34.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:44.387 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:54.468 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:04.474 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:14.504 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:24.550 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:34.523 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:44.580 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:54.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:04.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:14.718 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:24.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:34.769 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:44.689 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:54.762 UTC 200 GET /ping 0.000000 seconds


HOMESTEAD:

Log files:

20-02-2016 19:25:29.325 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:25:48.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:11.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:22.522 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:33.317 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:43.202 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:53.594 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:04.931 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:14.657 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:25.383 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:36.587 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:46.124 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:57.708 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:28:08.325 UTC 200 GET /ping 0.000000 seconds


Process 'ntp_process'               Running
System 'node-homestead'             Running
Process 'nginx_process'             Running
Process 'homestead_process'         Running
Program 'poll_homestead'            Status ok
Process 'homestead-prov_process'    Running
Program 'poll_homestead-prov'       Status ok
Process 'clearwater_queue_manager'  Running
Process 'etcd_process'              Running
Program 'poll_etcd_cluster'         Waiting
Program 'poll_etcd'                 Status ok
Process 'clearwater_diags_monitor_process' Running
Process 'clearwater_config_manager' Running
Process 'clearwater_cluster_manager' Running
Process 'cassandra_process'         Running
Program 'poll_cassandra'            Status ok


Ellis:

I can't to upload shared_config when i try to enter line:

/usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config

Some errors:

root at ellis:/home/ubuntu# /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config
Upload shared configuration failed to http://10.12.0.66:4000/v2/keys/clearwater/site1/configuration/shared_config
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   684  100    35  100   649      5    108  0:00:07  0:00:06  0:00:01     0

What's going on?

Thnaks?


Att.,
Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160223/53987517/attachment.html>

From moreira_r at outlook.com  Wed Feb 24 12:27:12 2016
From: moreira_r at outlook.com (Rodrigo Moreira)
Date: Wed, 24 Feb 2016 14:27:12 -0300
Subject: [Clearwater] [Error when i try to upload shared_config]
In-Reply-To: <SN1PR0201MB18567B8B6AC95395509454ABE1A40@SN1PR0201MB1856.namprd02.prod.outlook.com>
References: <BAY181-W555F40C53B4376D7CC39199EA10@phx.gbl>, ,
	<SN1PR0201MB18560AC68412A55E8ABA8B6EE1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>,
	,
	<SN1PR0201MB1856DD18B48A5E8F85FF5199E1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>,
	<BAY181-W4014AFE75D1DAD61C4DC059EA30@phx.gbl>,
	<SN1PR0201MB1856ACF6501A6E11756FABF3E1A40@SN1PR0201MB1856.namprd02.prod.outlook.com>,
	<BLUPR02MB12516DE3D0E4A41C24F9C82B9BA40@BLUPR02MB1251.namprd02.prod.outlook.com>,
	<SN1PR0201MB18567B8B6AC95395509454ABE1A40@SN1PR0201MB1856.namprd02.prod.outlook.com>
Message-ID: <BAY181-W773824E2650BF0ACEB4DD19EA50@phx.gbl>







Right,

When I try to access the Web Ellis I received the following error message:


404 Not Found
nginx/1.4.6 (Ubuntu)


When I try 'monit summary':

Process 'ntp_process'               Running
System 'node-ellis'                 Running
Process 'nginx_process'             Running
Process 'mysql_process'             Running
Process 'ellis_process'             Does not exist
Program 'poll_ellis'                Initializing
Program 'poll_ellis_https'          Initializing
Process 'clearwater_queue_manager'  Running
Process 'etcd_process'              Running
Program 'poll_etcd_cluster'         Waiting
Program 'poll_etcd'                 Status ok
Process 'clearwater_diags_monitor_process' Running
Process 'clearwater_config_manager' Running
Process 'clearwater_cluster_manager' Running


After to try reinstall Ellis node, follow the log files:

clearwater-etcdctl cluster-health

cluster is healthy
member 287826f92828e44 is healthy
member 54555c3e9f5e5117 is healthy
member b483fdec02f4e3c4 is healthy
member bd0a983db9f96bdf is healthy
member cadfb4ac52ff0cb3 is healthy
member f35d50c60a752f92 is healthy


  
clearwater-etcdctl member list

287826f92828e44: name=10-12-0-79 peerURLs=http://10.12.0.79:2380 clientURLs=http://10.12.0.79:4000
54555c3e9f5e5117: name=10-12-0-74 peerURLs=http://10.12.0.74:2380 clientURLs=http://10.12.0.74:4000
b483fdec02f4e3c4: name=10-12-0-76 peerURLs=http://10.12.0.76:2380 clientURLs=http://10.12.0.76:4000
bd0a983db9f96bdf: name=10-12-0-75 peerURLs=http://10.12.0.75:2380 clientURLs=http://10.12.0.75:4000
cadfb4ac52ff0cb3: name=10-12-0-77 peerURLs=http://10.12.0.77:2380 clientURLs=http://10.12.0.77:4000
f35d50c60a752f92: name=10-12-0-78 peerURLs=http://10.12.0.78:2380 clientURLs=http://10.12.0.78:4000

clearwater-config-manager:

23-02-2016 23:58:42.565 UTC INFO common_etcd_synchronizer.py:180 (thread SharedConfigPlugin): Watching for changes with 68


clearwater-cluster-manager:

23-02-2016 23:39:07.534 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-cluster-manager.pid.lockfile
23-02-2016 23:39:07.663 UTC ERROR logging_config.py:101 (thread MainThread): Uncaught exception:
  Exception: OSError
  Detail: [Errno 2] No such file or directory: '/usr/share/clearwater/clearwater-cluster-manager/plugins/'
  Traceback:
    File "/usr/share/clearwater/bin/clearwater-cluster-manager", line 40, in <module>
    main(sys.argv[1:])
  File "build_clustermgr/bdist.linux-x86_64/egg/metaswitch/clearwater/cluster_manager/main.py", line 169, in main
    etcd_cluster_key=etcd_cluster_key))
  File "build_shared/bdist.linux-x86_64/egg/metaswitch/clearwater/etcd_shared/plugin_loader.py", line 47, in load_plugins_in_dir
    files = os.listdir(dir)

23-02-2016 23:39:47.481 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-cluster-manager.pid.lockfile
23-02-2016 23:39:47.481 UTC INFO main.py:210 (thread MainThread): No plugin threads running, waiting for a SIGTERM or SIGQUIT

clearwater-etcd:

2016/02/24 00:20:01 etcdhttp: [GET] /v2/keys/clearwater/site1/configuration/shared_config?waitIndex=68&recursive=false&wait=true remote:10.12.0.77:49953
2016/02/24 00:20:01 etcdhttp: [GET] /v2/keys/clearwater/site1/configuration/apply_config?waitIndex=75&recursive=false&wait=true remote:10.12.0.77:49954
2016/02/24 00:20:06 etcdhttp: [GET] /v2/keys/clearwater/site1/configuration/shared_config?waitIndex=68&recursive=false&wait=true remote:10.12.0.77:49955
2016/02/24 00:20:06 etcdhttp: [GET] /v2/keys/clearwater/site1/configuration/apply_config?waitIndex=75&recursive=false&wait=true remote:10.12.0.77:49956
2016/02/24 00:20:11 etcdhttp: [GET] /v2/stats/self remote:10.12.0.77:49958
2016/02/24 00:20:11 etcdhttp: [GET] /v2/keys/clearwater/site1/configuration/apply_config?waitIndex=75&recursive=false&wait=true remote:10.12.0.77:49960
2016/02/24 00:20:11 etcdhttp: [GET] /v2/keys/clearwater/site1/configuration/shared_config?waitIndex=68&recursive=false&wait=true remote:10.12.0.77:49961


Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007

From: Chris.Elford at metaswitch.com
To: moreira_r at outlook.com
CC: clearwater at lists.projectclearwater.org; Eleanor.Merry at metaswitch.com
Subject: RE: [Clearwater] [Error when i try to upload shared_config]
Date: Tue, 23 Feb 2016 17:39:24 +0000









Hi Rodrigo,
 
It looks like the error message that I saw below is benign. We need to figure out what is wrong with etcd.

 
Please send over:
?      
the logs from /var/log/clearwater-config-manager
?      
your local config file (/etc/clearwater/local_config).
 
We might need some etcd diagnostics. Please send us the results of running the following commands.
?      
clearwater-etcdctl cluster-health
?      
clearwater-etcdctl member list
 
Yours,
 
Chris
 


From: Eleanor Merry


Sent: 23 February 2016 16:19

To: Chris Elford <Chris.Elford at metaswitch.com>; Rodrigo Moreira <moreira_r at outlook.com>

Cc: clearwater at lists.projectclearwater.org

Subject: RE: [Clearwater] [Error when i try to upload shared_config]


 
Hi Rodrigo,
 
We?re looking into this at the moment ? can you please not delete your Ellis node yet?
 
Thanks,
 
Ellie
 


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
On Behalf Of Chris Elford

Sent: 23 February 2016 15:57

To: Rodrigo Moreira

Cc: clearwater at lists.projectclearwater.org

Subject: Re: [Clearwater] [Error when i try to upload shared_config]


 
Hi Rodrigo,
 
It looks like your system does not have all of the directories that Clearwater expects to exist. In particular, you are missing the
 folder '/usr/share/clearwater/clearwater-cluster-manager/plugins/', which is normally created when clearwater-cluster-manager is installed.
 
I suspect that the easiest thing to do to get your Ellis node working might be to save off its configuration (/etc/clearwater/local_config
 and /etc/clearwater/shared_config), delete the node, and re-install it from scratch. You can then copy on the saved-off config and try uploading it again.
 
If that does not work (or if you are not able to do that) then we will need more details of how you installed your Ellis node.
 
Yours,
 
Chris
 


From: Rodrigo Moreira [mailto:moreira_r at outlook.com]


Sent: 22 February 2016 22:18

To: Chris Elford <Chris.Elford at metaswitch.com>

Cc: clearwater at lists.projectclearwater.org

Subject: RE: [Clearwater] [Error when i try to upload shared_config]


 

Sure,



The Ellis log:





cluster-maneger:



22-02-2016 22:13:26.514 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-cluster-manager.pid.lockfile

22-02-2016 22:13:26.527 UTC ERROR logging_config.py:101 (thread MainThread): Uncaught exception:

  Exception: OSError

  Detail: [Errno 2] No such file or directory: '/usr/share/clearwater/clearwater-cluster-manager/plugins/'

  Traceback:

    File "/usr/share/clearwater/bin/clearwater-cluster-manager", line 40, in <module>

    main(sys.argv[1:])

  File "build_clustermgr/bdist.linux-x86_64/egg/metaswitch/clearwater/cluster_manager/main.py", line 169, in main

    etcd_cluster_key=etcd_cluster_key))

  File "build_shared/bdist.linux-x86_64/egg/metaswitch/clearwater/etcd_shared/plugin_loader.py", line 47, in load_plugins_in_dir

    files = os.listdir(dir)



config-maneger:



22-02-2016 22:09:21.237 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-config-manager.pid.lockfile

22-02-2016 22:09:21.238 UTC INFO plugin_loader.py:50 (thread MainThread): Inspecting shared_config_plugin.py

22-02-2016 22:09:21.281 UTC INFO plugin_loader.py:58 (thread MainThread): Loading shared_config_plugin.py

22-02-2016 22:09:21.282 UTC INFO plugin_loader.py:60 (thread MainThread): Loaded shared_config_plugin.py successfully

22-02-2016 22:09:21.314 UTC INFO alarms.py:55 (thread MainThread): Imported /usr/share/clearwater/bin/alarms.py

22-02-2016 22:09:23.418 UTC INFO main.py:136 (thread MainThread): Loaded plugin <shared_config_plugin.SharedConfigPlugin object at 0x7f3324bbe850>

22-02-2016 22:09:31.138 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $

22-02-2016 22:10:08.493 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $

22-02-2016 22:10:44.514 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $

22-02-2016 22:11:20.546 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $

22-02-2016 22:11:56.578 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $

22-02-2016 22:12:32.596 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $

22-02-2016 22:13:08.628 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $

22-02-2016 22:13:44.654 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $

22-02-2016 22:14:20.687 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $

22-02-2016 22:14:56.715 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $

22-02-2016 22:15:32.740 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $





What's wrong?



Thanks.



Att.,

Rodrigo M.

+ 55 37 - 9 9132-4539

Skype: rodrigo.moreira2007




From:
Chris.Elford at metaswitch.com

To: Chris.Elford at metaswitch.com;
moreira_r at outlook.com

CC: clearwater at lists.projectclearwater.org

Subject: RE: [Clearwater] [Error when i try to upload shared_config]

Date: Mon, 22 Feb 2016 17:29:23 +0000

Sorry, I slipped up there.

 
The etcd logs are in /var/log/clearwater-etcd, /var/log/clearwater-cluster-manager, and /var/log/clearwater-config-manager.
 
Chris
 


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
On Behalf Of Chris Elford

Sent: 22 February 2016 17:25

To: Rodrigo Moreira <moreira_r at outlook.com>

Cc: clearwater at lists.projectclearwater.org

Subject: Re: [Clearwater] [Error when i try to upload shared_config]


 
Hi Rodrigo,
 
It looks like your Ellis node is having trouble talking to the etcd cluster and uploading its configuration. Can you please send us the etcd logs from your Ellis
 node? You can find these at /var/log/ellis. It would also help to know when you attempted to upload shared config ? do you have a note of the time at which you ran the command?
 
Yours,
 
Chris
 


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
On Behalf Of Rodrigo Moreira

Sent: 20 February 2016 19:48

To: Eleanor Merry <Eleanor.Merry at metaswitch.com>;
clearwater at lists.projectclearwater.org

Subject: [Clearwater] [Error when i try to upload shared_config]


 

Hi, 


Could you
please
tell
what happens?

Follows logs:



BONO:


I can ping
sprout.nyc3.example.com on the host bono.nyc3.example.com


Log files:





20-02-2016 13:57:21.194 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 13:57:21.195 UTC Status bono.cpp:3316: Create list of PBXes

20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins

20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:148: Finished loading plug-ins

20-02-2016 13:57:21.197 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:57:36.207 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:57:51.214 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:58:06.228 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:58:21.235 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:58:36.249 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:58:51.263 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:59:06.277 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:59:21.279 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:59:36.294 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 13:59:51.306 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)




SPROUT:



Log files:



20-02-2016 19:19:04.990 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:19:14.962 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:19:25.086 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:19:35.096 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:19:45.181 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:19:55.167 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:05.169 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:15.221 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:25.345 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:35.360 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:45.342 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:55.359 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:05.475 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:15.465 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:25.501 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:35.547 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:45.606 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:55.596 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:05.568 UTC 200 GET /ping 0.000000 seconds





RALF:



Log files:





20-02-2016 19:19:54.153 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:04.177 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:14.175 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:24.216 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:34.266 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:44.287 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:20:54.368 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:04.329 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:14.369 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:24.370 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:34.368 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:44.387 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:21:54.468 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:04.474 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:14.504 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:24.550 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:34.523 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:44.580 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:22:54.561 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:23:04.648 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:23:14.718 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:23:24.648 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:23:34.769 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:23:44.689 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:23:54.762 UTC 200 GET /ping 0.000000 seconds





HOMESTEAD:



Log files:



20-02-2016 19:25:29.325 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:25:48.561 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:26:11.169 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:26:22.522 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:26:33.317 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:26:43.202 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:26:53.594 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:27:04.931 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:27:14.657 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:27:25.383 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:27:36.587 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:27:46.124 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:27:57.708 UTC 200 GET /ping 0.000000 seconds

20-02-2016 19:28:08.325 UTC 200 GET /ping 0.000000 seconds





Process 'ntp_process'               Running

System 'node-homestead'             Running

Process 'nginx_process'             Running

Process 'homestead_process'         Running

Program 'poll_homestead'            Status ok

Process 'homestead-prov_process'    Running

Program 'poll_homestead-prov'       Status ok

Process 'clearwater_queue_manager'  Running

Process 'etcd_process'              Running

Program 'poll_etcd_cluster'         Waiting

Program 'poll_etcd'                 Status ok

Process 'clearwater_diags_monitor_process' Running

Process 'clearwater_config_manager' Running

Process 'clearwater_cluster_manager' Running

Process 'cassandra_process'         Running

Program 'poll_cassandra'            Status ok





Ellis:



I can't to upload shared_config when i try to enter line:
/usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config

Some errors:

root at ellis:/home/ubuntu# /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config 
Upload shared configuration failed to http://10.12.0.66:4000/v2/keys/clearwater/site1/configuration/shared_config
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   684  100    35  100   649      5    108  0:00:07  0:00:06  0:00:01     0


What's going on?



Thnaks?





Att.,

Rodrigo M.

+ 55 37 - 9 9132-4539

Skype: rodrigo.moreira2007 







 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160224/baf26cdb/attachment.html>

From jiaxuan at chinamobile.com  Thu Feb 25 03:44:17 2016
From: jiaxuan at chinamobile.com (jiaxuan)
Date: Thu, 25 Feb 2016 16:44:17 +0800
Subject: [Project Clearwater] Test Clearwater on Openshift
Message-ID: <008501d16fa8$b7658e50$2630aaf0$@com>

Hi list:

  

  I have installed clearwater, following "Manual Install Instrutions". The
difference is that it's running on Openshift which is using Kurbernetes.
Currently the Ellis model is running well, as I can make my X-Lite client
registered in. The problem is that: The call function doesn't work properly.
I list what I have found:

  1. Ralf: from /var/log/ralf/ralf_2016XXX

    24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1413: Sending
Diameter message of type 271 on transaction 0x7f17d0005100

24-02-2016 13:00:34.447 UTC Debug diameterstack.cpp:397: Routing out
callback from freeDiameter

24-02-2016 13:00:34.447 UTC Error diameterstack.cpp:293: Routing error: 'No
remaining suitable candidate to route the message to' for message with
Command-Code 271, Destination-Host  and Destination-Realm example.com

24-02-2016 13:00:34.447 UTC Debug freeDiameter: Iterating on rules of
COMMAND: '(generic error format)'.

24-02-2016 13:00:34.447 UTC Debug freeDiameter: Calling callback registered
when query was sent (0x437910, 0x7f17d0005100)

24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1093: Got Diameter
response of type 271 - calling callback on transaction 0x7f17d0005100

24-02-2016 13:00:34.447 UTC Warning peer_message_sender.cpp:125: Failed to
send ACR to  (number 0)

24-02-2016 13:00:34.447 UTC Error peer_message_sender.cpp:145: Failed to
connect to all CCFs, message not sent

24-02-2016 13:00:34.447 UTC Warning session_manager.cpp:319: Session for
79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY received error from CDF

24-02-2016 13:00:34.470 UTC Verbose httpstack.cpp:286: Process request for
URL /call-id/79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY, args (null)

  2. Sprout:from /var/log/sprout/sprout_current.txt . We can see there're
several 'tombstone' element in this json. 

25-02-2016 08:05:00.076 UTC Debug avstore.cpp:72: Set AV for
6505550502 at example.com\7b66730d304dcdcb

{"digest":{"ha1":"3fb2ca6bb49c77b0907e080079d325a0","realm":"example.com","q
op":"auth"},"branch":"z9hG4bKPjbALlVrXogjQKxNGPi1wmdGhDKXTFBr7h","tombstone"
:true,"tombstone":true,"tombstone":true}

 

  I attached all the log files in this email. 

We appreciate your response. Thanks. 

                                                      

Jia Xuan

China Mobile Research Institute

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160225/9dbcf6b3/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: clearwater-log-ose.tgz
Type: application/octet-stream
Size: 2007310 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160225/9dbcf6b3/attachment.obj>

From Chris.Elford at metaswitch.com  Thu Feb 25 10:41:17 2016
From: Chris.Elford at metaswitch.com (Chris Elford)
Date: Thu, 25 Feb 2016 15:41:17 +0000
Subject: [Project Clearwater] [Clearwater] [Error when i try to upload
	shared_config]
In-Reply-To: <BAY181-W773824E2650BF0ACEB4DD19EA50@phx.gbl>
References: <BAY181-W555F40C53B4376D7CC39199EA10@phx.gbl>, ,
	<SN1PR0201MB18560AC68412A55E8ABA8B6EE1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>,
	,
	<SN1PR0201MB1856DD18B48A5E8F85FF5199E1A30@SN1PR0201MB1856.namprd02.prod.outlook.com>,
	<BAY181-W4014AFE75D1DAD61C4DC059EA30@phx.gbl>,
	<SN1PR0201MB1856ACF6501A6E11756FABF3E1A40@SN1PR0201MB1856.namprd02.prod.outlook.com>,
	<BLUPR02MB12516DE3D0E4A41C24F9C82B9BA40@BLUPR02MB1251.namprd02.prod.outlook.com>,
	<SN1PR0201MB18567B8B6AC95395509454ABE1A40@SN1PR0201MB1856.namprd02.prod.outlook.com>
	<BAY181-W773824E2650BF0ACEB4DD19EA50@phx.gbl>
Message-ID: <SN1PR0201MB18564934106A48DF36B13480E1A60@SN1PR0201MB1856.namprd02.prod.outlook.com>

Thanks,

The output from `monit summary` shows that Ellis is no longer running on your node. The next step is to find out why it is not running. For that, we will need to look at the Ellis logs. You can find these at /var/log/ellis.

Yours,

Chris

From: Rodrigo Moreira [mailto:moreira_r at outlook.com]
Sent: 24 February 2016 17:27
To: Chris Elford <Chris.Elford at metaswitch.com>; Eleanor Merry <Eleanor.Merry at metaswitch.com>; clearwater at lists.projectclearwater.org
Subject: RE: [Clearwater] [Error when i try to upload shared_config]

Right,

When I try to access the Web Ellis I received the following error message:


404 Not Found
________________________________
nginx/1.4.6 (Ubuntu)



When I try 'monit summary':

Process 'ntp_process'               Running
System 'node-ellis'                 Running
Process 'nginx_process'             Running
Process 'mysql_process'             Running
Process 'ellis_process'             Does not exist
Program 'poll_ellis'                Initializing
Program 'poll_ellis_https'          Initializing
Process 'clearwater_queue_manager'  Running
Process 'etcd_process'              Running
Program 'poll_etcd_cluster'         Waiting
Program 'poll_etcd'                 Status ok
Process 'clearwater_diags_monitor_process' Running
Process 'clearwater_config_manager' Running
Process 'clearwater_cluster_manager' Running


After to try reinstall Ellis node, follow the log files:

clearwater-etcdctl cluster-health

cluster is healthy
member 287826f92828e44 is healthy
member 54555c3e9f5e5117 is healthy
member b483fdec02f4e3c4 is healthy
member bd0a983db9f96bdf is healthy
member cadfb4ac52ff0cb3 is healthy
member f35d50c60a752f92 is healthy


   clearwater-etcdctl member list

287826f92828e44: name=10-12-0-79 peerURLs=http://10.12.0.79:2380 clientURLs=http://10.12.0.79:4000
54555c3e9f5e5117: name=10-12-0-74 peerURLs=http://10.12.0.74:2380 clientURLs=http://10.12.0.74:4000
b483fdec02f4e3c4: name=10-12-0-76 peerURLs=http://10.12.0.76:2380 clientURLs=http://10.12.0.76:4000
bd0a983db9f96bdf: name=10-12-0-75 peerURLs=http://10.12.0.75:2380 clientURLs=http://10.12.0.75:4000
cadfb4ac52ff0cb3: name=10-12-0-77 peerURLs=http://10.12.0.77:2380 clientURLs=http://10.12.0.77:4000
f35d50c60a752f92: name=10-12-0-78 peerURLs=http://10.12.0.78:2380 clientURLs=http://10.12.0.78:4000

clearwater-config-manager:

23-02-2016 23:58:42.565 UTC INFO common_etcd_synchronizer.py:180 (thread SharedConfigPlugin): Watching for changes with 68


clearwater-cluster-manager:

23-02-2016 23:39:07.534 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-cluster-manager.pid.lockfile
23-02-2016 23:39:07.663 UTC ERROR logging_config.py:101 (thread MainThread): Uncaught exception:
  Exception: OSError
  Detail: [Errno 2] No such file or directory: '/usr/share/clearwater/clearwater-cluster-manager/plugins/'
  Traceback:
    File "/usr/share/clearwater/bin/clearwater-cluster-manager", line 40, in <module>
    main(sys.argv[1:])
  File "build_clustermgr/bdist.linux-x86_64/egg/metaswitch/clearwater/cluster_manager/main.py", line 169, in main
    etcd_cluster_key=etcd_cluster_key))
  File "build_shared/bdist.linux-x86_64/egg/metaswitch/clearwater/etcd_shared/plugin_loader.py", line 47, in load_plugins_in_dir
    files = os.listdir(dir)

23-02-2016 23:39:47.481 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-cluster-manager.pid.lockfile
23-02-2016 23:39:47.481 UTC INFO main.py:210 (thread MainThread): No plugin threads running, waiting for a SIGTERM or SIGQUIT

clearwater-etcd:

2016/02/24 00:20:01 etcdhttp: [GET] /v2/keys/clearwater/site1/configuration/shared_config?waitIndex=68&recursive=false&wait=true remote:10.12.0.77:49953
2016/02/24 00:20:01 etcdhttp: [GET] /v2/keys/clearwater/site1/configuration/apply_config?waitIndex=75&recursive=false&wait=true remote:10.12.0.77:49954
2016/02/24 00:20:06 etcdhttp: [GET] /v2/keys/clearwater/site1/configuration/shared_config?waitIndex=68&recursive=false&wait=true remote:10.12.0.77:49955
2016/02/24 00:20:06 etcdhttp: [GET] /v2/keys/clearwater/site1/configuration/apply_config?waitIndex=75&recursive=false&wait=true remote:10.12.0.77:49956
2016/02/24 00:20:11 etcdhttp: [GET] /v2/stats/self remote:10.12.0.77:49958
2016/02/24 00:20:11 etcdhttp: [GET] /v2/keys/clearwater/site1/configuration/apply_config?waitIndex=75&recursive=false&wait=true remote:10.12.0.77:49960
2016/02/24 00:20:11 etcdhttp: [GET] /v2/keys/clearwater/site1/configuration/shared_config?waitIndex=68&recursive=false&wait=true remote:10.12.0.77:49961


Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007
________________________________
From: Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>
To: moreira_r at outlook.com<mailto:moreira_r at outlook.com>
CC: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Eleanor.Merry at metaswitch.com<mailto:Eleanor.Merry at metaswitch.com>
Subject: RE: [Clearwater] [Error when i try to upload shared_config]
Date: Tue, 23 Feb 2016 17:39:24 +0000

Hi Rodrigo,



It looks like the error message that I saw below is benign. We need to figure out what is wrong with etcd.



Please send over:

*       the logs from /var/log/clearwater-config-manager

*       your local config file (/etc/clearwater/local_config).



We might need some etcd diagnostics. Please send us the results of running the following commands.

*       clearwater-etcdctl cluster-health

*       clearwater-etcdctl member list



Yours,



Chris



From: Eleanor Merry
Sent: 23 February 2016 16:19
To: Chris Elford <Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>>; Rodrigo Moreira <moreira_r at outlook.com<mailto:moreira_r at outlook.com>>
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Clearwater] [Error when i try to upload shared_config]



Hi Rodrigo,



We're looking into this at the moment - can you please not delete your Ellis node yet?



Thanks,



Ellie



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Chris Elford
Sent: 23 February 2016 15:57
To: Rodrigo Moreira
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Clearwater] [Error when i try to upload shared_config]



Hi Rodrigo,



It looks like your system does not have all of the directories that Clearwater expects to exist. In particular, you are missing the folder '/usr/share/clearwater/clearwater-cluster-manager/plugins/', which is normally created when clearwater-cluster-manager is installed.



I suspect that the easiest thing to do to get your Ellis node working might be to save off its configuration (/etc/clearwater/local_config and /etc/clearwater/shared_config), delete the node, and re-install it from scratch. You can then copy on the saved-off config and try uploading it again.



If that does not work (or if you are not able to do that) then we will need more details of how you installed your Ellis node.



Yours,



Chris



From: Rodrigo Moreira [mailto:moreira_r at outlook.com]
Sent: 22 February 2016 22:18
To: Chris Elford <Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>>
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Clearwater] [Error when i try to upload shared_config]



Sure,

The Ellis log:


cluster-maneger:

22-02-2016 22:13:26.514 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-cluster-manager.pid.lockfile
22-02-2016 22:13:26.527 UTC ERROR logging_config.py:101 (thread MainThread): Uncaught exception:
  Exception: OSError
  Detail: [Errno 2] No such file or directory: '/usr/share/clearwater/clearwater-cluster-manager/plugins/'
  Traceback:
    File "/usr/share/clearwater/bin/clearwater-cluster-manager", line 40, in <module>
    main(sys.argv[1:])
  File "build_clustermgr/bdist.linux-x86_64/egg/metaswitch/clearwater/cluster_manager/main.py", line 169, in main
    etcd_cluster_key=etcd_cluster_key))
  File "build_shared/bdist.linux-x86_64/egg/metaswitch/clearwater/etcd_shared/plugin_loader.py", line 47, in load_plugins_in_dir
    files = os.listdir(dir)

config-maneger:

22-02-2016 22:09:21.237 UTC INFO utils.py:417 (thread MainThread): Acquired exclusive lock on /var/run/clearwater-config-manager.pid.lockfile
22-02-2016 22:09:21.238 UTC INFO plugin_loader.py:50 (thread MainThread): Inspecting shared_config_plugin.py
22-02-2016 22:09:21.281 UTC INFO plugin_loader.py:58 (thread MainThread): Loading shared_config_plugin.py
22-02-2016 22:09:21.282 UTC INFO plugin_loader.py:60 (thread MainThread): Loaded shared_config_plugin.py successfully
22-02-2016 22:09:21.314 UTC INFO alarms.py:55 (thread MainThread): Imported /usr/share/clearwater/bin/alarms.py
22-02-2016 22:09:23.418 UTC INFO main.py:136 (thread MainThread): Loaded plugin <shared_config_plugin.SharedConfigPlugin object at 0x7f3324bbe850>
22-02-2016 22:09:31.138 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:10:08.493 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:10:44.514 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:11:20.546 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:11:56.578 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:12:32.596 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:13:08.628 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:13:44.654 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:14:20.687 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:14:56.715 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $
22-02-2016 22:15:32.740 UTC ERROR common_etcd_synchronizer.py:216 (thread SharedConfigPlugin): 10.12.0.70 caught EtcdException('Internal Server Error : None',) when trying to read with index None - pause $


What's wrong?

Thanks.

Att.,
Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007

________________________________

From: Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>
To: Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>; moreira_r at outlook.com<mailto:moreira_r at outlook.com>
CC: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Clearwater] [Error when i try to upload shared_config]
Date: Mon, 22 Feb 2016 17:29:23 +0000

Sorry, I slipped up there.



The etcd logs are in /var/log/clearwater-etcd, /var/log/clearwater-cluster-manager, and /var/log/clearwater-config-manager.



Chris



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Chris Elford
Sent: 22 February 2016 17:25
To: Rodrigo Moreira <moreira_r at outlook.com<mailto:moreira_r at outlook.com>>
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Clearwater] [Error when i try to upload shared_config]



Hi Rodrigo,



It looks like your Ellis node is having trouble talking to the etcd cluster and uploading its configuration. Can you please send us the etcd logs from your Ellis node? You can find these at /var/log/ellis. It would also help to know when you attempted to upload shared config - do you have a note of the time at which you ran the command?



Yours,



Chris



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Rodrigo Moreira
Sent: 20 February 2016 19:48
To: Eleanor Merry <Eleanor.Merry at metaswitch.com<mailto:Eleanor.Merry at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] [Error when i try to upload shared_config]



Hi,

Could you please tell what happens?
Follows logs:

BONO:

  *   I can ping sprout.nyc3.example.com on the host bono.nyc3.example.com

Log files:


20-02-2016 13:57:21.194 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Error connection_pool.cpp:214: Failed to resolve 5054 to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 13:57:21.195 UTC Status bono.cpp:3316: Create list of PBXes
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins
20-02-2016 13:57:21.195 UTC Status pluginloader.cpp:148: Finished loading plug-ins
20-02-2016 13:57:21.197 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:36.207 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:57:51.214 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:06.228 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:21.235 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:36.249 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:58:51.263 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:06.277 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:21.279 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:36.294 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 13:59:51.306 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)
20-02-2016 19:17:18.040 UTC Error connection_pool.cpp:214: Failed to resolve sprout.nyc3.example.com to an IP address - Not found (PJ_ENOTFOUND)

SPROUT:

Log files:

20-02-2016 19:19:04.990 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:14.962 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:25.086 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:35.096 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:45.181 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:19:55.167 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:05.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:15.221 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:25.345 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:35.360 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:45.342 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:55.359 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:05.475 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:15.465 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:25.501 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:35.547 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:45.606 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:55.596 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:05.568 UTC 200 GET /ping 0.000000 seconds


RALF:

Log files:


20-02-2016 19:19:54.153 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:04.177 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:14.175 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:24.216 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:34.266 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:44.287 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:20:54.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:04.329 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:14.369 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:24.370 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:34.368 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:44.387 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:21:54.468 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:04.474 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:14.504 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:24.550 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:34.523 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:44.580 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:22:54.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:04.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:14.718 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:24.648 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:34.769 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:44.689 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:23:54.762 UTC 200 GET /ping 0.000000 seconds


HOMESTEAD:

Log files:

20-02-2016 19:25:29.325 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:25:48.561 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:11.169 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:22.522 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:33.317 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:43.202 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:26:53.594 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:04.931 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:14.657 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:25.383 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:36.587 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:46.124 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:27:57.708 UTC 200 GET /ping 0.000000 seconds
20-02-2016 19:28:08.325 UTC 200 GET /ping 0.000000 seconds


Process 'ntp_process'               Running
System 'node-homestead'             Running
Process 'nginx_process'             Running
Process 'homestead_process'         Running
Program 'poll_homestead'            Status ok
Process 'homestead-prov_process'    Running
Program 'poll_homestead-prov'       Status ok
Process 'clearwater_queue_manager'  Running
Process 'etcd_process'              Running
Program 'poll_etcd_cluster'         Waiting
Program 'poll_etcd'                 Status ok
Process 'clearwater_diags_monitor_process' Running
Process 'clearwater_config_manager' Running
Process 'clearwater_cluster_manager' Running
Process 'cassandra_process'         Running
Program 'poll_cassandra'            Status ok


Ellis:

I can't to upload shared_config when i try to enter line:

/usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config

Some errors:

root at ellis:/home/ubuntu# /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config
Upload shared configuration failed to http://10.12.0.66:4000/v2/keys/clearwater/site1/configuration/shared_config
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   684  100    35  100   649      5    108  0:00:07  0:00:06  0:00:01     0

What's going on?

Thnaks?


Att.,
Rodrigo M.
+ 55 37 - 9 9132-4539
Skype: rodrigo.moreira2007
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160225/e765280e/attachment.html>

From Chris.Elford at metaswitch.com  Thu Feb 25 11:27:19 2016
From: Chris.Elford at metaswitch.com (Chris Elford)
Date: Thu, 25 Feb 2016 16:27:19 +0000
Subject: [Project Clearwater] Test Clearwater on Openshift
In-Reply-To: <008501d16fa8$b7658e50$2630aaf0$@com>
References: <008501d16fa8$b7658e50$2630aaf0$@com>
Message-ID: <SN1PR0201MB1856FD019CDE8B7CF5ACABD1E1A60@SN1PR0201MB1856.namprd02.prod.outlook.com>

Hi,

I've taken a look at your Sprout logs. In order to diagnose your problem, it would be useful to have some more information about what you are seeing.

*       What are the subscribers are you trying to call between?

*       Are you seeing all calls fail or just occasional ones?

We've looked into the two issues that you mentioned below.

It looks like Ralf is configured to send billing data to a CDF at example.com. The logs in your message show that it is failing to do so. You can configure a real CDF by changing the value of 'cdf_identity' in the deployment's shared config. See http://clearwater.readthedocs.org/en/latest/Clearwater_Configuration_Options_Reference/index.html for details.

We have looked into the tombstone issue that you mention below. We do not think that this will cause any problems. We have raised https://github.com/Metaswitch/sprout/issues/1334 to track working around it and cleaning up those entries.

Yours,

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of jiaxuan
Sent: 25 February 2016 08:44
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Test Clearwater on Openshift

Hi list:

  I have installed clearwater, following "Manual Install Instrutions". The difference is that it's running on Openshift which is using Kurbernetes. Currently the Ellis model is running well, as I can make my X-Lite client registered in. The problem is that: The call function doesn't work properly. I list what I have found:
  1. Ralf: from /var/log/ralf/ralf_2016XXX
    24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1413: Sending Diameter message of type 271 on transaction 0x7f17d0005100
24-02-2016 13:00:34.447 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
24-02-2016 13:00:34.447 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm example.com
24-02-2016 13:00:34.447 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
24-02-2016 13:00:34.447 UTC Debug freeDiameter: Calling callback registered when query was sent (0x437910, 0x7f17d0005100)
24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1093: Got Diameter response of type 271 - calling callback on transaction 0x7f17d0005100
24-02-2016 13:00:34.447 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
24-02-2016 13:00:34.447 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
24-02-2016 13:00:34.447 UTC Warning session_manager.cpp:319: Session for 79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY received error from CDF
24-02-2016 13:00:34.470 UTC Verbose httpstack.cpp:286: Process request for URL /call-id/79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY, args (null)
  2. Sprout:from /var/log/sprout/sprout_current.txt . We can see there're several 'tombstone' element in this json.
25-02-2016 08:05:00.076 UTC Debug avstore.cpp:72: Set AV for 6505550502 at example.com\7b66730d304dcdcb<mailto:6505550502 at example.com\7b66730d304dcdcb>
{"digest":{"ha1":"3fb2ca6bb49c77b0907e080079d325a0","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjbALlVrXogjQKxNGPi1wmdGhDKXTFBr7h","tombstone":true,"tombstone":true,"tombstone":true}

  I attached all the log files in this email.
We appreciate your response. Thanks.

Jia Xuan
China Mobile Research Institute
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160225/d92ed1c7/attachment.html>

From Chris.Elford at metaswitch.com  Fri Feb 26 06:59:25 2016
From: Chris.Elford at metaswitch.com (Chris Elford)
Date: Fri, 26 Feb 2016 11:59:25 +0000
Subject: [Project Clearwater] Test Clearwater on Openshift
In-Reply-To: <002101d17074$5c862ef0$15928cd0$@com>
References: <008501d16fa8$b7658e50$2630aaf0$@com>
	<SN1PR0201MB1856FD019CDE8B7CF5ACABD1E1A60@SN1PR0201MB1856.namprd02.prod.outlook.com>
	<002101d17074$5c862ef0$15928cd0$@com>
Message-ID: <SN1PR0201MB185664708CA9B1CD9778401EE1A70@SN1PR0201MB1856.namprd02.prod.outlook.com>

I notice below that you say you are using Openshift and Kubernetes. As far as I know, nobody is currently running Project Clearwater using these platforms, and I would not expect it to work without some development work to port it to this new platform.

I think that we need to understand what you did to allow you to install Project Clearwater on this new platform before we can provide any help.

Yours,

Chris

From: jiaxuan [mailto:jiaxuan at chinamobile.com]
Sent: 26 February 2016 09:02
To: Chris Elford <Chris.Elford at metaswitch.com>
Cc: clearwater at lists.projectclearwater.org
Subject: ??: [Project Clearwater] Test Clearwater on Openshift

Sorry? I have no idea about what?s ?subscriber?. It?s almost same with the environment which is setup by docker-compose (https://github.com/Metaswitch/clearwater-docker).  But all call fail.

I found a PUBLISH message : The content-Length is zero.

PUBLISH sip:6505550219 at example.com;transport=TCP SIP/2.0^M
Via: SIP/2.0/TCP 175.128.0.113:55316;branch=z9hG4bK-524287-1---4b2b34f926a6c876;rport^M
Max-Forwards: 70^M
Route: <sip:10.1.0.217:5054;transport=TCP;lr;orig>^M
Contact: <sip:6505550219 at 175.128.0.113:55316;transport=TCP>^M
To: "2323"<sip:6505550219 at example.com;transport=TCP>^M
From: "2323"<sip:6505550219 at example.com;transport=TCP>;tag=ea19e074^M
Call-ID: ZuNcniFDAQplqlNX3qyFJQ..^M
CSeq: 319 PUBLISH^M
Expires: 600^M
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE^M
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri^M
User-Agent: Z 3.9.32144 r32121^M
Event: presence^M
Allow-Events: presence, kpml^M
Content-Length: 0^M
^M
While checking the docker-compose env, I got
Max-Forwards: 70^M
Route: <sip:172.17.0.5:5054;transport=TCP;lr;orig>^M
Contact: <sip:6505550782 at 192.168.34.178:51158;transport=TCP>^M
To: "123"<sip:6505550782 at example.com;transport=TCP>^M
From: "123"<sip:6505550782 at example.com;transport=TCP>;tag=e72f4e32^M
Call-ID: vqETasJUaN4hD1h5scKyUA..^M
CSeq: 1 PUBLISH^M
Expires: 600^M
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE^M
Content-Type: application/pidf+xml^M
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri^M
User-Agent: Z 3.9.32144 r32121^M
Event: presence^M
Allow-Events: presence, kpml^M
Content-Length: 271^M
^M
<?xml version="1.0" encoding="UTF-8"?>^M
<presence xmlns="urn:ietf:params:xml:ns:pidf"^M
          entity="sip:6505550782 at example.com;transport=TCP">^M
  <tuple id="6505550782" >^M
     <status><basic>open</basic></status>^M
     <note>Online</note>^M
  </tuple>^M
</presence>^M

Is that helpful? Thanks.

Jia Xuan

???: Chris Elford [mailto:Chris.Elford at metaswitch.com]
????: 2016?2?26? 0:27
???: jiaxuan
??: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
??: RE: [Project Clearwater] Test Clearwater on Openshift

Hi,

I?ve taken a look at your Sprout logs. In order to diagnose your problem, it would be useful to have some more information about what you are seeing.

*       What are the subscribers are you trying to call between?

*       Are you seeing all calls fail or just occasional ones?

We?ve looked into the two issues that you mentioned below.

It looks like Ralf is configured to send billing data to a CDF at example.com. The logs in your message show that it is failing to do so. You can configure a real CDF by changing the value of ?cdf_identity? in the deployment?s shared config. See http://clearwater.readthedocs.org/en/latest/Clearwater_Configuration_Options_Reference/index.html for details.

We have looked into the tombstone issue that you mention below. We do not think that this will cause any problems. We have raised https://github.com/Metaswitch/sprout/issues/1334 to track working around it and cleaning up those entries.

Yours,

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of jiaxuan
Sent: 25 February 2016 08:44
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Test Clearwater on Openshift

Hi list:

  I have installed clearwater, following ?Manual Install Instrutions?. The difference is that it?s running on Openshift which is using Kurbernetes. Currently the Ellis model is running well, as I can make my X-Lite client registered in. The problem is that: The call function doesn?t work properly. I list what I have found:
  1. Ralf: from /var/log/ralf/ralf_2016XXX
    24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1413: Sending Diameter message of type 271 on transaction 0x7f17d0005100
24-02-2016 13:00:34.447 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
24-02-2016 13:00:34.447 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm example.com
24-02-2016 13:00:34.447 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
24-02-2016 13:00:34.447 UTC Debug freeDiameter: Calling callback registered when query was sent (0x437910, 0x7f17d0005100)
24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1093: Got Diameter response of type 271 - calling callback on transaction 0x7f17d0005100
24-02-2016 13:00:34.447 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
24-02-2016 13:00:34.447 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
24-02-2016 13:00:34.447 UTC Warning session_manager.cpp:319: Session for 79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY received error from CDF
24-02-2016 13:00:34.470 UTC Verbose httpstack.cpp:286: Process request for URL /call-id/79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY, args (null)
  2. Sprout:from /var/log/sprout/sprout_current.txt . We can see there?re several ?tombstone? element in this json.
25-02-2016 08:05:00.076 UTC Debug avstore.cpp:72: Set AV for 6505550502 at example.com\7b66730d304dcdcb<mailto:6505550502 at example.com\7b66730d304dcdcb>
{"digest":{"ha1":"3fb2ca6bb49c77b0907e080079d325a0","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjbALlVrXogjQKxNGPi1wmdGhDKXTFBr7h","tombstone":true,"tombstone":true,"tombstone":true}

  I attached all the log files in this email.
We appreciate your response. Thanks.

Jia Xuan
China Mobile Research Institute
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160226/944a69a3/attachment.html>

From Matt.Williams at metaswitch.com  Fri Feb 26 07:41:37 2016
From: Matt.Williams at metaswitch.com (Matt Williams)
Date: Fri, 26 Feb 2016 12:41:37 +0000
Subject: [Project Clearwater] Test Clearwater on Openshift
In-Reply-To: <SN1PR0201MB185664708CA9B1CD9778401EE1A70@SN1PR0201MB1856.namprd02.prod.outlook.com>
References: <008501d16fa8$b7658e50$2630aaf0$@com>
	<SN1PR0201MB1856FD019CDE8B7CF5ACABD1E1A60@SN1PR0201MB1856.namprd02.prod.outlook.com>
	<002101d17074$5c862ef0$15928cd0$@com>
	<SN1PR0201MB185664708CA9B1CD9778401EE1A70@SN1PR0201MB1856.namprd02.prod.outlook.com>
Message-ID: <CY1PR0201MB15639CE35CB2C5DEDDCB9E3390A70@CY1PR0201MB1563.namprd02.prod.outlook.com>

Jia,

I think I'm a bit more positive that we can make this work than Chris is - my understanding is that OpenShift is a Platform as a Service (Paas) built on Docker, and we've successfully run Clearwater under Docker before, so unless OpenShift disables something we rely on, Clearwater should run on OpenShift.

His concern was that you were trying to port Clearwater to run on Red Hat Enterprise Linux (RHEL), rather than under Ubuntu.  However, I think it should be possible to run Clearwater in its native Ubuntu, even if the host OS is RHEL.  Please can you confirm which guest OS your containers are running?  ...and have you made any changes to the clearwater-docker scripts?

When Chris asked about "subscribers", he was referring to the telephone numbers you're calling between.  How did you pick the numbers 6505550219 and 6505550782, for example?  Did you create them through the Ellis web UI?  Are these the only 2 subscribers you've created?

The diagnostics we've got so far haven't been conclusive, but there are three things that would be useful.

?         The complete logs from the output from the "docker build" and "docker compose" commands, showing all the packages being installed to build the images and then the images being deployed, just in case there's something odd going on there.

?         On the Sprout node, stop the sprout process (with "sudo service sprout stop"), wait 30s so that it has definitely restarted and then attempt a call, and send us the logs from that - it's possible that something is going wrong on start-up.

?         Clearwater has a built-in tool to gather the key diagnostics together.  On the Sprout node, run "/usr/share/clearwater/bin/gather_diags" and then gather the diagnostic dump from /var/clearwater-diags-monitor/dumps/*.tar.gz and send it to us.

Please let us know.

Thanks,

Matt

--

Matt Williams
Lead Architect, Project Clearwater
+44 (0) 20 8366 1177

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Chris Elford
Sent: 26 February 2016 11:59
To: jiaxuan <jiaxuan at chinamobile.com>
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Test Clearwater on Openshift

I notice below that you say you are using Openshift and Kubernetes. As far as I know, nobody is currently running Project Clearwater using these platforms, and I would not expect it to work without some development work to port it to this new platform.

I think that we need to understand what you did to allow you to install Project Clearwater on this new platform before we can provide any help.

Yours,

Chris

From: jiaxuan [mailto:jiaxuan at chinamobile.com]
Sent: 26 February 2016 09:02
To: Chris Elford <Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>>
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: ??: [Project Clearwater] Test Clearwater on Openshift

Sorry? I have no idea about what?s ?subscriber?. It?s almost same with the environment which is setup by docker-compose (https://github.com/Metaswitch/clearwater-docker).  But all call fail.

I found a PUBLISH message : The content-Length is zero.

PUBLISH sip:6505550219 at example.com;transport=TCP SIP/2.0^M
Via: SIP/2.0/TCP 175.128.0.113:55316;branch=z9hG4bK-524287-1---4b2b34f926a6c876;rport^M
Max-Forwards: 70^M
Route: <sip:10.1.0.217:5054;transport=TCP;lr;orig>^M
Contact: <sip:6505550219 at 175.128.0.113:55316;transport=TCP>^M
To: "2323"<sip:6505550219 at example.com;transport=TCP>^M
From: "2323"<sip:6505550219 at example.com;transport=TCP>;tag=ea19e074^M
Call-ID: ZuNcniFDAQplqlNX3qyFJQ..^M
CSeq: 319 PUBLISH^M
Expires: 600^M
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE^M
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri^M
User-Agent: Z 3.9.32144 r32121^M
Event: presence^M
Allow-Events: presence, kpml^M
Content-Length: 0^M
^M
While checking the docker-compose env, I got
Max-Forwards: 70^M
Route: <sip:172.17.0.5:5054;transport=TCP;lr;orig>^M
Contact: <sip:6505550782 at 192.168.34.178:51158;transport=TCP>^M
To: "123"<sip:6505550782 at example.com;transport=TCP>^M
From: "123"<sip:6505550782 at example.com;transport=TCP>;tag=e72f4e32^M
Call-ID: vqETasJUaN4hD1h5scKyUA..^M
CSeq: 1 PUBLISH^M
Expires: 600^M
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE^M
Content-Type: application/pidf+xml^M
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri^M
User-Agent: Z 3.9.32144 r32121^M
Event: presence^M
Allow-Events: presence, kpml^M
Content-Length: 271^M
^M
<?xml version="1.0" encoding="UTF-8"?>^M
<presence xmlns="urn:ietf:params:xml:ns:pidf"^M
          entity="sip:6505550782 at example.com;transport=TCP">^M
  <tuple id="6505550782" >^M
     <status><basic>open</basic></status>^M
     <note>Online</note>^M
  </tuple>^M
</presence>^M

Is that helpful? Thanks.

Jia Xuan

???: Chris Elford [mailto:Chris.Elford at metaswitch.com]
????: 2016?2?26? 0:27
???: jiaxuan
??: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
??: RE: [Project Clearwater] Test Clearwater on Openshift

Hi,

I?ve taken a look at your Sprout logs. In order to diagnose your problem, it would be useful to have some more information about what you are seeing.

*       What are the subscribers are you trying to call between?

*       Are you seeing all calls fail or just occasional ones?

We?ve looked into the two issues that you mentioned below.

It looks like Ralf is configured to send billing data to a CDF at example.com. The logs in your message show that it is failing to do so. You can configure a real CDF by changing the value of ?cdf_identity? in the deployment?s shared config. See http://clearwater.readthedocs.org/en/latest/Clearwater_Configuration_Options_Reference/index.html for details.

We have looked into the tombstone issue that you mention below. We do not think that this will cause any problems. We have raised https://github.com/Metaswitch/sprout/issues/1334 to track working around it and cleaning up those entries.

Yours,

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of jiaxuan
Sent: 25 February 2016 08:44
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Test Clearwater on Openshift

Hi list:

  I have installed clearwater, following ?Manual Install Instrutions?. The difference is that it?s running on Openshift which is using Kurbernetes. Currently the Ellis model is running well, as I can make my X-Lite client registered in. The problem is that: The call function doesn?t work properly. I list what I have found:
  1. Ralf: from /var/log/ralf/ralf_2016XXX
    24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1413: Sending Diameter message of type 271 on transaction 0x7f17d0005100
24-02-2016 13:00:34.447 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
24-02-2016 13:00:34.447 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm example.com
24-02-2016 13:00:34.447 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
24-02-2016 13:00:34.447 UTC Debug freeDiameter: Calling callback registered when query was sent (0x437910, 0x7f17d0005100)
24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1093: Got Diameter response of type 271 - calling callback on transaction 0x7f17d0005100
24-02-2016 13:00:34.447 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
24-02-2016 13:00:34.447 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
24-02-2016 13:00:34.447 UTC Warning session_manager.cpp:319: Session for 79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY received error from CDF
24-02-2016 13:00:34.470 UTC Verbose httpstack.cpp:286: Process request for URL /call-id/79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY, args (null)
  2. Sprout:from /var/log/sprout/sprout_current.txt . We can see there?re several ?tombstone? element in this json.
25-02-2016 08:05:00.076 UTC Debug avstore.cpp:72: Set AV for 6505550502 at example.com\7b66730d304dcdcb<mailto:6505550502 at example.com\7b66730d304dcdcb>
{"digest":{"ha1":"3fb2ca6bb49c77b0907e080079d325a0","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjbALlVrXogjQKxNGPi1wmdGhDKXTFBr7h","tombstone":true,"tombstone":true,"tombstone":true}

  I attached all the log files in this email.
We appreciate your response. Thanks.

Jia Xuan
China Mobile Research Institute
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160226/29a29e88/attachment.html>

From jiaxuan at chinamobile.com  Fri Feb 26 04:02:02 2016
From: jiaxuan at chinamobile.com (jiaxuan)
Date: Fri, 26 Feb 2016 17:02:02 +0800
Subject: [Project Clearwater] =?gb2312?b?tPC4tDogIFRlc3QgQ2xlYXJ3YXRlciBv?=
	=?gb2312?b?biBPcGVuc2hpZnQ=?=
In-Reply-To: <SN1PR0201MB1856FD019CDE8B7CF5ACABD1E1A60@SN1PR0201MB1856.namprd02.prod.outlook.com>
References: <008501d16fa8$b7658e50$2630aaf0$@com>
	<SN1PR0201MB1856FD019CDE8B7CF5ACABD1E1A60@SN1PR0201MB1856.namprd02.prod.outlook.com>
Message-ID: <002101d17074$5c862ef0$15928cd0$@com>

Sorry? I have no idea about what?s ?subscriber?. It?s almost same with
the environment which is setup by docker-compose
(https://github.com/Metaswitch/clearwater-docker).  But all call fail. 

 

I found a PUBLISH message : The content-Length is zero.  

 

PUBLISH sip:6505550219 at example.com;transport=TCP SIP/2.0^M

Via: SIP/2.0/TCP
175.128.0.113:55316;branch=z9hG4bK-524287-1---4b2b34f926a6c876;rport^M

Max-Forwards: 70^M

Route: <sip:10.1.0.217:5054;transport=TCP;lr;orig>^M

Contact: <sip:6505550219 at 175.128.0.113:55316;transport=TCP>^M

To: "2323"<sip:6505550219 at example.com;transport=TCP>^M

From: "2323"<sip:6505550219 at example.com;transport=TCP>;tag=ea19e074^M

Call-ID: ZuNcniFDAQplqlNX3qyFJQ..^M

CSeq: 319 PUBLISH^M

Expires: 600^M

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO,
SUBSCRIBE^M

Supported: replaces, norefersub, extended-refer, timer, outbound, path,
X-cisco-serviceuri^M

User-Agent: Z 3.9.32144 r32121^M

Event: presence^M

Allow-Events: presence, kpml^M

Content-Length: 0^M

^M



While checking the docker-compose env, I got           

Max-Forwards: 70^M

Route: <sip:172.17.0.5:5054;transport=TCP;lr;orig>^M

Contact: <sip:6505550782 at 192.168.34.178:51158;transport=TCP>^M

To: "123"<sip:6505550782 at example.com;transport=TCP>^M

From: "123"<sip:6505550782 at example.com;transport=TCP>;tag=e72f4e32^M

Call-ID: vqETasJUaN4hD1h5scKyUA..^M

CSeq: 1 PUBLISH^M

Expires: 600^M

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO,
SUBSCRIBE^M

Content-Type: application/pidf+xml^M

Supported: replaces, norefersub, extended-refer, timer, outbound, path,
X-cisco-serviceuri^M

User-Agent: Z 3.9.32144 r32121^M

Event: presence^M

Allow-Events: presence, kpml^M

Content-Length: 271^M

^M

<?xml version="1.0" encoding="UTF-8"?>^M

<presence xmlns="urn:ietf:params:xml:ns:pidf"^M

          entity="sip:6505550782 at example.com;transport=TCP">^M

  <tuple id="6505550782" >^M

     <status><basic>open</basic></status>^M

     <note>Online</note>^M

  </tuple>^M

</presence>^M

 

Is that helpful? Thanks. 

 

Jia Xuan

 

???: Chris Elford [mailto:Chris.Elford at metaswitch.com] 
????: 2016?2?26? 0:27
???: jiaxuan
??: clearwater at lists.projectclearwater.org
??: RE: [Project Clearwater] Test Clearwater on Openshift

 

Hi,

 

I?ve taken a look at your Sprout logs. In order to diagnose your problem,
it would be useful to have some more information about what you are seeing.

?       What are the subscribers are you trying to call between?

?       Are you seeing all calls fail or just occasional ones?

 

We?ve looked into the two issues that you mentioned below.

 

It looks like Ralf is configured to send billing data to a CDF at
example.com. The logs in your message show that it is failing to do so. You
can configure a real CDF by changing the value of ?cdf_identity? in the
deployment?s shared config. See
http://clearwater.readthedocs.org/en/latest/Clearwater_Configuration_Options
_Reference/index.html for details.

 

We have looked into the tombstone issue that you mention below. We do not
think that this will cause any problems. We have raised
https://github.com/Metaswitch/sprout/issues/1334 to track working around it
and cleaning up those entries.

 

Yours,

 

Chris

 

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On
Behalf Of jiaxuan
Sent: 25 February 2016 08:44
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Test Clearwater on Openshift

 

Hi list:

  

  I have installed clearwater, following ?Manual Install Instrutions?. The
difference is that it?s running on Openshift which is using Kurbernetes.
Currently the Ellis model is running well, as I can make my X-Lite client
registered in. The problem is that: The call function doesn?t work
properly. I list what I have found:

  1. Ralf: from /var/log/ralf/ralf_2016XXX

    24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1413: Sending
Diameter message of type 271 on transaction 0x7f17d0005100

24-02-2016 13:00:34.447 UTC Debug diameterstack.cpp:397: Routing out
callback from freeDiameter

24-02-2016 13:00:34.447 UTC Error diameterstack.cpp:293: Routing error: 'No
remaining suitable candidate to route the message to' for message with
Command-Code 271, Destination-Host  and Destination-Realm example.com

24-02-2016 13:00:34.447 UTC Debug freeDiameter: Iterating on rules of
COMMAND: '(generic error format)'.

24-02-2016 13:00:34.447 UTC Debug freeDiameter: Calling callback registered
when query was sent (0x437910, 0x7f17d0005100)

24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1093: Got Diameter
response of type 271 - calling callback on transaction 0x7f17d0005100

24-02-2016 13:00:34.447 UTC Warning peer_message_sender.cpp:125: Failed to
send ACR to  (number 0)

24-02-2016 13:00:34.447 UTC Error peer_message_sender.cpp:145: Failed to
connect to all CCFs, message not sent

24-02-2016 13:00:34.447 UTC Warning session_manager.cpp:319: Session for
79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY received error from CDF

24-02-2016 13:00:34.470 UTC Verbose httpstack.cpp:286: Process request for
URL /call-id/79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY, args (null)

  2. Sprout:from /var/log/sprout/sprout_current.txt . We can see there?re
several ?tombstone? element in this json. 

25-02-2016 08:05:00.076 UTC Debug avstore.cpp:72: Set AV for
6505550502 at example.com\7b66730d304dcdcb

{"digest":{"ha1":"3fb2ca6bb49c77b0907e080079d325a0","realm":"example.com","q
op":"auth"},"branch":"z9hG4bKPjbALlVrXogjQKxNGPi1wmdGhDKXTFBr7h","tombstone"
:true,"tombstone":true,"tombstone":true}

 

  I attached all the log files in this email. 

We appreciate your response. Thanks. 

                                                      

Jia Xuan

China Mobile Research Institute

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160226/747a1468/attachment.html>

From jiaxuan at chinamobile.com  Mon Feb 29 07:06:22 2016
From: jiaxuan at chinamobile.com (jiaxuan)
Date: Mon, 29 Feb 2016 20:06:22 +0800
Subject: [Project Clearwater] =?gb2312?b?tPC4tDogIFRlc3QgQ2xlYXJ3YXRlciBv?=
	=?gb2312?b?biBPcGVuc2hpZnQ=?=
In-Reply-To: <CY1PR0201MB15639CE35CB2C5DEDDCB9E3390A70@CY1PR0201MB1563.namprd02.prod.outlook.com>
References: <008501d16fa8$b7658e50$2630aaf0$@com>	<SN1PR0201MB1856FD019CDE8B7CF5ACABD1E1A60@SN1PR0201MB1856.namprd02.prod.outlook.com>	<002101d17074$5c862ef0$15928cd0$@com>
	<SN1PR0201MB185664708CA9B1CD9778401EE1A70@SN1PR0201MB1856.namprd02.prod.outlook.com>
	<CY1PR0201MB15639CE35CB2C5DEDDCB9E3390A70@CY1PR0201MB1563.namprd02.prod.outlook.com>
Message-ID: <002c01d172e9$9bc3ec70$d34bc550$@com>

Hi Matt, Chris,

Firstly , thanks for answering my questions. That makes me more confident to
move docker-compose into Openshift platform. 

Well, Openshift is using RHEL7 as its host OS. But the container is still
using ubuntu system, as far as I know the base image is Ubuntu14.04. I
change nothing in Dockerfile.  I am sorry I can?t find the log about
?docker build? and ?docker compose?. But all the call succeeds. So I
copy these images into Openshift Platform, and set some environment values,
make all the models can be connected by
requirements(https://raw.githubusercontent.com/Metaswitch/clearwater-docker/
master/minimal-distributed.yaml).

 

For openshift platform:

1. All the subscribers are from Ellis web UI. From the client(x-lite or
zoiper), I can see all the subscribers are registered.

2. The request ?INVITE? has been sent from bono to sprout. After at least
10 seconds, bono reply 408 to UE.

 

I attach the log file which you wanted, please check .

 

Great thanks.

 

Jia Xuan

 

 

???: Matt Williams [mailto:Matt.Williams at metaswitch.com] 
????: 2016?2?26? 20:42
???: jiaxuan
??: clearwater at lists.projectclearwater.org; Chris Elford
??: RE: [Project Clearwater] Test Clearwater on Openshift

 

Jia,

 

I think I'm a bit more positive that we can make this work than Chris is -
my understanding is that OpenShift is a Platform as a Service (Paas) built
on Docker, and we've successfully run Clearwater under Docker before, so
unless OpenShift disables something we rely on, Clearwater should run on
OpenShift.

 

His concern was that you were trying to port Clearwater to run on Red Hat
Enterprise Linux (RHEL), rather than under Ubuntu.  However, I think it
should be possible to run Clearwater in its native Ubuntu, even if the host
OS is RHEL.  Please can you confirm which guest OS your containers are
running?  ...and have you made any changes to the clearwater-docker scripts?

 

When Chris asked about "subscribers", he was referring to the telephone
numbers you're calling between.  How did you pick the numbers 6505550219 and
6505550782, for example?  Did you create them through the Ellis web UI?  Are
these the only 2 subscribers you've created?

 

The diagnostics we've got so far haven't been conclusive, but there are
three things that would be useful.

?         The complete logs from the output from the "docker build" and
"docker compose" commands, showing all the packages being installed to build
the images and then the images being deployed, just in case there's
something odd going on there.

?         On the Sprout node, stop the sprout process (with "sudo service
sprout stop"), wait 30s so that it has definitely restarted and then attempt
a call, and send us the logs from that - it's possible that something is
going wrong on start-up.

?         Clearwater has a built-in tool to gather the key diagnostics
together.  On the Sprout node, run "/usr/share/clearwater/bin/gather_diags"
and then gather the diagnostic dump from
/var/clearwater-diags-monitor/dumps/*.tar.gz and send it to us.

 

Please let us know.

 

Thanks,

 

Matt

 

--

 

Matt Williams
Lead Architect, Project Clearwater

+44 (0) 20 8366 1177

 

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On
Behalf Of Chris Elford
Sent: 26 February 2016 11:59
To: jiaxuan <jiaxuan at chinamobile.com>
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Test Clearwater on Openshift

 

I notice below that you say you are using Openshift and Kubernetes. As far
as I know, nobody is currently running Project Clearwater using these
platforms, and I would not expect it to work without some development work
to port it to this new platform.

 

I think that we need to understand what you did to allow you to install
Project Clearwater on this new platform before we can provide any help.

 

Yours,

 

Chris

 

From: jiaxuan [mailto:jiaxuan at chinamobile.com] 
Sent: 26 February 2016 09:02
To: Chris Elford <Chris.Elford at metaswitch.com>
Cc: clearwater at lists.projectclearwater.org
Subject: ??: [Project Clearwater] Test Clearwater on Openshift

 

Sorry? I have no idea about what?s ?subscriber?. It?s almost same with
the environment which is setup by docker-compose
(https://github.com/Metaswitch/clearwater-docker).  But all call fail. 

 

I found a PUBLISH message : The content-Length is zero.  

 

PUBLISH sip:6505550219 at example.com;transport=TCP SIP/2.0^M

Via: SIP/2.0/TCP
175.128.0.113:55316;branch=z9hG4bK-524287-1---4b2b34f926a6c876;rport^M

Max-Forwards: 70^M

Route: <sip:10.1.0.217:5054;transport=TCP;lr;orig>^M

Contact: <sip:6505550219 at 175.128.0.113:55316;transport=TCP>^M

To: "2323"<sip:6505550219 at example.com;transport=TCP>^M

From: "2323"<sip:6505550219 at example.com;transport=TCP>;tag=ea19e074^M

Call-ID: ZuNcniFDAQplqlNX3qyFJQ..^M

CSeq: 319 PUBLISH^M

Expires: 600^M

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO,
SUBSCRIBE^M

Supported: replaces, norefersub, extended-refer, timer, outbound, path,
X-cisco-serviceuri^M

User-Agent: Z 3.9.32144 r32121^M

Event: presence^M

Allow-Events: presence, kpml^M

Content-Length: 0^M

^M

While checking the docker-compose env, I got           

Max-Forwards: 70^M

Route: <sip:172.17.0.5:5054;transport=TCP;lr;orig>^M

Contact: <sip:6505550782 at 192.168.34.178:51158;transport=TCP>^M

To: "123"<sip:6505550782 at example.com;transport=TCP>^M

From: "123"<sip:6505550782 at example.com;transport=TCP>;tag=e72f4e32^M

Call-ID: vqETasJUaN4hD1h5scKyUA..^M

CSeq: 1 PUBLISH^M

Expires: 600^M

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO,
SUBSCRIBE^M

Content-Type: application/pidf+xml^M

Supported: replaces, norefersub, extended-refer, timer, outbound, path,
X-cisco-serviceuri^M

User-Agent: Z 3.9.32144 r32121^M

Event: presence^M

Allow-Events: presence, kpml^M

Content-Length: 271^M

^M

<?xml version="1.0" encoding="UTF-8"?>^M

<presence xmlns="urn:ietf:params:xml:ns:pidf"^M

          entity="sip:6505550782 at example.com;transport=TCP">^M

  <tuple id="6505550782" >^M

     <status><basic>open</basic></status>^M

     <note>Online</note>^M

  </tuple>^M

</presence>^M

 

Is that helpful? Thanks. 

 

Jia Xuan

 

???: Chris Elford [mailto:Chris.Elford at metaswitch.com] 
????: 2016?2?26? 0:27
???: jiaxuan
??: clearwater at lists.projectclearwater.org
??: RE: [Project Clearwater] Test Clearwater on Openshift

 

Hi,

 

I?ve taken a look at your Sprout logs. In order to diagnose your problem,
it would be useful to have some more information about what you are seeing.

?       What are the subscribers are you trying to call between?

?       Are you seeing all calls fail or just occasional ones?

 

We?ve looked into the two issues that you mentioned below.

 

It looks like Ralf is configured to send billing data to a CDF at
example.com. The logs in your message show that it is failing to do so. You
can configure a real CDF by changing the value of ?cdf_identity? in the
deployment?s shared config. See
http://clearwater.readthedocs.org/en/latest/Clearwater_Configuration_Options
_Reference/index.html for details.

 

We have looked into the tombstone issue that you mention below. We do not
think that this will cause any problems. We have raised
https://github.com/Metaswitch/sprout/issues/1334 to track working around it
and cleaning up those entries.

 

Yours,

 

Chris

 

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On
Behalf Of jiaxuan
Sent: 25 February 2016 08:44
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Test Clearwater on Openshift

 

Hi list:

  

  I have installed clearwater, following ?Manual Install Instrutions?. The
difference is that it?s running on Openshift which is using Kurbernetes.
Currently the Ellis model is running well, as I can make my X-Lite client
registered in. The problem is that: The call function doesn?t work
properly. I list what I have found:

  1. Ralf: from /var/log/ralf/ralf_2016XXX

    24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1413: Sending
Diameter message of type 271 on transaction 0x7f17d0005100

24-02-2016 13:00:34.447 UTC Debug diameterstack.cpp:397: Routing out
callback from freeDiameter

24-02-2016 13:00:34.447 UTC Error diameterstack.cpp:293: Routing error: 'No
remaining suitable candidate to route the message to' for message with
Command-Code 271, Destination-Host  and Destination-Realm example.com

24-02-2016 13:00:34.447 UTC Debug freeDiameter: Iterating on rules of
COMMAND: '(generic error format)'.

24-02-2016 13:00:34.447 UTC Debug freeDiameter: Calling callback registered
when query was sent (0x437910, 0x7f17d0005100)

24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1093: Got Diameter
response of type 271 - calling callback on transaction 0x7f17d0005100

24-02-2016 13:00:34.447 UTC Warning peer_message_sender.cpp:125: Failed to
send ACR to  (number 0)

24-02-2016 13:00:34.447 UTC Error peer_message_sender.cpp:145: Failed to
connect to all CCFs, message not sent

24-02-2016 13:00:34.447 UTC Warning session_manager.cpp:319: Session for
79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY received error from CDF

24-02-2016 13:00:34.470 UTC Verbose httpstack.cpp:286: Process request for
URL /call-id/79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY, args (null)

  2. Sprout:from /var/log/sprout/sprout_current.txt . We can see there?re
several ?tombstone? element in this json. 

25-02-2016 08:05:00.076 UTC Debug avstore.cpp:72: Set AV for
6505550502 at example.com\7b66730d304dcdcb

{"digest":{"ha1":"3fb2ca6bb49c77b0907e080079d325a0","realm":"example.com","q
op":"auth"},"branch":"z9hG4bKPjbALlVrXogjQKxNGPi1wmdGhDKXTFBr7h","tombstone"
:true,"tombstone":true,"tombstone":true}

 

  I attached all the log files in this email. 

We appreciate your response. Thanks. 

                                                      

Jia Xuan

China Mobile Research Institute

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160229/e4b29a91/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: jia.tar.gz
Type: application/octet-stream
Size: 1349732 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160229/e4b29a91/attachment.obj>

From matt at projectclearwater.org  Mon Feb 29 12:30:40 2016
From: matt at projectclearwater.org (Matt Williams (projectclearwater.org))
Date: Mon, 29 Feb 2016 17:30:40 +0000
Subject: [Project Clearwater] Test Clearwater on Openshift
In-Reply-To: <002c01d172e9$9bc3ec70$d34bc550$@com>
References: <008501d16fa8$b7658e50$2630aaf0$@com>
	<SN1PR0201MB1856FD019CDE8B7CF5ACABD1E1A60@SN1PR0201MB1856.namprd02.prod.outlook.com>
	<002101d17074$5c862ef0$15928cd0$@com>
	<SN1PR0201MB185664708CA9B1CD9778401EE1A70@SN1PR0201MB1856.namprd02.prod.outlook.com>
	<CY1PR0201MB15639CE35CB2C5DEDDCB9E3390A70@CY1PR0201MB1563.namprd02.prod.outlook.com>
	<002c01d172e9$9bc3ec70$d34bc550$@com>
Message-ID: <CY1PR0201MB1563C776A08F7BA3C9943C3E90BA0@CY1PR0201MB1563.namprd02.prod.outlook.com>

Jia,

Thanks for these diagnostics.  To me, the interesting bit is (in sprout_*.txt):

29-02-2016 11:20:03.678 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins
29-02-2016 11:20:03.679 UTC Status pluginloader.cpp:148: Finished loading plug-ins

Sprout is architected as a base executable with plug-ins (often referred to as "Sproutlets") on top of it to implement function.  The Sproutlets are loaded as shared objects (i.e. .so files) from the /usr/share/clearwater/sprout/plugins directory.  The log above indicates that it doesn't find any plugins.

Plugins are contained in separate Debian packages, such as sprout-icscf, sprout-scscf and sprout-bgcf (for each of the functions that Sprout provides).  I've checked the cw_package_info.txt file that you provided, and this shows that these packages are installed, so the shared objects should be present.

So I think the key question is whether these shared objects are present.  On the Sprout node, please can you
-   run "ls -l /usr/share/clearwater/sprout/plugins" to list the contents of this directory
-   run "ls -l /usr/share/clearwater/sprout" to check that the permissions on the containing directory?

Please let me know.

Thanks,

Matt

--

Matt Williams
Lead Architect, Project Clearwater
+44 (0) 20 8366 1177

From: jiaxuan [mailto:jiaxuan at chinamobile.com]
Sent: 29 February 2016 12:06
To: Matt Williams <Matt.Williams at metaswitch.com>
Cc: clearwater at lists.projectclearwater.org; Chris Elford <Chris.Elford at metaswitch.com>
Subject: ??: [Project Clearwater] Test Clearwater on Openshift

Hi Matt, Chris,
Firstly , thanks for answering my questions. That makes me more confident to move docker-compose into Openshift platform.
Well, Openshift is using RHEL7 as its host OS. But the container is still using ubuntu system, as far as I know the base image is Ubuntu14.04. I change nothing in Dockerfile.  I am sorry I can?t find the log about ?docker build? and ?docker compose?. But all the call succeeds. So I copy these images into Openshift Platform, and set some environment values, make all the models can be connected by requirements(https://raw.githubusercontent.com/Metaswitch/clearwater-docker/master/minimal-distributed.yaml).

For openshift platform:
1. All the subscribers are from Ellis web UI. From the client(x-lite or zoiper), I can see all the subscribers are registered.
2. The request ?INVITE? has been sent from bono to sprout. After at least 10 seconds, bono reply 408 to UE.

I attach the log file which you wanted, please check .

Great thanks.

Jia Xuan


???: Matt Williams [mailto:Matt.Williams at metaswitch.com]
????: 2016?2?26? 20:42
???: jiaxuan
??: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Chris Elford
??: RE: [Project Clearwater] Test Clearwater on Openshift

Jia,

I think I'm a bit more positive that we can make this work than Chris is - my understanding is that OpenShift is a Platform as a Service (Paas) built on Docker, and we've successfully run Clearwater under Docker before, so unless OpenShift disables something we rely on, Clearwater should run on OpenShift.

His concern was that you were trying to port Clearwater to run on Red Hat Enterprise Linux (RHEL), rather than under Ubuntu.  However, I think it should be possible to run Clearwater in its native Ubuntu, even if the host OS is RHEL.  Please can you confirm which guest OS your containers are running?  ...and have you made any changes to the clearwater-docker scripts?

When Chris asked about "subscribers", he was referring to the telephone numbers you're calling between.  How did you pick the numbers 6505550219 and 6505550782, for example?  Did you create them through the Ellis web UI?  Are these the only 2 subscribers you've created?

The diagnostics we've got so far haven't been conclusive, but there are three things that would be useful.

*         The complete logs from the output from the "docker build" and "docker compose" commands, showing all the packages being installed to build the images and then the images being deployed, just in case there's something odd going on there.

*         On the Sprout node, stop the sprout process (with "sudo service sprout stop"), wait 30s so that it has definitely restarted and then attempt a call, and send us the logs from that - it's possible that something is going wrong on start-up.

*         Clearwater has a built-in tool to gather the key diagnostics together.  On the Sprout node, run "/usr/share/clearwater/bin/gather_diags" and then gather the diagnostic dump from /var/clearwater-diags-monitor/dumps/*.tar.gz and send it to us.

Please let us know.

Thanks,

Matt

--

Matt Williams
Lead Architect, Project Clearwater
+44 (0) 20 8366 1177

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Chris Elford
Sent: 26 February 2016 11:59
To: jiaxuan <jiaxuan at chinamobile.com<mailto:jiaxuan at chinamobile.com>>
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Test Clearwater on Openshift

I notice below that you say you are using Openshift and Kubernetes. As far as I know, nobody is currently running Project Clearwater using these platforms, and I would not expect it to work without some development work to port it to this new platform.

I think that we need to understand what you did to allow you to install Project Clearwater on this new platform before we can provide any help.

Yours,

Chris

From: jiaxuan [mailto:jiaxuan at chinamobile.com]
Sent: 26 February 2016 09:02
To: Chris Elford <Chris.Elford at metaswitch.com<mailto:Chris.Elford at metaswitch.com>>
Cc: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: ??: [Project Clearwater] Test Clearwater on Openshift

Sorry? I have no idea about what?s ?subscriber?. It?s almost same with the environment which is setup by docker-compose (https://github.com/Metaswitch/clearwater-docker).  But all call fail.

I found a PUBLISH message : The content-Length is zero.

PUBLISH sip:6505550219 at example.com;transport=TCP SIP/2.0^M
Via: SIP/2.0/TCP 175.128.0.113:55316;branch=z9hG4bK-524287-1---4b2b34f926a6c876;rport^M
Max-Forwards: 70^M
Route: <sip:10.1.0.217:5054;transport=TCP;lr;orig>^M
Contact: <sip:6505550219 at 175.128.0.113:55316;transport=TCP>^M
To: "2323"<sip:6505550219 at example.com;transport=TCP>^M
From: "2323"<sip:6505550219 at example.com;transport=TCP>;tag=ea19e074^M
Call-ID: ZuNcniFDAQplqlNX3qyFJQ..^M
CSeq: 319 PUBLISH^M
Expires: 600^M
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE^M
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri^M
User-Agent: Z 3.9.32144 r32121^M
Event: presence^M
Allow-Events: presence, kpml^M
Content-Length: 0^M
^M
While checking the docker-compose env, I got
Max-Forwards: 70^M
Route: <sip:172.17.0.5:5054;transport=TCP;lr;orig>^M
Contact: <sip:6505550782 at 192.168.34.178:51158;transport=TCP>^M
To: "123"<sip:6505550782 at example.com;transport=TCP>^M
From: "123"<sip:6505550782 at example.com;transport=TCP>;tag=e72f4e32^M
Call-ID: vqETasJUaN4hD1h5scKyUA..^M
CSeq: 1 PUBLISH^M
Expires: 600^M
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE^M
Content-Type: application/pidf+xml^M
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri^M
User-Agent: Z 3.9.32144 r32121^M
Event: presence^M
Allow-Events: presence, kpml^M
Content-Length: 271^M
^M
<?xml version="1.0" encoding="UTF-8"?>^M
<presence xmlns="urn:ietf:params:xml:ns:pidf"^M
          entity="sip:6505550782 at example.com;transport=TCP">^M
  <tuple id="6505550782" >^M
     <status><basic>open</basic></status>^M
     <note>Online</note>^M
  </tuple>^M
</presence>^M

Is that helpful? Thanks.

Jia Xuan

???: Chris Elford [mailto:Chris.Elford at metaswitch.com]
????: 2016?2?26? 0:27
???: jiaxuan
??: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
??: RE: [Project Clearwater] Test Clearwater on Openshift

Hi,

I?ve taken a look at your Sprout logs. In order to diagnose your problem, it would be useful to have some more information about what you are seeing.

*       What are the subscribers are you trying to call between?

*       Are you seeing all calls fail or just occasional ones?

We?ve looked into the two issues that you mentioned below.

It looks like Ralf is configured to send billing data to a CDF at example.com. The logs in your message show that it is failing to do so. You can configure a real CDF by changing the value of ?cdf_identity? in the deployment?s shared config. See http://clearwater.readthedocs.org/en/latest/Clearwater_Configuration_Options_Reference/index.html for details.

We have looked into the tombstone issue that you mention below. We do not think that this will cause any problems. We have raised https://github.com/Metaswitch/sprout/issues/1334 to track working around it and cleaning up those entries.

Yours,

Chris

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of jiaxuan
Sent: 25 February 2016 08:44
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Test Clearwater on Openshift

Hi list:

  I have installed clearwater, following ?Manual Install Instrutions?. The difference is that it?s running on Openshift which is using Kurbernetes. Currently the Ellis model is running well, as I can make my X-Lite client registered in. The problem is that: The call function doesn?t work properly. I list what I have found:
  1. Ralf: from /var/log/ralf/ralf_2016XXX
    24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1413: Sending Diameter message of type 271 on transaction 0x7f17d0005100
24-02-2016 13:00:34.447 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
24-02-2016 13:00:34.447 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm example.com
24-02-2016 13:00:34.447 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
24-02-2016 13:00:34.447 UTC Debug freeDiameter: Calling callback registered when query was sent (0x437910, 0x7f17d0005100)
24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1093: Got Diameter response of type 271 - calling callback on transaction 0x7f17d0005100
24-02-2016 13:00:34.447 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
24-02-2016 13:00:34.447 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
24-02-2016 13:00:34.447 UTC Warning session_manager.cpp:319: Session for 79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY received error from CDF
24-02-2016 13:00:34.470 UTC Verbose httpstack.cpp:286: Process request for URL /call-id/79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY, args (null)
  2. Sprout:from /var/log/sprout/sprout_current.txt . We can see there?re several ?tombstone? element in this json.
25-02-2016 08:05:00.076 UTC Debug avstore.cpp:72: Set AV for 6505550502 at example.com\7b66730d304dcdcb<mailto:6505550502 at example.com\7b66730d304dcdcb>
{"digest":{"ha1":"3fb2ca6bb49c77b0907e080079d325a0","realm":"example.com","qop":"auth"},"branch":"z9hG4bKPjbALlVrXogjQKxNGPi1wmdGhDKXTFBr7h","tombstone":true,"tombstone":true,"tombstone":true}

  I attached all the log files in this email.
We appreciate your response. Thanks.

Jia Xuan
China Mobile Research Institute
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160229/82f97495/attachment.html>

From jiaxuan at chinamobile.com  Mon Feb 29 21:06:44 2016
From: jiaxuan at chinamobile.com (jiaxuan)
Date: Tue, 1 Mar 2016 10:06:44 +0800
Subject: [Project Clearwater] =?gb2312?b?tPC4tDogIFRlc3QgQ2xlYXJ3YXRlciBv?=
	=?gb2312?b?biBPcGVuc2hpZnQ=?=
In-Reply-To: <CY1PR0201MB1563C776A08F7BA3C9943C3E90BA0@CY1PR0201MB1563.namprd02.prod.outlook.com>
References: <008501d16fa8$b7658e50$2630aaf0$@com>	<SN1PR0201MB1856FD019CDE8B7CF5ACABD1E1A60@SN1PR0201MB1856.namprd02.prod.outlook.com>	<002101d17074$5c862ef0$15928cd0$@com>
	<SN1PR0201MB185664708CA9B1CD9778401EE1A70@SN1PR0201MB1856.namprd02.prod.outlook.com>
	<CY1PR0201MB15639CE35CB2C5DEDDCB9E3390A70@CY1PR0201MB1563.namprd02.prod.outlook.com>
	<002c01d172e9$9bc3ec70$d34bc550$@com>
	<CY1PR0201MB1563C776A08F7BA3C9943C3E90BA0@CY1PR0201MB1563.namprd02.prod.outlook.com>
Message-ID: <003d01d1735f$01fb2a60$05f17f20$@com>

Matt:

 

root at sprout-1-u9v45:~# cd /usr/share/clearwater/sprout

root at sprout-1-u9v45:/usr/share/clearwater/sprout# ls -l

total 4

drwxr-xr-x. 2 root root 4096 Dec 11 09:58 lib

drwxr-xr-x. 2 root root   96 Mar  1 02:00 plugins

root at sprout-1-u9v45:/usr/share/clearwater/sprout# cd plugins/

root at sprout-1-u9v45:/usr/share/clearwater/sprout/plugins# ls -l

total 608

-rw-r--r--. 1 root root 171456 Dec  7 16:00 sprout_bgcf.so

-rw-r--r--. 1 root root 212808 Dec  7 16:00 sprout_icscf.so

-rw-r--r--. 1 root root 113960 Dec  7 16:00 sprout_mmtel_as.so

-rw-r--r--. 1 root root 122504 Dec  7 16:00 sprout_scscf.so

root at sprout-1-u9v45:/usr/share/clearwater/sprout/plugins#

 

 

 

???: Matt Williams (projectclearwater.org)
[mailto:matt at projectclearwater.org] 
????: 2016?3?1? 1:31
???: jiaxuan
??: clearwater at lists.projectclearwater.org; Chris Elford
??: RE: [Project Clearwater] Test Clearwater on Openshift

 

Jia,

 

Thanks for these diagnostics.  To me, the interesting bit is (in
sprout_*.txt):

 

29-02-2016 11:20:03.678 UTC Status pluginloader.cpp:63: Loading plug-ins
from /usr/share/clearwater/sprout/plugins

29-02-2016 11:20:03.679 UTC Status pluginloader.cpp:148: Finished loading
plug-ins

 

Sprout is architected as a base executable with plug-ins (often referred to
as "Sproutlets") on top of it to implement function.  The Sproutlets are
loaded as shared objects (i.e. .so files) from the
/usr/share/clearwater/sprout/plugins directory.  The log above indicates
that it doesn't find any plugins.

 

Plugins are contained in separate Debian packages, such as sprout-icscf,
sprout-scscf and sprout-bgcf (for each of the functions that Sprout
provides).  I've checked the cw_package_info.txt file that you provided, and
this shows that these packages are installed, so the shared objects should
be present.

 

So I think the key question is whether these shared objects are present.  On
the Sprout node, please can you

-   run "ls -l /usr/share/clearwater/sprout/plugins" to list the contents of
this directory

-   run "ls -l /usr/share/clearwater/sprout" to check that the permissions
on the containing directory?

 

Please let me know.

 

Thanks,

 

Matt

 

--

 

Matt Williams
Lead Architect, Project Clearwater

+44 (0) 20 8366 1177

 

From: jiaxuan [mailto:jiaxuan at chinamobile.com] 
Sent: 29 February 2016 12:06
To: Matt Williams <Matt.Williams at metaswitch.com>
Cc: clearwater at lists.projectclearwater.org; Chris Elford
<Chris.Elford at metaswitch.com>
Subject: ??: [Project Clearwater] Test Clearwater on Openshift

 

Hi Matt, Chris,

Firstly , thanks for answering my questions. That makes me more confident to
move docker-compose into Openshift platform. 

Well, Openshift is using RHEL7 as its host OS. But the container is still
using ubuntu system, as far as I know the base image is Ubuntu14.04. I
change nothing in Dockerfile.  I am sorry I can?t find the log about
?docker build? and ?docker compose?. But all the call succeeds. So I
copy these images into Openshift Platform, and set some environment values,
make all the models can be connected by
requirements(https://raw.githubusercontent.com/Metaswitch/clearwater-docker/
master/minimal-distributed.yaml).

 

For openshift platform:

1. All the subscribers are from Ellis web UI. From the client(x-lite or
zoiper), I can see all the subscribers are registered.

2. The request ?INVITE? has been sent from bono to sprout. After at least
10 seconds, bono reply 408 to UE.

 

I attach the log file which you wanted, please check .

 

Great thanks.

 

Jia Xuan

 

 

???: Matt Williams [mailto:Matt.Williams at metaswitch.com] 
????: 2016?2?26? 20:42
???: jiaxuan
??: clearwater at lists.projectclearwater.org; Chris Elford
??: RE: [Project Clearwater] Test Clearwater on Openshift

 

Jia,

 

I think I'm a bit more positive that we can make this work than Chris is -
my understanding is that OpenShift is a Platform as a Service (Paas) built
on Docker, and we've successfully run Clearwater under Docker before, so
unless OpenShift disables something we rely on, Clearwater should run on
OpenShift.

 

His concern was that you were trying to port Clearwater to run on Red Hat
Enterprise Linux (RHEL), rather than under Ubuntu.  However, I think it
should be possible to run Clearwater in its native Ubuntu, even if the host
OS is RHEL.  Please can you confirm which guest OS your containers are
running?  ...and have you made any changes to the clearwater-docker scripts?

 

When Chris asked about "subscribers", he was referring to the telephone
numbers you're calling between.  How did you pick the numbers 6505550219 and
6505550782, for example?  Did you create them through the Ellis web UI?  Are
these the only 2 subscribers you've created?

 

The diagnostics we've got so far haven't been conclusive, but there are
three things that would be useful.

?         The complete logs from the output from the "docker build" and
"docker compose" commands, showing all the packages being installed to build
the images and then the images being deployed, just in case there's
something odd going on there.

?         On the Sprout node, stop the sprout process (with "sudo service
sprout stop"), wait 30s so that it has definitely restarted and then attempt
a call, and send us the logs from that - it's possible that something is
going wrong on start-up.

?         Clearwater has a built-in tool to gather the key diagnostics
together.  On the Sprout node, run "/usr/share/clearwater/bin/gather_diags"
and then gather the diagnostic dump from
/var/clearwater-diags-monitor/dumps/*.tar.gz and send it to us.

 

Please let us know.

 

Thanks,

 

Matt

 

--

 

Matt Williams
Lead Architect, Project Clearwater

+44 (0) 20 8366 1177

 

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On
Behalf Of Chris Elford
Sent: 26 February 2016 11:59
To: jiaxuan <jiaxuan at chinamobile.com>
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Test Clearwater on Openshift

 

I notice below that you say you are using Openshift and Kubernetes. As far
as I know, nobody is currently running Project Clearwater using these
platforms, and I would not expect it to work without some development work
to port it to this new platform.

 

I think that we need to understand what you did to allow you to install
Project Clearwater on this new platform before we can provide any help.

 

Yours,

 

Chris

 

From: jiaxuan [mailto:jiaxuan at chinamobile.com] 
Sent: 26 February 2016 09:02
To: Chris Elford <Chris.Elford at metaswitch.com>
Cc: clearwater at lists.projectclearwater.org
Subject: ??: [Project Clearwater] Test Clearwater on Openshift

 

Sorry? I have no idea about what?s ?subscriber?. It?s almost same with
the environment which is setup by docker-compose
(https://github.com/Metaswitch/clearwater-docker).  But all call fail. 

 

I found a PUBLISH message : The content-Length is zero.  

 

PUBLISH sip:6505550219 at example.com;transport=TCP SIP/2.0^M

Via: SIP/2.0/TCP
175.128.0.113:55316;branch=z9hG4bK-524287-1---4b2b34f926a6c876;rport^M

Max-Forwards: 70^M

Route: <sip:10.1.0.217:5054;transport=TCP;lr;orig>^M

Contact: <sip:6505550219 at 175.128.0.113:55316;transport=TCP>^M

To: "2323"<sip:6505550219 at example.com;transport=TCP>^M

From: "2323"<sip:6505550219 at example.com;transport=TCP>;tag=ea19e074^M

Call-ID: ZuNcniFDAQplqlNX3qyFJQ..^M

CSeq: 319 PUBLISH^M

Expires: 600^M

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO,
SUBSCRIBE^M

Supported: replaces, norefersub, extended-refer, timer, outbound, path,
X-cisco-serviceuri^M

User-Agent: Z 3.9.32144 r32121^M

Event: presence^M

Allow-Events: presence, kpml^M

Content-Length: 0^M

^M

While checking the docker-compose env, I got           

Max-Forwards: 70^M

Route: <sip:172.17.0.5:5054;transport=TCP;lr;orig>^M

Contact: <sip:6505550782 at 192.168.34.178:51158;transport=TCP>^M

To: "123"<sip:6505550782 at example.com;transport=TCP>^M

From: "123"<sip:6505550782 at example.com;transport=TCP>;tag=e72f4e32^M

Call-ID: vqETasJUaN4hD1h5scKyUA..^M

CSeq: 1 PUBLISH^M

Expires: 600^M

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO,
SUBSCRIBE^M

Content-Type: application/pidf+xml^M

Supported: replaces, norefersub, extended-refer, timer, outbound, path,
X-cisco-serviceuri^M

User-Agent: Z 3.9.32144 r32121^M

Event: presence^M

Allow-Events: presence, kpml^M

Content-Length: 271^M

^M

<?xml version="1.0" encoding="UTF-8"?>^M

<presence xmlns="urn:ietf:params:xml:ns:pidf"^M

          entity="sip:6505550782 at example.com;transport=TCP">^M

  <tuple id="6505550782" >^M

     <status><basic>open</basic></status>^M

     <note>Online</note>^M

  </tuple>^M

</presence>^M

 

Is that helpful? Thanks. 

 

Jia Xuan

 

???: Chris Elford [mailto:Chris.Elford at metaswitch.com] 
????: 2016?2?26? 0:27
???: jiaxuan
??: clearwater at lists.projectclearwater.org
??: RE: [Project Clearwater] Test Clearwater on Openshift

 

Hi,

 

I?ve taken a look at your Sprout logs. In order to diagnose your problem,
it would be useful to have some more information about what you are seeing.

?       What are the subscribers are you trying to call between?

?       Are you seeing all calls fail or just occasional ones?

 

We?ve looked into the two issues that you mentioned below.

 

It looks like Ralf is configured to send billing data to a CDF at
example.com. The logs in your message show that it is failing to do so. You
can configure a real CDF by changing the value of ?cdf_identity? in the
deployment?s shared config. See
http://clearwater.readthedocs.org/en/latest/Clearwater_Configuration_Options
_Reference/index.html for details.

 

We have looked into the tombstone issue that you mention below. We do not
think that this will cause any problems. We have raised
https://github.com/Metaswitch/sprout/issues/1334 to track working around it
and cleaning up those entries.

 

Yours,

 

Chris

 

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On
Behalf Of jiaxuan
Sent: 25 February 2016 08:44
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Test Clearwater on Openshift

 

Hi list:

  

  I have installed clearwater, following ?Manual Install Instrutions?. The
difference is that it?s running on Openshift which is using Kurbernetes.
Currently the Ellis model is running well, as I can make my X-Lite client
registered in. The problem is that: The call function doesn?t work
properly. I list what I have found:

  1. Ralf: from /var/log/ralf/ralf_2016XXX

    24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1413: Sending
Diameter message of type 271 on transaction 0x7f17d0005100

24-02-2016 13:00:34.447 UTC Debug diameterstack.cpp:397: Routing out
callback from freeDiameter

24-02-2016 13:00:34.447 UTC Error diameterstack.cpp:293: Routing error: 'No
remaining suitable candidate to route the message to' for message with
Command-Code 271, Destination-Host  and Destination-Realm example.com

24-02-2016 13:00:34.447 UTC Debug freeDiameter: Iterating on rules of
COMMAND: '(generic error format)'.

24-02-2016 13:00:34.447 UTC Debug freeDiameter: Calling callback registered
when query was sent (0x437910, 0x7f17d0005100)

24-02-2016 13:00:34.447 UTC Verbose diameterstack.cpp:1093: Got Diameter
response of type 271 - calling callback on transaction 0x7f17d0005100

24-02-2016 13:00:34.447 UTC Warning peer_message_sender.cpp:125: Failed to
send ACR to  (number 0)

24-02-2016 13:00:34.447 UTC Error peer_message_sender.cpp:145: Failed to
connect to all CCFs, message not sent

24-02-2016 13:00:34.447 UTC Warning session_manager.cpp:319: Session for
79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY received error from CDF

24-02-2016 13:00:34.470 UTC Verbose httpstack.cpp:286: Process request for
URL /call-id/79048MzlhMGNjMzc4YmEyY2NlMGEyZmU2ODNmYjZjM2E1ZGY, args (null)

  2. Sprout:from /var/log/sprout/sprout_current.txt . We can see there?re
several ?tombstone? element in this json. 

25-02-2016 08:05:00.076 UTC Debug avstore.cpp:72: Set AV for
6505550502 at example.com\7b66730d304dcdcb

{"digest":{"ha1":"3fb2ca6bb49c77b0907e080079d325a0","realm":"example.com","q
op":"auth"},"branch":"z9hG4bKPjbALlVrXogjQKxNGPi1wmdGhDKXTFBr7h","tombstone"
:true,"tombstone":true,"tombstone":true}

 

  I attached all the log files in this email. 

We appreciate your response. Thanks. 

                                                      

Jia Xuan

China Mobile Research Institute

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160301/558fca00/attachment.html>

