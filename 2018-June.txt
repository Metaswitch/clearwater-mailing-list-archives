From navdeep.uniyal at bristol.ac.uk  Fri Jun  1 06:27:55 2018
From: navdeep.uniyal at bristol.ac.uk (Navdeep Uniyal)
Date: Fri, 1 Jun 2018 10:27:55 +0000
Subject: [Project Clearwater] Clearwater Create Identity Error
Message-ID: <DB5PR06MB15596725F411657A5CA113F6DE620@DB5PR06MB1559.eurprd06.prod.outlook.com>

Hi all,

I am trying clearwaterIMS  using docker containers. (https://github.com/Metaswitch/clearwater-docker)
While trying to create an identity, I am getting the error: Failed to update the server (see detailed diagnostics in developer console). Please refresh the page.

On accessing the ellis server (docker), I am getting below listed error:

root at 7fc7f4487efb:/usr/share/clearwater/ellis/src/metaswitch/ellis/tools# python create_numbers.py
Traceback (most recent call last):
  File "create_numbers.py", line 17, in <module>
    from metaswitch.ellis.data import numbers, connection
ImportError: No module named metaswitch.ellis.data


Please suggest how can I resolve the issue.

Kind Regards,
Navdeep

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180601/b15f9627/attachment.html>

From tcchindeka at gmail.com  Sat Jun  2 14:40:53 2018
From: tcchindeka at gmail.com (Tapiwa Chindeka)
Date: Sat, 2 Jun 2018 20:40:53 +0200
Subject: [Project Clearwater] OpenStack version the Heat templates have been
 tested on
Message-ID: <CAFFdzJTDEaQVZtdtumx=3d6+p9JyB+SBt_6OH8CCT9ZmkXf6uQ@mail.gmail.com>

Hi

I have been having problems installing Clearwater using the Heat templates
from GitHub in OpenStack Ocata and I suspect it might be a version
compatibility issue.

Can someone please tell which OpenStack versions the templates have been
tested on?

Regards
Tapiwa
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180602/60527d07/attachment.html>

From Matthew.Davis.2 at team.telstra.com  Mon Jun  4 03:32:01 2018
From: Matthew.Davis.2 at team.telstra.com (Davis, Matthew)
Date: Mon, 4 Jun 2018 07:32:01 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <CY1PR0201MB0971E10C3CE7DB95C3CDBF08E2630@CY1PR0201MB0971.namprd02.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971545314ED91FB7F233AC1E2900@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB288450EF460A39FEA9E1D724C2940@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<BY1PR0201MB09689BB9A1645396B16AE810E2940@BY1PR0201MB0968.namprd02.prod.outlook.com>
	<ME2PR01MB2884F01278DC1CD5F7831DFEC26A0@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09710B4690B23BC5F7E8F786E26A0@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884BAB776270D40A8B06347C2690@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971049DFED6D952FFC21DE1E2690@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<MEAPR01MB2885D6C8E38E9D4E26723D4EC26C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971E10C3CE7DB95C3CDBF08E2630@CY1PR0201MB0971.namprd02.prod.outlook.com>
Message-ID: <ME2PR01MB2884B621E65E277DA1F8D36DC2670@ME2PR01MB2884.ausprd01.prod.outlook.com>

Hi Adam,

Let's focus on the 3 differences:

Changing the PUBLIC IP to match my rig IP.
Where is this? In bono-deply.yaml? Anywhere else? (should I have it in .env?) Should this include the port number?
Which IP should this be? Currently I have it equal to the Kubernetes API IP address.
When I run ` kubectl config view | grep 10` I see `server: https://10.3.1.76:6443`<https://10.3.1.76:6443%60> so I set PUBLIC_IP to 10.3.1.76.
What do you see? I think 6443 might be a non-standard port for Kubernetes. Could that be a source of the problem?
My .env file currently says "PUBLIC_IP=" (without an address)

Changed the image pull source to our internal one: Hmm, maybe my images were built wrong? Once my pull requests are merged, I'll do a fresh clone and rebuild.

Changed the zone in the config map to my own one, 'ajl.svc.cw-k8s.test': Why are you using the non-default zone ajl.svc.cw-k8s.test? Where did that come from? I thought my original Azure issue was because I used a zone which wasn't default.svc.cluster.local. What is that parameter? What does it mean? Why is the default what it is?

My test command is:
rake test[default.svc.cw-k8s.test] PROXY=10.3.1.76 PROXY_PORT=30060 SIGNUP_CODE='secret' ELLIS=10.3.1.76:30080 TESTS="Basic call - mainline"

I'm wrapping secret in single apostrophes. Without the apostrophes the result is the same.

Every pod (except etcd) has a shared_config file which says:

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret



Thanks,
Matt

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 1 June 2018 3:29 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com>; clearwater at lists.projectclearwater.org
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

Sorry to hear it isn't working yet. Looking at the output of rake test, you're hitting a forbidden in the RestClient, and so it's something on the Http calls. I think your issue may just be a mismatch in the value of 'SIGNUP_CODE' that you're passing in to the rake test command, and what's on your deployment. This should default to 'secret', and I don't see anything in your configmap that would change that, so can you double check the test command includes " SIGNUP_CODE='secret' ". If you have made any changes to the signup key then obviously it'll need to match those. If you want to double check the value, take a look in the /etc/clearwater/shared_config file in one of the pods.

I have double checked a deployment on our rig, copying your provided yaml files over directly to make sure there isn't anything odd in there, and the live tests were able to run fine, following the deployment steps you're using. I did have some trouble getting the script to work as copied over, but think that's just outlook formatting quotes wrong etc. Manually running the commands it all worked as expected. The only changes I made were:

  *   Changing the PUBLIC IP to match my rig IP.
  *   Changed the image pull source to our internal one
  *   Changed the zone in the config map to my own one, 'ajl.svc.cw-k8s.test'

With this all deployed, the following test command passed with no issue:
    rake test[ajl.svc.cw-k8s.test] PROXY=10.230.16.1 PROXY_PORT=30060 SIGNUP_CODE='secret' ELLIS=10.230.16.1:30080 TESTS="Basic call - mainline"

If the issue isn't the signup key, can we try getting some more diags that we can take a look into? In particular, I think we would benefit from:

  *   A packet capture on the node you are running the live tests on, when you hit the errors below
  *   The bono logs, at debug level, from the same time. To set up debug logging, you need to add 'log_level=5' to /etc/clearwater/user_settings (creating if needed), and restart the bono service
  *   The ellis logs from the same time

Running the tcpdump on the test node should mean we get to see the full set of flows, and you can likely read through that yourself to work out any following issues you find hiding behind this next one.
Any other diagnostics you can gather would obviously also be useful, but with the above, assuming traffic is reaching the pods, we should be able to work out the issue.

On your connectivity tests, you won't be able to connect to the bono service using 'nc localhost 30060', because that attempts to connect using the localhost IP. We have set the bono service up to listen on the 'PUBLIC_IP', i.e. the host IP. If you try running 'nc 10.3.1.76 30060 -v' you should see successful connection. (Or on whichever host IP you have configured it to listen).
The log output you are seeing on restarting Bono is also benign. These are again simply an artefact of some behaviour we want to have in VMs, but that is not needed in these containers. You can safely ignore this output.

Good luck, and let us know where you get with debugging.
Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 30 May 2018 08:27
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,
# Openstack Install

I only mentioned the helm charts just in case the almost empty charts on my machine were the source of the error. I personally have no experience with Helm, so I can't help you with any development.
I applied that latest change to the bono service port number. It still doesn't work.

How can I check whether the rake tests are failing on the bono side, as opposed to failing on the ellis side? Maybe the reason tcpdumps shows nothing in Bono during the rake tests is because the tests failed to create a user in ellis, and never got to the bono part?

Rake output:
```
Basic Call - Mainline (TCP) - Failed
  RestClient::Forbidden thrown:
   - 403 Forbidden
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/abstract_response.rb:74:in `return!'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:495:in `process_result'
...
```
If I go inside the ellis container and run `nc localhost 80 -v` I see that it establishes a connection.
If I go inside the bono container and run `nc localhost 5060 -v` or `nc localhost 30060 -v` it fails to connect. So from within the bono pod I cannot connect to the localhost. To me that suggests that the problem is caused by something inside bono, not the networking between pods. What happens when you try `nc localhost 32060 -v` in your deployment?
The logs inside bono are an echo of an error message from sprout. Does that matter?

```
30-05-2018 06:46:59.432 UTC [7f45527e4700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 4
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:244: Reraising all alarms with a known state
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:37: sprout issued 1012.3 alarm
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:37: sprout issued 1013.3 alarm
```

Those timestamps don't correspond to the rake tests. They just happen every 30 seconds.

When I restart the bono service it says: `63: ulimit: open files: cannot modify limit: Invalid argument`
Does that matter? (I've seen that error message everywhere. I have no idea what it means)

I've appended the yaml files to the end of this email.

# Azure Install

I had a chat to Microsoft. It seems that your hunch was correct. HTTP Application Routing only works on port 80 and 443. Furthermore, I cannot simply route SIP calls through port 443 because the routing does some HTTP specific packet inspection things. So I'll have to give up on that approach and go for a more vanilla, manually configured NodePort approach (either still on AKS but without HTTP Application Routing, or on Openstack). So I'm even more keen to solve the aforementioned issues.


# Yamls and stuff

I'm pasting them again just in case I've forgotten something. 10.3.1.76 is the ip address of my cluster.

Here's a script I'm using to tear down and rebuild everything. (Just incase `kubectl apply -f something.yaml` doesn't actually propagate the change fully) The while loops in this script are just to wait until the previous step has finished.

```
set -x
cd clearwater-docker-master/kubernetes
kubectl delete -f ./
kubectl delete configmap env-vars
set -e
echo 'waiting until old pods are all deleted'
while [ $(kubectl get pods | grep ^NAME -v | wc -l ) -neq 0]
do
   sleep 5
done
echo "creating new pods"
kubectl create configmap env-vars --from-literal=ZONE=default.svc.cluster.local
kubectl apply -f ./
while [ $(kubectl get pods | grep "2/2" | grep bono | wc -l) -neq 1 ]
do
   sleep 5
done
BONO=$(kubectl get pods | grep "2/2" | grep bono | awk '{ print $1 }')
echo "Bono is up as $BONO"
kubectl exec -it $BONO -- apt-get -y install vim
kubectl exec -it $BONO -- sed -i -e 's/--pcscf=5060,5058/--pcscf=30060,5058/g' /etc/init.d/bono
kubectl exec -it $BONO service bono restart
while [ $(kubectl get pods | grep "0/" | wc -l) -neq 0 ]
do
   sleep 5
done
echo "All pods are up now"
kubectl get pods
echo "Done"
```

`kubectl get services`

```
NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                         AGE
astaire          ClusterIP   None           <none>        11311/TCP                                       1h
bono             NodePort    10.0.168.197   <none>        3478:32214/TCP,30060:30060/TCP,5062:30144/TCP   1h
cassandra        ClusterIP   None           <none>        7001/TCP,7000/TCP,9042/TCP,9160/TCP             1h
chronos          ClusterIP   None           <none>        7253/TCP                                        1h
ellis            NodePort    10.0.53.199    <none>        80:30080/TCP                                    1h
etcd             ClusterIP   None           <none>        2379/TCP,2380/TCP,4001/TCP                      1h
homer            ClusterIP   None           <none>        7888/TCP                                        1h
homestead        ClusterIP   None           <none>        8888/TCP                                        1h
homestead-prov   ClusterIP   None           <none>        8889/TCP                                        1h
kubernetes       ClusterIP   10.0.0.1       <none>        443/TCP                                         5d
ralf             ClusterIP   None           <none>        10888/TCP                                       1h
sprout           ClusterIP   None           <none>        5052/TCP,5054/TCP                               1h
```

bono-depl.yaml

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: bono
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bono
  template:
    metadata:
      labels:
        service: bono
        snmp: enabled
    spec:
      containers:
      - image: "mlda065/bono:latest"
        imagePullPolicy: Always
        name: bono
        ports:
        - containerPort: 22
        - containerPort: 3478
        - containerPort: 5060
        - containerPort: 5062
        - containerPort: 5060
          protocol: "UDP"
        - containerPort: 5062
          protocol: "UDP"
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PUBLIC_IP
          value: 10.3.1.76
        livenessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
          initialDelaySeconds: 30
        readinessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      - image: busybox
        name: tailer
        command: [ "tail", "-F", "/var/log/bono/bono_current.txt" ]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      volumes:
      - name: bonologs
        emptyDir: {}
      imagePullSecrets:
      - name: ~
      restartPolicy: Always
```

Bono-svc.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: bono
spec:
  type: NodePort
  ports:
  - name: "3478"
    port: 3478
  - name: "5060"
    port: 30060
    nodePort: 30060
  - name: "5062"
    port: 5062
  selector:
    service: bono
```

Ellis service

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: ellis
spec:
  replicas: 1
  template:
    metadata:
      labels:
        service: ellis
    spec:
      containers:
      - image: "mlda065/ellis:latest"
        imagePullPolicy: Always
        name: ellis
        ports:
        - containerPort: 22
        - containerPort: 80
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        livenessProbe:
          tcpSocket:
            port: 80
          initialDelaySeconds: 30
        readinessProbe:
          tcpSocket:
            port: 80
      imagePullSecrets:
      - name: ~
      restartPolicy: Always
```

Ellis-svc.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: ellis
spec:
  type: NodePort
  ports:
  - name: "http"
    port: 80
    nodePort: 30080
  selector:
    service: ellis
```

`kubectl describe configmap env-vars`

```
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","data":{"ZONE":"default.svc.cluster.local"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"env-vars","namespace":"default"}...

Data
====
ZONE:
----
default.svc.cluster.local
Events:  <none>
```



Thanks,

Matt
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 25 May 2018 6:42 PM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

Our Helm support is a recent addition, and came from another external contributor. See the Pull Request at https://github.com/Metaswitch/clearwater-docker/pull/85 for the details :)
As it stands at the moment, the chart is good enough for deploying and re-creating a full standard deployment through Helm, but I don't believe it handles more of the complexities of upgrading a clearwater deployment that it potentially could.

We haven't yet done any significant work in setting up Helm charts, or integrating with them in a more detailed manner, so if that's something you're interested in as well, we'd love to work with you to get some more enhancements in. Especially if you have other expert contacts who know more in this area.

(I'm removing some of the thread in the email below, to keep us below the list limits. The online archives will keep all the info though)

Cheers,
Adam

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180604/92ea260a/attachment.html>

From Matthew.Davis.2 at team.telstra.com  Mon Jun  4 04:23:31 2018
From: Matthew.Davis.2 at team.telstra.com (Davis, Matthew)
Date: Mon, 4 Jun 2018 08:23:31 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971545314ED91FB7F233AC1E2900@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB288450EF460A39FEA9E1D724C2940@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<BY1PR0201MB09689BB9A1645396B16AE810E2940@BY1PR0201MB0968.namprd02.prod.outlook.com>
	<ME2PR01MB2884F01278DC1CD5F7831DFEC26A0@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09710B4690B23BC5F7E8F786E26A0@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884BAB776270D40A8B06347C2690@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971049DFED6D952FFC21DE1E2690@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<MEAPR01MB2885D6C8E38E9D4E26723D4EC26C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971E10C3CE7DB95C3CDBF08E2630@CY1PR0201MB0971.namprd02.prod.outlook.com>
Message-ID: <ME2PR01MB2884BEA7F75746D055BB7329C2670@ME2PR01MB2884.ausprd01.prod.outlook.com>

I forgot to mention,

When I run ` nc 10.3.1.76 32060 -v` I see nothing (not success, not failure. Just no output and it hangs)

Also, my bono-depl.yaml file says "containerPort: 5060" not 30060. Is that right?



When I run a tcp dump on the test machine I see dump.txt (attached)
(I'm not sure how this mailing list will cope with attachments)

The bono logs from that time are attached as bono_log.txt. (I inserted comments with ### to make it clear when the test was running.)

For some reason there is no soft link called /var/log/ellis/ellis_current.txt
The contents of ellis-err.log is "No handlers could be found for logger "metaswitch.utils"" I'm not sure whether that appeared during the test or not.
ellis_20180604T070000Z.txt is empty. (No new entries during the test)

I modified the log level on both ellis and bono, and restarted the bono and ellis service respectively, prior to running the test.

Regards,
Matt



From: Davis, Matthew
Sent: Monday, 4 June 2018 5:32 PM
To: 'Adam Lindley' <Adam.Lindley at metaswitch.com>; clearwater at lists.projectclearwater.org
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,
Let's focus on the 3 differences:

Changing the PUBLIC IP to match my rig IP.
Where is this? In bono-deply.yaml? Anywhere else? (should I have it in .env?) Should this include the port number?
Which IP should this be? Currently I have it equal to the Kubernetes API IP address.
When I run ` kubectl config view | grep 10` I see `server: https://10.3.1.76:6443`<https://10.3.1.76:6443%60> so I set PUBLIC_IP to 10.3.1.76.
What do you see? I think 6443 might be a non-standard port for Kubernetes. Could that be a source of the problem?
My .env file currently says "PUBLIC_IP=" (without an address)

Changed the image pull source to our internal one: Hmm, maybe my images were built wrong? Once my pull requests are merged, I'll do a fresh clone and rebuild.

Changed the zone in the config map to my own one, 'ajl.svc.cw-k8s.test': Why are you using the non-default zone ajl.svc.cw-k8s.test? Where did that come from? I thought my original Azure issue was because I used a zone which wasn't default.svc.cluster.local. What is that parameter? What does it mean? Why is the default what it is?

My test command is:
rake test[default.svc.cw-k8s.test] PROXY=10.3.1.76 PROXY_PORT=30060 SIGNUP_CODE='secret' ELLIS=10.3.1.76:30080 TESTS="Basic call - mainline"

I'm wrapping secret in single apostrophes. Without the apostrophes the result is the same.

Every pod (except etcd) has a shared_config file which says:

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


Thanks,
Matt

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 1 June 2018 3:29 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

Sorry to hear it isn't working yet. Looking at the output of rake test, you're hitting a forbidden in the RestClient, and so it's something on the Http calls. I think your issue may just be a mismatch in the value of 'SIGNUP_CODE' that you're passing in to the rake test command, and what's on your deployment. This should default to 'secret', and I don't see anything in your configmap that would change that, so can you double check the test command includes " SIGNUP_CODE='secret' ". If you have made any changes to the signup key then obviously it'll need to match those. If you want to double check the value, take a look in the /etc/clearwater/shared_config file in one of the pods.

I have double checked a deployment on our rig, copying your provided yaml files over directly to make sure there isn't anything odd in there, and the live tests were able to run fine, following the deployment steps you're using. I did have some trouble getting the script to work as copied over, but think that's just outlook formatting quotes wrong etc. Manually running the commands it all worked as expected. The only changes I made were:

  *   Changing the PUBLIC IP to match my rig IP.
  *   Changed the image pull source to our internal one
  *   Changed the zone in the config map to my own one, 'ajl.svc.cw-k8s.test'

With this all deployed, the following test command passed with no issue:
    rake test[ajl.svc.cw-k8s.test] PROXY=10.230.16.1 PROXY_PORT=30060 SIGNUP_CODE='secret' ELLIS=10.230.16.1:30080 TESTS="Basic call - mainline"

If the issue isn't the signup key, can we try getting some more diags that we can take a look into? In particular, I think we would benefit from:

  *   A packet capture on the node you are running the live tests on, when you hit the errors below
  *   The bono logs, at debug level, from the same time. To set up debug logging, you need to add 'log_level=5' to /etc/clearwater/user_settings (creating if needed), and restart the bono service
  *   The ellis logs from the same time

Running the tcpdump on the test node should mean we get to see the full set of flows, and you can likely read through that yourself to work out any following issues you find hiding behind this next one.
Any other diagnostics you can gather would obviously also be useful, but with the above, assuming traffic is reaching the pods, we should be able to work out the issue.

On your connectivity tests, you won't be able to connect to the bono service using 'nc localhost 30060', because that attempts to connect using the localhost IP. We have set the bono service up to listen on the 'PUBLIC_IP', i.e. the host IP. If you try running 'nc 10.3.1.76 30060 -v' you should see successful connection. (Or on whichever host IP you have configured it to listen).
The log output you are seeing on restarting Bono is also benign. These are again simply an artefact of some behaviour we want to have in VMs, but that is not needed in these containers. You can safely ignore this output.

Good luck, and let us know where you get with debugging.
Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 30 May 2018 08:27
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,
# Openstack Install

I only mentioned the helm charts just in case the almost empty charts on my machine were the source of the error. I personally have no experience with Helm, so I can't help you with any development.
I applied that latest change to the bono service port number. It still doesn't work.

How can I check whether the rake tests are failing on the bono side, as opposed to failing on the ellis side? Maybe the reason tcpdumps shows nothing in Bono during the rake tests is because the tests failed to create a user in ellis, and never got to the bono part?

Rake output:
```
Basic Call - Mainline (TCP) - Failed
  RestClient::Forbidden thrown:
   - 403 Forbidden
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/abstract_response.rb:74:in `return!'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:495:in `process_result'
...
```
If I go inside the ellis container and run `nc localhost 80 -v` I see that it establishes a connection.
If I go inside the bono container and run `nc localhost 5060 -v` or `nc localhost 30060 -v` it fails to connect. So from within the bono pod I cannot connect to the localhost. To me that suggests that the problem is caused by something inside bono, not the networking between pods. What happens when you try `nc localhost 32060 -v` in your deployment?
The logs inside bono are an echo of an error message from sprout. Does that matter?

```
30-05-2018 06:46:59.432 UTC [7f45527e4700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 4
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:244: Reraising all alarms with a known state
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:37: sprout issued 1012.3 alarm
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:37: sprout issued 1013.3 alarm
```

Those timestamps don't correspond to the rake tests. They just happen every 30 seconds.

When I restart the bono service it says: `63: ulimit: open files: cannot modify limit: Invalid argument`
Does that matter? (I've seen that error message everywhere. I have no idea what it means)

I've appended the yaml files to the end of this email.

# Azure Install

I had a chat to Microsoft. It seems that your hunch was correct. HTTP Application Routing only works on port 80 and 443. Furthermore, I cannot simply route SIP calls through port 443 because the routing does some HTTP specific packet inspection things. So I'll have to give up on that approach and go for a more vanilla, manually configured NodePort approach (either still on AKS but without HTTP Application Routing, or on Openstack). So I'm even more keen to solve the aforementioned issues.


# Yamls and stuff

I'm pasting them again just in case I've forgotten something. 10.3.1.76 is the ip address of my cluster.

Here's a script I'm using to tear down and rebuild everything. (Just incase `kubectl apply -f something.yaml` doesn't actually propagate the change fully) The while loops in this script are just to wait until the previous step has finished.

```
set -x
cd clearwater-docker-master/kubernetes
kubectl delete -f ./
kubectl delete configmap env-vars
set -e
echo 'waiting until old pods are all deleted'
while [ $(kubectl get pods | grep ^NAME -v | wc -l ) -neq 0]
do
   sleep 5
done
echo "creating new pods"
kubectl create configmap env-vars --from-literal=ZONE=default.svc.cluster.local
kubectl apply -f ./
while [ $(kubectl get pods | grep "2/2" | grep bono | wc -l) -neq 1 ]
do
   sleep 5
done
BONO=$(kubectl get pods | grep "2/2" | grep bono | awk '{ print $1 }')
echo "Bono is up as $BONO"
kubectl exec -it $BONO -- apt-get -y install vim
kubectl exec -it $BONO -- sed -i -e 's/--pcscf=5060,5058/--pcscf=30060,5058/g' /etc/init.d/bono
kubectl exec -it $BONO service bono restart
while [ $(kubectl get pods | grep "0/" | wc -l) -neq 0 ]
do
   sleep 5
done
echo "All pods are up now"
kubectl get pods
echo "Done"
```

`kubectl get services`

```
NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                         AGE
astaire          ClusterIP   None           <none>        11311/TCP                                       1h
bono             NodePort    10.0.168.197   <none>        3478:32214/TCP,30060:30060/TCP,5062:30144/TCP   1h
cassandra        ClusterIP   None           <none>        7001/TCP,7000/TCP,9042/TCP,9160/TCP             1h
chronos          ClusterIP   None           <none>        7253/TCP                                        1h
ellis            NodePort    10.0.53.199    <none>        80:30080/TCP                                    1h
etcd             ClusterIP   None           <none>        2379/TCP,2380/TCP,4001/TCP                      1h
homer            ClusterIP   None           <none>        7888/TCP                                        1h
homestead        ClusterIP   None           <none>        8888/TCP                                        1h
homestead-prov   ClusterIP   None           <none>        8889/TCP                                        1h
kubernetes       ClusterIP   10.0.0.1       <none>        443/TCP                                         5d
ralf             ClusterIP   None           <none>        10888/TCP                                       1h
sprout           ClusterIP   None           <none>        5052/TCP,5054/TCP                               1h
```

bono-depl.yaml

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: bono
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bono
  template:
    metadata:
      labels:
        service: bono
        snmp: enabled
    spec:
      containers:
      - image: "mlda065/bono:latest"
        imagePullPolicy: Always
        name: bono
        ports:
        - containerPort: 22
        - containerPort: 3478
        - containerPort: 5060
        - containerPort: 5062
        - containerPort: 5060
          protocol: "UDP"
        - containerPort: 5062
          protocol: "UDP"
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PUBLIC_IP
          value: 10.3.1.76
        livenessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
          initialDelaySeconds: 30
        readinessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      - image: busybox
        name: tailer
        command: [ "tail", "-F", "/var/log/bono/bono_current.txt" ]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      volumes:
      - name: bonologs
        emptyDir: {}
      imagePullSecrets:
      - name: ~
      restartPolicy: Always
```

Bono-svc.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: bono
spec:
  type: NodePort
  ports:
  - name: "3478"
    port: 3478
  - name: "5060"
    port: 30060
    nodePort: 30060
  - name: "5062"
    port: 5062
  selector:
    service: bono
```

Ellis service

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: ellis
spec:
  replicas: 1
  template:
    metadata:
      labels:
        service: ellis
    spec:
      containers:
      - image: "mlda065/ellis:latest"
        imagePullPolicy: Always
        name: ellis
        ports:
        - containerPort: 22
        - containerPort: 80
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        livenessProbe:
          tcpSocket:
            port: 80
          initialDelaySeconds: 30
        readinessProbe:
          tcpSocket:
            port: 80
      imagePullSecrets:
      - name: ~
      restartPolicy: Always
```

Ellis-svc.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: ellis
spec:
  type: NodePort
  ports:
  - name: "http"
    port: 80
    nodePort: 30080
  selector:
    service: ellis
```

`kubectl describe configmap env-vars`

```
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","data":{"ZONE":"default.svc.cluster.local"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"env-vars","namespace":"default"}...

Data
====
ZONE:
----
default.svc.cluster.local
Events:  <none>
```



Thanks,

Matt
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 25 May 2018 6:42 PM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

Our Helm support is a recent addition, and came from another external contributor. See the Pull Request at https://github.com/Metaswitch/clearwater-docker/pull/85 for the details :)
As it stands at the moment, the chart is good enough for deploying and re-creating a full standard deployment through Helm, but I don't believe it handles more of the complexities of upgrading a clearwater deployment that it potentially could.

We haven't yet done any significant work in setting up Helm charts, or integrating with them in a more detailed manner, so if that's something you're interested in as well, we'd love to work with you to get some more enhancements in. Especially if you have other expert contacts who know more in this area.

(I'm removing some of the thread in the email below, to keep us below the list limits. The online archives will keep all the info though)

Cheers,
Adam

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180604/0c3b3e05/attachment.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: bono_log.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180604/0c3b3e05/attachment.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: dump.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180604/0c3b3e05/attachment-0001.txt>

From Benjamin.Laing at metaswitch.com  Mon Jun  4 08:31:40 2018
From: Benjamin.Laing at metaswitch.com (Benjamin Laing)
Date: Mon, 4 Jun 2018 12:31:40 +0000
Subject: [Project Clearwater] clearwater QoS
Message-ID: <BYAPR02MB4374206875EB6EF89519800BF1670@BYAPR02MB4374.namprd02.prod.outlook.com>

Hi Jaber,


I think there are many other advantages to IMS over other forms of VoIP - probably the biggest being standardization/interoperability between different vendors' solutions.  For example, VoLTE (i.e. mobile) requires IMS, and most fixed-line providers are either already running IMS or migrating to it.



In terms of QoS, media is arguably much more important than signaling.  Project Clearwater does not get involved with media directly.  A fully-featured P-CSCF (such as Metaswitch's Perimeta) would achieve this through the P-CSCF reserving bandwidth via the PCRF.  (Project Clearwater's Bono is a very basic P-CSCF and I don't believe it's been deployed in production.)



Cheers,

Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180604/ca958dc8/attachment.html>

From msc.jaber at gmail.com  Mon Jun  4 09:32:36 2018
From: msc.jaber at gmail.com (jaber daneshamooz)
Date: Mon, 4 Jun 2018 18:02:36 +0430
Subject: [Project Clearwater] clearwater QoS
Message-ID: <5E8787A3-950C-493E-B5F8-D1AE94187ADE@gmail.com>

Hi Benjamin
thanks a lot for your complete response
first of all, I think when we wanna eliminate Mobile Switching Centers and transmit subscribers multimedia data ( voice calls and etc) by packet switches rather than circuit switch, we MUST provide some degrees of QoS for the connection. base on 3gpp standard, IMS used diffserv and mpls to provide QoS for users based on type of connection (conversational, streaming, interactive & background) and based on user?s charging  type(Gold, Silver ?). I don?s see anything about diffserv and mpls in cw docs.


secondly, we?re gonna deploy CW IMS for university(Isfahan University of Technology) and there must be some reasons (like QoS) to prefer CW IMS to some VOIP servers like Freeswitch.

thirdly, we have implemented EPC by Open Air Interface open source and we?re gonna connect CW IMS to it but as you now, Metaswitch Perimeta is not open source, so we?re looking for an alternative. It seems that you know how we can overcome this problem (eg. an alternative open source)


From Adam.Lindley at metaswitch.com  Mon Jun  4 12:52:46 2018
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Mon, 4 Jun 2018 16:52:46 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <ME2PR01MB2884BEA7F75746D055BB7329C2670@ME2PR01MB2884.ausprd01.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971545314ED91FB7F233AC1E2900@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB288450EF460A39FEA9E1D724C2940@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<BY1PR0201MB09689BB9A1645396B16AE810E2940@BY1PR0201MB0968.namprd02.prod.outlook.com>
	<ME2PR01MB2884F01278DC1CD5F7831DFEC26A0@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09710B4690B23BC5F7E8F786E26A0@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884BAB776270D40A8B06347C2690@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971049DFED6D952FFC21DE1E2690@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<MEAPR01MB2885D6C8E38E9D4E26723D4EC26C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971E10C3CE7DB95C3CDBF08E2630@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884BEA7F75746D055BB7329C2670@ME2PR01MB2884.ausprd01.prod.outlook.com>
Message-ID: <CY1PR0201MB09711B8CCD0D9D5870B6B8BFE2670@CY1PR0201MB0971.namprd02.prod.outlook.com>

Hey Matthew,

Quite a lot of questions, so hopefully all the answers are clear. Is the issue still the same Rest Client failure?
At a high level, a lot of this looks like networking issues on the Kubernetes rig side, not the Clearwater side.  I don't have any experience with Weave, which I believe you said your set up was using for the overlay network, and so my ability to help there may be fairly limited. Definitely keen to get everything up and running, but especially without access to the rig to poke around it may be more difficult.

Also, a couple of thoughts on diags:

  *   At the moment I don't think there's much interesting in the clearwater logs, as there's no network traffic hitting the processes. I suspect at the moment you're just sending over short snippets as the rest of the log is nearly identical, however when we get past network related issues, it will be much more useful to have the full log, or much larger excerpts at least.
  *   The tcpdump output you've sent over is pretty difficult to get anything from, with it both just being the CLI output and with no context on what the IPs involved are. If it's possible for future runs, can you grab the full packet ouput by having tcpdump write to a file? Something like `tcpdump -i any port xxxx -w dump.pcap` should do nicely. We can then open that up and see the full contents of all the messages. This will make debugging issues much easier


So, to answer your questions:

  *   When I run ` nc 10.3.1.76 32060 -v` I see nothing (not success, not failure. Just no output and it hangs)
Is the `32060` here just a bad copy-paste, as you've got `30060` below? If not, that's the issue.
If not, then this seems a likely symptom of some network issue.
On our rig, I see:
```root at bono-2597123395-s919s:/# nc 10.230.16.1 -v 30060
Connection to 10.230.16.1 30060 port [tcp/*] succeeded!```
And my bono-depl says the same, and is working, so don't think that's the problem.
If you're unable to get traffic to connect to the bono pod/process, we're not going to be able to get any further, so that'll be the thing to look into. You'll need to track the packets here, and see where they're hitting some issues.


Changing the PUBLIC IP to match my rig IP.
Where is this? In bono-deply.yaml? Anywhere else? (should I have it in .env?) Should this include the port number?
I have only change the depl file here. Nowhere else.
What is this `.env` file. I have not come across these, and don't think it's got any place here. Can you give some more info on what it is and how/where you're using it?

Which IP should this be? Currently I have it equal to the Kubernetes API IP address.
When I run ` kubectl config view | grep 10` I see `server: https://10.3.1.76:6443`<https://10.3.1.76:6443%60> so I set PUBLIC_IP to 10.3.1.76.
What do you see? I think 6443 might be a non-standard port for Kubernetes. Could that be a source of the problem?
I'm afraid I'm not entirely certain here. This may well be some of the answer a bit later on down the line.
Looking at the network setup on our host, I have this IP as the IP of interface cbr0. As I mentioned before, our rig is set up with directly routable networking, rather than e.g. weave or flannel overlay networks. The IP I am using is of the default gateway on the pod network.
However, I don't think this is the reason you are seeing nothing reach your bono pod. I've just run a test setting the public IP to the kubernetes host VM IP, and am seeing the same results as before, with the live tests passing.

My .env file currently says "PUBLIC_IP=" (without an address)
As above, I'm not really sure what this is, or what you're using it for.

Changed the image pull source to our internal one: Hmm, maybe my images were built wrong? Once my pull requests are merged, I'll do a fresh clone and rebuild.
I doubt this will help. The clearwater software is running fine, just receiving no data. If you aren't able to get packets routed into the bono pod, we won't be hitting any of the software in there, so images aren't the issue. If you're able to confirm traffic is reaching bono, but being rejected, then we can look at this

Changed the zone in the config map to my own one, 'ajl.svc.cw-k8s.test': Why are you using the non-default zone ajl.svc.cw-k8s.test? Where did that come from? I thought my original Azure issue was because I used a zone which wasn't default.svc.cluster.local. What is that parameter? What does it mean? Why is the default what it is?
Our rig is configured to allow multiple users running in their own namespaces. This simple stops my deployment clashing with any other.

My test command is:
rake test[default.svc.cw-k8s.test] PROXY=10.3.1.76 PROXY_PORT=30060 SIGNUP_CODE='secret' ELLIS=10.3.1.76:30080 TESTS="Basic call - mainline"

I'm wrapping secret in single apostrophes. Without the apostrophes the result is the same.
This looks reasonable. However, if you're still getting the Rest client error, I suspect something strange is happening. Would be very helpful to have full pcap files, possible on the test box, bono, and ellis all at the same time. Can you get these using a command like above?
You should be able to open them up in Wireshark, and take a deeper look into them; see if you can find anything unusual in the messages between the test box and ellis.
As I said before, a lot of this seems to be network related, and so even with diagnostics like these I'm going to be less able to help. You may well have better luck, and a faster turn around, digging in to them there, when you'll be able to see what's missing and grab that too. Follow the flow between test box and clearwater, and see when the message start dropping or getting error responses back, and you should be able to track it down a fair bit. If you're not sure how to go about that, ping back :)

Every pod (except etcd) has a shared_config file which says:

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 04 June 2018 09:24
To: Adam Lindley <Adam.Lindley at metaswitch.com>; clearwater at lists.projectclearwater.org
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

I forgot to mention,

When I run ` nc 10.3.1.76 32060 -v` I see nothing (not success, not failure. Just no output and it hangs)

Also, my bono-depl.yaml file says "containerPort: 5060" not 30060. Is that right?



When I run a tcp dump on the test machine I see dump.txt (attached)
(I'm not sure how this mailing list will cope with attachments)

The bono logs from that time are attached as bono_log.txt. (I inserted comments with ### to make it clear when the test was running.)

For some reason there is no soft link called /var/log/ellis/ellis_current.txt
The contents of ellis-err.log is "No handlers could be found for logger "metaswitch.utils"" I'm not sure whether that appeared during the test or not.
ellis_20180604T070000Z.txt is empty. (No new entries during the test)

I modified the log level on both ellis and bono, and restarted the bono and ellis service respectively, prior to running the test.

Regards,
Matt



From: Davis, Matthew
Sent: Monday, 4 June 2018 5:32 PM
To: 'Adam Lindley' <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,
Let's focus on the 3 differences:

Changing the PUBLIC IP to match my rig IP.
Where is this? In bono-deply.yaml? Anywhere else? (should I have it in .env?) Should this include the port number?
Which IP should this be? Currently I have it equal to the Kubernetes API IP address.
When I run ` kubectl config view | grep 10` I see `server: https://10.3.1.76:6443`<https://10.3.1.76:6443%60> so I set PUBLIC_IP to 10.3.1.76.
What do you see? I think 6443 might be a non-standard port for Kubernetes. Could that be a source of the problem?
My .env file currently says "PUBLIC_IP=" (without an address)

Changed the image pull source to our internal one: Hmm, maybe my images were built wrong? Once my pull requests are merged, I'll do a fresh clone and rebuild.

Changed the zone in the config map to my own one, 'ajl.svc.cw-k8s.test': Why are you using the non-default zone ajl.svc.cw-k8s.test? Where did that come from? I thought my original Azure issue was because I used a zone which wasn't default.svc.cluster.local. What is that parameter? What does it mean? Why is the default what it is?

My test command is:
rake test[default.svc.cw-k8s.test] PROXY=10.3.1.76 PROXY_PORT=30060 SIGNUP_CODE='secret' ELLIS=10.3.1.76:30080 TESTS="Basic call - mainline"

I'm wrapping secret in single apostrophes. Without the apostrophes the result is the same.

Every pod (except etcd) has a shared_config file which says:

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


Thanks,
Matt

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 1 June 2018 3:29 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

Sorry to hear it isn't working yet. Looking at the output of rake test, you're hitting a forbidden in the RestClient, and so it's something on the Http calls. I think your issue may just be a mismatch in the value of 'SIGNUP_CODE' that you're passing in to the rake test command, and what's on your deployment. This should default to 'secret', and I don't see anything in your configmap that would change that, so can you double check the test command includes " SIGNUP_CODE='secret' ". If you have made any changes to the signup key then obviously it'll need to match those. If you want to double check the value, take a look in the /etc/clearwater/shared_config file in one of the pods.

I have double checked a deployment on our rig, copying your provided yaml files over directly to make sure there isn't anything odd in there, and the live tests were able to run fine, following the deployment steps you're using. I did have some trouble getting the script to work as copied over, but think that's just outlook formatting quotes wrong etc. Manually running the commands it all worked as expected. The only changes I made were:

  *   Changing the PUBLIC IP to match my rig IP.
  *   Changed the image pull source to our internal one
  *   Changed the zone in the config map to my own one, 'ajl.svc.cw-k8s.test'

With this all deployed, the following test command passed with no issue:
    rake test[ajl.svc.cw-k8s.test] PROXY=10.230.16.1 PROXY_PORT=30060 SIGNUP_CODE='secret' ELLIS=10.230.16.1:30080 TESTS="Basic call - mainline"

If the issue isn't the signup key, can we try getting some more diags that we can take a look into? In particular, I think we would benefit from:

  *   A packet capture on the node you are running the live tests on, when you hit the errors below
  *   The bono logs, at debug level, from the same time. To set up debug logging, you need to add 'log_level=5' to /etc/clearwater/user_settings (creating if needed), and restart the bono service
  *   The ellis logs from the same time

Running the tcpdump on the test node should mean we get to see the full set of flows, and you can likely read through that yourself to work out any following issues you find hiding behind this next one.
Any other diagnostics you can gather would obviously also be useful, but with the above, assuming traffic is reaching the pods, we should be able to work out the issue.

On your connectivity tests, you won't be able to connect to the bono service using 'nc localhost 30060', because that attempts to connect using the localhost IP. We have set the bono service up to listen on the 'PUBLIC_IP', i.e. the host IP. If you try running 'nc 10.3.1.76 30060 -v' you should see successful connection. (Or on whichever host IP you have configured it to listen).
The log output you are seeing on restarting Bono is also benign. These are again simply an artefact of some behaviour we want to have in VMs, but that is not needed in these containers. You can safely ignore this output.

Good luck, and let us know where you get with debugging.
Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 30 May 2018 08:27
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,
# Openstack Install

I only mentioned the helm charts just in case the almost empty charts on my machine were the source of the error. I personally have no experience with Helm, so I can't help you with any development.
I applied that latest change to the bono service port number. It still doesn't work.

How can I check whether the rake tests are failing on the bono side, as opposed to failing on the ellis side? Maybe the reason tcpdumps shows nothing in Bono during the rake tests is because the tests failed to create a user in ellis, and never got to the bono part?

Rake output:
```
Basic Call - Mainline (TCP) - Failed
  RestClient::Forbidden thrown:
   - 403 Forbidden
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/abstract_response.rb:74:in `return!'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:495:in `process_result'
...
```
If I go inside the ellis container and run `nc localhost 80 -v` I see that it establishes a connection.
If I go inside the bono container and run `nc localhost 5060 -v` or `nc localhost 30060 -v` it fails to connect. So from within the bono pod I cannot connect to the localhost. To me that suggests that the problem is caused by something inside bono, not the networking between pods. What happens when you try `nc localhost 32060 -v` in your deployment?
The logs inside bono are an echo of an error message from sprout. Does that matter?

```
30-05-2018 06:46:59.432 UTC [7f45527e4700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 4
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:244: Reraising all alarms with a known state
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:37: sprout issued 1012.3 alarm
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:37: sprout issued 1013.3 alarm
```

Those timestamps don't correspond to the rake tests. They just happen every 30 seconds.

When I restart the bono service it says: `63: ulimit: open files: cannot modify limit: Invalid argument`
Does that matter? (I've seen that error message everywhere. I have no idea what it means)

I've appended the yaml files to the end of this email.

# Azure Install

I had a chat to Microsoft. It seems that your hunch was correct. HTTP Application Routing only works on port 80 and 443. Furthermore, I cannot simply route SIP calls through port 443 because the routing does some HTTP specific packet inspection things. So I'll have to give up on that approach and go for a more vanilla, manually configured NodePort approach (either still on AKS but without HTTP Application Routing, or on Openstack). So I'm even more keen to solve the aforementioned issues.


# Yamls and stuff

I'm pasting them again just in case I've forgotten something. 10.3.1.76 is the ip address of my cluster.

Here's a script I'm using to tear down and rebuild everything. (Just incase `kubectl apply -f something.yaml` doesn't actually propagate the change fully) The while loops in this script are just to wait until the previous step has finished.

```
set -x
cd clearwater-docker-master/kubernetes
kubectl delete -f ./
kubectl delete configmap env-vars
set -e
echo 'waiting until old pods are all deleted'
while [ $(kubectl get pods | grep ^NAME -v | wc -l ) -neq 0]
do
   sleep 5
done
echo "creating new pods"
kubectl create configmap env-vars --from-literal=ZONE=default.svc.cluster.local
kubectl apply -f ./
while [ $(kubectl get pods | grep "2/2" | grep bono | wc -l) -neq 1 ]
do
   sleep 5
done
BONO=$(kubectl get pods | grep "2/2" | grep bono | awk '{ print $1 }')
echo "Bono is up as $BONO"
kubectl exec -it $BONO -- apt-get -y install vim
kubectl exec -it $BONO -- sed -i -e 's/--pcscf=5060,5058/--pcscf=30060,5058/g' /etc/init.d/bono
kubectl exec -it $BONO service bono restart
while [ $(kubectl get pods | grep "0/" | wc -l) -neq 0 ]
do
   sleep 5
done
echo "All pods are up now"
kubectl get pods
echo "Done"
```

`kubectl get services`

```
NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                         AGE
astaire          ClusterIP   None           <none>        11311/TCP                                       1h
bono             NodePort    10.0.168.197   <none>        3478:32214/TCP,30060:30060/TCP,5062:30144/TCP   1h
cassandra        ClusterIP   None           <none>        7001/TCP,7000/TCP,9042/TCP,9160/TCP             1h
chronos          ClusterIP   None           <none>        7253/TCP                                        1h
ellis            NodePort    10.0.53.199    <none>        80:30080/TCP                                    1h
etcd             ClusterIP   None           <none>        2379/TCP,2380/TCP,4001/TCP                      1h
homer            ClusterIP   None           <none>        7888/TCP                                        1h
homestead        ClusterIP   None           <none>        8888/TCP                                        1h
homestead-prov   ClusterIP   None           <none>        8889/TCP                                        1h
kubernetes       ClusterIP   10.0.0.1       <none>        443/TCP                                         5d
ralf             ClusterIP   None           <none>        10888/TCP                                       1h
sprout           ClusterIP   None           <none>        5052/TCP,5054/TCP                               1h
```

bono-depl.yaml

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: bono
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bono
  template:
    metadata:
      labels:
        service: bono
        snmp: enabled
    spec:
      containers:
      - image: "mlda065/bono:latest"
        imagePullPolicy: Always
        name: bono
        ports:
        - containerPort: 22
        - containerPort: 3478
        - containerPort: 5060
        - containerPort: 5062
        - containerPort: 5060
          protocol: "UDP"
        - containerPort: 5062
          protocol: "UDP"
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PUBLIC_IP
          value: 10.3.1.76
        livenessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
          initialDelaySeconds: 30
        readinessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      - image: busybox
        name: tailer
        command: [ "tail", "-F", "/var/log/bono/bono_current.txt" ]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      volumes:
      - name: bonologs
        emptyDir: {}
      imagePullSecrets:
      - name: ~
      restartPolicy: Always
```

Bono-svc.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: bono
spec:
  type: NodePort
  ports:
  - name: "3478"
    port: 3478
  - name: "5060"
    port: 30060
    nodePort: 30060
  - name: "5062"
    port: 5062
  selector:
    service: bono
```

Ellis service

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: ellis
spec:
  replicas: 1
  template:
    metadata:
      labels:
        service: ellis
    spec:
      containers:
      - image: "mlda065/ellis:latest"
        imagePullPolicy: Always
        name: ellis
        ports:
        - containerPort: 22
        - containerPort: 80
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        livenessProbe:
          tcpSocket:
            port: 80
          initialDelaySeconds: 30
        readinessProbe:
          tcpSocket:
            port: 80
      imagePullSecrets:
      - name: ~
      restartPolicy: Always
```

Ellis-svc.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: ellis
spec:
  type: NodePort
  ports:
  - name: "http"
    port: 80
    nodePort: 30080
  selector:
    service: ellis
```

`kubectl describe configmap env-vars`

```
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","data":{"ZONE":"default.svc.cluster.local"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"env-vars","namespace":"default"}...

Data
====
ZONE:
----
default.svc.cluster.local
Events:  <none>
```



Thanks,

Matt
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 25 May 2018 6:42 PM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

Our Helm support is a recent addition, and came from another external contributor. See the Pull Request at https://github.com/Metaswitch/clearwater-docker/pull/85 for the details :)
As it stands at the moment, the chart is good enough for deploying and re-creating a full standard deployment through Helm, but I don't believe it handles more of the complexities of upgrading a clearwater deployment that it potentially could.

We haven't yet done any significant work in setting up Helm charts, or integrating with them in a more detailed manner, so if that's something you're interested in as well, we'd love to work with you to get some more enhancements in. Especially if you have other expert contacts who know more in this area.

(I'm removing some of the thread in the email below, to keep us below the list limits. The online archives will keep all the info though)

Cheers,
Adam

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180604/a5d1ae16/attachment.html>

From denise at Optonline.net  Tue Jun  5 13:33:48 2018
From: denise at Optonline.net (Denise Antonio)
Date: Tue, 05 Jun 2018 10:33:48 -0700
Subject: [Project Clearwater] fw: Re: Re: Swift
Message-ID: <mailman.0.1561167848.26269.clearwater_lists.projectclearwater.org@lists.projectclearwater.org>

 Denise Antonio
Account Manager 
Teco Diagnostics | 1268 N. Lakeview Ave. Anaheim, CA 92807 USA |
t: 714.463.1111 | f: 714.463.1169
 
 Confidentiality Notice:
This Email, and any attachments, may contain internal or confidential information and is intended solely for the individual to whom it is addressed. It may contain sensitive or protectively marked material and should be handled accordingly. If this Email has been misdirected, please notify the author immediately. If you are not the intended recipient you must not disclose, distribute, copy, print or rely on any of the information contained in it or attached, and all copies must be deleted immediately.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180605/da5c037c/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: paymentreceipt.doc
Type: application/msword
Size: 11210 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180605/da5c037c/attachment.doc>

From William.Yates at metaswitch.com  Tue Jun  5 10:39:18 2018
From: William.Yates at metaswitch.com (William Yates)
Date: Tue, 5 Jun 2018 14:39:18 +0000
Subject: [Project Clearwater] OpenStack version the Heat templates have
 been tested on
In-Reply-To: <CAFFdzJTDEaQVZtdtumx=3d6+p9JyB+SBt_6OH8CCT9ZmkXf6uQ@mail.gmail.com>
References: <CAFFdzJTDEaQVZtdtumx=3d6+p9JyB+SBt_6OH8CCT9ZmkXf6uQ@mail.gmail.com>
Message-ID: <BLUPR0201MB149076776ECC3AB9BE9D51B895660@BLUPR0201MB1490.namprd02.prod.outlook.com>

Hi Tapiwa,

The templates have been tested against Openstack Liberty.

Are there any more diagnostics output that you can provide? Passing '--debug' to the heat stack-create command might help.

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Tapiwa Chindeka
Sent: 02 June 2018 19:41
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] OpenStack version the Heat templates have been tested on

Hi

I have been having problems installing Clearwater using the Heat templates from GitHub in OpenStack Ocata and I suspect it might be a version compatibility issue.

Can someone please tell which OpenStack versions the templates have been tested on?

Regards
Tapiwa
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180605/b208e981/attachment.html>

From rajib.024415 at gmail.com  Thu Jun  7 00:08:05 2018
From: rajib.024415 at gmail.com (Rajib Rajkhowa)
Date: Thu, 7 Jun 2018 09:38:05 +0530
Subject: [Project Clearwater] Call getting disconnected in 30 sec using
 Clearwater vIMS (old architecture)
Message-ID: <CAK1gR+aP7ds7hyeaohkswmuU7N98Cpy4AWj_mcGuT-Nh=1zC1Q@mail.gmail.com>

 Hi,
  I am using old vIMS architecture (for some business reason) where Vellum
& Dime are not present. I am using a multi VM setup on OpenStack and for
SIP client I am using XLite as SIP client. After a lot of struggle and
poring over Clearwater mailing lists, I could resolve most of the initial
issues. But now I am facing an issue of call getting disconnected within 30
secs of establishment.

Interesting observation made, the calling party session continues to be
established only the called party session gets disconnected after 30 sec or
32 sec

Anticipating your kind support and suggestion



Thanks & regards,
Rajib
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180607/5c53f935/attachment.html>

From William.Yates at metaswitch.com  Thu Jun  7 11:59:53 2018
From: William.Yates at metaswitch.com (William Yates)
Date: Thu, 7 Jun 2018 15:59:53 +0000
Subject: [Project Clearwater] Call getting disconnected in 30 sec using
 Clearwater vIMS (old architecture)
In-Reply-To: <CAK1gR+aP7ds7hyeaohkswmuU7N98Cpy4AWj_mcGuT-Nh=1zC1Q@mail.gmail.com>
References: <CAK1gR+aP7ds7hyeaohkswmuU7N98Cpy4AWj_mcGuT-Nh=1zC1Q@mail.gmail.com>
Message-ID: <BLUPR0201MB149090591F7723D18768100795640@BLUPR0201MB1490.namprd02.prod.outlook.com>

Hi Rajib,

You can investigate further by turning on trace as described here, and taking a look through the logs around the time of the disconnection.

https://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html#bono
https://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html#sprout

Inspecting the diags, poring over a tcpdump capture, should help.


Also, this thread might be helpful - in this case, the ACK is being rejected by bono: http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/2015-September/002437.html

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Rajib Rajkhowa
Sent: 07 June 2018 05:08
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Call getting disconnected in 30 sec using Clearwater vIMS (old architecture)

Hi,
  I am using old vIMS architecture (for some business reason) where Vellum & Dime are not present. I am using a multi VM setup on OpenStack and for SIP client I am using XLite as SIP client. After a lot of struggle and poring over Clearwater mailing lists, I could resolve most of the initial issues. But now I am facing an issue of call getting disconnected within 30 secs of establishment.

Interesting observation made, the calling party session continues to be established only the called party session gets disconnected after 30 sec or 32 sec

Anticipating your kind support and suggestion



Thanks & regards,
Rajib

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180607/598ad27e/attachment.html>

From rajib.024415 at gmail.com  Thu Jun  7 12:23:17 2018
From: rajib.024415 at gmail.com (Rajib Rajkhowa)
Date: Thu, 7 Jun 2018 21:53:17 +0530
Subject: [Project Clearwater] Call getting disconnected in 30 sec using
 Clearwater vIMS (old architecture)
In-Reply-To: <BLUPR0201MB149090591F7723D18768100795640@BLUPR0201MB1490.namprd02.prod.outlook.com>
References: <CAK1gR+aP7ds7hyeaohkswmuU7N98Cpy4AWj_mcGuT-Nh=1zC1Q@mail.gmail.com>
	<BLUPR0201MB149090591F7723D18768100795640@BLUPR0201MB1490.namprd02.prod.outlook.com>
Message-ID: <CAK1gR+Y+gR2FmdCMAtuB9eOZf4zSW81Hm77JjLCcgV7j4JpBxQ@mail.gmail.com>

Hi Will,
       Thanks for sharing your valuable views. I have captured Wireshark
traces and TCPdump in addition to the components logs and Clearwater
mailing archives.
Currently doing some deep dive into the logs and traces, hopefully
something turns up in the logs and catches my eye.

Thanks & Regards,
Rajib

On Thu 7 Jun, 2018, 21:31 William Yates, <William.Yates at metaswitch.com>
wrote:

> Hi Rajib,
>
>
>
> You can investigate further by turning on trace as described here, and
> taking a look through the logs around the time of the disconnection.
>
>
>
>
> https://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html#bono
>
> https://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html#sprout
>
> Inspecting the diags, poring over a tcpdump capture, should help.
>
>
>
> Also, this thread might be helpful - in this case, the ACK is being rejected by bono: http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/2015-September/002437.html
>
> Cheers,
>
> Will
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Rajib Rajkhowa
> *Sent:* 07 June 2018 05:08
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Call getting disconnected in 30 sec using
> Clearwater vIMS (old architecture)
>
>
>
> Hi,
>
>   I am using old vIMS architecture (for some business reason) where Vellum
> & Dime are not present. I am using a multi VM setup on OpenStack and for
> SIP client I am using XLite as SIP client. After a lot of struggle and
> poring over Clearwater mailing lists, I could resolve most of the initial
> issues. But now I am facing an issue of call getting disconnected within 30
> secs of establishment.
>
>
>
> Interesting observation made, the calling party session continues to be
> established only the called party session gets disconnected after 30 sec or
> 32 sec
>
>
>
> Anticipating your kind support and suggestion
>
>
>
>
>
>
>
> Thanks & regards,
>
> Rajib
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
>
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180607/50a5bf79/attachment.html>

From Benjamin.Laing at metaswitch.com  Fri Jun  8 02:43:57 2018
From: Benjamin.Laing at metaswitch.com (Benjamin Laing)
Date: Fri, 8 Jun 2018 06:43:57 +0000
Subject: [Project Clearwater] Clearwater Create Identity Error
Message-ID: <BYAPR02MB4374A59067BA6F4AE8F2FFC1F17B0@BYAPR02MB4374.namprd02.prod.outlook.com>

Hi Navdeep,

You need to run create_numbers.sh using python in the venv (see https://github.com/Metaswitch/clearwater-docker/blob/master/ellis/create_numbers.sh).

If this fails to work, then it's worth reading the Ellis troubleshooting page (https://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html#ellis) .

Thanks,

Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180608/cda6aa4d/attachment.html>

From Benjamin.Laing at metaswitch.com  Fri Jun  8 03:07:25 2018
From: Benjamin.Laing at metaswitch.com (Benjamin Laing)
Date: Fri, 8 Jun 2018 07:07:25 +0000
Subject: [Project Clearwater] Chef server and clients are down at the
 end of installation
Message-ID: <BYAPR02MB4374DC515C8C468F5C0B9667F17B0@BYAPR02MB4374.namprd02.prod.outlook.com>

Hello Amir,

Apologies in the delay in getting back to you.

Have you tried following the steps in the knife troubleshooting guide - https://github.com/Metaswitch/chef/blob/master/docs/knife_commands.md#troubleshooting ? Specifically, have you checked for rogue entries using *<knife client list>* and *<knife node list>*? There could also be an issue with your cookbooks - ensure that these are uploaded as per instructions in the doc.

Let me know how you get on.

Thanks,

Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180608/ea985633/attachment.html>

From Benjamin.Laing at metaswitch.com  Fri Jun  8 08:59:06 2018
From: Benjamin.Laing at metaswitch.com (Benjamin Laing)
Date: Fri, 8 Jun 2018 12:59:06 +0000
Subject: [Project Clearwater] Ellis failed to update server error
Message-ID: <BYAPR02MB4374F1E641F41DB5D23E36C3F17B0@BYAPR02MB4374.namprd02.prod.outlook.com>

Hi Tapiwa,

It may be that Cassandra isn't clustered correctly - you can check following instructions here https://clearwater.readthedocs.io/en/stable/Handling_Multiple_Failed_Nodes.html?#vellum-cassandra-configuration.

If that doesn't throw anything up, the homestead prov logs (/var/log/homestead-prov) are a good place to look. You can also turn on debugging on Ellis as per https://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html?#ellis

Thanks,

Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180608/b130b5a1/attachment.html>

From amirhatami1000 at gmail.com  Sat Jun  9 05:30:41 2018
From: amirhatami1000 at gmail.com (Amir Moahammad Hatami)
Date: Sat, 9 Jun 2018 14:00:41 +0430
Subject: [Project Clearwater] Clearwater Digest, Vol 61, Issue 33
In-Reply-To: <mailman.1.1527609601.14833.clearwater_lists.projectclearwater.org@lists.projectclearwater.org>
References: <mailman.1.1527609601.14833.clearwater_lists.projectclearwater.org@lists.projectclearwater.org>
Message-ID: <CAEtN4MDWmRTV5BaVJX_YjJbBRXg6E1savzSpN3-dSRV=kMWM_Q@mail.gmail.com>

I think users should be provisioned automatically. Because in the stress
testing there is nothing said for provisioning users. If its not so kindly
let me know how to do it?

Thanks





On Tue, May 29, 2018 at 8:30 PM, <
clearwater-request at lists.projectclearwater.org> wrote:

> Send Clearwater mailing list submissions to
>         clearwater at lists.projectclearwater.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.projectclearwater.org/mailman/
> listinfo/clearwater_lists.projectclearwater.org
>
> or, via email, send a message with subject or body 'help' to
>         clearwater-request at lists.projectclearwater.org
>
> You can reach the person managing the list at
>         clearwater-owner at lists.projectclearwater.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Clearwater digest..."
>
>
> Today's Topics:
>
>    1. Re: getting 403 Forbidden instead of 401 in SIPp stress
>       testing. Please help (Mark Perryman)
>    2. clearwater QoS (jaber daneshamooz)
>    3. Re: Calls are failure with SIP 503 Service Unavailable
>       (Mark Perryman)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 29 May 2018 08:45:07 +0000
> From: Mark Perryman <Mark.Perryman at metaswitch.com>
> To: "clearwater at lists.projectclearwater.org"
>         <clearwater at lists.projectclearwater.org>
> Subject: Re: [Project Clearwater] getting 403 Forbidden instead of 401
>         in SIPp stress testing. Please help
> Message-ID:
>         <CY4PR02MB2518D06D19BECE40B0853BBF8F6D0 at CY4PR02MB2518.
> namprd02.prod.outlook.com>
>
> Content-Type: text/plain; charset="us-ascii"
>
> rajapaue,
>
> A 403 Forbidden when trying to register would often indicate that the
> username or password are wrong, or that the subscriber has not been
> provisioned correctly.
>
>
>
> sip:2010000000 at clearwater.amir.test.com
>
>
> How have you added that subscriber?
>
> Mark.
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.
> projectclearwater.org/attachments/20180529/7c15f2ce/attachment-0001.html>
>
> ------------------------------
>
> Message: 2
> Date: Sat, 26 May 2018 12:06:27 +0430
> From: jaber daneshamooz <msc.jaber at gmail.com>
> To: clearwater at lists.projectclearwater.org
> Subject: [Project Clearwater] clearwater QoS
> Message-ID: <69566153-E1D8-4C90-A0F8-B1CFCE20C60E at gmail.com>
> Content-Type: text/plain; charset=utf-8
>
> Hi,
> does clearwater provide QoS by using method such as MPLS and Differentiate
> Service?
> if it does, how it works?
> if it doesn?t , can we add QoS to it?
>
>
> ------------------------------
>
> Message: 3
> Date: Tue, 29 May 2018 10:00:11 +0000
> From: Mark Perryman <Mark.Perryman at metaswitch.com>
> To: "clearwater at lists.projectclearwater.org"
>         <clearwater at lists.projectclearwater.org>
> Subject: Re: [Project Clearwater] Calls are failure with SIP 503
>         Service Unavailable
> Message-ID:
>         <CY4PR02MB25184904FF439F918C79D7958F6D0 at CY4PR02MB2518.
> namprd02.prod.outlook.com>
>
> Content-Type: text/plain; charset="us-ascii"
>
> Hi Kapil,
>
> It is hard to debug this without more logging information.  Are you able
> to reproduce this error and send the log files?
>
> Thanks,
>
> Mark Perryman.
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.
> projectclearwater.org/attachments/20180529/288f68f3/attachment-0001.html>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
> ------------------------------
>
> End of Clearwater Digest, Vol 61, Issue 33
> ******************************************
>



-- 
Amir Mohammad Hatami
B.Sc in Electrical Engineering
Sharif University of technology
http://ee.sharif.ir/~hatami
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180609/95376c1d/attachment.html>

From Mark.Perryman at metaswitch.com  Tue Jun 12 05:00:37 2018
From: Mark.Perryman at metaswitch.com (Mark Perryman)
Date: Tue, 12 Jun 2018 09:00:37 +0000
Subject: [Project Clearwater] getting 403 Forbidden instead of 401 in
 SIPp stress testing. Please help
In-Reply-To: <578442439.1484281.1527342266916@mail.yahoo.com>
References: <578442439.1484281.1527342266916.ref@mail.yahoo.com>
	<578442439.1484281.1527342266916@mail.yahoo.com>
Message-ID: <BN7PR02MB42432AC0532031BFDF9D2F348F7F0@BN7PR02MB4243.namprd02.prod.outlook.com>

Amir,

Have you followed the process on http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html#deploying-a-stress-node to provision subscribers?

> Follow this process<https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%20Numbers.md> to bulk provision the number of subscribers you want. As a general guideline, we?d expect a small deployment (with one Sprout, Vellum and Dime, each with one full CPU core) to handle at least 30,000 subscribers.

If that doesn?t help, then try looking in sprout and homestead logs in debug mode.


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of rajapaue at yahoo.com
Sent: 26 May 2018 14:44
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] getting 403 Forbidden instead of 401 in SIPp stress testing. Please help

Hi,

I am trying to test my IMS core which is ran in AWS. I installed the sipp module using automated chef. But I run the script to test I get the following error. It is a long time I am working to find the reason. Kindly let me know the reason.

sipp: The following events occurred:
2018-05-23 13:24:18.329759 1527081858.329759: Aborting call on unexpected message for Call-Id '1-4325 at 172.31.42.209': while expecting '401' (index 1), received 'SIP/2.0 403 Forbidden
Via: SIP/2.0/TCP 172.31.42.209:36173;received=172.31.42.209;branch=z9hG4bK-4325-1-0
Call-ID: 1-4325 at 172.31.42.209<mailto:1-4325 at 172.31.42.209>
From: <sip:2010000000 at clearwater.amir.test.com>;tag=4325SIPpTag001
To: <sip:2010000000 at clearwater.amir.test.com>;tag=z9hG4bKPjz9AXA2brIljLxzEMQ9Akc1jTESHj18Sm
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
Content-Length:  0

Thank you in advance.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180612/c0030bd8/attachment.html>

From sean.cook at lohengrin.net  Tue Jun 12 12:30:57 2018
From: sean.cook at lohengrin.net (Sean Cook)
Date: Tue, 12 Jun 2018 11:30:57 -0500
Subject: [Project Clearwater] Invalid OFV checksum on cw-aio.ova
Message-ID: <CAP_euyVQ+fH5974r1RjBZyiSL8=SCOzbasjx_53oK7Tcp06DfQ@mail.gmail.com>

Downloaded the cw-aio.ova from the following site:
http://vm-images.cw-ngv.com/

Direct URL: http://vm-images.cw-ngv.com/cw-aio.ova

When deploying OFV template in vSphere 6.5 I get an error indicating the
provided manifest is invalid: Invalid OFV checksum algorithm: SHA1


Any thoughts on this?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180612/0614d4d0/attachment.html>

From fescarosbuechs at vmware.com  Tue Jun 12 12:33:48 2018
From: fescarosbuechs at vmware.com (Frank Escaros-Buechsel)
Date: Tue, 12 Jun 2018 16:33:48 +0000
Subject: [Project Clearwater] Invalid OFV checksum on cw-aio.ova
In-Reply-To: <CAP_euyVQ+fH5974r1RjBZyiSL8=SCOzbasjx_53oK7Tcp06DfQ@mail.gmail.com>
References: <CAP_euyVQ+fH5974r1RjBZyiSL8=SCOzbasjx_53oK7Tcp06DfQ@mail.gmail.com>
Message-ID: <BN6PR05MB30742B950AFB01ECFE3CA820B97F0@BN6PR05MB3074.namprd05.prod.outlook.com>

Hi Sean,
This should help ?

https://github.com/Metaswitch/project-clearwater-issues/issues/31

Kind Regards,
Frank Escaros-Buechsel
Consulting Architect NFV
fescarosbuechs at vmware.com<mailto:fescarosbuechs at vmware.com>
Parnell  House, Barrack Square, Ballincollig, Co. Cork, P31 PF68, IRL
Twitter: @fbuechsel

From: Clearwater <clearwater-bounces at lists.projectclearwater.org> On Behalf Of Sean Cook
Sent: Tuesday 12 June 2018 17:31
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Invalid OFV checksum on cw-aio.ova

Downloaded the cw-aio.ova from the following site:
http://vm-images.cw-ngv.com/<https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fvm-images.cw-ngv.com%2F&data=02%7C01%7Cfescarosbuechs%40vmware.com%7C0947717ec33c4f74ff9308d5d0821753%7Cb39138ca3cee4b4aa4d6cd83d9dd62f0%7C1%7C1%7C636644179520181374&sdata=E1ufLO9t9292FT9xlymMP84E36S9WPhkgmWD9HnAFYQ%3D&reserved=0>

Direct URL: http://vm-images.cw-ngv.com/cw-aio.ova<https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fvm-images.cw-ngv.com%2Fcw-aio.ova&data=02%7C01%7Cfescarosbuechs%40vmware.com%7C0947717ec33c4f74ff9308d5d0821753%7Cb39138ca3cee4b4aa4d6cd83d9dd62f0%7C1%7C1%7C636644179520181374&sdata=as1Kgi74YC8Cf2yZ%2FDzVWjqu41ItSYboM3SrA1wCQXE%3D&reserved=0>

When deploying OFV template in vSphere 6.5 I get an error indicating the provided manifest is invalid: Invalid OFV checksum algorithm: SHA1


Any thoughts on this?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180612/2f82c031/attachment.html>

From David.Luong at interoptechnologies.com  Fri Jun 15 17:27:21 2018
From: David.Luong at interoptechnologies.com (Luong, David)
Date: Fri, 15 Jun 2018 17:27:21 -0400
Subject: [Project Clearwater] To header in INVITE is being changed
Message-ID: <D7499969.19E89%David.Luong@interoptechnologies.com>

Hi,

We encountered a situation where the To field of the INVITE we sent to sprout is being changed when routing to the next node.  It is our understanding that the To and From header should not be altered unless behaving as B2BUA. Please see the log snippet below where I highlighted the header in question as the port information was dropped from the URI when the INVITE is being routed to the next node. Any feedback is appreciated.

15-06-2018 20:34:40.609 UTC [7f13b53d1700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg INVITE/cseq=1 (rdata0x7f13b0ade090)
15-06-2018 20:34:40.609 UTC [7f13b53d1700] Verbose common_sip_processing.cpp:87: RX 1688 bytes Request msg INVITE/cseq=1 (rdata0x7f13b0ade090) from TCP 172.27.0.81:54858:
--start msg--

INVITE sip:ImService at sc01.ivc.iot1.com:5510;as.session=2 SIP/2.0
Via: SIP/2.0/TCP sc01.sales2.iot1.com:5510;branch=z9hG4bKc3e26892-2a39-4ba3-887d-5545a9ef68c6
From: <tel:+18152579478>;tag=46bb82fb-4f96-42b7-9dcc-9f67d20c0d55
To: <sip:ImService at sc01.ivc.iot1.com:5510;as.session=2>
Call-ID: 13d56e67-87aa-4ae5-be66-8fc11e800eee
CSeq: 1 INVITE
Allow-Events: conference
Supported: timer, recipient-list-invite
Contact: <sip:ImService at sc01.sales2.iot1.com:5510;as.session=1>;isfocus;+g.oma.sip-im;+g.3gpp.iari-ref="urn%3Aurn-7%3A3gpp-application.ims.iari.rcse.im,urn%3Aurn-7%3A3gpp-application.ims.iari.rcs.fthttp,urn%3Aurn-7%3A3gpp-application.ims.iari.rcs.geopush"
Accept-Contact: *;+g.oma.sip-im
Allow: INVITE,ACK,CANCEL,BYE,UPDATE,REFER,SUBSCRIBE,MESSAGE,REGISTER,INFO
User-Agent: IM-serv/OMA2.0 Sales2-RMS/5.0
Session-Expires: 600;refresher=uas
P-Asserted-Identity: <sip:+18152579478 at sales2.iot1.com>
Subject: Football teams
Contribution-ID: d285786190f6407ebcd0f861f1645183
Route: <sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com:5054;transport=TCP;lr;orig;service=scscf>
Content-Type: application/sdp
P-Charging-Vector: icid-value="58b84e00-61f9-1036-00-00-00-50-56-bb-20-00";orig-ioi=sales2.iot1.com;icid-generated-at=172.27.0.140
Content-Length: 397
Max-Forwards: 70

v=0
o=- 16054947157924516231 16054947157924563475 IN IP4 172.28.0.81
s=-
c=IN IP4 172.28.0.81
t=0 0
m=message 9000 TCP/MSRP *
a=accept-types:message/cpim
a=accept-wrapped-types:text/plain message/imdn+xml application/im-iscomposing+xml application/vnd.gsma.rcs-ft-http+xml application/vnd.gsma.rcspushlocation+xml
a=msrp-cema
a=path:msrp://172.28.0.81:9000/Cw2E0W2t;tcp
a=setup:actpass

--end msg--
15-06-2018 20:34:40.609 UTC [7f13b53d1700] Debug pjutils.cpp:1771: Logging SAS Call-ID marker, Call-ID 13d56e67-87aa-4ae5-be66-8fc11e800eee
15-06-2018 20:34:40.609 UTC [7f13b53d1700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f13b0ade090 on worker thread
15-06-2018 20:34:40.609 UTC [7f13b53d1700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f13b0ade090 on worker thread
15-06-2018 20:34:40.609 UTC [7f13b53d1700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f13b0ade090 cloned to 0x7f13b0ae4598
15-06-2018 20:34:40.609 UTC [7f13b53d1700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f13b0ae4598 for worker threads with priority 0
15-06-2018 20:34:40.609 UTC [7f13b53d1700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1a24748
15-06-2018 20:34:40.609 UTC [7f13b53d1700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1a247c0
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug utils.cpp:872: Added IOHook 0x7f14414e8e30 to stack. There are now 1 hooks
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f13b0ae4598
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug thread_dispatcher.cpp:183: Request latency so far = 59us
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg INVITE/cseq=1 (rdata0x7f13b0ae4598)
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug uri_classifier.cpp:172: Classified URI as 5
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug basicproxy.cpp:62: Process INVITE request
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:631: Sproutlet Proxy transaction (0x7f13840db0f0) created. There are now 2 instances
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug basicproxy.cpp:1318: Report SAS start marker - trail (15ddc)
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug pjutils.cpp:719: Cloned Request msg INVITE/cseq=1 (rdata0x7f13b0ae4598) to tdta0x7f13842c6640
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug pjsip: tsx0x7f13840a0 Transaction created for Request msg INVITE/cseq=1 (rdata0x7f13b0ae4598)
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug pjsip: tsx0x7f13840a0 Incoming Request msg INVITE/cseq=1 (rdata0x7f13b0ae4598) in state Null
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug pjsip: tsx0x7f13840a0 State changed from Null to Trying, event=RX_MSG
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug basicproxy.cpp:183: tsx0x7f13840a0148 - tu_on_tsx_state UAS, TSX_STATE RX_MSG state=Trying
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug pjsip:       endpoint Response msg 408/INVITE/cseq=1 (tdta0x7f13842274b0) created
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug basicproxy.cpp:598: Send immediate 100 Trying response
15-06-2018 20:34:40.609 UTC [7f14414e9700] Info pjutils.cpp:1719: Cloning header! 139721798612272
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug pjsip: tsx0x7f13840a0 Sending Response msg 100/INVITE/cseq=1 (tdta0x7f138421d5a0) in state Trying
15-06-2018 20:34:40.609 UTC [7f14414e9700] Verbose common_sip_processing.cpp:103: TX 472 bytes Response msg 100/INVITE/cseq=1 (tdta0x7f138421d5a0) to TCP 172.27.0.81:54858:
--start msg--

SIP/2.0 100 Trying
Via: SIP/2.0/TCP sc01.sales2.iot1.com:5510;received=172.27.0.81;branch=z9hG4bKc3e26892-2a39-4ba3-887d-5545a9ef68c6
Call-ID: 13d56e67-87aa-4ae5-be66-8fc11e800eee
From: <tel:+18152579478>;tag=46bb82fb-4f96-42b7-9dcc-9f67d20c0d55
To: <sip:ImService at sc01.ivc.iot1.com;as.session=2>
CSeq: 1 INVITE
P-Charging-Vector: icid-value="58b84e00-61f9-1036-00-00-00-50-56-bb-20-00";orig-ioi=sales2.iot1.com;icid-generated-at=172.27.0.140
Content-Length:  0


--end msg--
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug pjsip: tsx0x7f13840a0 State changed from Trying to Proceeding, event=TX_MSG
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug basicproxy.cpp:183: tsx0x7f13840a0148 - tu_on_tsx_state UAS, TSX_STATE TX_MSG state=Proceeding
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com:5054;transport=TCP;lr;orig;service=scscf
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:302: Found services param - scscf
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:429: Creating URI for service registrar
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:302: Found services param - scscf
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:455: Constructed URI sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com;transport=TCP;lr;orig;service=registrar
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:1276: Remove top Route header Route: <sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com:5054;transport=TCP;lr;orig;service=scscf>
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com;transport=TCP;lr;orig;service=registrar
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:302: Found services param - registrar
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug uri_classifier.cpp:172: Classified URI as 5
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:429: Creating URI for service subscription
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:302: Found services param - registrar
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:455: Constructed URI sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com;transport=TCP;lr;orig;service=subscription
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:1276: Remove top Route header Route: <sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com;transport=TCP;lr;orig;service=registrar>
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com;transport=TCP;lr;orig;service=subscription
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:302: Found services param - subscription
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:429: Creating URI for service scscf-proxy
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:302: Found services param - subscription
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:455: Constructed URI sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com;transport=TCP;lr;orig;service=scscf-proxy
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:1276: Remove top Route header Route: <sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com;transport=TCP;lr;orig;service=subscription>
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com;transport=TCP;lr;orig;service=scscf-proxy
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:302: Found services param - scscf-proxy
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug scscfsproutlet.cpp:424: S-CSCF Transaction (0x7f138414a7f0) created
15-06-2018 20:34:40.609 UTC [7f14414e9700] Verbose sproutletproxy.cpp:1384: Created Sproutlet scscf-proxy-0x7f138414a7f0 for Request msg INVITE/cseq=1 (tdta0x7f13842c6640)
15-06-2018 20:34:40.609 UTC [7f14414e9700] Verbose sproutletproxy.cpp:2487: Routing Request msg INVITE/cseq=1 (tdta0x7f13842c6640) (1716 bytes) to downstream sproutlet scscf-proxy:
--start msg--

INVITE sip:ImService at sc01.ivc.iot1.com:5510;as.session=2 SIP/2.0
Route: <sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com;transport=TCP;lr;orig;service=scscf-proxy>
Via: SIP/2.0/TCP sc01.sales2.iot1.com:5510;received=172.27.0.81;branch=z9hG4bKc3e26892-2a39-4ba3-887d-5545a9ef68c6
From: <tel:+18152579478>;tag=46bb82fb-4f96-42b7-9dcc-9f67d20c0d55
To: <sip:ImService at sc01.ivc.iot1.com;as.session=2>
Call-ID: 13d56e67-87aa-4ae5-be66-8fc11e800eee
CSeq: 1 INVITE
Allow-Events: conference
Supported: timer, recipient-list-invite
Contact: <sip:ImService at sc01.sales2.iot1.com:5510;as.session=1>;isfocus;+g.oma.sip-im;+g.3gpp.iari-ref="urn%3Aurn-7%3A3gpp-application.ims.iari.rcse.im,urn%3Aurn-7%3A3gpp-application.ims.iari.rcs.fthttp,urn%3Aurn-7%3A3gpp-application.ims.iari.rcs.geopush"
Accept-Contact: *;+g.oma.sip-im
Allow: INVITE, ACK, CANCEL, BYE, UPDATE, REFER, SUBSCRIBE, MESSAGE, REGISTER, INFO
User-Agent: IM-serv/OMA2.0 Sales2-RMS/5.0
Session-Expires: 600;refresher=uas
P-Asserted-Identity: <sip:+18152579478 at sales2.iot1.com>
Subject: Football teams
Contribution-ID: d285786190f6407ebcd0f861f1645183
P-Charging-Vector: icid-value="58b84e00-61f9-1036-00-00-00-50-56-bb-20-00";orig-ioi=sales2.iot1.com;icid-generated-at=172.27.0.140
Max-Forwards: 70
Content-Type: application/sdp
Content-Length:   397

v=0
o=- 16054947157924516231 16054947157924563475 IN IP4 172.28.0.81
s=-
c=IN IP4 172.28.0.81
t=0 0
m=message 9000 TCP/MSRP *
a=accept-types:message/cpim
a=accept-wrapped-types:text/plain message/imdn+xml application/im-iscomposing+xml application/vnd.gsma.rcs-ft-http+xml application/vnd.gsma.rcspushlocation+xml
a=msrp-cema
a=path:msrp://172.28.0.81:9000/Cw2E0W2t;tcp
a=setup:actpass

--end msg--
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('EXTERNAL'->'scscf'/'scscf-proxy')
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('EXTERNAL'->'scscf'/'scscf-proxy')
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:2517: Internal network function boundary: no
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug pjutils.cpp:736: Cloned tdta0x7f13842c6640 to tdta0x7f13842bb9a0
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:1450: Remove top Route header Route: <sip:odi_KCVomZHKWX at sprout01.sales2.iot1.com;transport=TCP;lr;orig;service=scscf-proxy>
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug sproutletproxy.cpp:2115: Adding message 0x7f13842bbfb0 => txdata 0x7f13842bba48 mapping
15-06-2018 20:34:40.609 UTC [7f14414e9700] Verbose sproutletproxy.cpp:1946: scscf-proxy-0x7f138414a7f0 pass initial request Request msg INVITE/cseq=1 (tdta0x7f13842bb9a0) to Sproutlet
15-06-2018 20:34:40.609 UTC [7f14414e9700] Info scscfsproutlet.cpp:471: S-CSCF received initial request
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug uri_classifier.cpp:172: Classified URI as 3
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug scscfsproutlet.cpp:945: Route header references this system
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug scscfsproutlet.cpp:958: Found ODI token KCVomZHKWX
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug aschain.h:109: AsChain inc ref 0x7f138009e4d0 -> 2
15-06-2018 20:34:40.609 UTC [7f14414e9700] Info scscfsproutlet.cpp:965: Original dialog for odi_KCVomZHKWX found: AsChain-orig[0x7f138009e4d0]:2/2
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug scscfsproutlet.cpp:1004: Got our Route header, session case orig, OD=AsChain-orig[0x7f138009e4d0]:2/2
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug pjutils.cpp:294: Served user from P-Asserted-Identity header
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug uri_classifier.cpp:139: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug uri_classifier.cpp:172: Classified URI as 4
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug uri_classifier.cpp:139: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug uri_classifier.cpp:172: Classified URI as 4
15-06-2018 20:34:40.609 UTC [7f14414e9700] Info scscfsproutlet.cpp:635: Found served user, so apply services
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug scscfsproutlet.cpp:1384: Performing originating initiating request processing
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug aschain.cpp:190: Asssociating original SAS trail 89563 with new message SAS trail 89564
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug ifc.cpp:428: SPT class Method: result false
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug ifc.cpp:428: SPT class Method: result false
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug ifc.cpp:582: iFC does not match
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug ifc.cpp:589: AND each SPT match result to determine group result.
OR each group result to determine overall iFC match.

SPT in group 0 is not matched.
SPT in group 1 is not matched.
Group 0 is not matched.
Group 1 is not matched.

15-06-2018 20:34:40.609 UTC [7f14414e9700] Info scscfsproutlet.cpp:1420: Completed applying originating services
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true
15-06-2018 20:34:40.609 UTC [7f14414e9700] Debug uri_classifier.cpp:172: Classified URI as 5
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug uri_classifier.cpp:172: Classified URI as 5
15-06-2018 20:34:40.610 UTC [7f14414e9700] Info scscfsproutlet.cpp:1436: New URI string is sip:ImService at sc01.ivc.iot1.com:5510;as.session=2
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug scscfsproutlet.cpp:1452: Routing to BGCF
15-06-2018 20:34:40.610 UTC [7f14414e9700] Info scscfsproutlet.cpp:1719: Routing to BGCF sip:bgcf.sprout.sales2.iot1.com;transport=TCP
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:1621: Sproutlet send_request 0x7f13842bbfb0
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose sproutletproxy.cpp:1662: scscf-proxy-0x7f138414a7f0 sending Request msg INVITE/cseq=1 (tdta0x7f13842bb9a0) on fork 0
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2130: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2170: Processing request 0x7f13842bba48, fork = 0
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2334: scscf-proxy-0x7f138414a7f0 transmitting request on fork 0
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2349: scscf-proxy-0x7f138414a7f0 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f13842bb9a0) on fork 0
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2122: Removing message 0x7f13842bbfb0 => txdata 0x7f13842bba48 mapping
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:bgcf.sprout.sales2.iot1.com;transport=TCP;lr
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:342: Possible service name bgcf will be used if sprout.sales2.iot1.com is a local hostname
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose sproutletproxy.cpp:1384: Created Sproutlet bgcf-0x7f138421a4d0 for Request msg INVITE/cseq=1 (tdta0x7f13842bb9a0)
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('scscf'->'bgcf'/'bgcf')
15-06-2018 20:34:40.610 UTC [7f14414e9700] Info pjutils.cpp:1719: Cloning header! 139721798572184
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose sproutletproxy.cpp:2487: Routing Response msg 100/INVITE/cseq=1 (tdta0x7f13842c2940) (472 bytes) to upstream sproutlet scscf-proxy:
--start msg--

SIP/2.0 100 Trying
Via: SIP/2.0/TCP sc01.sales2.iot1.com:5510;received=172.27.0.81;branch=z9hG4bKc3e26892-2a39-4ba3-887d-5545a9ef68c6
Call-ID: 13d56e67-87aa-4ae5-be66-8fc11e800eee
From: <tel:+18152579478>;tag=46bb82fb-4f96-42b7-9dcc-9f67d20c0d55
To: <sip:ImService at sc01.ivc.iot1.com;as.session=2>
CSeq: 1 INVITE
P-Charging-Vector: icid-value="58b84e00-61f9-1036-00-00-00-50-56-bb-20-00";orig-ioi=sales2.iot1.com;icid-generated-at=172.27.0.140
Content-Length:  0


--end msg--
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2115: Adding message 0x7f13842c2f50 => txdata 0x7f13842c29e8 mapping
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose sproutletproxy.cpp:1991: scscf-proxy-0x7f138414a7f0 received provisional response Response msg 100/INVITE/cseq=1 (tdta0x7f13842c2940) on fork 0, state = Proceeding
15-06-2018 20:34:40.610 UTC [7f14414e9700] Info scscfsproutlet.cpp:728: S-CSCF received response
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('EXTERNAL'->'scscf'/'scscf-proxy')
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2517: Internal network function boundary: no
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose sproutletproxy.cpp:1698: scscf-proxy-0x7f138414a7f0 sending Response msg 100/INVITE/cseq=1 (tdta0x7f13842c2940)
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2130: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2216: Aggregating response with status code 100
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('EXTERNAL'->'scscf'/'scscf-proxy')
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2235: Discard 100/INVITE response (tdta0x7f13842c2940)
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2122: Removing message 0x7f13842c2f50 => txdata 0x7f13842c29e8 mapping
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug pjsip: tdta0x7f13842c Destroying txdata Response msg 100/INVITE/cseq=1 (tdta0x7f13842c2940)
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose sproutletproxy.cpp:2487: Routing Request msg INVITE/cseq=1 (tdta0x7f13842bb9a0) (1679 bytes) to downstream sproutlet bgcf:
--start msg--

INVITE sip:ImService at sc01.ivc.iot1.com:5510;as.session=2 SIP/2.0
Via: SIP/2.0/TCP sc01.sales2.iot1.com:5510;received=172.27.0.81;branch=z9hG4bKc3e26892-2a39-4ba3-887d-5545a9ef68c6
From: <tel:+18152579478>;tag=46bb82fb-4f96-42b7-9dcc-9f67d20c0d55
To: <sip:ImService at sc01.ivc.iot1.com;as.session=2>
Call-ID: 13d56e67-87aa-4ae5-be66-8fc11e800eee
CSeq: 1 INVITE
Allow-Events: conference
Supported: timer, recipient-list-invite
Contact: <sip:ImService at sc01.sales2.iot1.com:5510;as.session=1>;isfocus;+g.oma.sip-im;+g.3gpp.iari-ref="urn%3Aurn-7%3A3gpp-application.ims.iari.rcse.im,urn%3Aurn-7%3A3gpp-application.ims.iari.rcs.fthttp,urn%3Aurn-7%3A3gpp-application.ims.iari.rcs.geopush"
Accept-Contact: *;+g.oma.sip-im
Allow: INVITE, ACK, CANCEL, BYE, UPDATE, REFER, SUBSCRIBE, MESSAGE, REGISTER, INFO
User-Agent: IM-serv/OMA2.0 Sales2-RMS/5.0
Session-Expires: 600;refresher=uas
P-Asserted-Identity: <sip:+18152579478 at sales2.iot1.com>
Subject: Football teams
Contribution-ID: d285786190f6407ebcd0f861f1645183
P-Charging-Vector: icid-value="58b84e00-61f9-1036-00-00-00-50-56-bb-20-00";orig-ioi=sales2.iot1.com;icid-generated-at=172.27.0.140
Max-Forwards: 69
Route: <sip:bgcf.sprout.sales2.iot1.com;transport=TCP;lr>
Content-Type: application/sdp
Content-Length:   397

v=0
o=- 16054947157924516231 16054947157924563475 IN IP4 172.28.0.81
s=-
c=IN IP4 172.28.0.81
t=0 0
m=message 9000 TCP/MSRP *
a=accept-types:message/cpim
a=accept-wrapped-types:text/plain message/imdn+xml application/im-iscomposing+xml application/vnd.gsma.rcs-ft-http+xml application/vnd.gsma.rcspushlocation+xml
a=msrp-cema
a=path:msrp://172.28.0.81:9000/Cw2E0W2t;tcp
a=setup:actpass

--end msg--
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('scscf'->'bgcf'/'bgcf')
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('scscf'->'bgcf'/'bgcf')
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2517: Internal network function boundary: yes
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug pjutils.cpp:736: Cloned tdta0x7f13842bb9a0 to tdta0x7f13842c2940
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:1450: Remove top Route header Route: <sip:bgcf.sprout.sales2.iot1.com;transport=TCP;lr>
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2115: Adding message 0x7f13842c2f50 => txdata 0x7f13842c29e8 mapping
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose sproutletproxy.cpp:1946: bgcf-0x7f138421a4d0 pass initial request Request msg INVITE/cseq=1 (tdta0x7f13842c2940) to Sproutlet
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug acr.cpp:1797: Create RalfACR for node type BGCF with role Terminating
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug acr.cpp:24: Created ACR (0x7f138415f780)
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug acr.cpp:170: Created BGCF Ralf ACR
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug acr.cpp:250: Set record type for I-CSCF, BGCF, IBCF, AS to EVENT_RECORD
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug acr.cpp:1596: Found P-Charging-Vector header, store information
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug uri_classifier.cpp:172: Classified URI as 5
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug pjutils.cpp:2536: Not translating URI
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug uri_classifier.cpp:172: Classified URI as 5
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug bgcfservice.cpp:163: Getting route for URI domain sc01.ivc.iot1.com via BGCF lookup
15-06-2018 20:34:40.610 UTC [7f14414e9700] Info bgcfservice.cpp:194: Found default route
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:1621: Sproutlet send_request 0x7f13842c2f50
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose sproutletproxy.cpp:1662: bgcf-0x7f138421a4d0 sending Request msg INVITE/cseq=1 (tdta0x7f13842c2940) on fork 0
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2130: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2170: Processing request 0x7f13842c29e8, fork = 0
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2334: bgcf-0x7f138421a4d0 transmitting request on fork 0
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2349: bgcf-0x7f138421a4d0 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f13842c2940) on fork 0
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:2122: Removing message 0x7f13842c2f50 => txdata 0x7f13842c29e8 mapping
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:kambgcf.sales2.iot1.com;lr
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:342: Possible service name kambgcf will be used if sales2.iot1.com is a local hostname
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sproutletproxy.cpp:1007: No local sproutlet matches request
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug pjsip: tsx0x7f1384016 Transaction created for Request msg INVITE/cseq=1 (tdta0x7f13842c2940)
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug basicproxy.cpp:1669: Added trail identifier 89564 to UAC transaction
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug pjutils.cpp:510: Next hop node is encoded in top route header
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sipresolver.cpp:84: SIPResolver::resolve for name kambgcf.sales2.iot1.com, port 0, transport -1, family 2
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug utils.cpp:446: Attempt to parse kambgcf.sales2.iot1.com as IP address
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sipresolver.cpp:147: Do NAPTR look-up for kambgcf.sales2.iot1.com
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug ttlcache.h:123: Found the entry in the cache
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sipresolver.cpp:161: NAPTR resolved to transport 6
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug sipresolver.cpp:281: Do SRV lookup for _sip._tcp.kambgcf.sales2.iot1.com
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug baseresolver.cpp:156: Creating a lazy iterator for SRV Resolution
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug ttlcache.h:123: Found the entry in the cache
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug baseresolver.cpp:994: Found SRV records at 2 priority levels
15-06-2018 20:34:40.610 UTC [7f14414e9700] Info pjutils.cpp:989: Resolved destination URI sip:kambgcf.sales2.iot1.com;lr
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug pjutils.cpp:510: Next hop node is encoded in top route header
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug basicproxy.cpp:1693: Next hop kambgcf.sales2.iot1.com is not a stateless proxy
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug basicproxy.cpp:1707: Sending request for sip:ImService at sc01.ivc.iot1.com:5510;as.session=2
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose baseresolver.cpp:1127: Processing 1 SRVs with priority 0
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug baseresolver.cpp:1155: Selected SRV kambgcf01.sales2.iot1.com:5060, weight = 1
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose baseresolver.cpp:1170: Do A record look-ups for 1 SRVs
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose dnscachedresolver.cpp:468: Check cache for kambgcf01.sales2.iot1.com type 1
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug dnscachedresolver.cpp:578: Pulling 1 records from cache for kambgcf01.sales2.iot1.com A
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug baseresolver.cpp:1186: SRV kambgcf01.sales2.iot1.com:5060 returned 1 IP addresses
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug baseresolver.cpp:587: 172.27.0.146:5060;transport=TCP has state: WHITE
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug baseresolver.cpp:587: 172.27.0.146:5060;transport=TCP has state: WHITE
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug baseresolver.cpp:1365: Added a whitelisted server to targets, now have 1 of 1
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug basicproxy.cpp:2391: Selected host 172.27.0.146:5060;transport=TCP (will be blacklisted by default)
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug pjsip: tsx0x7f1384016 Sending Request msg INVITE/cseq=1 (tdta0x7f13842c2940) in state Null
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug pjsip:       endpoint Request msg INVITE/cseq=1 (tdta0x7f13842c2940): skipping target resolution because address is already set
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose pjsip: tcpc0x7f138427 tcp->base.local_name: 172.27.0.41
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose pjsip: tcpc0x7f138427 TCP client transport created
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose pjsip: tcpc0x7f138427 TCP transport 172.27.0.41:5052 is connecting to 172.27.0.146:5060...
15-06-2018 20:34:40.610 UTC [7f14414e9700] Verbose common_sip_processing.cpp:103: TX 1847 bytes Request msg INVITE/cseq=1 (tdta0x7f13842c2940) to TCP 172.27.0.146:5060:
--start msg--

INVITE sip:ImService at sc01.ivc.iot1.com:5510;as.session=2 SIP/2.0
Via: SIP/2.0/TCP 172.27.0.41:5052;rport;branch=z9hG4bKPjITQ.nIYhWd3YTUq9YXS6JDB6sfrKq5Yj
Via: SIP/2.0/TCP scscf.sprout.sales2.iot1.com;branch=z9hG4bKPjqNHu5LgfrVs3vsIS7SNT1UzfjEmpR6qz
Via: SIP/2.0/TCP sc01.sales2.iot1.com:5510;received=172.27.0.81;branch=z9hG4bKc3e26892-2a39-4ba3-887d-5545a9ef68c6
From: <tel:+18152579478>;tag=46bb82fb-4f96-42b7-9dcc-9f67d20c0d55
To: <sip:ImService at sc01.ivc.iot1.com;as.session=2>
Call-ID: 13d56e67-87aa-4ae5-be66-8fc11e800eee
CSeq: 1 INVITE
Allow-Events: conference
Supported: timer, recipient-list-invite
Contact: <sip:ImService at sc01.sales2.iot1.com:5510;as.session=1>;isfocus;+g.oma.sip-im;+g.3gpp.iari-ref="urn%3Aurn-7%3A3gpp-application.ims.iari.rcse.im,urn%3Aurn-7%3A3gpp-application.ims.iari.rcs.fthttp,urn%3Aurn-7%3A3gpp-application.ims.iari.rcs.geopush"
Accept-Contact: *;+g.oma.sip-im
Allow: INVITE, ACK, CANCEL, BYE, UPDATE, REFER, SUBSCRIBE, MESSAGE, REGISTER, INFO
User-Agent: IM-serv/OMA2.0 Sales2-RMS/5.0
Session-Expires: 600;refresher=uas
P-Asserted-Identity: <sip:+18152579478 at sales2.iot1.com>
Subject: Football teams
Contribution-ID: d285786190f6407ebcd0f861f1645183
P-Charging-Vector: icid-value="58b84e00-61f9-1036-00-00-00-50-56-bb-20-00";orig-ioi=sales2.iot1.com;icid-generated-at=172.27.0.140
Max-Forwards: 68
Route: <sip:kambgcf.sales2.iot1.com;lr>
Content-Type: application/sdp
Content-Length:   397

v=0
o=- 16054947157924516231 16054947157924563475 IN IP4 172.28.0.81
s=-
c=IN IP4 172.28.0.81
t=0 0
m=message 9000 TCP/MSRP *
a=accept-types:message/cpim
a=accept-wrapped-types:text/plain message/imdn+xml application/im-iscomposing+xml application/vnd.gsma.rcs-ft-http+xml application/vnd.gsma.rcspushlocation+xml
a=msrp-cema
a=path:msrp://172.28.0.81:9000/Cw2E0W2t;tcp
a=setup:actpass

--end msg--
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug pjsip: tsx0x7f1384016 State changed from Null to Calling, event=TX_MSG
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug basicproxy.cpp:183: tsx0x7f13840162a8 - tu_on_tsx_state UAC, TSX_STATE TX_MSG state=Calling
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug basicproxy.cpp:1942: tsx0x7f13840162a8 - uac_tsx = 0x7f138423aa60, uas_tsx = 0x7f13840db0f0
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug basicproxy.cpp:1950: TX_MSG event on current UAC transaction
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug basicproxy.cpp:2280: Starting timer C
15-06-2018 20:34:40.610 UTC [7f14414e9700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f13b0ae4598
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180615/cf86c954/attachment.html>

From amirhatami1000 at gmail.com  Sun Jun 17 03:33:07 2018
From: amirhatami1000 at gmail.com (Amir Moahammad Hatami)
Date: Sun, 17 Jun 2018 12:03:07 +0430
Subject: [Project Clearwater] Clearwater registrations problem (log file
 included.)
Message-ID: <CAEtN4MD7Xx_r1imTHfYNfWgvcQXyLfBhOANLuGN76sKC4E+P7Q@mail.gmail.com>

Hi,

I am trying to register a user that I get the user name and password from
ellis using jitsi and zoiper.
The registration is not successful. I can see that there is an error that
the sprout failed to get authentication vector. here is a peice of the log
file (there is only error)

17-06-2018 07:07:40.430 UTC [7fdaa6f7d700] Debug
event_statistic_accumulator.cpp:32: Accumulate 17750 for 0x3a5c070
17-06-2018 07:07:40.430 UTC [7fdaa6f7d700] Debug
event_statistic_accumulator.cpp:32: Accumulate 17750 for 0x3a5c0e8
17-06-2018 07:07:40.430 UTC [7fdaa6f7d700] Error hssconnection.cpp:138:
Failed to get Authentication Vector for 6505550353 at 34.201.104.195
17-06-2018 07:07:40.430 UTC [7fdaa6f7d700] Error hssconnection.cpp:138:
Failed to get Authentication Vector for 6505550353 at 34.201.104.195
17-06-2018 07:07:40.431 UTC [7fdaa6f7d700] Debug acr.cpp:1483: Found a
P-Charging-Function-Address header
17-06-2018 07:07:40.431 UTC [7fdaa6f7d700] Debug acr.cpp:1502: 1 ccfs and 0
ecfs
17-06-2018 07:07:40.431 UTC [7fdaa6f7d700] Verbose acr.cpp:641: Sending
S-CSCF Ralf ACR (0x7fda540f0900)


I also mention here that since I am running clearwater in AWS, and I am
using private zone. I don't use  6505550353 at clearwater.amir.test.com and
instead I use the IP address of  clearwater.amir.test.com which is the same
as bono IP address.

Please kindly let me know if you know the reason.

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180617/c2e17f9c/attachment.html>

From navdeep.uniyal at bristol.ac.uk  Mon Jun 18 05:34:15 2018
From: navdeep.uniyal at bristol.ac.uk (Navdeep Uniyal)
Date: Mon, 18 Jun 2018 09:34:15 +0000
Subject: [Project Clearwater] Clearwater RakeTest Fails
Message-ID: <DB5PR06MB1559C691DD2B7ECB824FDFE2DE710@DB5PR06MB1559.eurprd06.prod.outlook.com>

Dear All,

I am new to the community and the clearwater ims.
I have installed the clearwater solution manually on the OpenStack VMs.
In my setup I have 2 networks, one is public and other is private(only inside openstack).

I have configured the machines and DNS according to the private network.
All the tests are failing with connection refused!
I tried 2 scenarios:

  1.  Test machine in the public network.
  2.  Test machine in the private network with correct DNS hostname entry.

Can someone please suggest looking at the below part of log, what could be the issue:

Basic Call - Mainline (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `initialize'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `open'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `block in connect'
     - /usr/lib/ruby/1.9.1/timeout.rb:55:in `timeout'
     - /usr/lib/ruby/1.9.1/timeout.rb:100:in `timeout'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `connect'
     - /usr/lib/ruby/1.9.1/net/http.rb:756:in `do_start'
     - /usr/lib/ruby/1.9.1/net/http.rb:745:in `start'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:413:in `transmit'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:176:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient.rb:69:in `post'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:166:in `rescue in get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:159:in `get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:67:in `initialize'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `new'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `provision_line'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:181:in `add_endpoint'
     - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:19:in `block in <top (required)>'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `call'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `run'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:126:in `block (2 levels) in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `collect'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `block in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `each'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `run_all'
     - /home/cloud/clearwater-live-test/lib/live-test.rb:23:in `run_tests'
     - /home/cloud/clearwater-live-test/Rakefile:18:in `block in <top (required)>'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `call'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `block in execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:179:in `block in invoke_with_call_chain'
     - /usr/lib/ruby/1.9.1/monitor.rb:211:in `mon_synchronize'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:172:in `invoke_with_call_chain'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:165:in `invoke'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:150:in `invoke_task'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block (2 levels) in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:115:in `run_with_threads'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:100:in `top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:78:in `block in run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:176:in `standard_exception_handling'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:75:in `run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/bin/rake:33:in `<top (required)>'
     - /usr/local/bin/rake:23:in `load'
     - /usr/local/bin/rake:23:in `<main>'
Basic Call - SDP (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)




--------------------------------------------
Navdeep Uniyal
Email: Navdeep.uniyal at bristol.ac.uk
Senior Research Associate
High Performance Networks Group
University of Bristol

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180618/4175a52a/attachment.html>

From Mark.Perryman at metaswitch.com  Mon Jun 18 09:34:04 2018
From: Mark.Perryman at metaswitch.com (Mark Perryman)
Date: Mon, 18 Jun 2018 13:34:04 +0000
Subject: [Project Clearwater] Clearwater registrations problem (log file
 included.)
In-Reply-To: <CAEtN4MD7Xx_r1imTHfYNfWgvcQXyLfBhOANLuGN76sKC4E+P7Q@mail.gmail.com>
References: <CAEtN4MD7Xx_r1imTHfYNfWgvcQXyLfBhOANLuGN76sKC4E+P7Q@mail.gmail.com>
Message-ID: <BYAPR02MB42477C36C73CC7863B2D26838F710@BYAPR02MB4247.namprd02.prod.outlook.com>

Does it work if you register with 6505550353 at clearwater.amir.test.com?  You cannot just replace the host section of a SIP identity with an IP address.

Mark Perryman

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Amir Moahammad Hatami
Sent: 17 June 2018 08:33
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Clearwater registrations problem (log file included.)

[Image removed by sender.]
Hi,

I am trying to register a user that I get the user name and password from ellis using jitsi and zoiper.
The registration is not successful. I can see that there is an error that the sprout failed to get authentication vector. here is a peice of the log file (there is only error)

17-06-2018 07:07:40.430 UTC [7fdaa6f7d700] Debug event_statistic_accumulator.cpp:32: Accumulate 17750 for 0x3a5c070
17-06-2018 07:07:40.430 UTC [7fdaa6f7d700] Debug event_statistic_accumulator.cpp:32: Accumulate 17750 for 0x3a5c0e8
17-06-2018 07:07:40.430 UTC [7fdaa6f7d700] Error hssconnection.cpp:138: Failed to get Authentication Vector for 6505550353 at 34.201.104.195<mailto:6505550353 at 34.201.104.195>
17-06-2018 07:07:40.430 UTC [7fdaa6f7d700] Error hssconnection.cpp:138: Failed to get Authentication Vector for 6505550353 at 34.201.104.195<mailto:6505550353 at 34.201.104.195>
17-06-2018 07:07:40.431 UTC [7fdaa6f7d700] Debug acr.cpp:1483: Found a P-Charging-Function-Address header
17-06-2018 07:07:40.431 UTC [7fdaa6f7d700] Debug acr.cpp:1502: 1 ccfs and 0 ecfs
17-06-2018 07:07:40.431 UTC [7fdaa6f7d700] Verbose acr.cpp:641: Sending S-CSCF Ralf ACR (0x7fda540f0900)


I also mention here that since I am running clearwater in AWS, and I am using private zone. I don't use  6505550353 at clearwater.amir.test.com<mailto:6505550353 at clearwater.amir.test.com> and instead I use the IP address of  clearwater.amir.test.com<http://clearwater.amir.test.com> which is the same as bono IP address.

Please kindly let me know if you know the reason.

Thanks

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180618/eaad9b62/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ~WRD000.jpg
Type: image/jpeg
Size: 823 bytes
Desc: ~WRD000.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180618/eaad9b62/attachment.jpg>

From navdeep.uniyal at bristol.ac.uk  Mon Jun 18 11:28:43 2018
From: navdeep.uniyal at bristol.ac.uk (Navdeep Uniyal)
Date: Mon, 18 Jun 2018 15:28:43 +0000
Subject: [Project Clearwater] Clearwater RakeTest Fails
In-Reply-To: <DB5PR06MB1559C691DD2B7ECB824FDFE2DE710@DB5PR06MB1559.eurprd06.prod.outlook.com>
References: <DB5PR06MB1559C691DD2B7ECB824FDFE2DE710@DB5PR06MB1559.eurprd06.prod.outlook.com>
Message-ID: <DB5PR06MB1559D40AFC240A0576B85D93DE710@DB5PR06MB1559.eurprd06.prod.outlook.com>

Dear All,

I have been able to resolve some of the issues to proceed only to get stuck in another issue while running the Live Test.
The test is not yet successful and I am getting error in logs in dime(ralf):

18-06-2018 15:26:02.068 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:67: DiameterResolver::resolve for realm hpn.com, host , family 2
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:72: Do NAPTR look-up for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:230: Current time is 1529335567, expiry time of the entry at the head of the expiry list is 1529335562
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:128: Entry not in cache, so create new entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:252: NAPTR cache factory called for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:264: Sending DNS NAPTR query for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching hpn.com in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: hpn.com not found in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for hpn.com type 35
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 2 records from cache for hpn.com NAPTR
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query hpn.com (canonical domain: hpn.com)
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:139: DNS query has returned, populate the cache entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:273: Adding entry to expiry list, TTL=0, expiry time = 1529335567
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:97: NAPTR lookup failed, so do SRV lookups for TCP and SCTP
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._tcp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._tcp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._sctp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._sctp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._tcp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._sctp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._tcp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._sctp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._tcp.hpn.com (canonical domain: _diameter._tcp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._sctp.hpn.com (canonical domain: _diameter._sctp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:106: TCP SRV record _diameter._tcp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:109: SCTP SRV record _diameter._sctp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found

I am not using any external HSS and my shared_config is as below:

# Deployment definitions
home_domain=hpn.com
sprout_hostname=sprout.hpn.com
sprout_registration_store=vellum.hpn.com
hs_hostname=hs.hpn.com:8888
hs_provisioning_hostname=hs.hpn.com:8889
homestead_impu_store=vellum.hpn.com
ralf_hostname=ralf.hpn.com:10888
ralf_session_store=vellum.hpn.com
xdms_hostname=homer.hpn.com:7888
chronos_hostname=vellum.hpn.com
cassandra_hostname=vellum.hpn.com

# Email server configuration
#smtp_smarthost=<smtp server>
#smtp_username=<username>
#smtp_password=<password>
#email_recovery_sender=clearwater at example.org

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

# Application Servers
#gemini=<gemini port>
memento=5055
memento_auth_store=vellum.hpn.com


Please suggest how can I resolve the issue.

Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 10:34
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I am new to the community and the clearwater ims.
I have installed the clearwater solution manually on the OpenStack VMs.
In my setup I have 2 networks, one is public and other is private(only inside openstack).

I have configured the machines and DNS according to the private network.
All the tests are failing with connection refused!
I tried 2 scenarios:

  1.  Test machine in the public network.
  2.  Test machine in the private network with correct DNS hostname entry.

Can someone please suggest looking at the below part of log, what could be the issue:

Basic Call - Mainline (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `initialize'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `open'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `block in connect'
     - /usr/lib/ruby/1.9.1/timeout.rb:55:in `timeout'
     - /usr/lib/ruby/1.9.1/timeout.rb:100:in `timeout'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `connect'
     - /usr/lib/ruby/1.9.1/net/http.rb:756:in `do_start'
     - /usr/lib/ruby/1.9.1/net/http.rb:745:in `start'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:413:in `transmit'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:176:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient.rb:69:in `post'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:166:in `rescue in get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:159:in `get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:67:in `initialize'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `new'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `provision_line'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:181:in `add_endpoint'
     - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:19:in `block in <top (required)>'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `call'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `run'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:126:in `block (2 levels) in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `collect'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `block in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `each'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `run_all'
     - /home/cloud/clearwater-live-test/lib/live-test.rb:23:in `run_tests'
     - /home/cloud/clearwater-live-test/Rakefile:18:in `block in <top (required)>'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `call'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `block in execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:179:in `block in invoke_with_call_chain'
     - /usr/lib/ruby/1.9.1/monitor.rb:211:in `mon_synchronize'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:172:in `invoke_with_call_chain'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:165:in `invoke'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:150:in `invoke_task'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block (2 levels) in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:115:in `run_with_threads'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:100:in `top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:78:in `block in run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:176:in `standard_exception_handling'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:75:in `run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/bin/rake:33:in `<top (required)>'
     - /usr/local/bin/rake:23:in `load'
     - /usr/local/bin/rake:23:in `<main>'
Basic Call - SDP (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)




--------------------------------------------
Navdeep Uniyal
Email: Navdeep.uniyal at bristol.ac.uk<mailto:Navdeep.uniyal at bristol.ac.uk>
Senior Research Associate
High Performance Networks Group
University of Bristol

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180618/79583d21/attachment.html>

From navdeep.uniyal at bristol.ac.uk  Tue Jun 19 07:29:45 2018
From: navdeep.uniyal at bristol.ac.uk (Navdeep Uniyal)
Date: Tue, 19 Jun 2018 11:29:45 +0000
Subject: [Project Clearwater] Clearwater Live Test Fails
Message-ID: <DB5PR06MB1559504FD60C336243499D0CDE700@DB5PR06MB1559.eurprd06.prod.outlook.com>

Dear All,

Sorry for putting so many emails!
It would be really helpful if someone from the community could please suggest me on how to resolve the issue.

Live Test Results:

cloud at imstestserver:~/clearwater-live-test$ sudo rake test[hpn.com] SIGNUP_CODE=secret
Basic Call - Mainline (TCP) - (6505550786, 6505550816) Failed
Endpoint threw exception:
- sip:6505550816 at cw-ngv.com timed out waiting for new incoming call
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:68:in `rescue in incoming_call'
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:65:in `incoming_call'
   - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:55:in `block (2 levels) in <top (required)>'
Terminating other threads after failure

Bono Logs :

19-06-2018 11:20:18.666 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:20:33.677 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:20:48.678 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:03.687 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:21:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:21:18.700 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:33.715 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:21:48.724 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:03.730 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:22:18.736 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:33.751 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:22:48.763 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:03.774 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:23:18.781 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:31.700 UTC [7fc2cf4d4700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 23
19-06-2018 11:23:33.797 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:23:48.808 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:24:03.818 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm


Sprout Logs:

--start msg--

OPTIONS sip:poll-sip at 20.0.0.5:5054 SIP/2.0
Via: SIP/2.0/TCP 20.0.0.5;rport;branch=z9hG4bK-498767
Max-Forwards: 2
To: <sip:poll-sip at 20.0.0.5:5054>
From: poll-sip <sip:poll-sip at 20.0.0.5>;tag=498767
Call-ID: poll-sip-498767
CSeq: 498767 OPTIONS
Contact: <sip:20.0.0.5>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug uri_classifier.cpp:173: Classified URI sip:poll-sip at 20.0.0.5:5054 as 3
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:568: Received message 0x7faa8c03d4d0
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:585: Admitted request 0x7faa8c03d4d0
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:620: Incoming message 0x7faa8c03d4d0 cloned to 0x7faa8c0a4b98
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:639: Queuing cloned received message 0x7faa8c0a4b98 for worker threads with priority 15
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x22a03b8
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x22a0430
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug utils.cpp:872: Added IOHook 0x7faaaccebdf0 to stack. There are now 1 hooks
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:181: Worker thread dequeue message 0x7faa8c0a4b98
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:186: Request latency so far = 227us
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=498767 (rdata0x7faa8c0a4b98)
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug uri_classifier.cpp:173: Classified URI sip:poll-sip at 20.0.0.5:5054 as 3
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80) created
19-06-2018 11:26:01.816 UTC [7faaaccec700] Verbose common_sip_processing.cpp:103: TX 266 bytes Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80) to TCP 20.0.0.5:41266:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 20.0.0.5;rport=41266;received=20.0.0.5;branch=z9hG4bK-498767
Call-ID: poll-sip-498767
From: "poll-sip" <sip:poll-sip at 20.0.0.5>;tag=498767
To: <sip:poll-sip at 20.0.0.5>;tag=z9hG4bK-498767
CSeq: 498767 OPTIONS
Content-Length:  0


--end msg--
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip: tdta0x7faa8c05 Destroying txdata Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80)
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:273: Worker thread completed processing message 0x7faa8c0a4b98
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:287: Request latency = 494us
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug event_statistic_accumulator.cpp:32: Accumulate 494 for 0x229c428
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug event_statistic_accumulator.cpp:32: Accumulate 494 for 0x229c4a0
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 19).
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug utils.cpp:878: Removed IOHook 0x7faaaccebdf0 to stack. There are now 0 hooks
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:161: Attempting to process queue element
19-06-2018 11:26:01.875 UTC [7faa8bfff700] Verbose httpstack.cpp:308: Process request for URL /ping, args (null)
19-06-2018 11:26:01.875 UTC [7faa8bfff700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)




Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 16:29
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I have been able to resolve some of the issues to proceed only to get stuck in another issue while running the Live Test.
The test is not yet successful and I am getting error in logs in dime(ralf):

18-06-2018 15:26:02.068 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:67: DiameterResolver::resolve for realm hpn.com, host , family 2
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:72: Do NAPTR look-up for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:230: Current time is 1529335567, expiry time of the entry at the head of the expiry list is 1529335562
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:128: Entry not in cache, so create new entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:252: NAPTR cache factory called for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:264: Sending DNS NAPTR query for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching hpn.com in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: hpn.com not found in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for hpn.com type 35
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 2 records from cache for hpn.com NAPTR
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query hpn.com (canonical domain: hpn.com)
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:139: DNS query has returned, populate the cache entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:273: Adding entry to expiry list, TTL=0, expiry time = 1529335567
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:97: NAPTR lookup failed, so do SRV lookups for TCP and SCTP
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._tcp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._tcp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._sctp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._sctp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._tcp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._sctp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._tcp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._sctp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._tcp.hpn.com (canonical domain: _diameter._tcp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._sctp.hpn.com (canonical domain: _diameter._sctp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:106: TCP SRV record _diameter._tcp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:109: SCTP SRV record _diameter._sctp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found

I am not using any external HSS and my shared_config is as below:

# Deployment definitions
home_domain=hpn.com
sprout_hostname=sprout.hpn.com
sprout_registration_store=vellum.hpn.com
hs_hostname=hs.hpn.com:8888
hs_provisioning_hostname=hs.hpn.com:8889
homestead_impu_store=vellum.hpn.com
ralf_hostname=ralf.hpn.com:10888
ralf_session_store=vellum.hpn.com
xdms_hostname=homer.hpn.com:7888
chronos_hostname=vellum.hpn.com
cassandra_hostname=vellum.hpn.com

# Email server configuration
#smtp_smarthost=<smtp server>
#smtp_username=<username>
#smtp_password=<password>
#email_recovery_sender=clearwater at example.org<mailto:#email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

# Application Servers
#gemini=<gemini port>
memento=5055
memento_auth_store=vellum.hpn.com


Please suggest how can I resolve the issue.

Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 10:34
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I am new to the community and the clearwater ims.
I have installed the clearwater solution manually on the OpenStack VMs.
In my setup I have 2 networks, one is public and other is private(only inside openstack).

I have configured the machines and DNS according to the private network.
All the tests are failing with connection refused!
I tried 2 scenarios:

  1.  Test machine in the public network.
  2.  Test machine in the private network with correct DNS hostname entry.

Can someone please suggest looking at the below part of log, what could be the issue:

Basic Call - Mainline (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `initialize'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `open'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `block in connect'
     - /usr/lib/ruby/1.9.1/timeout.rb:55:in `timeout'
     - /usr/lib/ruby/1.9.1/timeout.rb:100:in `timeout'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `connect'
     - /usr/lib/ruby/1.9.1/net/http.rb:756:in `do_start'
     - /usr/lib/ruby/1.9.1/net/http.rb:745:in `start'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:413:in `transmit'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:176:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient.rb:69:in `post'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:166:in `rescue in get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:159:in `get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:67:in `initialize'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `new'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `provision_line'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:181:in `add_endpoint'
     - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:19:in `block in <top (required)>'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `call'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `run'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:126:in `block (2 levels) in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `collect'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `block in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `each'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `run_all'
     - /home/cloud/clearwater-live-test/lib/live-test.rb:23:in `run_tests'
     - /home/cloud/clearwater-live-test/Rakefile:18:in `block in <top (required)>'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `call'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `block in execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:179:in `block in invoke_with_call_chain'
     - /usr/lib/ruby/1.9.1/monitor.rb:211:in `mon_synchronize'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:172:in `invoke_with_call_chain'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:165:in `invoke'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:150:in `invoke_task'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block (2 levels) in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:115:in `run_with_threads'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:100:in `top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:78:in `block in run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:176:in `standard_exception_handling'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:75:in `run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/bin/rake:33:in `<top (required)>'
     - /usr/local/bin/rake:23:in `load'
     - /usr/local/bin/rake:23:in `<main>'
Basic Call - SDP (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)




--------------------------------------------
Navdeep Uniyal
Email: Navdeep.uniyal at bristol.ac.uk<mailto:Navdeep.uniyal at bristol.ac.uk>
Senior Research Associate
High Performance Networks Group
University of Bristol

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180619/0062a94d/attachment.html>

From navdeep.uniyal at bristol.ac.uk  Tue Jun 19 09:57:52 2018
From: navdeep.uniyal at bristol.ac.uk (Navdeep Uniyal)
Date: Tue, 19 Jun 2018 13:57:52 +0000
Subject: [Project Clearwater] Clearwater Live Test Fails
In-Reply-To: <DB5PR06MB1559504FD60C336243499D0CDE700@DB5PR06MB1559.eurprd06.prod.outlook.com>
References: <DB5PR06MB1559504FD60C336243499D0CDE700@DB5PR06MB1559.eurprd06.prod.outlook.com>
Message-ID: <DB5PR06MB155995DA8E7B707A7B965075DE700@DB5PR06MB1559.eurprd06.prod.outlook.com>

Dear All,

It would be great if someone could share the expertise in the ims and would help me resolve the issues.

Kind Regards,
Navdeep

From: Clearwater <clearwater-bounces at lists.projectclearwater.org> On Behalf Of Navdeep Uniyal
Sent: 19 June 2018 12:30
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Dear All,

Sorry for putting so many emails!
It would be really helpful if someone from the community could please suggest me on how to resolve the issue.

Live Test Results:

cloud at imstestserver:~/clearwater-live-test$ sudo rake test[hpn.com] SIGNUP_CODE=secret
Basic Call - Mainline (TCP) - (6505550786, 6505550816) Failed
Endpoint threw exception:
- sip:6505550816 at cw-ngv.com timed out waiting for new incoming call
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:68:in `rescue in incoming_call'
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:65:in `incoming_call'
   - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:55:in `block (2 levels) in <top (required)>'
Terminating other threads after failure

Bono Logs :

19-06-2018 11:20:18.666 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:20:33.677 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:20:48.678 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:03.687 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:21:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:21:18.700 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:33.715 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:21:48.724 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:03.730 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:22:18.736 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:33.751 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:22:48.763 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:03.774 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:23:18.781 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:31.700 UTC [7fc2cf4d4700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 23
19-06-2018 11:23:33.797 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:23:48.808 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:24:03.818 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm


Sprout Logs:

--start msg--

OPTIONS sip:poll-sip at 20.0.0.5:5054 SIP/2.0
Via: SIP/2.0/TCP 20.0.0.5;rport;branch=z9hG4bK-498767
Max-Forwards: 2
To: <sip:poll-sip at 20.0.0.5:5054>
From: poll-sip <sip:poll-sip at 20.0.0.5>;tag=498767
Call-ID: poll-sip-498767
CSeq: 498767 OPTIONS
Contact: <sip:20.0.0.5>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug uri_classifier.cpp:173: Classified URI sip:poll-sip at 20.0.0.5:5054 as 3
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:568: Received message 0x7faa8c03d4d0
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:585: Admitted request 0x7faa8c03d4d0
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:620: Incoming message 0x7faa8c03d4d0 cloned to 0x7faa8c0a4b98
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:639: Queuing cloned received message 0x7faa8c0a4b98 for worker threads with priority 15
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x22a03b8
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x22a0430
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug utils.cpp:872: Added IOHook 0x7faaaccebdf0 to stack. There are now 1 hooks
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:181: Worker thread dequeue message 0x7faa8c0a4b98
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:186: Request latency so far = 227us
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=498767 (rdata0x7faa8c0a4b98)
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug uri_classifier.cpp:173: Classified URI sip:poll-sip at 20.0.0.5:5054 as 3
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80) created
19-06-2018 11:26:01.816 UTC [7faaaccec700] Verbose common_sip_processing.cpp:103: TX 266 bytes Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80) to TCP 20.0.0.5:41266:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 20.0.0.5;rport=41266;received=20.0.0.5;branch=z9hG4bK-498767
Call-ID: poll-sip-498767
From: "poll-sip" <sip:poll-sip at 20.0.0.5>;tag=498767
To: <sip:poll-sip at 20.0.0.5>;tag=z9hG4bK-498767
CSeq: 498767 OPTIONS
Content-Length:  0


--end msg--
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip: tdta0x7faa8c05 Destroying txdata Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80)
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:273: Worker thread completed processing message 0x7faa8c0a4b98
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:287: Request latency = 494us
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug event_statistic_accumulator.cpp:32: Accumulate 494 for 0x229c428
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug event_statistic_accumulator.cpp:32: Accumulate 494 for 0x229c4a0
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 19).
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug utils.cpp:878: Removed IOHook 0x7faaaccebdf0 to stack. There are now 0 hooks
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:161: Attempting to process queue element
19-06-2018 11:26:01.875 UTC [7faa8bfff700] Verbose httpstack.cpp:308: Process request for URL /ping, args (null)
19-06-2018 11:26:01.875 UTC [7faa8bfff700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)




Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 16:29
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I have been able to resolve some of the issues to proceed only to get stuck in another issue while running the Live Test.
The test is not yet successful and I am getting error in logs in dime(ralf):

18-06-2018 15:26:02.068 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:67: DiameterResolver::resolve for realm hpn.com, host , family 2
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:72: Do NAPTR look-up for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:230: Current time is 1529335567, expiry time of the entry at the head of the expiry list is 1529335562
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:128: Entry not in cache, so create new entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:252: NAPTR cache factory called for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:264: Sending DNS NAPTR query for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching hpn.com in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: hpn.com not found in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for hpn.com type 35
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 2 records from cache for hpn.com NAPTR
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query hpn.com (canonical domain: hpn.com)
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:139: DNS query has returned, populate the cache entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:273: Adding entry to expiry list, TTL=0, expiry time = 1529335567
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:97: NAPTR lookup failed, so do SRV lookups for TCP and SCTP
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._tcp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._tcp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._sctp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._sctp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._tcp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._sctp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._tcp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._sctp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._tcp.hpn.com (canonical domain: _diameter._tcp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._sctp.hpn.com (canonical domain: _diameter._sctp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:106: TCP SRV record _diameter._tcp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:109: SCTP SRV record _diameter._sctp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found

I am not using any external HSS and my shared_config is as below:

# Deployment definitions
home_domain=hpn.com
sprout_hostname=sprout.hpn.com
sprout_registration_store=vellum.hpn.com
hs_hostname=hs.hpn.com:8888
hs_provisioning_hostname=hs.hpn.com:8889
homestead_impu_store=vellum.hpn.com
ralf_hostname=ralf.hpn.com:10888
ralf_session_store=vellum.hpn.com
xdms_hostname=homer.hpn.com:7888
chronos_hostname=vellum.hpn.com
cassandra_hostname=vellum.hpn.com

# Email server configuration
#smtp_smarthost=<smtp server>
#smtp_username=<username>
#smtp_password=<password>
#email_recovery_sender=clearwater at example.org<mailto:#email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

# Application Servers
#gemini=<gemini port>
memento=5055
memento_auth_store=vellum.hpn.com


Please suggest how can I resolve the issue.

Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 10:34
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I am new to the community and the clearwater ims.
I have installed the clearwater solution manually on the OpenStack VMs.
In my setup I have 2 networks, one is public and other is private(only inside openstack).

I have configured the machines and DNS according to the private network.
All the tests are failing with connection refused!
I tried 2 scenarios:

  1.  Test machine in the public network.
  2.  Test machine in the private network with correct DNS hostname entry.

Can someone please suggest looking at the below part of log, what could be the issue:

Basic Call - Mainline (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `initialize'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `open'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `block in connect'
     - /usr/lib/ruby/1.9.1/timeout.rb:55:in `timeout'
     - /usr/lib/ruby/1.9.1/timeout.rb:100:in `timeout'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `connect'
     - /usr/lib/ruby/1.9.1/net/http.rb:756:in `do_start'
     - /usr/lib/ruby/1.9.1/net/http.rb:745:in `start'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:413:in `transmit'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:176:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient.rb:69:in `post'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:166:in `rescue in get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:159:in `get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:67:in `initialize'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `new'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `provision_line'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:181:in `add_endpoint'
     - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:19:in `block in <top (required)>'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `call'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `run'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:126:in `block (2 levels) in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `collect'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `block in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `each'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `run_all'
     - /home/cloud/clearwater-live-test/lib/live-test.rb:23:in `run_tests'
     - /home/cloud/clearwater-live-test/Rakefile:18:in `block in <top (required)>'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `call'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `block in execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:179:in `block in invoke_with_call_chain'
     - /usr/lib/ruby/1.9.1/monitor.rb:211:in `mon_synchronize'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:172:in `invoke_with_call_chain'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:165:in `invoke'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:150:in `invoke_task'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block (2 levels) in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:115:in `run_with_threads'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:100:in `top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:78:in `block in run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:176:in `standard_exception_handling'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:75:in `run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/bin/rake:33:in `<top (required)>'
     - /usr/local/bin/rake:23:in `load'
     - /usr/local/bin/rake:23:in `<main>'
Basic Call - SDP (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)




--------------------------------------------
Navdeep Uniyal
Email: Navdeep.uniyal at bristol.ac.uk<mailto:Navdeep.uniyal at bristol.ac.uk>
Senior Research Associate
High Performance Networks Group
University of Bristol

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180619/cc380720/attachment.html>

From William.Yates at metaswitch.com  Wed Jun 20 05:59:47 2018
From: William.Yates at metaswitch.com (William Yates)
Date: Wed, 20 Jun 2018 09:59:47 +0000
Subject: [Project Clearwater] Clearwater Live Test Fails
In-Reply-To: <DB5PR06MB155995DA8E7B707A7B965075DE700@DB5PR06MB1559.eurprd06.prod.outlook.com>
References: <DB5PR06MB1559504FD60C336243499D0CDE700@DB5PR06MB1559.eurprd06.prod.outlook.com>
	<DB5PR06MB155995DA8E7B707A7B965075DE700@DB5PR06MB1559.eurprd06.prod.outlook.com>
Message-ID: <BLUPR0201MB1490D3D5511E2F793DE0DF3F95770@BLUPR0201MB1490.namprd02.prod.outlook.com>

Hi Navdeep,

Regarding diameter peer errors, it looks like your dime node is trying to resolve to an external HSS, via DNS.

Do you have hss_realm and/or hss_hostname configured?

>From https://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options :
*  hss_realm - this sets the Destination-Realm of your external HSS. When this field is set, the homestead process on Dime will then attempt to set up multiple Diameter connections using an SRV lookup on this realm.
*  hss_hostname - this sets the Destination-Host of your external HSS, if you have one. The homestead process on Dime will also try and establish a Diameter connection to this host (on port 3868) if no SRV-discovered peers exist.

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Navdeep Uniyal
Sent: 19 June 2018 14:58
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Dear All,

It would be great if someone could share the expertise in the ims and would help me resolve the issues.

Kind Regards,
Navdeep

From: Clearwater <clearwater-bounces at lists.projectclearwater.org> On Behalf Of Navdeep Uniyal
Sent: 19 June 2018 12:30
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Dear All,

Sorry for putting so many emails!
It would be really helpful if someone from the community could please suggest me on how to resolve the issue.

Live Test Results:

cloud at imstestserver:~/clearwater-live-test$ sudo rake test[hpn.com] SIGNUP_CODE=secret
Basic Call - Mainline (TCP) - (6505550786, 6505550816) Failed
Endpoint threw exception:
- sip:6505550816 at cw-ngv.com timed out waiting for new incoming call
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:68:in `rescue in incoming_call'
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:65:in `incoming_call'
   - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:55:in `block (2 levels) in <top (required)>'
Terminating other threads after failure

Bono Logs :

19-06-2018 11:20:18.666 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:20:33.677 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:20:48.678 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:03.687 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:21:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:21:18.700 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:33.715 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:21:48.724 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:03.730 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:22:18.736 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:33.751 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:22:48.763 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:03.774 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:23:18.781 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:31.700 UTC [7fc2cf4d4700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 23
19-06-2018 11:23:33.797 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:23:48.808 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:24:03.818 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm


Sprout Logs:

--start msg--

OPTIONS sip:poll-sip at 20.0.0.5:5054 SIP/2.0
Via: SIP/2.0/TCP 20.0.0.5;rport;branch=z9hG4bK-498767
Max-Forwards: 2
To: <sip:poll-sip at 20.0.0.5:5054>
From: poll-sip <sip:poll-sip at 20.0.0.5>;tag=498767
Call-ID: poll-sip-498767
CSeq: 498767 OPTIONS
Contact: <sip:20.0.0.5>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug uri_classifier.cpp:173: Classified URI sip:poll-sip at 20.0.0.5:5054 as 3
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:568: Received message 0x7faa8c03d4d0
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:585: Admitted request 0x7faa8c03d4d0
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:620: Incoming message 0x7faa8c03d4d0 cloned to 0x7faa8c0a4b98
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:639: Queuing cloned received message 0x7faa8c0a4b98 for worker threads with priority 15
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x22a03b8
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x22a0430
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug utils.cpp:872: Added IOHook 0x7faaaccebdf0 to stack. There are now 1 hooks
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:181: Worker thread dequeue message 0x7faa8c0a4b98
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:186: Request latency so far = 227us
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=498767 (rdata0x7faa8c0a4b98)
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug uri_classifier.cpp:173: Classified URI sip:poll-sip at 20.0.0.5:5054 as 3
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80) created
19-06-2018 11:26:01.816 UTC [7faaaccec700] Verbose common_sip_processing.cpp:103: TX 266 bytes Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80) to TCP 20.0.0.5:41266:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 20.0.0.5;rport=41266;received=20.0.0.5;branch=z9hG4bK-498767
Call-ID: poll-sip-498767
From: "poll-sip" <sip:poll-sip at 20.0.0.5>;tag=498767
To: <sip:poll-sip at 20.0.0.5>;tag=z9hG4bK-498767
CSeq: 498767 OPTIONS
Content-Length:  0


--end msg--
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip: tdta0x7faa8c05 Destroying txdata Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80)
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:273: Worker thread completed processing message 0x7faa8c0a4b98
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:287: Request latency = 494us
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug event_statistic_accumulator.cpp:32: Accumulate 494 for 0x229c428
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug event_statistic_accumulator.cpp:32: Accumulate 494 for 0x229c4a0
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 19).
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug utils.cpp:878: Removed IOHook 0x7faaaccebdf0 to stack. There are now 0 hooks
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:161: Attempting to process queue element
19-06-2018 11:26:01.875 UTC [7faa8bfff700] Verbose httpstack.cpp:308: Process request for URL /ping, args (null)
19-06-2018 11:26:01.875 UTC [7faa8bfff700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)




Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 16:29
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I have been able to resolve some of the issues to proceed only to get stuck in another issue while running the Live Test.
The test is not yet successful and I am getting error in logs in dime(ralf):

18-06-2018 15:26:02.068 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:67: DiameterResolver::resolve for realm hpn.com, host , family 2
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:72: Do NAPTR look-up for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:230: Current time is 1529335567, expiry time of the entry at the head of the expiry list is 1529335562
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:128: Entry not in cache, so create new entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:252: NAPTR cache factory called for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:264: Sending DNS NAPTR query for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching hpn.com in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: hpn.com not found in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for hpn.com type 35
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 2 records from cache for hpn.com NAPTR
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query hpn.com (canonical domain: hpn.com)
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:139: DNS query has returned, populate the cache entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:273: Adding entry to expiry list, TTL=0, expiry time = 1529335567
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:97: NAPTR lookup failed, so do SRV lookups for TCP and SCTP
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._tcp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._tcp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._sctp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._sctp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._tcp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._sctp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._tcp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._sctp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._tcp.hpn.com (canonical domain: _diameter._tcp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._sctp.hpn.com (canonical domain: _diameter._sctp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:106: TCP SRV record _diameter._tcp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:109: SCTP SRV record _diameter._sctp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found

I am not using any external HSS and my shared_config is as below:

# Deployment definitions
home_domain=hpn.com
sprout_hostname=sprout.hpn.com
sprout_registration_store=vellum.hpn.com
hs_hostname=hs.hpn.com:8888
hs_provisioning_hostname=hs.hpn.com:8889
homestead_impu_store=vellum.hpn.com
ralf_hostname=ralf.hpn.com:10888
ralf_session_store=vellum.hpn.com
xdms_hostname=homer.hpn.com:7888
chronos_hostname=vellum.hpn.com
cassandra_hostname=vellum.hpn.com

# Email server configuration
#smtp_smarthost=<smtp server>
#smtp_username=<username>
#smtp_password=<password>
#email_recovery_sender=clearwater at example.org<mailto:#email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

# Application Servers
#gemini=<gemini port>
memento=5055
memento_auth_store=vellum.hpn.com


Please suggest how can I resolve the issue.

Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 10:34
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I am new to the community and the clearwater ims.
I have installed the clearwater solution manually on the OpenStack VMs.
In my setup I have 2 networks, one is public and other is private(only inside openstack).

I have configured the machines and DNS according to the private network.
All the tests are failing with connection refused!
I tried 2 scenarios:

  1.  Test machine in the public network.
  2.  Test machine in the private network with correct DNS hostname entry.

Can someone please suggest looking at the below part of log, what could be the issue:

Basic Call - Mainline (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `initialize'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `open'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `block in connect'
     - /usr/lib/ruby/1.9.1/timeout.rb:55:in `timeout'
     - /usr/lib/ruby/1.9.1/timeout.rb:100:in `timeout'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `connect'
     - /usr/lib/ruby/1.9.1/net/http.rb:756:in `do_start'
     - /usr/lib/ruby/1.9.1/net/http.rb:745:in `start'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:413:in `transmit'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:176:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient.rb:69:in `post'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:166:in `rescue in get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:159:in `get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:67:in `initialize'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `new'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `provision_line'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:181:in `add_endpoint'
     - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:19:in `block in <top (required)>'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `call'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `run'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:126:in `block (2 levels) in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `collect'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `block in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `each'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `run_all'
     - /home/cloud/clearwater-live-test/lib/live-test.rb:23:in `run_tests'
     - /home/cloud/clearwater-live-test/Rakefile:18:in `block in <top (required)>'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `call'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `block in execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:179:in `block in invoke_with_call_chain'
     - /usr/lib/ruby/1.9.1/monitor.rb:211:in `mon_synchronize'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:172:in `invoke_with_call_chain'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:165:in `invoke'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:150:in `invoke_task'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block (2 levels) in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:115:in `run_with_threads'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:100:in `top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:78:in `block in run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:176:in `standard_exception_handling'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:75:in `run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/bin/rake:33:in `<top (required)>'
     - /usr/local/bin/rake:23:in `load'
     - /usr/local/bin/rake:23:in `<main>'
Basic Call - SDP (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)




--------------------------------------------
Navdeep Uniyal
Email: Navdeep.uniyal at bristol.ac.uk<mailto:Navdeep.uniyal at bristol.ac.uk>
Senior Research Associate
High Performance Networks Group
University of Bristol

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180620/a1928404/attachment.html>

From navdeep.uniyal at bristol.ac.uk  Wed Jun 20 07:57:37 2018
From: navdeep.uniyal at bristol.ac.uk (Navdeep Uniyal)
Date: Wed, 20 Jun 2018 11:57:37 +0000
Subject: [Project Clearwater] Clearwater Live Test Fails
In-Reply-To: <BLUPR0201MB1490D3D5511E2F793DE0DF3F95770@BLUPR0201MB1490.namprd02.prod.outlook.com>
References: <DB5PR06MB1559504FD60C336243499D0CDE700@DB5PR06MB1559.eurprd06.prod.outlook.com>
	<DB5PR06MB155995DA8E7B707A7B965075DE700@DB5PR06MB1559.eurprd06.prod.outlook.com>
	<BLUPR0201MB1490D3D5511E2F793DE0DF3F95770@BLUPR0201MB1490.namprd02.prod.outlook.com>
Message-ID: <DB5PR06MB1559CDE0E92FBB3932D3E8FADE770@DB5PR06MB1559.eurprd06.prod.outlook.com>

Hi Will,

Thank you for your response.

I have not set hss_realm or hss_hostname in my configuration.

I have done the basic install of the clearwater manually on openstack VMs.
All VMs can talk to each other using the private network. Also they are connected to the public network where the SIP client would be running.

All the calls I am making with clearwater-live-test are failing with following output:
Basic Call - Mainline (TCP) - (6505550090, 6505550593) Failed
Endpoint threw exception:
- sip:6505550593 at cw-ngv.com timed out waiting for new incoming call
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:68:in `rescue in incoming_call'
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:65:in `incoming_call'
   - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:55:in `block (2 levels) in <top (required)>'
Terminating other threads after failure

Please suggest how can I resolve the issue.

Kind Regards,
Navdeep
From: Clearwater <clearwater-bounces at lists.projectclearwater.org> On Behalf Of William Yates
Sent: 20 June 2018 11:00
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Hi Navdeep,

Regarding diameter peer errors, it looks like your dime node is trying to resolve to an external HSS, via DNS.

Do you have hss_realm and/or hss_hostname configured?

>From https://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options :
*  hss_realm - this sets the Destination-Realm of your external HSS. When this field is set, the homestead process on Dime will then attempt to set up multiple Diameter connections using an SRV lookup on this realm.
*  hss_hostname - this sets the Destination-Host of your external HSS, if you have one. The homestead process on Dime will also try and establish a Diameter connection to this host (on port 3868) if no SRV-discovered peers exist.

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Navdeep Uniyal
Sent: 19 June 2018 14:58
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Dear All,

It would be great if someone could share the expertise in the ims and would help me resolve the issues.

Kind Regards,
Navdeep

From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 19 June 2018 12:30
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Dear All,

Sorry for putting so many emails!
It would be really helpful if someone from the community could please suggest me on how to resolve the issue.

Live Test Results:

cloud at imstestserver:~/clearwater-live-test$ sudo rake test[hpn.com] SIGNUP_CODE=secret
Basic Call - Mainline (TCP) - (6505550786, 6505550816) Failed
Endpoint threw exception:
- sip:6505550816 at cw-ngv.com timed out waiting for new incoming call
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:68:in `rescue in incoming_call'
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:65:in `incoming_call'
   - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:55:in `block (2 levels) in <top (required)>'
Terminating other threads after failure

Bono Logs :

19-06-2018 11:20:18.666 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:20:33.677 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:20:48.678 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:03.687 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:21:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:21:18.700 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:33.715 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:21:48.724 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:03.730 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:22:18.736 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:33.751 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:22:48.763 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:03.774 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:23:18.781 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:31.700 UTC [7fc2cf4d4700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 23
19-06-2018 11:23:33.797 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:23:48.808 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:24:03.818 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm


Sprout Logs:

--start msg--

OPTIONS sip:poll-sip at 20.0.0.5:5054 SIP/2.0
Via: SIP/2.0/TCP 20.0.0.5;rport;branch=z9hG4bK-498767
Max-Forwards: 2
To: <sip:poll-sip at 20.0.0.5:5054>
From: poll-sip <sip:poll-sip at 20.0.0.5>;tag=498767
Call-ID: poll-sip-498767
CSeq: 498767 OPTIONS
Contact: <sip:20.0.0.5>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug uri_classifier.cpp:173: Classified URI sip:poll-sip at 20.0.0.5:5054 as 3
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:568: Received message 0x7faa8c03d4d0
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:585: Admitted request 0x7faa8c03d4d0
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:620: Incoming message 0x7faa8c03d4d0 cloned to 0x7faa8c0a4b98
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:639: Queuing cloned received message 0x7faa8c0a4b98 for worker threads with priority 15
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x22a03b8
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x22a0430
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug utils.cpp:872: Added IOHook 0x7faaaccebdf0 to stack. There are now 1 hooks
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:181: Worker thread dequeue message 0x7faa8c0a4b98
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:186: Request latency so far = 227us
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=498767 (rdata0x7faa8c0a4b98)
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug uri_classifier.cpp:173: Classified URI sip:poll-sip at 20.0.0.5:5054 as 3
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80) created
19-06-2018 11:26:01.816 UTC [7faaaccec700] Verbose common_sip_processing.cpp:103: TX 266 bytes Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80) to TCP 20.0.0.5:41266:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 20.0.0.5;rport=41266;received=20.0.0.5;branch=z9hG4bK-498767
Call-ID: poll-sip-498767
From: "poll-sip" <sip:poll-sip at 20.0.0.5>;tag=498767
To: <sip:poll-sip at 20.0.0.5>;tag=z9hG4bK-498767
CSeq: 498767 OPTIONS
Content-Length:  0


--end msg--
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip: tdta0x7faa8c05 Destroying txdata Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80)
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:273: Worker thread completed processing message 0x7faa8c0a4b98
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:287: Request latency = 494us
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug event_statistic_accumulator.cpp:32: Accumulate 494 for 0x229c428
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug event_statistic_accumulator.cpp:32: Accumulate 494 for 0x229c4a0
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 19).
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug utils.cpp:878: Removed IOHook 0x7faaaccebdf0 to stack. There are now 0 hooks
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:161: Attempting to process queue element
19-06-2018 11:26:01.875 UTC [7faa8bfff700] Verbose httpstack.cpp:308: Process request for URL /ping, args (null)
19-06-2018 11:26:01.875 UTC [7faa8bfff700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)




Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 16:29
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I have been able to resolve some of the issues to proceed only to get stuck in another issue while running the Live Test.
The test is not yet successful and I am getting error in logs in dime(ralf):

18-06-2018 15:26:02.068 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:67: DiameterResolver::resolve for realm hpn.com, host , family 2
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:72: Do NAPTR look-up for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:230: Current time is 1529335567, expiry time of the entry at the head of the expiry list is 1529335562
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:128: Entry not in cache, so create new entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:252: NAPTR cache factory called for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:264: Sending DNS NAPTR query for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching hpn.com in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: hpn.com not found in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for hpn.com type 35
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 2 records from cache for hpn.com NAPTR
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query hpn.com (canonical domain: hpn.com)
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:139: DNS query has returned, populate the cache entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:273: Adding entry to expiry list, TTL=0, expiry time = 1529335567
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:97: NAPTR lookup failed, so do SRV lookups for TCP and SCTP
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._tcp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._tcp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._sctp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._sctp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._tcp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._sctp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._tcp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._sctp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._tcp.hpn.com (canonical domain: _diameter._tcp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._sctp.hpn.com (canonical domain: _diameter._sctp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:106: TCP SRV record _diameter._tcp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:109: SCTP SRV record _diameter._sctp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found

I am not using any external HSS and my shared_config is as below:

# Deployment definitions
home_domain=hpn.com
sprout_hostname=sprout.hpn.com
sprout_registration_store=vellum.hpn.com
hs_hostname=hs.hpn.com:8888
hs_provisioning_hostname=hs.hpn.com:8889
homestead_impu_store=vellum.hpn.com
ralf_hostname=ralf.hpn.com:10888
ralf_session_store=vellum.hpn.com
xdms_hostname=homer.hpn.com:7888
chronos_hostname=vellum.hpn.com
cassandra_hostname=vellum.hpn.com

# Email server configuration
#smtp_smarthost=<smtp server>
#smtp_username=<username>
#smtp_password=<password>
#email_recovery_sender=clearwater at example.org<mailto:#email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

# Application Servers
#gemini=<gemini port>
memento=5055
memento_auth_store=vellum.hpn.com


Please suggest how can I resolve the issue.

Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 10:34
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I am new to the community and the clearwater ims.
I have installed the clearwater solution manually on the OpenStack VMs.
In my setup I have 2 networks, one is public and other is private(only inside openstack).

I have configured the machines and DNS according to the private network.
All the tests are failing with connection refused!
I tried 2 scenarios:

  1.  Test machine in the public network.
  2.  Test machine in the private network with correct DNS hostname entry.

Can someone please suggest looking at the below part of log, what could be the issue:

Basic Call - Mainline (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `initialize'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `open'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `block in connect'
     - /usr/lib/ruby/1.9.1/timeout.rb:55:in `timeout'
     - /usr/lib/ruby/1.9.1/timeout.rb:100:in `timeout'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `connect'
     - /usr/lib/ruby/1.9.1/net/http.rb:756:in `do_start'
     - /usr/lib/ruby/1.9.1/net/http.rb:745:in `start'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:413:in `transmit'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:176:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient.rb:69:in `post'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:166:in `rescue in get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:159:in `get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:67:in `initialize'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `new'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `provision_line'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:181:in `add_endpoint'
     - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:19:in `block in <top (required)>'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `call'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `run'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:126:in `block (2 levels) in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `collect'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `block in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `each'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `run_all'
     - /home/cloud/clearwater-live-test/lib/live-test.rb:23:in `run_tests'
     - /home/cloud/clearwater-live-test/Rakefile:18:in `block in <top (required)>'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `call'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `block in execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:179:in `block in invoke_with_call_chain'
     - /usr/lib/ruby/1.9.1/monitor.rb:211:in `mon_synchronize'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:172:in `invoke_with_call_chain'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:165:in `invoke'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:150:in `invoke_task'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block (2 levels) in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:115:in `run_with_threads'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:100:in `top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:78:in `block in run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:176:in `standard_exception_handling'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:75:in `run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/bin/rake:33:in `<top (required)>'
     - /usr/local/bin/rake:23:in `load'
     - /usr/local/bin/rake:23:in `<main>'
Basic Call - SDP (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)




--------------------------------------------
Navdeep Uniyal
Email: Navdeep.uniyal at bristol.ac.uk<mailto:Navdeep.uniyal at bristol.ac.uk>
Senior Research Associate
High Performance Networks Group
University of Bristol

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180620/f7ec2a2b/attachment.html>

From William.Yates at metaswitch.com  Wed Jun 20 10:00:08 2018
From: William.Yates at metaswitch.com (William Yates)
Date: Wed, 20 Jun 2018 14:00:08 +0000
Subject: [Project Clearwater] Clearwater Live Test Fails
In-Reply-To: <DB5PR06MB1559CDE0E92FBB3932D3E8FADE770@DB5PR06MB1559.eurprd06.prod.outlook.com>
References: <DB5PR06MB1559504FD60C336243499D0CDE700@DB5PR06MB1559.eurprd06.prod.outlook.com>
	<DB5PR06MB155995DA8E7B707A7B965075DE700@DB5PR06MB1559.eurprd06.prod.outlook.com>
	<BLUPR0201MB1490D3D5511E2F793DE0DF3F95770@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<DB5PR06MB1559CDE0E92FBB3932D3E8FADE770@DB5PR06MB1559.eurprd06.prod.outlook.com>
Message-ID: <BLUPR0201MB1490510C28CDF9FD38CF7D0095770@BLUPR0201MB1490.namprd02.prod.outlook.com>

Hi Navdeep,

Dime should not be trying to talk over the Cx interface if those are not set - can you double-check your config has been applied on the sprout node? (you can use ps to see the arguments to sprout)
I'm just wondering if you have made shared config changes but not uploaded them with cw-config upload.
I would resolve this first.



Regarding the quaff output, it's telling you that the call is not making it back out to the B party on the live test node.
I suggest turning on trace through the call path, turning on trace as described in relevant sections under https://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html, to see where the call is being dropped.
[ A tcpdump capture should get you to the right node more quickly ]

Also, it *might* be easier to run through this process to make a single call, before moving on to live test: https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html
.. and again enabling/inspecting trace along the call path, to find out where the failure is occurring.

Good luck!

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Navdeep Uniyal
Sent: 20 June 2018 12:58
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Hi Will,

Thank you for your response.

I have not set hss_realm or hss_hostname in my configuration.

I have done the basic install of the clearwater manually on openstack VMs.
All VMs can talk to each other using the private network. Also they are connected to the public network where the SIP client would be running.

All the calls I am making with clearwater-live-test are failing with following output:
Basic Call - Mainline (TCP) - (6505550090, 6505550593) Failed
Endpoint threw exception:
- sip:6505550593 at cw-ngv.com timed out waiting for new incoming call
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:68:in `rescue in incoming_call'
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:65:in `incoming_call'
   - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:55:in `block (2 levels) in <top (required)>'
Terminating other threads after failure

Please suggest how can I resolve the issue.

Kind Regards,
Navdeep
From: Clearwater <clearwater-bounces at lists.projectclearwater.org> On Behalf Of William Yates
Sent: 20 June 2018 11:00
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Hi Navdeep,

Regarding diameter peer errors, it looks like your dime node is trying to resolve to an external HSS, via DNS.

Do you have hss_realm and/or hss_hostname configured?

>From https://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options :
*  hss_realm - this sets the Destination-Realm of your external HSS. When this field is set, the homestead process on Dime will then attempt to set up multiple Diameter connections using an SRV lookup on this realm.
*  hss_hostname - this sets the Destination-Host of your external HSS, if you have one. The homestead process on Dime will also try and establish a Diameter connection to this host (on port 3868) if no SRV-discovered peers exist.

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Navdeep Uniyal
Sent: 19 June 2018 14:58
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Dear All,

It would be great if someone could share the expertise in the ims and would help me resolve the issues.

Kind Regards,
Navdeep

From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 19 June 2018 12:30
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Dear All,

Sorry for putting so many emails!
It would be really helpful if someone from the community could please suggest me on how to resolve the issue.

Live Test Results:

cloud at imstestserver:~/clearwater-live-test$ sudo rake test[hpn.com] SIGNUP_CODE=secret
Basic Call - Mainline (TCP) - (6505550786, 6505550816) Failed
Endpoint threw exception:
- sip:6505550816 at cw-ngv.com timed out waiting for new incoming call
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:68:in `rescue in incoming_call'
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:65:in `incoming_call'
   - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:55:in `block (2 levels) in <top (required)>'
Terminating other threads after failure

Bono Logs :

19-06-2018 11:20:18.666 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:20:33.677 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:20:48.678 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:03.687 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:21:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:21:18.700 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:33.715 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:21:48.724 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:03.730 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:22:18.736 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:33.751 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:22:48.763 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:03.774 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:23:18.781 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:31.700 UTC [7fc2cf4d4700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 23
19-06-2018 11:23:33.797 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:23:48.808 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:24:03.818 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm


Sprout Logs:

--start msg--

OPTIONS sip:poll-sip at 20.0.0.5:5054 SIP/2.0
Via: SIP/2.0/TCP 20.0.0.5;rport;branch=z9hG4bK-498767
Max-Forwards: 2
To: <sip:poll-sip at 20.0.0.5:5054>
From: poll-sip <sip:poll-sip at 20.0.0.5>;tag=498767
Call-ID: poll-sip-498767
CSeq: 498767 OPTIONS
Contact: <sip:20.0.0.5>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug uri_classifier.cpp:173: Classified URI sip:poll-sip at 20.0.0.5:5054 as 3
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:568: Received message 0x7faa8c03d4d0
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:585: Admitted request 0x7faa8c03d4d0
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:620: Incoming message 0x7faa8c03d4d0 cloned to 0x7faa8c0a4b98
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:639: Queuing cloned received message 0x7faa8c0a4b98 for worker threads with priority 15
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x22a03b8
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x22a0430
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug utils.cpp:872: Added IOHook 0x7faaaccebdf0 to stack. There are now 1 hooks
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:181: Worker thread dequeue message 0x7faa8c0a4b98
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:186: Request latency so far = 227us
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=498767 (rdata0x7faa8c0a4b98)
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug uri_classifier.cpp:173: Classified URI sip:poll-sip at 20.0.0.5:5054 as 3
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80) created
19-06-2018 11:26:01.816 UTC [7faaaccec700] Verbose common_sip_processing.cpp:103: TX 266 bytes Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80) to TCP 20.0.0.5:41266:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 20.0.0.5;rport=41266;received=20.0.0.5;branch=z9hG4bK-498767
Call-ID: poll-sip-498767
From: "poll-sip" <sip:poll-sip at 20.0.0.5>;tag=498767
To: <sip:poll-sip at 20.0.0.5>;tag=z9hG4bK-498767
CSeq: 498767 OPTIONS
Content-Length:  0


--end msg--
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip: tdta0x7faa8c05 Destroying txdata Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80)
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:273: Worker thread completed processing message 0x7faa8c0a4b98
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:287: Request latency = 494us
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug event_statistic_accumulator.cpp:32: Accumulate 494 for 0x229c428
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug event_statistic_accumulator.cpp:32: Accumulate 494 for 0x229c4a0
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 19).
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug utils.cpp:878: Removed IOHook 0x7faaaccebdf0 to stack. There are now 0 hooks
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:161: Attempting to process queue element
19-06-2018 11:26:01.875 UTC [7faa8bfff700] Verbose httpstack.cpp:308: Process request for URL /ping, args (null)
19-06-2018 11:26:01.875 UTC [7faa8bfff700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)




Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 16:29
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I have been able to resolve some of the issues to proceed only to get stuck in another issue while running the Live Test.
The test is not yet successful and I am getting error in logs in dime(ralf):

18-06-2018 15:26:02.068 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:67: DiameterResolver::resolve for realm hpn.com, host , family 2
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:72: Do NAPTR look-up for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:230: Current time is 1529335567, expiry time of the entry at the head of the expiry list is 1529335562
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:128: Entry not in cache, so create new entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:252: NAPTR cache factory called for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:264: Sending DNS NAPTR query for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching hpn.com in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: hpn.com not found in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for hpn.com type 35
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 2 records from cache for hpn.com NAPTR
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query hpn.com (canonical domain: hpn.com)
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:139: DNS query has returned, populate the cache entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:273: Adding entry to expiry list, TTL=0, expiry time = 1529335567
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:97: NAPTR lookup failed, so do SRV lookups for TCP and SCTP
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._tcp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._tcp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._sctp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._sctp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._tcp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._sctp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._tcp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._sctp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._tcp.hpn.com (canonical domain: _diameter._tcp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._sctp.hpn.com (canonical domain: _diameter._sctp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:106: TCP SRV record _diameter._tcp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:109: SCTP SRV record _diameter._sctp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found

I am not using any external HSS and my shared_config is as below:

# Deployment definitions
home_domain=hpn.com
sprout_hostname=sprout.hpn.com
sprout_registration_store=vellum.hpn.com
hs_hostname=hs.hpn.com:8888
hs_provisioning_hostname=hs.hpn.com:8889
homestead_impu_store=vellum.hpn.com
ralf_hostname=ralf.hpn.com:10888
ralf_session_store=vellum.hpn.com
xdms_hostname=homer.hpn.com:7888
chronos_hostname=vellum.hpn.com
cassandra_hostname=vellum.hpn.com

# Email server configuration
#smtp_smarthost=<smtp server>
#smtp_username=<username>
#smtp_password=<password>
#email_recovery_sender=clearwater at example.org<mailto:#email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

# Application Servers
#gemini=<gemini port>
memento=5055
memento_auth_store=vellum.hpn.com


Please suggest how can I resolve the issue.

Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 10:34
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I am new to the community and the clearwater ims.
I have installed the clearwater solution manually on the OpenStack VMs.
In my setup I have 2 networks, one is public and other is private(only inside openstack).

I have configured the machines and DNS according to the private network.
All the tests are failing with connection refused!
I tried 2 scenarios:

  1.  Test machine in the public network.
  2.  Test machine in the private network with correct DNS hostname entry.

Can someone please suggest looking at the below part of log, what could be the issue:

Basic Call - Mainline (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `initialize'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `open'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `block in connect'
     - /usr/lib/ruby/1.9.1/timeout.rb:55:in `timeout'
     - /usr/lib/ruby/1.9.1/timeout.rb:100:in `timeout'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `connect'
     - /usr/lib/ruby/1.9.1/net/http.rb:756:in `do_start'
     - /usr/lib/ruby/1.9.1/net/http.rb:745:in `start'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:413:in `transmit'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:176:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient.rb:69:in `post'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:166:in `rescue in get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:159:in `get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:67:in `initialize'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `new'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `provision_line'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:181:in `add_endpoint'
     - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:19:in `block in <top (required)>'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `call'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `run'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:126:in `block (2 levels) in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `collect'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `block in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `each'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `run_all'
     - /home/cloud/clearwater-live-test/lib/live-test.rb:23:in `run_tests'
     - /home/cloud/clearwater-live-test/Rakefile:18:in `block in <top (required)>'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `call'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `block in execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:179:in `block in invoke_with_call_chain'
     - /usr/lib/ruby/1.9.1/monitor.rb:211:in `mon_synchronize'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:172:in `invoke_with_call_chain'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:165:in `invoke'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:150:in `invoke_task'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block (2 levels) in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:115:in `run_with_threads'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:100:in `top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:78:in `block in run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:176:in `standard_exception_handling'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:75:in `run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/bin/rake:33:in `<top (required)>'
     - /usr/local/bin/rake:23:in `load'
     - /usr/local/bin/rake:23:in `<main>'
Basic Call - SDP (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)




--------------------------------------------
Navdeep Uniyal
Email: Navdeep.uniyal at bristol.ac.uk<mailto:Navdeep.uniyal at bristol.ac.uk>
Senior Research Associate
High Performance Networks Group
University of Bristol

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180620/10885ce0/attachment.html>

From navdeep.uniyal at bristol.ac.uk  Fri Jun 22 07:56:25 2018
From: navdeep.uniyal at bristol.ac.uk (Navdeep Uniyal)
Date: Fri, 22 Jun 2018 11:56:25 +0000
Subject: [Project Clearwater] Clearwater Live Test Fails
In-Reply-To: <BLUPR0201MB1490510C28CDF9FD38CF7D0095770@BLUPR0201MB1490.namprd02.prod.outlook.com>
References: <DB5PR06MB1559504FD60C336243499D0CDE700@DB5PR06MB1559.eurprd06.prod.outlook.com>
	<DB5PR06MB155995DA8E7B707A7B965075DE700@DB5PR06MB1559.eurprd06.prod.outlook.com>
	<BLUPR0201MB1490D3D5511E2F793DE0DF3F95770@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<DB5PR06MB1559CDE0E92FBB3932D3E8FADE770@DB5PR06MB1559.eurprd06.prod.outlook.com>
	<BLUPR0201MB1490510C28CDF9FD38CF7D0095770@BLUPR0201MB1490.namprd02.prod.outlook.com>
Message-ID: <DB5PR06MB155950F681FCD5992225CC3DDE750@DB5PR06MB1559.eurprd06.prod.outlook.com>

Hi William,

My shared config is as below, please check if it is fine:

# Deployment definitions
home_domain=hpn.com
sprout_hostname=sprout.hpn.com
sprout_registration_store=vellum.hpn.com
hs_hostname=hs.hpn.com:8888
hs_provisioning_hostname=hs.hpn.com:8889
homestead_impu_store=vellum.hpn.com
ralf_hostname=ralf.hpn.com:10888
ralf_session_store=vellum.hpn.com
xdms_hostname=homer.hpn.com:7888
chronos_hostname=vellum.hpn.com
cassandra_hostname=vellum.hpn.com

# Email server configuration
#smtp_smarthost=<smtp server>
#smtp_username=<username>
#smtp_password=<password>
#email_recovery_sender=clearwater at example.org

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

# Application Servers
#gemini=<gemini port>
memento=5055
memento_auth_store=vellum.hpn.com

I have uploaded it using cw-config upload. I cannot see the parameters on sprout using 'ps' or 'ps -a'.

I have less ideas on troubleshooting this issue.
One potential issue I noticed is about the domain which is getting created while I am using ellis:
6505550387 at cw-ngv.com<mailto:6505550387 at cw-ngv.com>

But the domain I configured in my setup is hpn.com.

Please suggest.

Using the below listed link, I could see how to increase the log levels in the various components (which I already did) but I cannot see any specific mention to trace the call (Sorry, maybe I am missing something). https://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html,


Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org> On Behalf Of William Yates
Sent: 20 June 2018 15:00
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Hi Navdeep,

Dime should not be trying to talk over the Cx interface if those are not set - can you double-check your config has been applied on the sprout node? (you can use ps to see the arguments to sprout)
I'm just wondering if you have made shared config changes but not uploaded them with cw-config upload.
I would resolve this first.



Regarding the quaff output, it's telling you that the call is not making it back out to the B party on the live test node.
I suggest turning on trace through the call path, turning on trace as described in relevant sections under https://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html, to see where the call is being dropped.
[ A tcpdump capture should get you to the right node more quickly ]

Also, it *might* be easier to run through this process to make a single call, before moving on to live test: https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html
.. and again enabling/inspecting trace along the call path, to find out where the failure is occurring.

Good luck!

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Navdeep Uniyal
Sent: 20 June 2018 12:58
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Hi Will,

Thank you for your response.

I have not set hss_realm or hss_hostname in my configuration.

I have done the basic install of the clearwater manually on openstack VMs.
All VMs can talk to each other using the private network. Also they are connected to the public network where the SIP client would be running.

All the calls I am making with clearwater-live-test are failing with following output:
Basic Call - Mainline (TCP) - (6505550090, 6505550593) Failed
Endpoint threw exception:
- sip:6505550593 at cw-ngv.com timed out waiting for new incoming call
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:68:in `rescue in incoming_call'
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:65:in `incoming_call'
   - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:55:in `block (2 levels) in <top (required)>'
Terminating other threads after failure

Please suggest how can I resolve the issue.

Kind Regards,
Navdeep
From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of William Yates
Sent: 20 June 2018 11:00
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Hi Navdeep,

Regarding diameter peer errors, it looks like your dime node is trying to resolve to an external HSS, via DNS.

Do you have hss_realm and/or hss_hostname configured?

>From https://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options :
*  hss_realm - this sets the Destination-Realm of your external HSS. When this field is set, the homestead process on Dime will then attempt to set up multiple Diameter connections using an SRV lookup on this realm.
*  hss_hostname - this sets the Destination-Host of your external HSS, if you have one. The homestead process on Dime will also try and establish a Diameter connection to this host (on port 3868) if no SRV-discovered peers exist.

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Navdeep Uniyal
Sent: 19 June 2018 14:58
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Dear All,

It would be great if someone could share the expertise in the ims and would help me resolve the issues.

Kind Regards,
Navdeep

From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 19 June 2018 12:30
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater Live Test Fails

Dear All,

Sorry for putting so many emails!
It would be really helpful if someone from the community could please suggest me on how to resolve the issue.

Live Test Results:

cloud at imstestserver:~/clearwater-live-test$ sudo rake test[hpn.com] SIGNUP_CODE=secret
Basic Call - Mainline (TCP) - (6505550786, 6505550816) Failed
Endpoint threw exception:
- sip:6505550816 at cw-ngv.com timed out waiting for new incoming call
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:68:in `rescue in incoming_call'
   - /home/cloud/clearwater-live-test/quaff/lib/endpoint.rb:65:in `incoming_call'
   - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:55:in `block (2 levels) in <top (required)>'
Terminating other threads after failure

Bono Logs :

19-06-2018 11:20:18.666 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:20:33.677 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:20:40.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:20:48.678 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:03.687 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:21:10.115 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:21:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:21:18.700 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:33.715 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:21:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:21:48.724 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:03.730 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:22:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:22:18.736 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:33.751 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:22:40.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:22:48.763 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:03.774 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:23:10.116 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:23:18.781 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:31.700 UTC [7fc2cf4d4700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 23
19-06-2018 11:23:33.797 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:23:40.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm
19-06-2018 11:23:48.808 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:24:03.818 UTC [7fc2ce4d2700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:244: Reraising all alarms with a known state
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1005.1 alarm
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1012.3 alarm
19-06-2018 11:24:10.117 UTC [7fc2eadf2700] Status alarm.cpp:37: sprout issued 1013.3 alarm


Sprout Logs:

--start msg--

OPTIONS sip:poll-sip at 20.0.0.5:5054 SIP/2.0
Via: SIP/2.0/TCP 20.0.0.5;rport;branch=z9hG4bK-498767
Max-Forwards: 2
To: <sip:poll-sip at 20.0.0.5:5054>
From: poll-sip <sip:poll-sip at 20.0.0.5>;tag=498767
Call-ID: poll-sip-498767
CSeq: 498767 OPTIONS
Contact: <sip:20.0.0.5>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug uri_classifier.cpp:173: Classified URI sip:poll-sip at 20.0.0.5:5054 as 3
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:568: Received message 0x7faa8c03d4d0
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:585: Admitted request 0x7faa8c03d4d0
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:620: Incoming message 0x7faa8c03d4d0 cloned to 0x7faa8c0a4b98
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug thread_dispatcher.cpp:639: Queuing cloned received message 0x7faa8c0a4b98 for worker threads with priority 15
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x22a03b8
19-06-2018 11:26:01.815 UTC [7faa90cb4700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x22a0430
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug utils.cpp:872: Added IOHook 0x7faaaccebdf0 to stack. There are now 1 hooks
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:181: Worker thread dequeue message 0x7faa8c0a4b98
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:186: Request latency so far = 227us
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=498767 (rdata0x7faa8c0a4b98)
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug uri_classifier.cpp:173: Classified URI sip:poll-sip at 20.0.0.5:5054 as 3
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80) created
19-06-2018 11:26:01.816 UTC [7faaaccec700] Verbose common_sip_processing.cpp:103: TX 266 bytes Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80) to TCP 20.0.0.5:41266:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 20.0.0.5;rport=41266;received=20.0.0.5;branch=z9hG4bK-498767
Call-ID: poll-sip-498767
From: "poll-sip" <sip:poll-sip at 20.0.0.5>;tag=498767
To: <sip:poll-sip at 20.0.0.5>;tag=z9hG4bK-498767
CSeq: 498767 OPTIONS
Content-Length:  0


--end msg--
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug pjsip: tdta0x7faa8c05 Destroying txdata Response msg 200/OPTIONS/cseq=498767 (tdta0x7faa8c05ce80)
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:273: Worker thread completed processing message 0x7faa8c0a4b98
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:287: Request latency = 494us
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug event_statistic_accumulator.cpp:32: Accumulate 494 for 0x229c428
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug event_statistic_accumulator.cpp:32: Accumulate 494 for 0x229c4a0
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 19).
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug utils.cpp:878: Removed IOHook 0x7faaaccebdf0 to stack. There are now 0 hooks
19-06-2018 11:26:01.816 UTC [7faaaccec700] Debug thread_dispatcher.cpp:161: Attempting to process queue element
19-06-2018 11:26:01.875 UTC [7faa8bfff700] Verbose httpstack.cpp:308: Process request for URL /ping, args (null)
19-06-2018 11:26:01.875 UTC [7faa8bfff700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)




Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 16:29
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I have been able to resolve some of the issues to proceed only to get stuck in another issue while running the Live Test.
The test is not yet successful and I am getting error in logs in dime(ralf):

18-06-2018 15:26:02.068 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:67: DiameterResolver::resolve for realm hpn.com, host , family 2
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:72: Do NAPTR look-up for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:230: Current time is 1529335567, expiry time of the entry at the head of the expiry list is 1529335562
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:128: Entry not in cache, so create new entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:252: NAPTR cache factory called for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug baseresolver.cpp:264: Sending DNS NAPTR query for hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching hpn.com in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching hpn.com
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: hpn.com not found in the static cache
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for hpn.com type 35
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 2 records from cache for hpn.com NAPTR
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query hpn.com (canonical domain: hpn.com)
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:139: DNS query has returned, populate the cache entry
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug ttlcache.h:273: Adding entry to expiry list, TTL=0, expiry time = 1529335567
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug diameterresolver.cpp:97: NAPTR lookup failed, so do SRV lookups for TCP and SCTP
18-06-2018 15:26:07.068 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._tcp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._tcp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._tcp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:250: Searching for DNS record matching _diameter._sctp.hpn.com in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose static_dns_cache.cpp:327: No matching CNAME record found in static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug static_dns_cache.cpp:303: No static records found matching _diameter._sctp.hpn.com
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:269: _diameter._sctp.hpn.com not found in the static cache
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._tcp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Verbose dnscachedresolver.cpp:314: Check cache for _diameter._sctp.hpn.com type 33
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._tcp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:424: Pulling 0 records from cache for _diameter._sctp.hpn.com SRV
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._tcp.hpn.com (canonical domain: _diameter._tcp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug dnscachedresolver.cpp:287: Found result for query _diameter._sctp.hpn.com (canonical domain: _diameter._sctp.hpn.com)
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:106: TCP SRV record _diameter._tcp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Debug diameterresolver.cpp:109: SCTP SRV record _diameter._sctp.hpn.com returned 0 records
18-06-2018 15:26:07.069 UTC [7f8ff0f91700] Error diameterstack.cpp:862: No Diameter peers have been found

I am not using any external HSS and my shared_config is as below:

# Deployment definitions
home_domain=hpn.com
sprout_hostname=sprout.hpn.com
sprout_registration_store=vellum.hpn.com
hs_hostname=hs.hpn.com:8888
hs_provisioning_hostname=hs.hpn.com:8889
homestead_impu_store=vellum.hpn.com
ralf_hostname=ralf.hpn.com:10888
ralf_session_store=vellum.hpn.com
xdms_hostname=homer.hpn.com:7888
chronos_hostname=vellum.hpn.com
cassandra_hostname=vellum.hpn.com

# Email server configuration
#smtp_smarthost=<smtp server>
#smtp_username=<username>
#smtp_password=<password>
#email_recovery_sender=clearwater at example.org<mailto:#email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

# Application Servers
#gemini=<gemini port>
memento=5055
memento_auth_store=vellum.hpn.com


Please suggest how can I resolve the issue.

Kind Regards,
Navdeep


From: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> On Behalf Of Navdeep Uniyal
Sent: 18 June 2018 10:34
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Clearwater RakeTest Fails

Dear All,

I am new to the community and the clearwater ims.
I have installed the clearwater solution manually on the OpenStack VMs.
In my setup I have 2 networks, one is public and other is private(only inside openstack).

I have configured the machines and DNS according to the private network.
All the tests are failing with connection refused!
I tried 2 scenarios:

  1.  Test machine in the public network.
  2.  Test machine in the private network with correct DNS hostname entry.

Can someone please suggest looking at the below part of log, what could be the issue:

Basic Call - Mainline (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `initialize'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `open'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `block in connect'
     - /usr/lib/ruby/1.9.1/timeout.rb:55:in `timeout'
     - /usr/lib/ruby/1.9.1/timeout.rb:100:in `timeout'
     - /usr/lib/ruby/1.9.1/net/http.rb:763:in `connect'
     - /usr/lib/ruby/1.9.1/net/http.rb:756:in `do_start'
     - /usr/lib/ruby/1.9.1/net/http.rb:745:in `start'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:413:in `transmit'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:176:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
     - /var/lib/gems/1.9.1/gems/rest-client-1.8.0/lib/restclient.rb:69:in `post'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:166:in `rescue in get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:159:in `get_security_cookie'
     - /home/cloud/clearwater-live-test/lib/ellis.rb:67:in `initialize'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `new'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:354:in `provision_line'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:181:in `add_endpoint'
     - /home/cloud/clearwater-live-test/lib/tests/basic-call.rb:19:in `block in <top (required)>'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `call'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:256:in `run'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:126:in `block (2 levels) in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `collect'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:112:in `block in run_all'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `each'
     - /home/cloud/clearwater-live-test/lib/test-definition.rb:111:in `run_all'
     - /home/cloud/clearwater-live-test/lib/live-test.rb:23:in `run_tests'
     - /home/cloud/clearwater-live-test/Rakefile:18:in `block in <top (required)>'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `call'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:240:in `block in execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:235:in `execute'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:179:in `block in invoke_with_call_chain'
     - /usr/lib/ruby/1.9.1/monitor.rb:211:in `mon_synchronize'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:172:in `invoke_with_call_chain'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/task.rb:165:in `invoke'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:150:in `invoke_task'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block (2 levels) in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `each'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:106:in `block in top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:115:in `run_with_threads'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:100:in `top_level'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:78:in `block in run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:176:in `standard_exception_handling'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/lib/rake/application.rb:75:in `run'
     - /var/lib/gems/1.9.1/gems/rake-10.4.2/bin/rake:33:in `<top (required)>'
     - /usr/local/bin/rake:23:in `load'
     - /usr/local/bin/rake:23:in `<main>'
Basic Call - SDP (TCP) - ^[[37;41mFailed^[[37;0m
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)




--------------------------------------------
Navdeep Uniyal
Email: Navdeep.uniyal at bristol.ac.uk<mailto:Navdeep.uniyal at bristol.ac.uk>
Senior Research Associate
High Performance Networks Group
University of Bristol

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180622/5168aa49/attachment.html>

From bf1936 at att.com  Mon Jun 11 12:54:03 2018
From: bf1936 at att.com (FREEMAN, BRIAN D)
Date: Mon, 11 Jun 2018 16:54:03 -0000
Subject: [Project Clearwater] Clearwater vIMS with ONAP Beijing
Message-ID: <D7DB3CAC16035F4882B69E2CCA1B039644B69278@MISOUT7MSGUSRDF.ITServices.sbc.com>

Trying to update the onap wiki to show installation of Clearwater vIMS on ONAP Beijing.

A minor update to the heat templates was required to make the server names unique with the nested heat templates (and adding a default dns for our windriver lab)
In Beijing SDC checks to make sure the resources are unique across the nested template per the VNF guidelines.

bono.yaml: bono_server:

bono.yaml: value: { get_attr: [ bono_server, accessIPv4 ] }
dns.yaml: dns_server:
dns.yaml: value: { get_attr: [ dns_server, accessIPv4 ] }
ellis.yaml: ellis_server:
ellis.yaml: value: { get_attr: [ ellis_server, accessIPv4 ] }
homer.yaml: homer_server:
homer.yaml: value: { get_attr: [ homer_server, accessIPv4 ] }
homestead.yaml: homestead_server:
homestead.yaml: value: { get_attr: [ homestead_server, accessIPv4 ] }
ralf.yaml: ralf_server:
ralf.yaml: value: { get_attr: [ ralf_server, accessIPv4 ] }
sprout.yaml: sprout_server:
sprout.yaml: value: { get_attr: [ sprout_server, accessIPv4 ] }



The templates onboard to SDC and instantiate but the resulting instances dont seem to work with the clearwater test suite.

Homer calls fail because the homer_process fails to start.

Any suggestions ?

Brian

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180611/8a8d0486/attachment.html>

From salvatore.cmp at gmail.com  Thu Jun 28 12:33:22 2018
From: salvatore.cmp at gmail.com (Salvatore Campanella)
Date: Thu, 28 Jun 2018 18:33:22 +0200
Subject: [Project Clearwater]  Delete pool of numbers provisioned in Ellis
Message-ID: <CAH21nexSr=RNRnPR3CDpB3jF1wUBkkUGwnBaHf9BFxr0akWdXg@mail.gmail.com>

Hi all,

I am new on Clearwater.
I have installed Clearwater manually and I have provisioned two pools of
numbers using create_nambers.py.

Is it possible to delete one of those pools of numbers?
Could anyone describe the procedure?

Thanks in advance
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180628/b67bfcfd/attachment.html>

From Alan.Kwon at interoptechnologies.com  Thu Jun 28 18:09:52 2018
From: Alan.Kwon at interoptechnologies.com (Kwon, Alan)
Date: Thu, 28 Jun 2018 18:09:52 -0400
Subject: [Project Clearwater] iFC match on SessionDescription
Message-ID: <D75AC6DF.56E93%Alan.Kwon@interoptechnologies.com>

Hi,

I have a SIP INVITE with ?multipart/mixed? Content-Type that?s failing to match iFC for SessionDescription. Below is the log from Sprout.

The fourth SPT in group 0 for SessionDescription?s m line = ?message? should evaluate to match, but it?s evaluating to not match. I looked at ifc.cpp file based on the log and it looks like the codeline will only evaluate this if the Content-Type is ?application/sdp?:


    if (msg->body &&

        (!pj_stricmp2(&msg->body->content_type.type, "application")) &&

        (!pj_stricmp2(&msg->body->content_type.subtype, "sdp")))


Shouldn't it also consider multipart body and look for the sdp to run the SPT evaluation?


Thank you,

Alan Kwon

------------------------------------------------------------------------------------------------------------------------------------------------

28-06-2018 21:34:33.338 UTC [7f670ae0d700] Verbose sproutletproxy.cpp:2487: Routing Request msg INVITE/cseq=864586 (tdta0x7f66d42a1f80) (2504 bytes) to downstream sproutlet scscf-proxy:
--start msg--

INVITE sip:+18152570701 at demo.iot1.com SIP/2.0
Route: <sip:sprout.demo.iot1.com;lr;service=scscf-proxy>
Record-Route: <sip:scscf.sprout.demo.iot1.com:5054;lr;billing-role=charge-orig>
Via: SIP/2.0/TCP scscf.sprout.demo.iot1.com;branch=z9hG4bKPjsQV9J8nikwOXxD.rNAbtVLrTLc7i61DY
Via: SIP/2.0/UDP xxx.xx.0.140:5060;received=xxx.xx.0.140;branch=z9hG4bK0cB0af77d08536ec87f
From: <tel:+18152579481>;tag=gK0c0b571d
To: <sip:+18152570701 at demo.iot1.com;user=phone>
Call-ID: 786629_134204932 at 172.27.0.140
CSeq: 864586 INVITE
Max-Forwards: 68
Allow: INVITE, ACK, CANCEL, BYE, REGISTER, REFER, INFO, SUBSCRIBE, NOTIFY, UPDATE, OPTIONS, MESSAGE, PUBLISH
P-Asserted-Identity: <sip:+18152579481 at demo.iot1.com>
Contact: <sip:+18152579481 at demo.iot1.com:5762;transport=tcp;gr=urn:gsma:imei:35197909-002422->;+sip.instance="<urn:gsma:imei:35197909-002422->";+g.oma.sip-im
Record-Route: <sip:xxx.xx.0.140:5060;transport=udp;lr>
P-Preferred-Identity: <tel:+18152579481>
Allow: INVITE, ACK, BYE, CANCEL, NOTIFY, OPTIONS, MESSAGE, UPDATE
Contribution-ID: 6ec6a25a6e2a494ca799b33bd4e39999
Accept-Contact: *;+g.oma.sip-im
Subject: Pump
Supported: timer, replaces
Session-Expires: 3600
Min-SE: 90
MIME-Version: 1.0
P-Asserted-Identity: <tel:+18152579481>
Content-Type: multipart/mixed;boundary="sonus-content-delim"
Content-Length:   972


--sonus-content-delim
Content-Disposition: session; handling=required
Content-Length: 422
Content-Type: application/sdp

v=0
o=Sonus_UAC 582334 96029 IN IP4 172.27.0.140
s=SIP Media Capabilities
c=IN IP4 172.28.0.140
t=0 0
m=message 16559 TCP/MSRP *
a=setup:actpass
a=path:msrp://172.25.50.95:9/15b04feec8aa494f95cb6bcee06e2d55;tcp
a=accept-types:message/cpim application/im-iscomposing+xml
a=accept-wrapped-types:text/plain application/vnd.gsma.rcs-ft-http+xml message/imdn+xml application/vnd.gsma.rcspushlocation+xml
a=msrp-cema

--sonus-content-delim
Content-Length: 319
Content-Type: message/cpim

NS: imdn <urn:ietf:params:imdn>
imdn.Disposition-Notification: positive-delivery, display
imdn.Message-ID: MsbsAhPj5lTXmoMSRd81dQqg
To: <sip:anonymous at anonymous.invalid>
From: <sip:anonymous at anonymous.invalid>
DateTime: 2018-06-28T21:34:32.767Z

Content-Length: 4
Content-Type: text/plain; charset=utf-8

Pump
--sonus-content-delim--

--end msg--
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('icscf'->'scscf'/'scscf-proxy')
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('icscf'->'scscf'/'scscf-proxy')
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug sproutletproxy.cpp:2517: Internal network function boundary: yes
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug pjutils.cpp:736: Cloned tdta0x7f66d42a1f80 to tdta0x7f66d4365620
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug sproutletproxy.cpp:1450: Remove top Route header Route: <sip:sprout.demo.iot1.com;lr;service=scscf-proxy>
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug sproutletproxy.cpp:2115: Adding message 0x7f66d4365c30 => txdata 0x7f66d43656c8 mapping
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Verbose sproutletproxy.cpp:1946: scscf-proxy-0x7f66d4255170 pass initial request Request msg INVITE/cseq=864586 (tdta0x7f66d4365620) to Sproutlet
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Info scscfsproutlet.cpp:471: S-CSCF received initial request
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug uri_classifier.cpp:172: Classified URI as 3
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug scscfsproutlet.cpp:945: Route header references this system
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug scscfsproutlet.cpp:991: No ODI token, or invalid ODI token, on request - logging ICID marker 739955c0-5d49-1036-00-00-00-50-56-bb-09-00 for B2BUA AS correlation
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug scscfsproutlet.cpp:1004: Got our Route header, session case term, OD=None
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug uri_classifier.cpp:139: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug uri_classifier.cpp:172: Classified URI as 4
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug acr.cpp:1797: Create RalfACR for node type S-CSCF with role Terminating
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug acr.cpp:24: Created ACR (0x7f66d4211ef0)
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug acr.cpp:170: Created S-CSCF Ralf ACR
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug sproutletproxy.cpp:342: Possible service name scscf will be used if sprout.demo.iot1.com is a local hostname
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug sproutletproxy.cpp:302: Found services param - scscf-proxy
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug scscfsproutlet.cpp:1242: Looking up iFCs for sip:+18152570701 at demo.iot1.com for new AS chain
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug hssconnection.cpp:448: Making Homestead request for /impu/sip%3A%2B18152570701%40demo.iot1.com/reg-data
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug a_record_resolver.cpp:57: ARecordResolver::resolve_iter for host dime.demo.iot1.com, port 8888, family 2
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug utils.cpp:446: Attempt to parse dime.demo.iot1.com as IP address
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Verbose dnscachedresolver.cpp:468: Check cache for dime.demo.iot1.com type 1
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug dnscachedresolver.cpp:578: Pulling 2 records from cache for dime.demo.iot1.com A
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug baseresolver.cpp:192: Found 2 A/AAAA records, creating iterator
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug utils.cpp:446: Attempt to parse dime.demo.iot1.com as IP address
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug baseresolver.cpp:812: Attempting to get 1 targets for host:dime.demo.iot1.com. allowed_host_state = 3
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug baseresolver.cpp:587: 172.27.0.52:8888;transport=TCP has state: WHITE
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug baseresolver.cpp:587: 172.27.0.51:8888;transport=TCP has state: WHITE
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug baseresolver.cpp:587: 172.27.0.52:8888;transport=TCP has state: WHITE
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug baseresolver.cpp:883: Added a whitelisted server to targets, now have 1 of 1
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug connection_pool.h:207: Request for connection to IP: 172.27.0.52, port: 8888
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug connection_pool.h:220: Found existing connection 0x7f6814028970 in pool
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug httpclient.cpp:557: Set CURLOPT_RESOLVE: dime.demo.iot1.com:8888:172.27.0.52
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug httpclient.cpp:588: Sending HTTP request : http://dime.demo.iot1.com:8888/impu/sip%3A%2B18152570701%40demo.iot1.com/reg-data (trying 172.27.0.52)
28-06-2018 21:34:33.338 UTC [7f670ae0d700] Debug thread_dispatcher.cpp:117: Pausing stopwatch due to HTTP request to http://dime.demo.iot1.com:8888/impu/sip%3A%2B18152570701%40demo.iot1.com/reg-data
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug httpclient.cpp:956: Received header http/1.1200ok with value
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug httpclient.cpp:957: Header pointer: 0x7f670ae0c3d0
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug httpclient.cpp:956: Received header content-length with value 3387
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug httpclient.cpp:957: Header pointer: 0x7f670ae0c3d0
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug httpclient.cpp:956: Received header content-type with value text/plain
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug httpclient.cpp:957: Header pointer: 0x7f670ae0c3d0
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug httpclient.cpp:956: Received header  with value
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug httpclient.cpp:957: Header pointer: 0x7f670ae0c3d0
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug thread_dispatcher.cpp:123: Resuming stopwatch after HTTP request to http://dime.demo.iot1.com:8888/impu/sip%3A%2B18152570701%40demo.iot1.com/reg-data
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug httpclient.cpp:628: Received HTTP response: status=200, doc=<ClearwaterRegData>
        <RegistrationState>REGISTERED</RegistrationState>
        <PreviousRegistrationState>REGISTERED</PreviousRegistrationState>
        <IMSSubscription xsi="http://www.w3.org/2001/XMLSchema-instance" noNamespaceSchemaLocation="CxDataType.xsd">
                <PrivateID>+18152570701 at demo.iot1.com</PrivateID>
                <ServiceProfile>
                        <PublicIdentity>
                                <Identity>sip:+18152570701 at demo.iot1.com</Identity>
                        </PublicIdentity>
                        <PublicIdentity>
                                <Identity>tel:+18152570701</Identity>
                        </PublicIdentity>
                        <InitialFilterCriteria>
                                <Priority>0</Priority>
                                <TriggerPoint>
                                        <ConditionTypeCNF>0</ConditionTypeCNF>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNegated>
                                                <Group>0</Group>
                                                <Method>INVITE</Method>
                                        </SPT>
                                        <SPT>
                                                <ConditionNegated>1</ConditionNegated>
                                                <Group>0</Group>
                                                <SIPHeader>
                                                        <Header>User-Agent</Header>
                                                        <Content>IVC</Content>
                                                </SIPHeader>
                                        </SPT>
                                        <SPT>
                                                <ConditionNegated>1</ConditionNegated>
                                                <Group>0</Group>
                                                <SIPHeader>
                                                        <Header>P-Preferred-Service</Header>
                                                        <Content>(callcomposer|callunanswered|sharedmap|sharedsketch)</Content>
                                                </SIPHeader>
                                        </SPT>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNegated>
                                                <Group>0</Group>
                                                <SessionDescription>
                                                        <Line>m</Line>
                                                        <Content>message</Content>
                                                </SessionDescription>
                                        </SPT>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNegated>
                                                <Group>1</Group>
                                                <Method>MESSAGE</Method>
                                        </SPT>
                                        <SPT>
                                                <ConditionNegated>1</ConditionNegated>
                                                <Group>1</Group>
                                                <SIPHeader>
                                                        <Header>User-Agent</Header>
                                                        <Content>IVC</Content>
                                                </SIPHeader>
                                        </SPT>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNegated>
                                                <Group>2</Group>
                                                <Method>SUBSCRIBE</Method>
                                        </SPT>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNegated>
                                                <Group>2</Group>
                                                <SIPHeader>
                                                        <Header>Event</Header>
                                                        <Content>conference</Content>
                                                </SIPHeader>
                                        </SPT>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNegated>
                                                <Group>3</Group>
                                                <Method>REFER</Method>
                                        </SPT>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNegated>
                                                <Group>3</Group>
                                                <SIPHeader>
                                                        <Header>Accept-Contact</Header>
                                                        <Content>(g.oma.sip-im|service.ims.icsi.oma.cpm.session)</Content>
                                                </SIPHeader>
                                        </SPT>
                                </TriggerPoint>
                                <ApplicationServer>
                                        <ServerName>sip:sc01.demo.iot1.com:5510;transport=TCP</ServerName>
                                        <DefaultHandling>1</DefaultHandling>
                                </ApplicationServer>
                        </InitialFilterCriteria>
                        <InitialFilterCriteria>
                                <Priority>2</Priority>
                                <TriggerPoint>
                                        <ConditionTypeCNF>0</ConditionTypeCNF>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNegated>
                                                <Group>0</Group>
                                                <Method>REGISTER</Method>
                                                <Extension>
                                                        <RegistrationType>0</RegistrationType>
                                                </Extension>
                                        </SPT>
                                        <SPT>
                                                <ConditionNegated>0</ConditionNegated>
                                                <Group>1</Group>
                                                <Method>REGISTER</Method>
                                                <Extension>
                                                        <RegistrationType>1</RegistrationType>
                                                </Extension>
                                        </SPT>
                                </TriggerPoint>
                                <ApplicationServer>
                                        <ServerName>sip:sc01.demo.iot1.com:5510;transport=TCP</ServerName>
                                        <DefaultHandling>1</DefaultHandling>
                                </ApplicationServer>
                        </InitialFilterCriteria>
                </ServiceProfile>
        </IMSSubscription>
</ClearwaterRegData>


28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug baseresolver.cpp:672: Successful response from  172.27.0.52:8888;transport=TCP
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug connection_pool.h:244: Release connection to IP: 172.27.0.52, port: 8888 to pool
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug event_statistic_accumulator.cpp:32: Accumulate 1761 for 0x2d4e720
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug event_statistic_accumulator.cpp:32: Accumulate 1761 for 0x2d4e798
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug event_statistic_accumulator.cpp:32: Accumulate 1761 for 0x2d53300
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug event_statistic_accumulator.cpp:32: Accumulate 1761 for 0x2d533a8
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug sprout_xml_utils.cpp:70: Processing Identity node from HSS XML - sip:+18152570701 at demo.iot1.com
28-06-2018 21:34:33.339 UTC [7f670ae0d700] Debug sprout_xml_utils.cpp:70: Processing Identity node from HSS XML - tel:+18152570701
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug scscfsproutlet.cpp:1248: Successfully looked up iFCs
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug aschain.cpp:55: Creating AsChain 0x7f66d4179500 with 2 iFCs and adding to map
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug aschain.cpp:57: Attached ACR (0x7f66d4211ef0) to chain
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug scscfsproutlet.cpp:1376: S-CSCF sproutlet transaction 0x7f66d4255170 linked to AsChain AsChain-term[0x7f66d4179500]:1/2
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug acr.cpp:210: Set record type for P/S-CSCF
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug acr.cpp:237: Dialog-initiating INVITE => START_RECORD
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug acr.cpp:1596: Found P-Charging-Vector header, store information
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Info scscfsproutlet.cpp:635: Found served user, so apply services
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:428: SPT class Method: result true
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:428: SPT class SIPHeader: result false
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:428: SPT class SIPHeader: result false
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:428: SPT class SessionDescription: result false
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:428: SPT class Method: result false
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:428: SPT class SIPHeader: result false
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:428: SPT class Method: result false
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:428: SPT class SIPHeader: result false
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:428: SPT class Method: result false
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:428: SPT class SIPHeader: result true
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:582: iFC does not match
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:589: AND each SPT match result to determine group result.
OR each group result to determine overall iFC match.

SPT in group 0 is matched.
SPT in group 0 is matched.
SPT in group 0 is matched.
SPT in group 0 is not matched.
SPT in group 1 is not matched.
SPT in group 1 is matched.
SPT in group 2 is not matched.
SPT in group 2 is not matched.
SPT in group 3 is not matched.
SPT in group 3 is matched.
Group 0 is not matched.
Group 1 is not matched.
Group 2 is not matched.
Group 3 is not matched.

28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:428: SPT class Method: result false
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:428: SPT class Method: result false
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:582: iFC does not match
28-06-2018 21:34:33.340 UTC [7f670ae0d700] Debug ifc.cpp:589: AND each SPT match result to determine group result.
OR each group result to determine overall iFC match.

SPT in group 0 is not matched.
SPT in group 1 is not matched.
Group 0 is not matched.
Group 1 is not matched.

28-06-2018 21:34:33.340 UTC [7f670ae0d700] Info scscfsproutlet.cpp:1529: Completed applying terminating services



[cid:7E8907A8-95D3-4E42-84B2-4DE94ABAB638]<http://www.interoptechnologies.com/>




ALAN KWON
Senior Software Engineer




T: +1 972-753-1865 (Texas)
F: +1 239-425-6845

Confidentiality Notice: The information in this e-mail and in any attachment may contain information which is legally privileged. It is intended only for the attention and use of the named recipient. If you are not the intended recipient, you are not authorized to retain, disclose, copy or distribute the message and/or any of its attachments. If you received this e-mail in error, please notify me and delete this message.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180628/fd547e43/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0E8CB2C0-CC34-4B10-814C-7B349D41FD79.png
Type: image/png
Size: 3922 bytes
Desc: 0E8CB2C0-CC34-4B10-814C-7B349D41FD79.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180628/fd547e43/attachment.png>

