From Sebastian.Rex at metaswitch.com  Wed Jul  5 08:25:50 2017
From: Sebastian.Rex at metaswitch.com (Sebastian Rex)
Date: Wed, 5 Jul 2017 12:25:50 +0000
Subject: [Project Clearwater] =?iso-8859-1?q?Release_note_for_sprint_Il=FA?=
 =?iso-8859-1?q?vatar?=
Message-ID: <SN1PR02MB1664629A7CDF69FD6E038F7F8FD40@SN1PR02MB1664.namprd02.prod.outlook.com>

The release for Project Clearwater sprint "Il?vatar" has been cut. The code for this release is tagged as release-126 in GitHub.

This release includes the following bug fixes:


?         Excess line breaks between Sprout logs

?         Dummy iFCs work on INVITEs but not on REGISTERs

?         Monit RAM thresholds for Ralf and Homestead are much too high

?         Sprout's "Connection refused" errors don't state the connection endpoint

?         Memory leak in Sprout when an AS is configured but has no DNS record

?         Sprout ran out of memory

?         Using Default iFCs causes an AS to be invoked one extra time

?         Chronos doesn't have a way to easily share shared configuration

?         Chronos doesn't listen on IPv6 by default

?         Chronos docs are misleading about the contents of a duplicated timer

To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.

Seb.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170705/bdffd746/attachment.html>

From hkaranjikar at apm.com  Thu Jul  6 05:22:22 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Thu, 6 Jul 2017 14:52:22 +0530
Subject: [Project Clearwater] restund_process Execution failed on bono
Message-ID: <CABmBNEafr-WsM7_E3i7cbhJujejjqfgs9aFp2dTRdr=R7Lsmww@mail.gmail.com>

Hello,

I have Manually installed all 6 nodes on VMs using virtualbox.
I followed the procedure given @
http://clearwater.readthedocs.io/en/stable/Manual_Install.html
Looks like all nodes except Dime are running fine.
I am getting error "restund_process Execution failed" in monit summary on
Bono node.

Here is the shared and local config file,

#####################################################################

[bono]cwbono at cwbono:~$ cat /etc/clearwater/shared_config

home_domain=example.com
sprout_hostname=cwsprout
sprout_registration_store=192.168.56.107 #vellum
hs_hostname=192.168.56.106:8888 #dime
hs_provisioning_hostname=192.168.56.106:8889 #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.105:7888 #homer
chronos_hostname=192.168.56.107 #vellum
cassandra_hostname=192.168.56.107 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret
[bono]cwbono at cwbono:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.104
public_ip=192.168.56.104
public_hostname=cwbono
etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,192.168.56.105,192.168.56.106,192.168.56.107"

#####################################################################

The logs for bono node are as follows,

#####################################################################
[bono]cwbono at cwbono:~$ sudo monit summary
[sudo] password for cwbono:
Monit 5.18.1 uptime: 2d 16h 39m
 Service Name                     Status                      Type
 node-cwbono                      Running                     System
 restund_process                  Execution failed | Does...  Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Wait parent                 Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program


05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.843 UTC Status main.cpp:1358: Quiesce signal received
05-07-2017 11:30:15.843 UTC Status stack.cpp:125: Setting quiescing =
PJ_TRUE
05-07-2017 11:30:15.851 UTC Status stack.cpp:156: Quiescing state changed
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing
Manager received input QUIESCE (0) when in state ACTIVE (0)
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:265: Close
untrusted listening port
05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport
for port 5060
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:273: Quiesce
FlowTable
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing
Manager received input FLOWS_GONE (1) when in state QUIESCING_FLOWS (1)
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:290: Closing
trusted port
05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport
for port 5058
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:296: Quiescing all
connections
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:162: Start
quiescing connections
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:175: Quiescing 0
transactions
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:180: Connection
quiescing complete
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing
Manager received input CONNS_GONE (2) when in state QUIESCING_CONNS (2)
05-07-2017 11:30:15.851 UTC Status main.cpp:1380: Quiesce complete
05-07-2017 11:30:15.853 UTC Status stack.cpp:171: PJSIP thread ended
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

[bono]cwbono at cwbono:~$ cat /var/log/monit.log

httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:47:46] error    : 'restund_process' process is not running
[IST Jul  6 14:47:46] info     : 'restund_process' trying to restart
[IST Jul  6 14:47:46] info     : 'restund_process' restart:
/etc/init.d/restund
[IST Jul  6 14:48:16] error    : 'restund_process' failed to restart (exit
status 0) -- /etc/init.d/restund: httpdb: configured url
http://hs.example.com:8888/impi/%s/digest
httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:48:26] error    : 'restund_process' process is not running
[IST Jul  6 14:48:26] info     : 'restund_process' trying to restart
[IST Jul  6 14:48:26] info     : 'restund_process' restart:
/etc/init.d/restund
[IST Jul  6 14:48:56] error    : 'restund_process' failed to restart (exit
status 0) -- /etc/init.d/restund: httpdb: configured url
http://hs.example.com:8888/impi/%s/digest
httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:49:06] error    : 'restund_process' process is not running
[IST Jul  6 14:49:06] info     : 'restund_process' trying to restart
[IST Jul  6 14:49:06] info     : 'restund_process' restart:
/etc/init.d/restund


[bono]cwbono at cwbono:~$ clearwater-etcdctl cluster-health
member 9c1928228d308a0f is healthy: got healthy result from
http://192.168.56.107:4000
member b0c9c017e0d47e14 is healthy: got healthy result from
http://192.168.56.106:4000
member d44832212a08c43f is healthy: got healthy result from
http://192.168.56.103:4000
member ef1a9a8a2fd05283 is healthy: got healthy result from
http://192.168.56.104:4000
member f63afbe816fb463d is healthy: got healthy result from
http://192.168.56.102:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[bono]cwbono at cwbono:~$ cw-check_cluster_state
This script prints out the status of the Chronos, Memcached and Cassandra
clusters.

Describing the Vellum Chronos cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

Describing the Vellum Memcached cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

Describing the Vellum Cassandra cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

[bono]cwbono at cwbono:~$ clearwater-etcdctl member list
9c1928228d308a0f: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
b0c9c017e0d47e14: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=true
d44832212a08c43f: name=192-168-56-103 peerURLs=http://192.168.56.103:2380
clientURLs=http://192.168.56.103:4000 isLeader=false
ef1a9a8a2fd05283: name=192-168-56-104 peerURLs=http://192.168.56.104:2380
clientURLs=http://192.168.56.104:4000 isLeader=false
f63afbe816fb463d: name=192-168-56-102 peerURLs=http://192.168.56.102:2380
clientURLs=http://192.168.56.102:4000 isLeader=false
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono at cwbono:~$ sudo cw-check_config_sync
[sudo] password for cwbono:
 - /etc/clearwater/dns.json is up to date
 - /etc/clearwater/shared_config is up to date


#####################################################################

The logs for other nodes are as follows

#####################################################################

[ellis]cwellis at cwellis:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 21h 15m
 Service Name                     Status                      Type
 node-cwellis                     Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[sprout]cwsprout at cwsprout:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 7m
 Service Name                     Status                      Type
 node-cwsprout                    Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[homer]cwhomer at cwhomer:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 2m
 Service Name                     Status                      Type
 node-cwhomer                     Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program

[dime]cwdime at cwdime:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 2m
 Service Name                     Status                      Type
 node-cwdime                      Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[vellum]cwvellum at cwvellum:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 3m
 Service Name                     Status                      Type
 node-cwvellum                    Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program


#####################################################################

Please let me know if I am missing any configuration.

Thanks
Hrishikesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170706/29fda463/attachment.html>

From Martin.Horne at arm.com  Fri Jul  7 11:12:15 2017
From: Martin.Horne at arm.com (Martin Horne)
Date: Fri, 7 Jul 2017 15:12:15 +0000
Subject: [Project Clearwater] aarch64 build
Message-ID: <AM5PR0801MB16330CC6DF17A8D2A1BD606896AA0@AM5PR0801MB1633.eurprd08.prod.outlook.com>

I want to build the Clearwater IMS using the Clearwater-docker project.
Can anybody give me a list of the required Metaswitch projects and branch/tags to use.
Thanks

         Martin Horne
[armlogo]
     System Architect
   Developer Advocate
Mobile: +1 (925) 519-3056

IMPORTANT NOTICE: The contents of this email and any attachments are confidential and may also be privileged. If you are not the intended recipient, please notify the sender immediately and do not disclose the contents to any other person, use it for any purpose, or store or copy the information in any medium. Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170707/4c4a0795/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 4901 bytes
Desc: image002.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170707/4c4a0795/attachment.png>

From Martin.Horne at arm.com  Fri Jul  7 11:49:30 2017
From: Martin.Horne at arm.com (Martin Horne)
Date: Fri, 7 Jul 2017 15:49:30 +0000
Subject: [Project Clearwater] aarch64 build
Message-ID: <AM5PR0801MB1633EDDC45BF64792B09483196AA0@AM5PR0801MB1633.eurprd08.prod.outlook.com>

Trying to build clearwater-docker base image, requires clearwater-manager deb package, which Metaswitch project builds this?

         Martin Horne
IMPORTANT NOTICE: The contents of this email and any attachments are confidential and may also be privileged. If you are not the intended recipient, please notify the sender immediately and do not disclose the contents to any other person, use it for any purpose, or store or copy the information in any medium. Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170707/53038af1/attachment.html>

From bharath_ves at hotmail.com  Sun Jul  9 01:10:29 2017
From: bharath_ves at hotmail.com (bharath thiruveedula)
Date: Sun, 9 Jul 2017 05:10:29 +0000
Subject: [Project Clearwater] Autorestart of Ellis
Message-ID: <MAXPR01MB00898EFA927CEA40A37AF39794A80@MAXPR01MB0089.INDPRD01.PROD.OUTLOOK.COM>

Hi,


I am trying to install clearwater using manual installation procedure(http://clearwater.readthedocs.io/en/stable/Manual_Install.html) . But when I try to create a number from ellis, it is automatically restarting and giving "failed to restart server" error.


Here is the logs from ellis

08-07-2017 20:47:21.822 UTC INFO main.py:88: Ellis process starting up
08-07-2017 20:47:22.041 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.45ms
08-07-2017 20:47:32.131 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.26ms
08-07-2017 20:47:32.143 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.23ms
08-07-2017 20:47:35.559 UTC DEBUG numbers.py:105: Number allocation API call (PSTN = false)
08-07-2017 20:47:35.568 UTC DEBUG numbers.py:100: non-PSTN
08-07-2017 20:47:35.572 UTC DEBUG numbers.py:122: Fetched be20628b-e9f9-4bdf-8e95-cba6d0be7921
08-07-2017 20:47:35.573 UTC DEBUG numbers.py:114: SIP URI sip:6505550494 at example.com
08-07-2017 20:47:35.632 UTC DEBUG numbers.py:144: Populating other servers...
08-07-2017 20:47:35.633 UTC DEBUG numbers.py:155: About to create private ID at Homestead
08-07-2017 20:47:35.634 UTC INFO homestead.py:272: Sending HTTP PUT request to http://hs.example.com:8889/private/6505550494%40example.com
08-07-2017 20:47:55.646 UTC DEBUG numbers.py:160: Created private ID at Homestead
08-07-2017 20:47:55.648 UTC INFO homestead.py:272: Sending HTTP GET request to http://hs.example.com:8889/private/6505550494%40example.com/associated_implicit_registration_sets
08-07-2017 20:48:12.690 UTC INFO main.py:88: Ellis process starting up
08-07-2017 20:48:12.904 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.41ms
08-07-2017 20:48:22.949 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.16ms
08-07-2017 20:48:22.951 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.23ms


And syslog from homestead and homer
Jul  9 10:37:47 ubuntu issue-alarm: zmq_msg_recv: Invalid argument
Jul  9 10:37:47 ubuntu issue-alarm: zmq_msg_recv: Invalid argument
Jul  9 10:37:47 ubuntu issue-alarm: zmq_msg_recv: Invalid argument
Jul  9 10:37:57 ubuntu issue-alarm: zmq_msg_recv: Invalid argument
Jul  9 10:37:57 ubuntu issue-alarm: zmq_msg_recv: Invalid argument
Jul  9 10:37:57 ubuntu cluster-manager[1595]: dropped request: 'issue-alarm cluster-manager 8003.1'
Jul  9 10:37:59 ubuntu queue-manager[10303]: dropped request: 'issue-alarm queue-manager 9001.1'
Jul  9 10:38:00 ubuntu cluster-manager[1595]: dropped request: 'issue-alarm cluster-manager 8000.1'
Jul  9 10:38:01 ubuntu queue-manager[10303]: dropped request: 'issue-alarm queue-manager 9002.1'
Jul  9 10:38:01 ubuntu CRON[26641]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Jul  9 10:38:07 ubuntu issue-alarm: zmq_msg_recv: Invalid argument
Jul  9 10:38:07 ubuntu issue-alarm: zmq_msg_recv: Invalid argument
Jul  9 10:38:07 ubuntu issue-alarm: zmq_msg_recv: Invalid argument




I can see all other services in other nodes are running.

I have one question, I have tried allinone build instructions as mentioned in http://clearwater.readthedocs.io/en/stable/All_in_one_Images.html#manual-build. It worked for me. The packages we used in aio setup are like homer-node, homestead-node and so on. But in manual build instructions, we are using homer, homestead etc., Does this make difference, which one to use? Do we need to update the manual build instructions?

And aio setup we are not using vellum, can we safely skip vellum in manual multinode setup too? If yes, how we can achieve that?


Any help/suggestion would be much appreciated.



Regards

Bharath T





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170709/12e2f260/attachment.html>

From Andrew.Edmonds at metaswitch.com  Tue Jul 11 04:58:11 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Tue, 11 Jul 2017 08:58:11 +0000
Subject: [Project Clearwater] aarch64 build
In-Reply-To: <AM5PR0801MB16330CC6DF17A8D2A1BD606896AA0@AM5PR0801MB1633.eurprd08.prod.outlook.com>
References: <AM5PR0801MB16330CC6DF17A8D2A1BD606896AA0@AM5PR0801MB1633.eurprd08.prod.outlook.com>
Message-ID: <BLUPR02MB437FADF67D24AAF9CF4574EE5AE0@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Martin,

Thank you for your interest in Clearwater.

Our Clearwater-docker repository (which you can access through GitHub here<https://github.com/Metaswitch/clearwater-docker>) should have everything you need to deploy Project Clearwater using Docker. It contains instructions as well as the Dockerfiles you will need to use.

Please let me know if you have any further questions about using this.

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Martin Horne
Sent: Friday, July 7, 2017 4:12 PM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] aarch64 build

I want to build the Clearwater IMS using the Clearwater-docker project.
Can anybody give me a list of the required Metaswitch projects and branch/tags to use.
Thanks

         Martin Horne
[armlogo]
     System Architect
   Developer Advocate
Mobile: +1 (925) 519-3056

IMPORTANT NOTICE: The contents of this email and any attachments are confidential and may also be privileged. If you are not the intended recipient, please notify the sender immediately and do not disclose the contents to any other person, use it for any purpose, or store or copy the information in any medium. Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170711/675cce03/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 4901 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170711/675cce03/attachment.png>

From Andrew.Edmonds at metaswitch.com  Tue Jul 11 05:12:35 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Tue, 11 Jul 2017 09:12:35 +0000
Subject: [Project Clearwater] restund_process Execution failed on bono
In-Reply-To: <CABmBNEafr-WsM7_E3i7cbhJujejjqfgs9aFp2dTRdr=R7Lsmww@mail.gmail.com>
References: <CABmBNEafr-WsM7_E3i7cbhJujejjqfgs9aFp2dTRdr=R7Lsmww@mail.gmail.com>
Message-ID: <BLUPR02MB4375A20455D0317FDCA0A97E5AE0@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Hrishikesh,

Thank you for your question and the detailed logs you have provided.

The issues appears to be caused by your shared config, in the manual installation instructions<http://clearwater.readthedocs.io/en/stable/Manual_Install.html> you?ll see that we advise that the entries in shared config have a format like:

sprout_hostname=sprout.<site_name>.<zone>
sprout_registration_store=vellum.<site_name>.<zone>
hs_hostname=hs.<site_name>.<zone>:8888

In your shared config you have:

sprout_hostname=cwsprout
sprout_registration_store=192.168.56.107 #vellum

I don?t think the sprout_hostname used here will resolve, we can see evidence for this in the Bono logs, Bono attempts to resolve icscf.<sprout_hostname> to find which location to forward SIP messages on you, you can see it is failing to do that here:

Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

To resolve this issue you should change your shared config to use hostnames which have been configured in your DNS server to resolve to the appropriate location. Once you have done this run the command ?sudo cw-upload_shared_config?. Please let me know if this does not resolve the issue.

Thanks,

Andrew


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, July 6, 2017 10:22 AM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] restund_process Execution failed on bono

Hello,
I have Manually installed all 6 nodes on VMs using virtualbox.
I followed the procedure given @ http://clearwater.readthedocs.io/en/stable/Manual_Install.html
Looks like all nodes except Dime are running fine.
I am getting error "restund_process Execution failed" in monit summary on Bono node.
Here is the shared and local config file,

#####################################################################

[bono]cwbono at cwbono:~$ cat /etc/clearwater/shared_config

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout
sprout_registration_store=192.168.56.107 #vellum
hs_hostname=192.168.56.106:8888<http://192.168.56.106:8888> #dime
hs_provisioning_hostname=192.168.56.106:8889<http://192.168.56.106:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.105:7888<http://192.168.56.105:7888> #homer
chronos_hostname=192.168.56.107 #vellum
cassandra_hostname=192.168.56.107 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret
[bono]cwbono at cwbono:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.104
public_ip=192.168.56.104
public_hostname=cwbono
etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,192.168.56.105,192.168.56.106,192.168.56.107"

#####################################################################
The logs for bono node are as follows,

#####################################################################
[bono]cwbono at cwbono:~$ sudo monit summary
[sudo] password for cwbono:
Monit 5.18.1 uptime: 2d 16h 39m
 Service Name                     Status                      Type
 node-cwbono                      Running                     System
 restund_process                  Execution failed | Does...  Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Wait parent                 Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program


05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.843 UTC Status main.cpp:1358: Quiesce signal received
05-07-2017 11:30:15.843 UTC Status stack.cpp:125: Setting quiescing = PJ_TRUE
05-07-2017 11:30:15.851 UTC Status stack.cpp:156: Quiescing state changed
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing Manager received input QUIESCE (0) when in state ACTIVE (0)
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:265: Close untrusted listening port
05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport for port 5060
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:273: Quiesce FlowTable
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing Manager received input FLOWS_GONE (1) when in state QUIESCING_FLOWS (1)
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:290: Closing trusted port
05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport for port 5058
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:296: Quiescing all connections
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:162: Start quiescing connections
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:175: Quiescing 0 transactions
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:180: Connection quiescing complete
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing Manager received input CONNS_GONE (2) when in state QUIESCING_CONNS (2)
05-07-2017 11:30:15.851 UTC Status main.cpp:1380: Quiesce complete
05-07-2017 11:30:15.853 UTC Status stack.cpp:171: PJSIP thread ended
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

[bono]cwbono at cwbono:~$ cat /var/log/monit.log

httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0<http://192.168.56.104:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:47:46] error    : 'restund_process' process is not running
[IST Jul  6 14:47:46] info     : 'restund_process' trying to restart
[IST Jul  6 14:47:46] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul  6 14:48:16] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0<http://192.168.56.104:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:48:26] error    : 'restund_process' process is not running
[IST Jul  6 14:48:26] info     : 'restund_process' trying to restart
[IST Jul  6 14:48:26] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul  6 14:48:56] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0<http://192.168.56.104:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:49:06] error    : 'restund_process' process is not running
[IST Jul  6 14:49:06] info     : 'restund_process' trying to restart
[IST Jul  6 14:49:06] info     : 'restund_process' restart: /etc/init.d/restund


[bono]cwbono at cwbono:~$ clearwater-etcdctl cluster-health
member 9c1928228d308a0f is healthy: got healthy result from http://192.168.56.107:4000
member b0c9c017e0d47e14 is healthy: got healthy result from http://192.168.56.106:4000
member d44832212a08c43f is healthy: got healthy result from http://192.168.56.103:4000
member ef1a9a8a2fd05283 is healthy: got healthy result from http://192.168.56.104:4000
member f63afbe816fb463d is healthy: got healthy result from http://192.168.56.102:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono at cwbono:~$ cw-check_cluster_state
This script prints out the status of the Chronos, Memcached and Cassandra clusters.

Describing the Vellum Chronos cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

Describing the Vellum Memcached cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

Describing the Vellum Cassandra cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

[bono]cwbono at cwbono:~$ clearwater-etcdctl member list
9c1928228d308a0f: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
b0c9c017e0d47e14: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=true
d44832212a08c43f: name=192-168-56-103 peerURLs=http://192.168.56.103:2380 clientURLs=http://192.168.56.103:4000 isLeader=false
ef1a9a8a2fd05283: name=192-168-56-104 peerURLs=http://192.168.56.104:2380 clientURLs=http://192.168.56.104:4000 isLeader=false
f63afbe816fb463d: name=192-168-56-102 peerURLs=http://192.168.56.102:2380 clientURLs=http://192.168.56.102:4000 isLeader=false
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono at cwbono:~$ sudo cw-check_config_sync
[sudo] password for cwbono:
 - /etc/clearwater/dns.json is up to date
 - /etc/clearwater/shared_config is up to date


#####################################################################
The logs for other nodes are as follows

#####################################################################

[ellis]cwellis at cwellis:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 21h 15m
 Service Name                     Status                      Type
 node-cwellis                     Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[sprout]cwsprout at cwsprout:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 7m
 Service Name                     Status                      Type
 node-cwsprout                    Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[homer]cwhomer at cwhomer:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 2m
 Service Name                     Status                      Type
 node-cwhomer                     Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program

[dime]cwdime at cwdime:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 2m
 Service Name                     Status                      Type
 node-cwdime                      Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[vellum]cwvellum at cwvellum:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 3m
 Service Name                     Status                      Type
 node-cwvellum                    Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program


#####################################################################
Please let me know if I am missing any configuration.
Thanks
Hrishikesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170711/2f66dee2/attachment.html>

From hkaranjikar at apm.com  Tue Jul 11 07:38:47 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Tue, 11 Jul 2017 17:08:47 +0530
Subject: [Project Clearwater] restund_process Execution failed on bono
In-Reply-To: <BLUPR02MB4375A20455D0317FDCA0A97E5AE0@BLUPR02MB437.namprd02.prod.outlook.com>
References: <CABmBNEafr-WsM7_E3i7cbhJujejjqfgs9aFp2dTRdr=R7Lsmww@mail.gmail.com>
	<BLUPR02MB4375A20455D0317FDCA0A97E5AE0@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <CABmBNEY-Saqs0REksQqk9=vfE=sOb8GtwV31JQKftBYn_y1NXQ@mail.gmail.com>

Hi,

Thanks a lot for your reply.

I am using virtualbox and host only network.
The DHCP server runs inside virtualbox and I can only configure the IP
address range.
However I am not using the DHCP server and assigning static IP addresses to
all nodes which are within the DHCP server IP address range.
I added entry of cwsprout in /ets/hosts of bono node as follows,

===============================================

127.0.0.1       localhost
127.0.1.1       cwbono

*192.168.56.103  cwsprout192.168.56.103  icscf.cwsprout*

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
192.168.56.103 cwsprout
192.168.56.103  icscf.cwsprout
::1 localhost # added by clearwater-infrastructure 1hosts script
192.168.56.104 cwbono #+clearwater-infrastructure

===============================================

I am also able to ping cwsprout and icscf.cwsprout from bono,
Here is the log,

===============================================

[bono]cwbono at cwbono:~$ ping cwsprout
PING cwsprout (192.168.56.103) 56(84) bytes of data.
64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.197 ms
64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.217 ms
^C
--- cwsprout ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.197/0.207/0.217/0.010 ms
[bono]cwbono at cwbono:~$
[bono]cwbono at cwbono:~$
[bono]cwbono at cwbono:~$
[bono]cwbono at cwbono:~$ ping icscf.cwsprout
PING icscf.cwsprout (192.168.56.103) 56(84) bytes of data.
64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.112 ms
64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.165 ms

===============================================

I am using static IP addresses as I have to specify them in local_config
file of each node.
If I use DHCP server of VirtualBox they might change. In that case I am not
sure how do I cope up with local_config.
Can I modify local_config after all nodes are up?

Thanks
Hrishikesh



On Tue, Jul 11, 2017 at 2:42 PM, Andrew Edmonds <
Andrew.Edmonds at metaswitch.com> wrote:

> Hi Hrishikesh,
>
>
>
> Thank you for your question and the detailed logs you have provided.
>
>
>
> The issues appears to be caused by your shared config, in the manual
> installation instructions
> <http://clearwater.readthedocs.io/en/stable/Manual_Install.html> you?ll
> see that we advise that the entries in shared config have a format like:
>
>
>
> sprout_hostname*=*sprout*.<*site_name*>.<*zone*>*
>
> sprout_registration_store*=*vellum*.<*site_name*>.<*zone*>*
>
> hs_hostname*=*hs*.<*site_name*>.<*zone*>*:8888
>
>
>
> In your shared config you have:
>
>
>
> sprout_hostname=cwsprout
> sprout_registration_store=192.168.56.107 #vellum
>
> I don?t think the sprout_hostname used here will resolve, we can see
> evidence for this in the Bono logs, Bono attempts to resolve
> icscf.<sprout_hostname> to find which location to forward SIP messages on
> you, you can see it is failing to do that here:
>
>
>
> Failed to resolve icscf.cwsprout to an IP address - Not found
> (PJ_ENOTFOUND)
>
>
>
> To resolve this issue you should change your shared config to use
> hostnames which have been configured in your DNS server to resolve to the
> appropriate location. Once you have done this run the command ?sudo
> cw-upload_shared_config?. Please let me know if this does not resolve the
> issue.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, July 6, 2017 10:22 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] restund_process Execution failed on bono
>
>
>
> Hello,
>
> I have Manually installed all 6 nodes on VMs using virtualbox.
>
> I followed the procedure given @ http://clearwater.readthedocs.
> io/en/stable/Manual_Install.html
>
> Looks like all nodes except Dime are running fine.
>
> I am getting error "restund_process Execution failed" in monit summary on
> Bono node.
>
> Here is the shared and local config file,
>
> #####################################################################
>
> [bono]cwbono at cwbono:~$ cat /etc/clearwater/shared_config
>
> home_domain=example.com
> sprout_hostname=cwsprout
> sprout_registration_store=192.168.56.107 #vellum
> hs_hostname=192.168.56.106:8888 #dime
> hs_provisioning_hostname=192.168.56.106:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.105:7888 #homer
> chronos_hostname=192.168.56.107 #vellum
> cassandra_hostname=192.168.56.107 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
> [bono]cwbono at cwbono:~$ cat /etc/clearwater/local_config
> local_ip=192.168.56.104
> public_ip=192.168.56.104
> public_hostname=cwbono
> etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,
> 192.168.56.105,192.168.56.106,192.168.56.107"
>
> #####################################################################
>
> The logs for bono node are as follows,
>
> #####################################################################
> [bono]cwbono at cwbono:~$ sudo monit summary
> [sudo] password for cwbono:
> Monit 5.18.1 uptime: 2d 16h 39m
>  Service Name                     Status
> Type
>  node-cwbono                      Running
> System
>  restund_process                  Execution failed | Does...
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Wait parent
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
>
>
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.843 UTC Status main.cpp:1358: Quiesce signal received
> 05-07-2017 11:30:15.843 UTC Status stack.cpp:125: Setting quiescing =
> PJ_TRUE
> 05-07-2017 11:30:15.851 UTC Status stack.cpp:156: Quiescing state changed
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The
> Quiescing Manager received input QUIESCE (0) when in state ACTIVE (0)
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:265: Close
> untrusted listening port
> 05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport
> for port 5060
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:273: Quiesce
> FlowTable
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The
> Quiescing Manager received input FLOWS_GONE (1) when in state
> QUIESCING_FLOWS (1)
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:290: Closing
> trusted port
> 05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport
> for port 5058
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:296: Quiescing
> all connections
> 05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:162: Start
> quiescing connections
> 05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:175: Quiescing 0
> transactions
> 05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:180: Connection
> quiescing complete
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The
> Quiescing Manager received input CONNS_GONE (2) when in state
> QUIESCING_CONNS (2)
> 05-07-2017 11:30:15.851 UTC Status main.cpp:1380: Quiesce complete
> 05-07-2017 11:30:15.853 UTC Status stack.cpp:171: PJSIP thread ended
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>
> [bono]cwbono at cwbono:~$ cat /var/log/monit.log
>
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=192.168.56.104:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in u
> [IST Jul  6 14:47:46] error    : 'restund_process' process is not running
> [IST Jul  6 14:47:46] info     : 'restund_process' trying to restart
> [IST Jul  6 14:47:46] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul  6 14:48:16] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=192.168.56.104:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in u
> [IST Jul  6 14:48:26] error    : 'restund_process' process is not running
> [IST Jul  6 14:48:26] info     : 'restund_process' trying to restart
> [IST Jul  6 14:48:26] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul  6 14:48:56] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=192.168.56.104:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in u
> [IST Jul  6 14:49:06] error    : 'restund_process' process is not running
> [IST Jul  6 14:49:06] info     : 'restund_process' trying to restart
> [IST Jul  6 14:49:06] info     : 'restund_process' restart:
> /etc/init.d/restund
>
>
> [bono]cwbono at cwbono:~$ clearwater-etcdctl cluster-health
> member 9c1928228d308a0f is healthy: got healthy result from
> http://192.168.56.107:4000
> member b0c9c017e0d47e14 is healthy: got healthy result from
> http://192.168.56.106:4000
> member d44832212a08c43f is healthy: got healthy result from
> http://192.168.56.103:4000
> member ef1a9a8a2fd05283 is healthy: got healthy result from
> http://192.168.56.104:4000
> member f63afbe816fb463d is healthy: got healthy result from
> http://192.168.56.102:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono at cwbono:~$ cw-check_cluster_state
> This script prints out the status of the Chronos, Memcached and Cassandra
> clusters.
>
> Describing the Vellum Chronos cluster:
>   The local node is *not* in this cluster
>   The cluster is stable
>     192.168.56.107 is in state normal
>
> Describing the Vellum Memcached cluster:
>   The local node is *not* in this cluster
>   The cluster is stable
>     192.168.56.107 is in state normal
>
> Describing the Vellum Cassandra cluster:
>   The local node is *not* in this cluster
>   The cluster is stable
>     192.168.56.107 is in state normal
>
> [bono]cwbono at cwbono:~$ clearwater-etcdctl member list
> 9c1928228d308a0f: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> b0c9c017e0d47e14: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=true
> d44832212a08c43f: name=192-168-56-103 peerURLs=http://192.168.56.103:2380
> clientURLs=http://192.168.56.103:4000 isLeader=false
> ef1a9a8a2fd05283: name=192-168-56-104 peerURLs=http://192.168.56.104:2380
> clientURLs=http://192.168.56.104:4000 isLeader=false
> f63afbe816fb463d: name=192-168-56-102 peerURLs=http://192.168.56.102:2380
> clientURLs=http://192.168.56.102:4000 isLeader=false
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono at cwbono:~$ sudo cw-check_config_sync
> [sudo] password for cwbono:
>  - /etc/clearwater/dns.json is up to date
>  - /etc/clearwater/shared_config is up to date
>
>
> #####################################################################
>
> The logs for other nodes are as follows
>
> #####################################################################
>
> [ellis]cwellis at cwellis:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 21h 15m
>  Service Name                     Status
> Type
>  node-cwellis                     Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>
>
> [sprout]cwsprout at cwsprout:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 20h 7m
>  Service Name                     Status
> Type
>  node-cwsprout                    Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok                   Program
>
>
> [homer]cwhomer at cwhomer:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 20h 2m
>  Service Name                     Status
> Type
>  node-cwhomer                     Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok                   Program
>
> [dime]cwdime at cwdime:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 20h 2m
>  Service Name                     Status
> Type
>  node-cwdime                      Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>
>
> [vellum]cwvellum at cwvellum:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 20h 3m
>  Service Name                     Status
> Type
>  node-cwvellum                    Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
>
>
> #####################################################################
>
> Please let me know if I am missing any configuration.
>
> Thanks
>
> Hrishikesh
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170711/3c4cf245/attachment.html>

From arvindas at hpe.com  Wed Jul 12 10:08:53 2017
From: arvindas at hpe.com (Shirabur, Aravind Ashok (CMS))
Date: Wed, 12 Jul 2017 14:08:53 +0000
Subject: [Project Clearwater] Sprout S-CSCF Forking disable
Message-ID: <TU4PR84MB006416CAD2E0D9AABA0C3E49DCAF0@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>

Hi Andrew,

How do we disable the S-CSCF Forking functionality ? more than one UE are registered to IMS-Core network with same IMPI and IMPU, S-CSCF try to fork the all registered UE for the MT call landing on this IMPU.


Thanks
Aravind

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170712/49ccfb41/attachment.html>

From jake at dccllc.net  Wed Jul 12 16:10:25 2017
From: jake at dccllc.net (Jake Brown)
Date: Wed, 12 Jul 2017 20:10:25 +0000
Subject: [Project Clearwater] Using Ellis to program call diversion isn't
 working
Message-ID: <CY4PR18MB1336647CD2933E09DE55EDACA9AF0@CY4PR18MB1336.namprd18.prod.outlook.com>

I am trying to use Ellis to provision the call diversion settings for subscribers.  I can enable and disable barring, but am unable to enter new call diversion rules.   When logged into Ellis,  I click on the public identity and then go to redirect.  I enter the new rules and then click save changes.   Nothing seems to happen.  I have checked the vellum db and see that there are only entries for barring and privacy settings.

Here is the call that happens when I click config in ellis.  It queries Homer for the subscriber info.

This invokes the following get to the xdms server:
12-07-2017 19:07:02.649 UTC INFO xdm.py:54: Sending HTTP GET request to http://xdms.ims.mncXXX.mccXXX.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs.xml<http://xdms.ims.mnc560.mcc311.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mnc560.mcc311.3gppnetwork.org/simservs.xml>
12-07-2017 19:07:02.651 UTC INFO web.py:1447: 200 GET /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/listed?cb=2604effcc6M15 (0.0.0.0) 1.68ms
12-07-2017 19:07:02.653 UTC INFO homestead.py:297: Sending HTTP GET request to http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/public/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/service_profile<http://hss-cache.ims.mnc560.mcc311.3gppnetwork.org:8889/public/sip%3A13204705108%40ims.mnc560.mcc311.3gppnetwork.org/service_profile>
12-07-2017 19:07:02.661 UTC INFO homestead.py:278: Sending HTTP GET request to http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria<http://hss-cache.ims.mnc560.mcc311.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria>
12-07-2017 19:07:02.662 UTC DEBUG static.py:80: Checking if js/app-servers.json is allowed
12-07-2017 19:07:02.664 UTC INFO web.py:1447: 200 GET /js/app-servers.json?cb=2604effcc6M17 (0.0.0.0) 1.77ms
12-07-2017 19:07:02.668 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.01893901824951172,buffer=<_io.BytesIO object at 0x7fe4c5901b30>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7fe4c58826d0>,effective_url='http://xdms.ims.mncXXX.mccXXX.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs.xml',headers={'Content-Length': '338', 'Content-Encoding': 'gzip', 'Server': 'nginx/1.4.6 (Ubuntu)', 'Connection': 'close', 'Etag': '"8cc8f6a2314a88e52ca3d3ee9d1bc2de04046c0a"', 'Date': 'Wed, 12 Jul 2017 19:07:02 GMT', 'Content-Type': 'text/html; charset=UTF-8'},error=None)
12-07-2017 19:07:02.668 UTC DEBUG utils.py:92: All requests successful.
12-07-2017 19:07:02.668 UTC DEBUG numbers.py:484: Successfully fetched from Homer (simservs)
12-07-2017 19:07:02.669 UTC INFO web.py:1447: 200 GET /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs?cb=2604effcc6M14 (0.0.0.0) 22.29ms
12-07-2017 19:07:02.669 UTC DEBUG utils.py:94: Still expecting 0 callbacks
12-07-2017 19:07:02.669 UTC DEBUG homestead.py:282: Received response from http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria<http://hss-cache.ims.mnc560.mcc311.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria> with code 200
12-07-2017 19:07:02.669 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.008014917373657227,buffer=<_io.BytesIO object at 0x7fe4c5901710>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7fe4c58e2210>,effective_url='http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria',headers={'Content-Length': '295', 'Content-Encoding': 'gzip', 'Server': 'nginx/1.4.6 (Ubuntu)', 'Connection': 'close', 'Etag': '"10b41239aa6911d9689d6e979a695ed552f6ede5"', 'Date': 'Wed, 12 Jul 2017 19:07:02 GMT', 'Content-Type': 'text/html; charset=UTF-8'},error=None)
12-07-2017 19:07:02.670 UTC DEBUG utils.py:92: All requests successful.
12-07-2017 19:07:02.670 UTC DEBUG numbers.py:484: Successfully fetched from Homestead (iFC)
12-07-2017 19:07:02.670 UTC INFO web.py:1447: 200 GET /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/ifcs?cb=2604effcc6M16 (0.0.0.0) 18.44ms
12-07-2017 19:07:02.670 UTC DEBUG utils.py:94: Still expecting 0 callbacks

Here is the info that is retrieved from Vellum:

sip:13204705108 at ims.mncXXX.mccXXX.3gppnetwork.org<sip:13204705108 at ims.mnc560.mcc311.3gppnetwork.org> | <?xml version="1.0" encoding="UTF-8"?>\r\n<simservs xmlns="http://uri.etsi.org/ngn/params/xml/simservs/xcap" xmlns:cp="urn:ietf:params:xml:ns:common-policy"><originating-identity-png-communication-barring><outgoing-communication-barring active="true"><cp:ruleset><cp:rule id="rule0"><cp:conditions/><cp:actions><allow>true</allow></cp:actions></cp:rule></cp:ruleset></outgoing-communication-barring></simservs>


Here is the put from Ellis when I change the redirect settings:

12-07-2017 20:05:37.115 UTC INFO xdm.py:54: Sending HTTP PUT request to http://xdms.ims.mncXXX.mccXXX.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs.xml
12-07-2017 20:05:37.120 UTC INFO web.py:1447: 200 PUT /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/listed/1/?cb=26051e9852_12 (0.0.0.0) 3.63ms
12-07-2017 20:05:37.121 UTC INFO homestead.py:297: Sending HTTP GET request to http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/public/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/service_profile
12-07-2017 20:05:37.127 UTC INFO homestead.py:278: Sending HTTP PUT request to http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria
12-07-2017 20:05:37.132 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.016077041625976562,buffer=<_io.BytesIO object at 0x7fe4c6391ef0>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7fe4c59038d0>,effective_url='http://xdms.ims.mncXXX.mccXXX.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs.xml',headers={'Date': 'Wed, 12 Jul 2017 20:05:37 GMT', 'Content-Length': '2', 'Content-Type': 'application/json', 'Connection': 'close', 'Server': 'nginx/1.4.6 (Ubuntu)'},error=None)
12-07-2017 20:05:37.132 UTC DEBUG utils.py:92: All requests successful.
12-07-2017 20:05:37.132 UTC DEBUG numbers.py:502: Successfully updated Homer (simservs)
12-07-2017 20:05:37.132 UTC INFO web.py:1447: 200 PUT /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs?cb=26051e9852_10 (0.0.0.0) 19.73ms
12-07-2017 20:05:37.133 UTC DEBUG utils.py:94: Still expecting 0 callbacks
12-07-2017 20:05:37.158 UTC DEBUG homestead.py:282: Received response from http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria with code 200
12-07-2017 20:05:37.158 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.029946088790893555,buffer=<_io.BytesIO object at 0x7fe4c5901410>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7fe4c5872a10>,effective_url='http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria',headers={'Date': 'Wed, 12 Jul 2017 20:05:37 GMT', 'Content-Length': '0', 'Content-Type': 'text/html; charset=UTF-8', 'Connection': 'close', 'Server': 'nginx/1.4.6 (Ubuntu)'},error=None)
12-07-2017 20:05:37.158 UTC DEBUG utils.py:92: All requests successful.
12-07-2017 20:05:37.158 UTC DEBUG numbers.py:502: Successfully updated Homestead (iFC)
12-07-2017 20:05:37.158 UTC INFO web.py:1447: 200 PUT /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/ifcs?cb=26051e9852_11 (0.0.0.0) 41.91ms

The problem is...nothing is written for the redirect.

Can you please help point me in the correct direction.  I'm either looking to sort out why Ellis is doing this, or if I could directly send this data to homer and skip Ellis. (I use an HSS so don't need the IFC functionality...just need the xcap functions)


Jake Brown
Principal Engineer/Owner
920-351-4054 x1001
jake at dccllc.net<mailto:jake at dccllc.net>
www.dccllc.net<http://www.dccllc.net/>

[Signature-1]

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170712/b5b442de/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 2635 bytes
Desc: image002.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170712/b5b442de/attachment.jpg>

From hkaranjikar at apm.com  Thu Jul 13 05:39:56 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Thu, 13 Jul 2017 15:09:56 +0530
Subject: [Project Clearwater] restund_process Execution failed on bono
In-Reply-To: <CABmBNEY-Saqs0REksQqk9=vfE=sOb8GtwV31JQKftBYn_y1NXQ@mail.gmail.com>
References: <CABmBNEafr-WsM7_E3i7cbhJujejjqfgs9aFp2dTRdr=R7Lsmww@mail.gmail.com>
	<BLUPR02MB4375A20455D0317FDCA0A97E5AE0@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEY-Saqs0REksQqk9=vfE=sOb8GtwV31JQKftBYn_y1NXQ@mail.gmail.com>
Message-ID: <CABmBNEZB2HzhJZT9bFCjLFiX_W-NgigK7DCb5T64Yc7Jnj__eg@mail.gmail.com>

Hi,

I have changed my deployment completely.
No I am not using static IP addresses any more and each node is getting IP
addresses from the DHCP server.
Each node is able to ping one another using hostname cwellis, cwsprout,
cwbono, cwvellum, cwhomer, cwdime.
>From bono node I am able to ping, ssh to cwsprout. Check the logs,

===============================================================================

cwbono at cwbono:~$ ping cwsprout
PING cwsprout.amcc.com (10.48.12.143) 56(84) bytes of data.
64 bytes from cwsprout.amcc.com (10.48.12.143): icmp_seq=1 ttl=64
time=0.126 ms
64 bytes from cwsprout.amcc.com (10.48.12.143): icmp_seq=2 ttl=64
time=0.203 ms
64 bytes from cwsprout.amcc.com (10.48.12.143): icmp_seq=3 ttl=64
time=0.210 ms

cwbono at cwbono:~$ ssh cwsprout at cwsprout
The authenticity of host 'cwsprout (10.48.12.143)' can't be established.
ECDSA key fingerprint is 03:f8:42:81:36:ef:b1:be:7a:d8:3d:52:b5:74:f9:ba.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'cwsprout,10.48.12.143' (ECDSA) to the list of
known hosts.
cwsprout at cwsprout's password:
Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-31-generic x86_64)

 * Documentation:  https://help.ubuntu.com/

  System information as of Thu Jul 13 14:08:07 IST 2017

  System load:  0.12              Processes:           115
  Usage of /:   27.7% of 7.26GB   Users logged in:     0
  Memory usage: 8%                IP address for eth0: 10.48.12.143
  Swap usage:   0%

  Graph this data and manage this system at:
    https://landscape.canonical.com/

New release '16.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Thu Jul 13 14:08:07 2017 from hdk-supermicro.amcc.com
[sprout]cwsprout at cwsprout:~$

[sprout]cwsprout at cwsprout:~$ exit
logout
Connection to cwsprout closed.
cwbono at cwbono:~$
cwbono at cwbono:~$
cwbono at cwbono:~$ cat /etc/clearwater/local_config
local_ip=10.48.12.173
public_ip=10.48.12.173
public_hostname=cwbono
etcd_cluster="10.48.12.142,10.48.12.143,10.48.12.173,10.48.12.140,10.48.12.139,10.48.12.120"
cwbono at cwbono:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com
sprout_hostname=cwsprout
sprout_registration_store=10.48.12.120 #vellum
hs_hostname=10.48.12.139:8888 #dime
hs_provisioning_hostname=10.48.12.139:8889 #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=10.48.12.140:7888 #homer
chronos_hostname=vellum
cassandra_hostname=10.48.12.120 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


cwbono at cwbono:~$ clearwater-etcdctl cluster-health
member 2d821d7a0a7736b4 is healthy: got healthy result from
http://10.48.12.142:4000
member 6201151ee7f99f5c is healthy: got healthy result from
http://10.48.12.139:4000
member 895efb70c4b1b8b4 is healthy: got healthy result from
http://10.48.12.120:4000
member c7f5d6485fb4735b is healthy: got healthy result from
http://10.48.12.143:4000
member f7726a4e29ec7d3d is healthy: got healthy result from
http://10.48.12.173:4000
member ffb968d2990c63f0 is healthy: got healthy result from
http://10.48.12.140:4000
cluster is healthy
cwbono at cwbono:~$ clearwater-etcdctl member list
2d821d7a0a7736b4: name=10-48-12-142 peerURLs=http://10.48.12.142:2380
clientURLs=http://10.48.12.142:4000 isLeader=false
6201151ee7f99f5c: name=10-48-12-139 peerURLs=http://10.48.12.139:2380
clientURLs=http://10.48.12.139:4000 isLeader=false
895efb70c4b1b8b4: name=10-48-12-120 peerURLs=http://10.48.12.120:2380
clientURLs=http://10.48.12.120:4000 isLeader=false
c7f5d6485fb4735b: name=10-48-12-143 peerURLs=http://10.48.12.143:2380
clientURLs=http://10.48.12.143:4000 isLeader=false
f7726a4e29ec7d3d: name=10-48-12-173 peerURLs=http://10.48.12.173:2380
clientURLs=http://10.48.12.173:4000 isLeader=true
ffb968d2990c63f0: name=10-48-12-140 peerURLs=http://10.48.12.140:2380
clientURLs=http://10.48.12.140:4000 isLeader=false

cwbono at cwbono:~$ cw-check_cluster_state
This script prints the status of the Cassandra, Chronos, and Memcached
clusters.
This node (10.48.12.173) should not be in any cluster.

Describing the Cassandra cluster:
  The cluster is stable
    10.48.12.120 is in state normal

Describing the Chronos cluster:
  The cluster is stable
    10.48.12.120 is in state normal

Describing the Memcached cluster:
  The cluster is stable
    10.48.12.120 is in state normal

cwbono at cwbono:~$ sudo cw-check_config_sync
 - /etc/clearwater/dns.json is up to date
 - /etc/clearwater/shared_config is up to date

cwbono at cwbono:~$ sudo monit summary
Monit 5.18.1 uptime: 54m
 Service Name                     Status                      Type
 node-cwbono                      Running                     System
 restund_process                  Execution failed | Does...  Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Wait parent                 Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program

cwbono at cwbono:~$ cat /var/log/bono/bono_current.txt
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

cwbono at cwbono:~$ cat /var/log/monit.log
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:03:29] error    : 'restund_process' process is not running
[IST Jul 13 15:03:29] info     : 'restund_process' trying to restart
[IST Jul 13 15:03:29] info     : 'restund_process' restart:
/etc/init.d/restund
[IST Jul 13 15:04:00] error    : 'restund_process' failed to restart (exit
status 0) -- /etc/init.d/restund: httpdb: configured url
http://hs.example.com:8888/impi/%s/digest
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:04:10] error    : 'restund_process' process is not running
[IST Jul 13 15:04:10] info     : 'restund_process' trying to restart
[IST Jul 13 15:04:10] info     : 'restund_process' restart:
/etc/init.d/restund
[IST Jul 13 15:04:40] error    : 'restund_process' failed to restart (exit
status 0) -- /etc/init.d/restund: httpdb: configured url
http://hs.example.com:8888/impi/%s/digest
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:04:50] error    : 'restund_process' process is not running
[IST Jul 13 15:04:50] info     : 'restund_process' trying to restart
[IST Jul 13 15:04:50] info     : 'restund_process' restart:
/etc/init.d/restund
[IST Jul 13 15:05:20] error    : 'restund_process' failed to restart (exit
status 0) -- /etc/init.d/restund: httpdb: configured url
http://hs.example.com:8888/impi/%s/digest
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:05:30] error    : 'restund_process' process is not running
[IST Jul 13 15:05:30] info     : 'restund_process' trying to restart
[IST Jul 13 15:05:30] info     : 'restund_process' restart:
/etc/init.d/restund
[IST Jul 13 15:06:01] error    : 'restund_process' failed to restart (exit
status 0) -- /etc/init.d/restund: httpdb: configured url
http://hs.example.com:8888/impi/%s/digest
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:06:11] error    : 'restund_process' process is not running
[IST Jul 13 15:06:11] info     : 'restund_process' trying to restart
[IST Jul 13 15:06:11] info     : 'restund_process' restart:
/etc/init.d/restund
[IST Jul 13 15:06:41] error    : 'restund_process' failed to restart (exit
status 0) -- /etc/init.d/restund: httpdb: configured url
http://hs.example.com:8888/impi/%s/digest
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:06:51] error    : 'restund_process' process is not running
[IST Jul 13 15:06:51] info     : 'restund_process' trying to restart
[IST Jul 13 15:06:51] info     : 'restund_process' restart:
/etc/init.d/restund
[IST Jul 13 15:07:21] error    : 'restund_process' failed to restart (exit
status 0) -- /etc/init.d/restund: httpdb: configured url
http://hs.example.com:8888/impi/%s/digest
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:07:31] error    : 'restund_process' process is not running
[IST Jul 13 15:07:31] info     : 'restund_process' trying to restart
[IST Jul 13 15:07:31] info     : 'restund_process' restart:
/etc/init.d/restund

===============================================================================

I don't know whats going wrong here.
What is icscf.cwsprout?

Thanks
Hrishikesh



On Tue, Jul 11, 2017 at 5:08 PM, Hrishikesh Karanjikar <hkaranjikar at apm.com>
wrote:

> Hi,
>
> Thanks a lot for your reply.
>
> I am using virtualbox and host only network.
> The DHCP server runs inside virtualbox and I can only configure the IP
> address range.
> However I am not using the DHCP server and assigning static IP addresses
> to all nodes which are within the DHCP server IP address range.
> I added entry of cwsprout in /ets/hosts of bono node as follows,
>
> ===============================================
>
> 127.0.0.1       localhost
> 127.0.1.1       cwbono
>
> *192.168.56.103  cwsprout192.168.56.103  icscf.cwsprout*
>
> # The following lines are desirable for IPv6 capable hosts
> ::1     localhost ip6-localhost ip6-loopback
> ff02::1 ip6-allnodes
> ff02::2 ip6-allrouters
> 192.168.56.103 cwsprout
> 192.168.56.103  icscf.cwsprout
> ::1 localhost # added by clearwater-infrastructure 1hosts script
> 192.168.56.104 cwbono #+clearwater-infrastructure
>
> ===============================================
>
> I am also able to ping cwsprout and icscf.cwsprout from bono,
> Here is the log,
>
> ===============================================
>
> [bono]cwbono at cwbono:~$ ping cwsprout
> PING cwsprout (192.168.56.103) 56(84) bytes of data.
> 64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.197 ms
> 64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.217 ms
> ^C
> --- cwsprout ping statistics ---
> 2 packets transmitted, 2 received, 0% packet loss, time 999ms
> rtt min/avg/max/mdev = 0.197/0.207/0.217/0.010 ms
> [bono]cwbono at cwbono:~$
> [bono]cwbono at cwbono:~$
> [bono]cwbono at cwbono:~$
> [bono]cwbono at cwbono:~$ ping icscf.cwsprout
> PING icscf.cwsprout (192.168.56.103) 56(84) bytes of data.
> 64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.112 ms
> 64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.165 ms
>
> ===============================================
>
> I am using static IP addresses as I have to specify them in local_config
> file of each node.
> If I use DHCP server of VirtualBox they might change. In that case I am
> not sure how do I cope up with local_config.
> Can I modify local_config after all nodes are up?
>
> Thanks
> Hrishikesh
>
>
>
> On Tue, Jul 11, 2017 at 2:42 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
>> Hi Hrishikesh,
>>
>>
>>
>> Thank you for your question and the detailed logs you have provided.
>>
>>
>>
>> The issues appears to be caused by your shared config, in the manual
>> installation instructions
>> <http://clearwater.readthedocs.io/en/stable/Manual_Install.html> you?ll
>> see that we advise that the entries in shared config have a format like:
>>
>>
>>
>> sprout_hostname*=*sprout*.<*site_name*>.<*zone*>*
>>
>> sprout_registration_store*=*vellum*.<*site_name*>.<*zone*>*
>>
>> hs_hostname*=*hs*.<*site_name*>.<*zone*>*:8888
>>
>>
>>
>> In your shared config you have:
>>
>>
>>
>> sprout_hostname=cwsprout
>> sprout_registration_store=192.168.56.107 #vellum
>>
>> I don?t think the sprout_hostname used here will resolve, we can see
>> evidence for this in the Bono logs, Bono attempts to resolve
>> icscf.<sprout_hostname> to find which location to forward SIP messages on
>> you, you can see it is failing to do that here:
>>
>>
>>
>> Failed to resolve icscf.cwsprout to an IP address - Not found
>> (PJ_ENOTFOUND)
>>
>>
>>
>> To resolve this issue you should change your shared config to use
>> hostnames which have been configured in your DNS server to resolve to the
>> appropriate location. Once you have done this run the command ?sudo
>> cw-upload_shared_config?. Please let me know if this does not resolve the
>> issue.
>>
>>
>>
>> Thanks,
>>
>>
>>
>> Andrew
>>
>>
>>
>>
>>
>> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
>> *On Behalf Of *Hrishikesh Karanjikar
>> *Sent:* Thursday, July 6, 2017 10:22 AM
>> *To:* clearwater at lists.projectclearwater.org
>> *Subject:* [Project Clearwater] restund_process Execution failed on bono
>>
>>
>>
>> Hello,
>>
>> I have Manually installed all 6 nodes on VMs using virtualbox.
>>
>> I followed the procedure given @ http://clearwater.readthedocs.
>> io/en/stable/Manual_Install.html
>>
>> Looks like all nodes except Dime are running fine.
>>
>> I am getting error "restund_process Execution failed" in monit summary on
>> Bono node.
>>
>> Here is the shared and local config file,
>>
>> #####################################################################
>>
>> [bono]cwbono at cwbono:~$ cat /etc/clearwater/shared_config
>>
>> home_domain=example.com
>> sprout_hostname=cwsprout
>> sprout_registration_store=192.168.56.107 #vellum
>> hs_hostname=192.168.56.106:8888 #dime
>> hs_provisioning_hostname=192.168.56.106:8889 #dime
>> ralf_hostname=
>> ralf_session_store=
>> xdms_hostname=192.168.56.105:7888 #homer
>> chronos_hostname=192.168.56.107 #vellum
>> cassandra_hostname=192.168.56.107 #vellum
>>
>> # Email server configuration
>> smtp_smarthost=localhost
>> smtp_username=username
>> smtp_password=password
>> email_recovery_sender=clearwater at example.org
>>
>> # Keys
>> signup_key=secret
>> turn_workaround=secret
>> ellis_api_key=secret
>> ellis_cookie_key=secret
>> [bono]cwbono at cwbono:~$ cat /etc/clearwater/local_config
>> local_ip=192.168.56.104
>> public_ip=192.168.56.104
>> public_hostname=cwbono
>> etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,1
>> 92.168.56.105,192.168.56.106,192.168.56.107"
>>
>> #####################################################################
>>
>> The logs for bono node are as follows,
>>
>> #####################################################################
>> [bono]cwbono at cwbono:~$ sudo monit summary
>> [sudo] password for cwbono:
>> Monit 5.18.1 uptime: 2d 16h 39m
>>  Service Name                     Status
>> Type
>>  node-cwbono                      Running
>> System
>>  restund_process                  Execution failed | Does...
>> Process
>>  ntp_process                      Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  bono_process                     Running
>> Process
>>  poll_restund                     Wait parent
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>>  poll_bono                        Status ok
>> Program
>>
>>
>> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:15.843 UTC Status main.cpp:1358: Quiesce signal received
>> 05-07-2017 11:30:15.843 UTC Status stack.cpp:125: Setting quiescing =
>> PJ_TRUE
>> 05-07-2017 11:30:15.851 UTC Status stack.cpp:156: Quiescing state changed
>> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The
>> Quiescing Manager received input QUIESCE (0) when in state ACTIVE (0)
>> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:265: Close
>> untrusted listening port
>> 05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport
>> for port 5060
>> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:273: Quiesce
>> FlowTable
>> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The
>> Quiescing Manager received input FLOWS_GONE (1) when in state
>> QUIESCING_FLOWS (1)
>> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:290: Closing
>> trusted port
>> 05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport
>> for port 5058
>> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:296: Quiescing
>> all connections
>> 05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:162: Start
>> quiescing connections
>> 05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:175: Quiescing
>> 0 transactions
>> 05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:180: Connection
>> quiescing complete
>> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The
>> Quiescing Manager received input CONNS_GONE (2) when in state
>> QUIESCING_CONNS (2)
>> 05-07-2017 11:30:15.851 UTC Status main.cpp:1380: Quiesce complete
>> 05-07-2017 11:30:15.853 UTC Status stack.cpp:171: PJSIP thread ended
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
>> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>>
>> [bono]cwbono at cwbono:~$ cat /var/log/monit.log
>>
>> httpd: using URI workaround
>> turn: server deployed behind static NAT addr=192.168.56.104:0
>> turn: extended channels enabled
>> tcp: sock_bind: bind: Address already in u
>> [IST Jul  6 14:47:46] error    : 'restund_process' process is not running
>> [IST Jul  6 14:47:46] info     : 'restund_process' trying to restart
>> [IST Jul  6 14:47:46] info     : 'restund_process' restart:
>> /etc/init.d/restund
>> [IST Jul  6 14:48:16] error    : 'restund_process' failed to restart
>> (exit status 0) -- /etc/init.d/restund: httpdb: configured url
>> http://hs.example.com:8888/impi/%s/digest
>> httpd: using URI workaround
>> turn: server deployed behind static NAT addr=192.168.56.104:0
>> turn: extended channels enabled
>> tcp: sock_bind: bind: Address already in u
>> [IST Jul  6 14:48:26] error    : 'restund_process' process is not running
>> [IST Jul  6 14:48:26] info     : 'restund_process' trying to restart
>> [IST Jul  6 14:48:26] info     : 'restund_process' restart:
>> /etc/init.d/restund
>> [IST Jul  6 14:48:56] error    : 'restund_process' failed to restart
>> (exit status 0) -- /etc/init.d/restund: httpdb: configured url
>> http://hs.example.com:8888/impi/%s/digest
>> httpd: using URI workaround
>> turn: server deployed behind static NAT addr=192.168.56.104:0
>> turn: extended channels enabled
>> tcp: sock_bind: bind: Address already in u
>> [IST Jul  6 14:49:06] error    : 'restund_process' process is not running
>> [IST Jul  6 14:49:06] info     : 'restund_process' trying to restart
>> [IST Jul  6 14:49:06] info     : 'restund_process' restart:
>> /etc/init.d/restund
>>
>>
>> [bono]cwbono at cwbono:~$ clearwater-etcdctl cluster-health
>> member 9c1928228d308a0f is healthy: got healthy result from
>> http://192.168.56.107:4000
>> member b0c9c017e0d47e14 is healthy: got healthy result from
>> http://192.168.56.106:4000
>> member d44832212a08c43f is healthy: got healthy result from
>> http://192.168.56.103:4000
>> member ef1a9a8a2fd05283 is healthy: got healthy result from
>> http://192.168.56.104:4000
>> member f63afbe816fb463d is healthy: got healthy result from
>> http://192.168.56.102:4000
>> member f7132cc88f7a39fa is healthy: got healthy result from
>> http://192.168.56.105:4000
>> cluster is healthy
>> [bono]cwbono at cwbono:~$ cw-check_cluster_state
>> This script prints out the status of the Chronos, Memcached and Cassandra
>> clusters.
>>
>> Describing the Vellum Chronos cluster:
>>   The local node is *not* in this cluster
>>   The cluster is stable
>>     192.168.56.107 is in state normal
>>
>> Describing the Vellum Memcached cluster:
>>   The local node is *not* in this cluster
>>   The cluster is stable
>>     192.168.56.107 is in state normal
>>
>> Describing the Vellum Cassandra cluster:
>>   The local node is *not* in this cluster
>>   The cluster is stable
>>     192.168.56.107 is in state normal
>>
>> [bono]cwbono at cwbono:~$ clearwater-etcdctl member list
>> 9c1928228d308a0f: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
>> clientURLs=http://192.168.56.107:4000 isLeader=false
>> b0c9c017e0d47e14: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
>> clientURLs=http://192.168.56.106:4000 isLeader=true
>> d44832212a08c43f: name=192-168-56-103 peerURLs=http://192.168.56.103:2380
>> clientURLs=http://192.168.56.103:4000 isLeader=false
>> ef1a9a8a2fd05283: name=192-168-56-104 peerURLs=http://192.168.56.104:2380
>> clientURLs=http://192.168.56.104:4000 isLeader=false
>> f63afbe816fb463d: name=192-168-56-102 peerURLs=http://192.168.56.102:2380
>> clientURLs=http://192.168.56.102:4000 isLeader=false
>> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
>> clientURLs=http://192.168.56.105:4000 isLeader=false
>> [bono]cwbono at cwbono:~$ sudo cw-check_config_sync
>> [sudo] password for cwbono:
>>  - /etc/clearwater/dns.json is up to date
>>  - /etc/clearwater/shared_config is up to date
>>
>>
>> #####################################################################
>>
>> The logs for other nodes are as follows
>>
>> #####################################################################
>>
>> [ellis]cwellis at cwellis:~$ sudo monit summary
>> Monit 5.18.1 uptime: 1d 21h 15m
>>  Service Name                     Status
>> Type
>>  node-cwellis                     Running
>> System
>>  ntp_process                      Running
>> Process
>>  nginx_process                    Running
>> Process
>>  mysql_process                    Running
>> Process
>>  ellis_process                    Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  nginx_ping                       Status ok
>> Program
>>  nginx_uptime                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  poll_ellis                       Status ok
>> Program
>>  poll_ellis_https                 Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>>
>>
>> [sprout]cwsprout at cwsprout:~$ sudo monit summary
>> Monit 5.18.1 uptime: 1d 20h 7m
>>  Service Name                     Status
>> Type
>>  node-cwsprout                    Running
>> System
>>  sprout_process                   Running
>> Process
>>  ntp_process                      Running
>> Process
>>  nginx_process                    Running
>> Process
>>  memento_process                  Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  sprout_uptime                    Status ok
>> Program
>>  poll_sprout_sip                  Status ok
>> Program
>>  poll_sprout_http                 Status ok
>> Program
>>  nginx_ping                       Status ok
>> Program
>>  nginx_uptime                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  memento_uptime                   Status ok
>> Program
>>  poll_memento                     Status ok
>> Program
>>  poll_memento_https               Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>>
>>
>> [homer]cwhomer at cwhomer:~$ sudo monit summary
>> Monit 5.18.1 uptime: 1d 20h 2m
>>  Service Name                     Status
>> Type
>>  node-cwhomer                     Running
>> System
>>  ntp_process                      Running
>> Process
>>  nginx_process                    Running
>> Process
>>  homer_process                    Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  nginx_ping                       Status ok
>> Program
>>  nginx_uptime                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  poll_homer                       Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok                   Program
>>
>> [dime]cwdime at cwdime:~$ sudo monit summary
>> Monit 5.18.1 uptime: 1d 20h 2m
>>  Service Name                     Status
>> Type
>>  node-cwdime                      Running
>> System
>>  snmpd_process                    Running
>> Process
>>  ntp_process                      Running
>> Process
>>  nginx_process                    Running
>> Process
>>  homestead_process                Running
>> Process
>>  homestead-prov_process           Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  nginx_ping                       Status ok
>> Program
>>  nginx_uptime                     Status ok
>> Program
>>  monit_uptime                     Status ok
>> Program
>>  homestead_uptime                 Status ok
>> Program
>>  poll_homestead                   Status ok
>> Program
>>  check_cx_health                  Status ok
>> Program
>>  poll_homestead-prov              Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>>
>>
>> [vellum]cwvellum at cwvellum:~$ sudo monit summary
>> Monit 5.18.1 uptime: 1d 20h 3m
>>  Service Name                     Status
>> Type
>>  node-cwvellum                    Running
>> System
>>  snmpd_process                    Running
>> Process
>>  ntp_process                      Running
>> Process
>>  memcached_process                Running
>> Process
>>  clearwater_queue_manager_pro...  Running
>> Process
>>  etcd_process                     Running
>> Process
>>  clearwater_diags_monitor_pro...  Running
>> Process
>>  clearwater_config_manager_pr...  Running
>> Process
>>  clearwater_cluster_manager_p...  Running
>> Process
>>  cassandra_process                Running
>> Process
>>  chronos_process                  Running
>> Process
>>  astaire_process                  Running
>> Process
>>  monit_uptime                     Status ok
>> Program
>>  memcached_uptime                 Status ok
>> Program
>>  poll_memcached                   Status ok
>> Program
>>  clearwater_queue_manager_uptime  Status ok
>> Program
>>  etcd_uptime                      Status ok
>> Program
>>  poll_etcd_cluster                Status ok
>> Program
>>  poll_etcd                        Status ok
>> Program
>>  cassandra_uptime                 Status ok
>> Program
>>  poll_cassandra                   Status ok
>> Program
>>  poll_cqlsh                       Status ok
>> Program
>>  chronos_uptime                   Status ok
>> Program
>>  poll_chronos                     Status ok
>> Program
>>  astaire_uptime                   Status ok
>> Program
>>
>>
>> #####################################################################
>>
>> Please let me know if I am missing any configuration.
>>
>> Thanks
>>
>> Hrishikesh
>>
>> _______________________________________________
>> Clearwater mailing list
>> Clearwater at lists.projectclearwater.org
>> http://lists.projectclearwater.org/mailman/listinfo/
>> clearwater_lists.projectclearwater.org
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170713/9bae39b0/attachment.html>

From Andrew.Edmonds at metaswitch.com  Thu Jul 13 10:08:47 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Thu, 13 Jul 2017 14:08:47 +0000
Subject: [Project Clearwater] aarch64 build
In-Reply-To: <AM5PR0801MB1633EDDC45BF64792B09483196AA0@AM5PR0801MB1633.eurprd08.prod.outlook.com>
References: <AM5PR0801MB1633EDDC45BF64792B09483196AA0@AM5PR0801MB1633.eurprd08.prod.outlook.com>
Message-ID: <BLUPR02MB437065125507AC83A90F9D3E5AC0@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Martin,

If we have a look at the Dockerfile<https://github.com/Metaswitch/clearwater-docker/blob/d0154def257d0982f203a2eecbad7194b74212fa/base/Dockerfile#L23> we can see that the debian package required is clearwater-management (not clearwater-manager).

clearwater-management is built by the clearwater-etcd repository<https://github.com/Metaswitch/clearwater-etcd/blob/dev/debian/control#L32>.

Please get in touch if you have any further questions,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Martin Horne
Sent: Friday, July 7, 2017 4:50 PM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] aarch64 build

Trying to build clearwater-docker base image, requires clearwater-manager deb package, which Metaswitch project builds this?

         Martin Horne
IMPORTANT NOTICE: The contents of this email and any attachments are confidential and may also be privileged. If you are not the intended recipient, please notify the sender immediately and do not disclose the contents to any other person, use it for any purpose, or store or copy the information in any medium. Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170713/d8c0bb19/attachment.html>

From jake at dccllc.net  Thu Jul 13 10:23:28 2017
From: jake at dccllc.net (Jake Brown)
Date: Thu, 13 Jul 2017 14:23:28 +0000
Subject: [Project Clearwater] Using Ellis to program call diversion
 isn't working
In-Reply-To: <CY4PR18MB1336647CD2933E09DE55EDACA9AF0@CY4PR18MB1336.namprd18.prod.outlook.com>
References: <CY4PR18MB1336647CD2933E09DE55EDACA9AF0@CY4PR18MB1336.namprd18.prod.outlook.com>
Message-ID: <CY4PR18MB13364623B39D09C179B78896A9AC0@CY4PR18MB1336.namprd18.prod.outlook.com>

This ended up being a firefox vs Chrome issue.

It doesn't work with Firefox, and works properly with Chrome.

How do I enable conference calling, and call waiting?

Jake Brown
Principal Engineer/Owner
920-351-4054 x1001
jake at dccllc.net<mailto:jake at dccllc.net>
www.dccllc.net<http://www.dccllc.net/>

[Signature-1]

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Jake Brown
Sent: Wednesday, July 12, 2017 3:10 PM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Using Ellis to program call diversion isn't working


This sender failed our fraud detection checks and may not be who they appear to be. Learn about spoofing<http://aka.ms/LearnAboutSpoofing>

Feedback<http://aka.ms/SafetyTipsFeedback>

I am trying to use Ellis to provision the call diversion settings for subscribers.  I can enable and disable barring, but am unable to enter new call diversion rules.   When logged into Ellis,  I click on the public identity and then go to redirect.  I enter the new rules and then click save changes.   Nothing seems to happen.  I have checked the vellum db and see that there are only entries for barring and privacy settings.

Here is the call that happens when I click config in ellis.  It queries Homer for the subscriber info.

This invokes the following get to the xdms server:
12-07-2017 19:07:02.649 UTC INFO xdm.py:54: Sending HTTP GET request to http://xdms.ims.mncXXX.mccXXX.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs.xml<http://xdms.ims.mnc560.mcc311.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mnc560.mcc311.3gppnetwork.org/simservs.xml>
12-07-2017 19:07:02.651 UTC INFO web.py:1447: 200 GET /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/listed?cb=2604effcc6M15 (0.0.0.0) 1.68ms
12-07-2017 19:07:02.653 UTC INFO homestead.py:297: Sending HTTP GET request to http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/public/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/service_profile<http://hss-cache.ims.mnc560.mcc311.3gppnetwork.org:8889/public/sip%3A13204705108%40ims.mnc560.mcc311.3gppnetwork.org/service_profile>
12-07-2017 19:07:02.661 UTC INFO homestead.py:278: Sending HTTP GET request to http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria<http://hss-cache.ims.mnc560.mcc311.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria>
12-07-2017 19:07:02.662 UTC DEBUG static.py:80: Checking if js/app-servers.json is allowed
12-07-2017 19:07:02.664 UTC INFO web.py:1447: 200 GET /js/app-servers.json?cb=2604effcc6M17 (0.0.0.0) 1.77ms
12-07-2017 19:07:02.668 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.01893901824951172,buffer=<_io.BytesIO object at 0x7fe4c5901b30>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7fe4c58826d0>,effective_url='http://xdms.ims.mncXXX.mccXXX.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs.xml',headers={'Content-Length': '338', 'Content-Encoding': 'gzip', 'Server': 'nginx/1.4.6 (Ubuntu)', 'Connection': 'close', 'Etag': '"8cc8f6a2314a88e52ca3d3ee9d1bc2de04046c0a"', 'Date': 'Wed, 12 Jul 2017 19:07:02 GMT', 'Content-Type': 'text/html; charset=UTF-8'},error=None)
12-07-2017 19:07:02.668 UTC DEBUG utils.py:92: All requests successful.
12-07-2017 19:07:02.668 UTC DEBUG numbers.py:484: Successfully fetched from Homer (simservs)
12-07-2017 19:07:02.669 UTC INFO web.py:1447: 200 GET /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs?cb=2604effcc6M14 (0.0.0.0) 22.29ms
12-07-2017 19:07:02.669 UTC DEBUG utils.py:94: Still expecting 0 callbacks
12-07-2017 19:07:02.669 UTC DEBUG homestead.py:282: Received response from http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria<http://hss-cache.ims.mnc560.mcc311.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria> with code 200
12-07-2017 19:07:02.669 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.008014917373657227,buffer=<_io.BytesIO object at 0x7fe4c5901710>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7fe4c58e2210>,effective_url='http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria',headers={'Content-Length': '295', 'Content-Encoding': 'gzip', 'Server': 'nginx/1.4.6 (Ubuntu)', 'Connection': 'close', 'Etag': '"10b41239aa6911d9689d6e979a695ed552f6ede5"', 'Date': 'Wed, 12 Jul 2017 19:07:02 GMT', 'Content-Type': 'text/html; charset=UTF-8'},error=None)
12-07-2017 19:07:02.670 UTC DEBUG utils.py:92: All requests successful.
12-07-2017 19:07:02.670 UTC DEBUG numbers.py:484: Successfully fetched from Homestead (iFC)
12-07-2017 19:07:02.670 UTC INFO web.py:1447: 200 GET /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/ifcs?cb=2604effcc6M16 (0.0.0.0) 18.44ms
12-07-2017 19:07:02.670 UTC DEBUG utils.py:94: Still expecting 0 callbacks

Here is the info that is retrieved from Vellum:

sip:13204705108 at ims.mncXXX.mccXXX.3gppnetwork.org<sip:13204705108 at ims.mnc560.mcc311.3gppnetwork.org> | <?xml version="1.0" encoding="UTF-8"?>\r\n<simservs xmlns="http://uri.etsi.org/ngn/params/xml/simservs/xcap" xmlns:cp="urn:ietf:params:xml:ns:common-policy"><originating-identity-png-communication-barring><outgoing-communication-barring active="true"><cp:ruleset><cp:rule id="rule0"><cp:conditions/><cp:actions><allow>true</allow></cp:actions></cp:rule></cp:ruleset></outgoing-communication-barring></simservs>


Here is the put from Ellis when I change the redirect settings:

12-07-2017 20:05:37.115 UTC INFO xdm.py:54: Sending HTTP PUT request to http://xdms.ims.mncXXX.mccXXX.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs.xml
12-07-2017 20:05:37.120 UTC INFO web.py:1447: 200 PUT /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/listed/1/?cb=26051e9852_12 (0.0.0.0) 3.63ms
12-07-2017 20:05:37.121 UTC INFO homestead.py:297: Sending HTTP GET request to http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/public/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/service_profile
12-07-2017 20:05:37.127 UTC INFO homestead.py:278: Sending HTTP PUT request to http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria
12-07-2017 20:05:37.132 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.016077041625976562,buffer=<_io.BytesIO object at 0x7fe4c6391ef0>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7fe4c59038d0>,effective_url='http://xdms.ims.mncXXX.mccXXX.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs.xml',headers={'Date': 'Wed, 12 Jul 2017 20:05:37 GMT', 'Content-Length': '2', 'Content-Type': 'application/json', 'Connection': 'close', 'Server': 'nginx/1.4.6 (Ubuntu)'},error=None)
12-07-2017 20:05:37.132 UTC DEBUG utils.py:92: All requests successful.
12-07-2017 20:05:37.132 UTC DEBUG numbers.py:502: Successfully updated Homer (simservs)
12-07-2017 20:05:37.132 UTC INFO web.py:1447: 200 PUT /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs?cb=26051e9852_10 (0.0.0.0) 19.73ms
12-07-2017 20:05:37.133 UTC DEBUG utils.py:94: Still expecting 0 callbacks
12-07-2017 20:05:37.158 UTC DEBUG homestead.py:282: Received response from http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria with code 200
12-07-2017 20:05:37.158 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.029946088790893555,buffer=<_io.BytesIO object at 0x7fe4c5901410>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7fe4c5872a10>,effective_url='http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria',headers={'Date': 'Wed, 12 Jul 2017 20:05:37 GMT', 'Content-Length': '0', 'Content-Type': 'text/html; charset=UTF-8', 'Connection': 'close', 'Server': 'nginx/1.4.6 (Ubuntu)'},error=None)
12-07-2017 20:05:37.158 UTC DEBUG utils.py:92: All requests successful.
12-07-2017 20:05:37.158 UTC DEBUG numbers.py:502: Successfully updated Homestead (iFC)
12-07-2017 20:05:37.158 UTC INFO web.py:1447: 200 PUT /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/ifcs?cb=26051e9852_11 (0.0.0.0) 41.91ms

The problem is...nothing is written for the redirect.

Can you please help point me in the correct direction.  I'm either looking to sort out why Ellis is doing this, or if I could directly send this data to homer and skip Ellis. (I use an HSS so don't need the IFC functionality...just need the xcap functions)


Jake Brown
Principal Engineer/Owner
920-351-4054 x1001
jake at dccllc.net<mailto:jake at dccllc.net>
www.dccllc.net<http://www.dccllc.net/>

[Signature-1]

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170713/bf67b575/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 2635 bytes
Desc: image002.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170713/bf67b575/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.jpg
Type: image/jpeg
Size: 2630 bytes
Desc: image003.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170713/bf67b575/attachment-0001.jpg>

From Andrew.Edmonds at metaswitch.com  Thu Jul 13 12:00:23 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Thu, 13 Jul 2017 16:00:23 +0000
Subject: [Project Clearwater] restund_process Execution failed on bono
In-Reply-To: <CABmBNEZB2HzhJZT9bFCjLFiX_W-NgigK7DCb5T64Yc7Jnj__eg@mail.gmail.com>
References: <CABmBNEafr-WsM7_E3i7cbhJujejjqfgs9aFp2dTRdr=R7Lsmww@mail.gmail.com>
	<BLUPR02MB4375A20455D0317FDCA0A97E5AE0@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEY-Saqs0REksQqk9=vfE=sOb8GtwV31JQKftBYn_y1NXQ@mail.gmail.com>
	<CABmBNEZB2HzhJZT9bFCjLFiX_W-NgigK7DCb5T64Yc7Jnj__eg@mail.gmail.com>
Message-ID: <BLUPR02MB437BB7A07944A9B02AA3075E5AC0@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Hrishikesh,

Thank you for the updated diagnostics.

We can still see the following appearing in log files:

?Failed to resolve icscf.cwsprout to an IP address?

The icscf.<sprout hostname> DNS record is used by the P-CSCF (in this case Bono) to identify which I-CSCF to forward requests on to.

Even when assigning nodes IP addresses through DHCP you must still configure your DNS with all the records that Clearwater nodes require to communicate with each other (such as icscf.<sprout hostname>). You can find a list of all the records that Clearwater requires here<http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html>.

Please could you try updating your DNS server to contain these records and restart your nodes, let me know if you still hit the issue.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, July 13, 2017 10:40 AM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] restund_process Execution failed on bono

Hi,
I have changed my deployment completely.
No I am not using static IP addresses any more and each node is getting IP addresses from the DHCP server.
Each node is able to ping one another using hostname cwellis, cwsprout, cwbono, cwvellum, cwhomer, cwdime.
From bono node I am able to ping, ssh to cwsprout. Check the logs,

===============================================================================

cwbono at cwbono:~$ ping cwsprout
PING cwsprout.amcc.com<http://cwsprout.amcc.com> (10.48.12.143) 56(84) bytes of data.
64 bytes from cwsprout.amcc.com<http://cwsprout.amcc.com> (10.48.12.143): icmp_seq=1 ttl=64 time=0.126 ms
64 bytes from cwsprout.amcc.com<http://cwsprout.amcc.com> (10.48.12.143): icmp_seq=2 ttl=64 time=0.203 ms
64 bytes from cwsprout.amcc.com<http://cwsprout.amcc.com> (10.48.12.143): icmp_seq=3 ttl=64 time=0.210 ms

cwbono at cwbono:~$ ssh cwsprout at cwsprout
The authenticity of host 'cwsprout (10.48.12.143)' can't be established.
ECDSA key fingerprint is 03:f8:42:81:36:ef:b1:be:7a:d8:3d:52:b5:74:f9:ba.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'cwsprout,10.48.12.143' (ECDSA) to the list of known hosts.
cwsprout at cwsprout's password:
Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-31-generic x86_64)

 * Documentation:  https://help.ubuntu.com/

  System information as of Thu Jul 13 14:08:07 IST 2017

  System load:  0.12              Processes:           115
  Usage of /:   27.7% of 7.26GB   Users logged in:     0
  Memory usage: 8%                IP address for eth0: 10.48.12.143
  Swap usage:   0%

  Graph this data and manage this system at:
    https://landscape.canonical.com/

New release '16.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Thu Jul 13 14:08:07 2017 from hdk-supermicro.amcc.com<http://hdk-supermicro.amcc.com>
[sprout]cwsprout at cwsprout:~$

[sprout]cwsprout at cwsprout:~$ exit
logout
Connection to cwsprout closed.
cwbono at cwbono:~$
cwbono at cwbono:~$
cwbono at cwbono:~$ cat /etc/clearwater/local_config
local_ip=10.48.12.173
public_ip=10.48.12.173
public_hostname=cwbono
etcd_cluster="10.48.12.142,10.48.12.143,10.48.12.173,10.48.12.140,10.48.12.139,10.48.12.120"
cwbono at cwbono:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout
sprout_registration_store=10.48.12.120 #vellum
hs_hostname=10.48.12.139:8888<http://10.48.12.139:8888> #dime
hs_provisioning_hostname=10.48.12.139:8889<http://10.48.12.139:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=10.48.12.140:7888<http://10.48.12.140:7888> #homer
chronos_hostname=vellum
cassandra_hostname=10.48.12.120 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


cwbono at cwbono:~$ clearwater-etcdctl cluster-health
member 2d821d7a0a7736b4 is healthy: got healthy result from http://10.48.12.142:4000
member 6201151ee7f99f5c is healthy: got healthy result from http://10.48.12.139:4000
member 895efb70c4b1b8b4 is healthy: got healthy result from http://10.48.12.120:4000
member c7f5d6485fb4735b is healthy: got healthy result from http://10.48.12.143:4000
member f7726a4e29ec7d3d is healthy: got healthy result from http://10.48.12.173:4000
member ffb968d2990c63f0 is healthy: got healthy result from http://10.48.12.140:4000
cluster is healthy
cwbono at cwbono:~$ clearwater-etcdctl member list
2d821d7a0a7736b4: name=10-48-12-142 peerURLs=http://10.48.12.142:2380 clientURLs=http://10.48.12.142:4000 isLeader=false
6201151ee7f99f5c: name=10-48-12-139 peerURLs=http://10.48.12.139:2380 clientURLs=http://10.48.12.139:4000 isLeader=false
895efb70c4b1b8b4: name=10-48-12-120 peerURLs=http://10.48.12.120:2380 clientURLs=http://10.48.12.120:4000 isLeader=false
c7f5d6485fb4735b: name=10-48-12-143 peerURLs=http://10.48.12.143:2380 clientURLs=http://10.48.12.143:4000 isLeader=false
f7726a4e29ec7d3d: name=10-48-12-173 peerURLs=http://10.48.12.173:2380 clientURLs=http://10.48.12.173:4000 isLeader=true
ffb968d2990c63f0: name=10-48-12-140 peerURLs=http://10.48.12.140:2380 clientURLs=http://10.48.12.140:4000 isLeader=false

cwbono at cwbono:~$ cw-check_cluster_state
This script prints the status of the Cassandra, Chronos, and Memcached clusters.
This node (10.48.12.173) should not be in any cluster.

Describing the Cassandra cluster:
  The cluster is stable
    10.48.12.120 is in state normal

Describing the Chronos cluster:
  The cluster is stable
    10.48.12.120 is in state normal

Describing the Memcached cluster:
  The cluster is stable
    10.48.12.120 is in state normal

cwbono at cwbono:~$ sudo cw-check_config_sync
 - /etc/clearwater/dns.json is up to date
 - /etc/clearwater/shared_config is up to date

cwbono at cwbono:~$ sudo monit summary
Monit 5.18.1 uptime: 54m
 Service Name                     Status                      Type
 node-cwbono                      Running                     System
 restund_process                  Execution failed | Does...  Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Wait parent                 Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program

cwbono at cwbono:~$ cat /var/log/bono/bono_current.txt
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

cwbono at cwbono:~$ cat /var/log/monit.log
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:03:29] error    : 'restund_process' process is not running
[IST Jul 13 15:03:29] info     : 'restund_process' trying to restart
[IST Jul 13 15:03:29] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:04:00] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:04:10] error    : 'restund_process' process is not running
[IST Jul 13 15:04:10] info     : 'restund_process' trying to restart
[IST Jul 13 15:04:10] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:04:40] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:04:50] error    : 'restund_process' process is not running
[IST Jul 13 15:04:50] info     : 'restund_process' trying to restart
[IST Jul 13 15:04:50] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:05:20] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:05:30] error    : 'restund_process' process is not running
[IST Jul 13 15:05:30] info     : 'restund_process' trying to restart
[IST Jul 13 15:05:30] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:06:01] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:06:11] error    : 'restund_process' process is not running
[IST Jul 13 15:06:11] info     : 'restund_process' trying to restart
[IST Jul 13 15:06:11] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:06:41] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:06:51] error    : 'restund_process' process is not running
[IST Jul 13 15:06:51] info     : 'restund_process' trying to restart
[IST Jul 13 15:06:51] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:07:21] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:07:31] error    : 'restund_process' process is not running
[IST Jul 13 15:07:31] info     : 'restund_process' trying to restart
[IST Jul 13 15:07:31] info     : 'restund_process' restart: /etc/init.d/restund

===============================================================================
I don't know whats going wrong here.
What is icscf.cwsprout?

Thanks
Hrishikesh


On Tue, Jul 11, 2017 at 5:08 PM, Hrishikesh Karanjikar <hkaranjikar at apm.com<mailto:hkaranjikar at apm.com>> wrote:
Hi,
Thanks a lot for your reply.
I am using virtualbox and host only network.
The DHCP server runs inside virtualbox and I can only configure the IP address range.
However I am not using the DHCP server and assigning static IP addresses to all nodes which are within the DHCP server IP address range.
I added entry of cwsprout in /ets/hosts of bono node as follows,

===============================================
127.0.0.1       localhost
127.0.1.1       cwbono
192.168.56.103  cwsprout
192.168.56.103  icscf.cwsprout

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
192.168.56.103 cwsprout
192.168.56.103  icscf.cwsprout
::1 localhost # added by clearwater-infrastructure 1hosts script
192.168.56.104 cwbono #+clearwater-infrastructure

===============================================
I am also able to ping cwsprout and icscf.cwsprout from bono,
Here is the log,

===============================================

[bono]cwbono at cwbono:~$ ping cwsprout
PING cwsprout (192.168.56.103) 56(84) bytes of data.
64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.197 ms
64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.217 ms
^C
--- cwsprout ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.197/0.207/0.217/0.010 ms
[bono]cwbono at cwbono:~$
[bono]cwbono at cwbono:~$
[bono]cwbono at cwbono:~$
[bono]cwbono at cwbono:~$ ping icscf.cwsprout
PING icscf.cwsprout (192.168.56.103) 56(84) bytes of data.
64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.112 ms
64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.165 ms
===============================================
I am using static IP addresses as I have to specify them in local_config file of each node.
If I use DHCP server of VirtualBox they might change. In that case I am not sure how do I cope up with local_config.
Can I modify local_config after all nodes are up?

Thanks
Hrishikesh


On Tue, Jul 11, 2017 at 2:42 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Thank you for your question and the detailed logs you have provided.

The issues appears to be caused by your shared config, in the manual installation instructions<http://clearwater.readthedocs.io/en/stable/Manual_Install.html> you?ll see that we advise that the entries in shared config have a format like:

sprout_hostname=sprout.<site_name>.<zone>
sprout_registration_store=vellum.<site_name>.<zone>
hs_hostname=hs.<site_name>.<zone>:8888

In your shared config you have:

sprout_hostname=cwsprout
sprout_registration_store=192.168.56.107 #vellum
I don?t think the sprout_hostname used here will resolve, we can see evidence for this in the Bono logs, Bono attempts to resolve icscf.<sprout_hostname> to find which location to forward SIP messages on you, you can see it is failing to do that here:

Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

To resolve this issue you should change your shared config to use hostnames which have been configured in your DNS server to resolve to the appropriate location. Once you have done this run the command ?sudo cw-upload_shared_config?. Please let me know if this does not resolve the issue.

Thanks,

Andrew


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, July 6, 2017 10:22 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] restund_process Execution failed on bono

Hello,
I have Manually installed all 6 nodes on VMs using virtualbox.
I followed the procedure given @ http://clearwater.readthedocs.io/en/stable/Manual_Install.html
Looks like all nodes except Dime are running fine.
I am getting error "restund_process Execution failed" in monit summary on Bono node.
Here is the shared and local config file,

#####################################################################

[bono]cwbono at cwbono:~$ cat /etc/clearwater/shared_config

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout
sprout_registration_store=192.168.56.107 #vellum
hs_hostname=192.168.56.106:8888<http://192.168.56.106:8888> #dime
hs_provisioning_hostname=192.168.56.106:8889<http://192.168.56.106:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.105:7888<http://192.168.56.105:7888> #homer
chronos_hostname=192.168.56.107 #vellum
cassandra_hostname=192.168.56.107 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret
[bono]cwbono at cwbono:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.104
public_ip=192.168.56.104
public_hostname=cwbono
etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,192.168.56.105,192.168.56.106,192.168.56.107"

#####################################################################
The logs for bono node are as follows,

#####################################################################
[bono]cwbono at cwbono:~$ sudo monit summary
[sudo] password for cwbono:
Monit 5.18.1 uptime: 2d 16h 39m
 Service Name                     Status                      Type
 node-cwbono                      Running                     System
 restund_process                  Execution failed | Does...  Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Wait parent                 Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program


05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.843 UTC Status main.cpp:1358: Quiesce signal received
05-07-2017 11:30:15.843 UTC Status stack.cpp:125: Setting quiescing = PJ_TRUE
05-07-2017 11:30:15.851 UTC Status stack.cpp:156: Quiescing state changed
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing Manager received input QUIESCE (0) when in state ACTIVE (0)
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:265: Close untrusted listening port
05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport for port 5060
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:273: Quiesce FlowTable
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing Manager received input FLOWS_GONE (1) when in state QUIESCING_FLOWS (1)
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:290: Closing trusted port
05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport for port 5058
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:296: Quiescing all connections
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:162: Start quiescing connections
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:175: Quiescing 0 transactions
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:180: Connection quiescing complete
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing Manager received input CONNS_GONE (2) when in state QUIESCING_CONNS (2)
05-07-2017 11:30:15.851 UTC Status main.cpp:1380: Quiesce complete
05-07-2017 11:30:15.853 UTC Status stack.cpp:171: PJSIP thread ended
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

[bono]cwbono at cwbono:~$ cat /var/log/monit.log

httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0<http://192.168.56.104:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:47:46] error    : 'restund_process' process is not running
[IST Jul  6 14:47:46] info     : 'restund_process' trying to restart
[IST Jul  6 14:47:46] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul  6 14:48:16] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0<http://192.168.56.104:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:48:26] error    : 'restund_process' process is not running
[IST Jul  6 14:48:26] info     : 'restund_process' trying to restart
[IST Jul  6 14:48:26] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul  6 14:48:56] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0<http://192.168.56.104:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:49:06] error    : 'restund_process' process is not running
[IST Jul  6 14:49:06] info     : 'restund_process' trying to restart
[IST Jul  6 14:49:06] info     : 'restund_process' restart: /etc/init.d/restund


[bono]cwbono at cwbono:~$ clearwater-etcdctl cluster-health
member 9c1928228d308a0f is healthy: got healthy result from http://192.168.56.107:4000
member b0c9c017e0d47e14 is healthy: got healthy result from http://192.168.56.106:4000
member d44832212a08c43f is healthy: got healthy result from http://192.168.56.103:4000
member ef1a9a8a2fd05283 is healthy: got healthy result from http://192.168.56.104:4000
member f63afbe816fb463d is healthy: got healthy result from http://192.168.56.102:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono at cwbono:~$ cw-check_cluster_state
This script prints out the status of the Chronos, Memcached and Cassandra clusters.

Describing the Vellum Chronos cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

Describing the Vellum Memcached cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

Describing the Vellum Cassandra cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

[bono]cwbono at cwbono:~$ clearwater-etcdctl member list
9c1928228d308a0f: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
b0c9c017e0d47e14: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=true
d44832212a08c43f: name=192-168-56-103 peerURLs=http://192.168.56.103:2380 clientURLs=http://192.168.56.103:4000 isLeader=false
ef1a9a8a2fd05283: name=192-168-56-104 peerURLs=http://192.168.56.104:2380 clientURLs=http://192.168.56.104:4000 isLeader=false
f63afbe816fb463d: name=192-168-56-102 peerURLs=http://192.168.56.102:2380 clientURLs=http://192.168.56.102:4000 isLeader=false
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono at cwbono:~$ sudo cw-check_config_sync
[sudo] password for cwbono:
 - /etc/clearwater/dns.json is up to date
 - /etc/clearwater/shared_config is up to date


#####################################################################
The logs for other nodes are as follows

#####################################################################

[ellis]cwellis at cwellis:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 21h 15m
 Service Name                     Status                      Type
 node-cwellis                     Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[sprout]cwsprout at cwsprout:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 7m
 Service Name                     Status                      Type
 node-cwsprout                    Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[homer]cwhomer at cwhomer:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 2m
 Service Name                     Status                      Type
 node-cwhomer                     Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program

[dime]cwdime at cwdime:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 2m
 Service Name                     Status                      Type
 node-cwdime                      Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[vellum]cwvellum at cwvellum:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 3m
 Service Name                     Status                      Type
 node-cwvellum                    Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program


#####################################################################
Please let me know if I am missing any configuration.
Thanks
Hrishikesh

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170713/588538e3/attachment.html>

From Andrew.Edmonds at metaswitch.com  Thu Jul 13 12:15:29 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Thu, 13 Jul 2017 16:15:29 +0000
Subject: [Project Clearwater] Using Ellis to program call diversion
 isn't working
In-Reply-To: <CY4PR18MB13364623B39D09C179B78896A9AC0@CY4PR18MB1336.namprd18.prod.outlook.com>
References: <CY4PR18MB1336647CD2933E09DE55EDACA9AF0@CY4PR18MB1336.namprd18.prod.outlook.com>
	<CY4PR18MB13364623B39D09C179B78896A9AC0@CY4PR18MB1336.namprd18.prod.outlook.com>
Message-ID: <BLUPR02MB43710B1F34C91EA3DE29384E5AC0@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Jake,

Many thanks for updating your issue with the diagnosis, I will raise it on my end and hopefully it will also be of some help to future readers of the mailing list.

Project Clearwater offers varying levels of support for the IR.92 supplementary call services<https://www.gsma.com/newsroom/wp-content/uploads/IR.92-v9.0.pdf>. This document<http://clearwater.readthedocs.io/en/stable/IR.92_Supplementary_Services.html> outlines what we do support.

We do support call waiting (aka communication waiting) which is implemented by the UE. This means in order to test call waiting you will require a SIP client which supports this feature.

Conference calling (aka Ad-Hoc Multi Party Conference) would require an external application server.

Please let me know if you have any further questions.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Jake Brown
Sent: Thursday, July 13, 2017 3:23 PM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Using Ellis to program call diversion isn't working

This ended up being a firefox vs Chrome issue.

It doesn't work with Firefox, and works properly with Chrome.

How do I enable conference calling, and call waiting?

Jake Brown
Principal Engineer/Owner
920-351-4054 x1001
jake at dccllc.net<mailto:jake at dccllc.net>
www.dccllc.net<http://www.dccllc.net/>

[Signature-1]

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Jake Brown
Sent: Wednesday, July 12, 2017 3:10 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Using Ellis to program call diversion isn't working


This sender failed our fraud detection checks and may not be who they appear to be. Learn about spoofing<http://aka.ms/LearnAboutSpoofing>

Feedback<http://aka.ms/SafetyTipsFeedback>

I am trying to use Ellis to provision the call diversion settings for subscribers.  I can enable and disable barring, but am unable to enter new call diversion rules.   When logged into Ellis,  I click on the public identity and then go to redirect.  I enter the new rules and then click save changes.   Nothing seems to happen.  I have checked the vellum db and see that there are only entries for barring and privacy settings.

Here is the call that happens when I click config in ellis.  It queries Homer for the subscriber info.

This invokes the following get to the xdms server:
12-07-2017 19:07:02.649 UTC INFO xdm.py:54: Sending HTTP GET request to http://xdms.ims.mncXXX.mccXXX.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs.xml<http://xdms.ims.mnc560.mcc311.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mnc560.mcc311.3gppnetwork.org/simservs.xml>
12-07-2017 19:07:02.651 UTC INFO web.py:1447: 200 GET /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/listed?cb=2604effcc6M15 (0.0.0.0) 1.68ms
12-07-2017 19:07:02.653 UTC INFO homestead.py:297: Sending HTTP GET request to http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/public/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/service_profile<http://hss-cache.ims.mnc560.mcc311.3gppnetwork.org:8889/public/sip%3A13204705108%40ims.mnc560.mcc311.3gppnetwork.org/service_profile>
12-07-2017 19:07:02.661 UTC INFO homestead.py:278: Sending HTTP GET request to http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria<http://hss-cache.ims.mnc560.mcc311.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria>
12-07-2017 19:07:02.662 UTC DEBUG static.py:80: Checking if js/app-servers.json is allowed
12-07-2017 19:07:02.664 UTC INFO web.py:1447: 200 GET /js/app-servers.json?cb=2604effcc6M17 (0.0.0.0) 1.77ms
12-07-2017 19:07:02.668 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.01893901824951172,buffer=<_io.BytesIO object at 0x7fe4c5901b30>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7fe4c58826d0>,effective_url='http://xdms.ims.mncXXX.mccXXX.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs.xml',headers={'Content-Length': '338', 'Content-Encoding': 'gzip', 'Server': 'nginx/1.4.6 (Ubuntu)', 'Connection': 'close', 'Etag': '"8cc8f6a2314a88e52ca3d3ee9d1bc2de04046c0a"', 'Date': 'Wed, 12 Jul 2017 19:07:02 GMT', 'Content-Type': 'text/html; charset=UTF-8'},error=None)
12-07-2017 19:07:02.668 UTC DEBUG utils.py:92: All requests successful.
12-07-2017 19:07:02.668 UTC DEBUG numbers.py:484: Successfully fetched from Homer (simservs)
12-07-2017 19:07:02.669 UTC INFO web.py:1447: 200 GET /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs?cb=2604effcc6M14 (0.0.0.0) 22.29ms
12-07-2017 19:07:02.669 UTC DEBUG utils.py:94: Still expecting 0 callbacks
12-07-2017 19:07:02.669 UTC DEBUG homestead.py:282: Received response from http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria<http://hss-cache.ims.mnc560.mcc311.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria> with code 200
12-07-2017 19:07:02.669 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.008014917373657227,buffer=<_io.BytesIO object at 0x7fe4c5901710>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7fe4c58e2210>,effective_url='http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria',headers={'Content-Length': '295', 'Content-Encoding': 'gzip', 'Server': 'nginx/1.4.6 (Ubuntu)', 'Connection': 'close', 'Etag': '"10b41239aa6911d9689d6e979a695ed552f6ede5"', 'Date': 'Wed, 12 Jul 2017 19:07:02 GMT', 'Content-Type': 'text/html; charset=UTF-8'},error=None)
12-07-2017 19:07:02.670 UTC DEBUG utils.py:92: All requests successful.
12-07-2017 19:07:02.670 UTC DEBUG numbers.py:484: Successfully fetched from Homestead (iFC)
12-07-2017 19:07:02.670 UTC INFO web.py:1447: 200 GET /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/ifcs?cb=2604effcc6M16 (0.0.0.0) 18.44ms
12-07-2017 19:07:02.670 UTC DEBUG utils.py:94: Still expecting 0 callbacks

Here is the info that is retrieved from Vellum:

sip:13204705108 at ims.mncXXX.mccXXX.3gppnetwork.org<sip:13204705108 at ims.mnc560.mcc311.3gppnetwork.org> | <?xml version="1.0" encoding="UTF-8"?>\r\n<simservs xmlns="http://uri.etsi.org/ngn/params/xml/simservs/xcap" xmlns:cp="urn:ietf:params:xml:ns:common-policy"><originating-identity-png-communication-barring><outgoing-communication-barring active="true"><cp:ruleset><cp:rule id="rule0"><cp:conditions/><cp:actions><allow>true</allow></cp:actions></cp:rule></cp:ruleset></outgoing-communication-barring></simservs>


Here is the put from Ellis when I change the redirect settings:

12-07-2017 20:05:37.115 UTC INFO xdm.py:54: Sending HTTP PUT request to http://xdms.ims.mncXXX.mccXXX.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs.xml
12-07-2017 20:05:37.120 UTC INFO web.py:1447: 200 PUT /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/listed/1/?cb=26051e9852_12 (0.0.0.0) 3.63ms
12-07-2017 20:05:37.121 UTC INFO homestead.py:297: Sending HTTP GET request to http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/public/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/service_profile
12-07-2017 20:05:37.127 UTC INFO homestead.py:278: Sending HTTP PUT request to http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria
12-07-2017 20:05:37.132 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.016077041625976562,buffer=<_io.BytesIO object at 0x7fe4c6391ef0>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7fe4c59038d0>,effective_url='http://xdms.ims.mncXXX.mccXXX.3gppnetwork.org:7888/org.etsi.ngn.simservs/users/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs.xml',headers={'Date': 'Wed, 12 Jul 2017 20:05:37 GMT', 'Content-Length': '2', 'Content-Type': 'application/json', 'Connection': 'close', 'Server': 'nginx/1.4.6 (Ubuntu)'},error=None)
12-07-2017 20:05:37.132 UTC DEBUG utils.py:92: All requests successful.
12-07-2017 20:05:37.132 UTC DEBUG numbers.py:502: Successfully updated Homer (simservs)
12-07-2017 20:05:37.132 UTC INFO web.py:1447: 200 PUT /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/simservs?cb=26051e9852_10 (0.0.0.0) 19.73ms
12-07-2017 20:05:37.133 UTC DEBUG utils.py:94: Still expecting 0 callbacks
12-07-2017 20:05:37.158 UTC DEBUG homestead.py:282: Received response from http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria with code 200
12-07-2017 20:05:37.158 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.029946088790893555,buffer=<_io.BytesIO object at 0x7fe4c5901410>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7fe4c5872a10>,effective_url='http://hss-cache.ims.mncXXX.mccXXX.3gppnetwork.org:8889/irs/dd439a12-1e78-4a39-a031-887992f82d83/service_profiles/fa8a23be-0581-4d06-b12b-7c606e395b67/filter_criteria',headers={'Date': 'Wed, 12 Jul 2017 20:05:37 GMT', 'Content-Length': '0', 'Content-Type': 'text/html; charset=UTF-8', 'Connection': 'close', 'Server': 'nginx/1.4.6 (Ubuntu)'},error=None)
12-07-2017 20:05:37.158 UTC DEBUG utils.py:92: All requests successful.
12-07-2017 20:05:37.158 UTC DEBUG numbers.py:502: Successfully updated Homestead (iFC)
12-07-2017 20:05:37.158 UTC INFO web.py:1447: 200 PUT /accounts/jake%40dccllc.net/numbers/sip%3A13204705108%40ims.mncXXX.mccXXX.3gppnetwork.org/ifcs?cb=26051e9852_11 (0.0.0.0) 41.91ms

The problem is...nothing is written for the redirect.

Can you please help point me in the correct direction.  I'm either looking to sort out why Ellis is doing this, or if I could directly send this data to homer and skip Ellis. (I use an HSS so don't need the IFC functionality...just need the xcap functions)


Jake Brown
Principal Engineer/Owner
920-351-4054 x1001
jake at dccllc.net<mailto:jake at dccllc.net>
www.dccllc.net<http://www.dccllc.net/>

[Signature-1]

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170713/63c2731a/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 2630 bytes
Desc: image001.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170713/63c2731a/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 2635 bytes
Desc: image002.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170713/63c2731a/attachment-0001.jpg>

From hkaranjikar at apm.com  Fri Jul 14 06:16:41 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Fri, 14 Jul 2017 15:46:41 +0530
Subject: [Project Clearwater] restund_process Execution failed on bono
In-Reply-To: <BLUPR02MB437BB7A07944A9B02AA3075E5AC0@BLUPR02MB437.namprd02.prod.outlook.com>
References: <CABmBNEafr-WsM7_E3i7cbhJujejjqfgs9aFp2dTRdr=R7Lsmww@mail.gmail.com>
	<BLUPR02MB4375A20455D0317FDCA0A97E5AE0@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEY-Saqs0REksQqk9=vfE=sOb8GtwV31JQKftBYn_y1NXQ@mail.gmail.com>
	<CABmBNEZB2HzhJZT9bFCjLFiX_W-NgigK7DCb5T64Yc7Jnj__eg@mail.gmail.com>
	<BLUPR02MB437BB7A07944A9B02AA3075E5AC0@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <CABmBNEZpKw62fEBwm_omy05nsOdhnQrPVhwhxn7+s_V6LJErWw@mail.gmail.com>

Hi Andrew,

Thanks for your support.
I am not in a position to modify the current dns server nor our admins
would do it.
I have added entry for icscf.cwsprout in /etc/hosts. I am able to ping
icscf.cwsprout after adding this entry.
Still i get same error in the log.

===========================================================
cwbono at cwbono:~$ ping icscf.cwsprout
PING icscf.cwsprout (10.48.12.143) 56(84) bytes of data.
64 bytes from cwsprout (10.48.12.143): icmp_seq=1 ttl=64 time=0.157 ms
64 bytes from cwsprout (10.48.12.143): icmp_seq=2 ttl=64 time=0.129 ms
^C
--- icscf.cwsprout ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.129/0.143/0.157/0.014 ms

cwbono at cwbono:~$ cat /var/log/bono/bono_current.txt

14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to
resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

===========================================================

If this is not working then I might have to create a dns server locally.
Then i would be able to modify or add entries
Is it possible for you to give me an example of how these entries are added?

Thanks
Hrishikesh

On Thu, Jul 13, 2017 at 9:30 PM, Andrew Edmonds <
Andrew.Edmonds at metaswitch.com> wrote:

> Hi Hrishikesh,
>
>
>
> Thank you for the updated diagnostics.
>
>
>
> We can still see the following appearing in log files:
>
>
>
> ?Failed to resolve icscf.cwsprout to an IP address?
>
>
>
> The icscf.<sprout hostname> DNS record is used by the P-CSCF (in this case
> Bono) to identify which I-CSCF to forward requests on to.
>
>
>
> Even when assigning nodes IP addresses through DHCP you must still
> configure your DNS with all the records that Clearwater nodes require to
> communicate with each other (such as icscf.<sprout hostname>). You can find
> a list of all the records that Clearwater requires here
> <http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html>.
>
>
>
> Please could you try updating your DNS server to contain these records and
> restart your nodes, let me know if you still hit the issue.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, July 13, 2017 10:40 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] restund_process Execution failed on
> bono
>
>
>
> Hi,
>
> I have changed my deployment completely.
>
> No I am not using static IP addresses any more and each node is getting IP
> addresses from the DHCP server.
>
> Each node is able to ping one another using hostname cwellis, cwsprout,
> cwbono, cwvellum, cwhomer, cwdime.
>
> From bono node I am able to ping, ssh to cwsprout. Check the logs,
>
> ============================================================
> ===================
>
> cwbono at cwbono:~$ ping cwsprout
> PING cwsprout.amcc.com (10.48.12.143) 56(84) bytes of data.
> 64 bytes from cwsprout.amcc.com (10.48.12.143): icmp_seq=1 ttl=64
> time=0.126 ms
> 64 bytes from cwsprout.amcc.com (10.48.12.143): icmp_seq=2 ttl=64
> time=0.203 ms
> 64 bytes from cwsprout.amcc.com (10.48.12.143): icmp_seq=3 ttl=64
> time=0.210 ms
>
> cwbono at cwbono:~$ ssh cwsprout at cwsprout
> The authenticity of host 'cwsprout (10.48.12.143)' can't be established.
> ECDSA key fingerprint is 03:f8:42:81:36:ef:b1:be:7a:d8:3d:52:b5:74:f9:ba.
> Are you sure you want to continue connecting (yes/no)? yes
> Warning: Permanently added 'cwsprout,10.48.12.143' (ECDSA) to the list of
> known hosts.
> cwsprout at cwsprout's password:
> Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-31-generic x86_64)
>
>  * Documentation:  https://help.ubuntu.com/
>
>   System information as of Thu Jul 13 14:08:07 IST 2017
>
>   System load:  0.12              Processes:           115
>   Usage of /:   27.7% of 7.26GB   Users logged in:     0
>   Memory usage: 8%                IP address for eth0: 10.48.12.143
>   Swap usage:   0%
>
>   Graph this data and manage this system at:
>     https://landscape.canonical.com/
>
> New release '16.04.2 LTS' available.
> Run 'do-release-upgrade' to upgrade to it.
>
> Last login: Thu Jul 13 14:08:07 2017 from hdk-supermicro.amcc.com
> [sprout]cwsprout at cwsprout:~$
>
> [sprout]cwsprout at cwsprout:~$ exit
> logout
> Connection to cwsprout closed.
> cwbono at cwbono:~$
> cwbono at cwbono:~$
> cwbono at cwbono:~$ cat /etc/clearwater/local_config
> local_ip=10.48.12.173
> public_ip=10.48.12.173
> public_hostname=cwbono
> etcd_cluster="10.48.12.142,10.48.12.143,10.48.12.173,10.48.
> 12.140,10.48.12.139,10.48.12.120"
> cwbono at cwbono:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout
> sprout_registration_store=10.48.12.120 #vellum
> hs_hostname=10.48.12.139:8888 #dime
> hs_provisioning_hostname=10.48.12.139:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=10.48.12.140:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=10.48.12.120 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> cwbono at cwbono:~$ clearwater-etcdctl cluster-health
> member 2d821d7a0a7736b4 is healthy: got healthy result from
> http://10.48.12.142:4000
> member 6201151ee7f99f5c is healthy: got healthy result from
> http://10.48.12.139:4000
> member 895efb70c4b1b8b4 is healthy: got healthy result from
> http://10.48.12.120:4000
> member c7f5d6485fb4735b is healthy: got healthy result from
> http://10.48.12.143:4000
> member f7726a4e29ec7d3d is healthy: got healthy result from
> http://10.48.12.173:4000
> member ffb968d2990c63f0 is healthy: got healthy result from
> http://10.48.12.140:4000
> cluster is healthy
> cwbono at cwbono:~$ clearwater-etcdctl member list
> 2d821d7a0a7736b4: name=10-48-12-142 peerURLs=http://10.48.12.142:2380
> clientURLs=http://10.48.12.142:4000 isLeader=false
> 6201151ee7f99f5c: name=10-48-12-139 peerURLs=http://10.48.12.139:2380
> clientURLs=http://10.48.12.139:4000 isLeader=false
> 895efb70c4b1b8b4: name=10-48-12-120 peerURLs=http://10.48.12.120:2380
> clientURLs=http://10.48.12.120:4000 isLeader=false
> c7f5d6485fb4735b: name=10-48-12-143 peerURLs=http://10.48.12.143:2380
> clientURLs=http://10.48.12.143:4000 isLeader=false
> f7726a4e29ec7d3d: name=10-48-12-173 peerURLs=http://10.48.12.173:2380
> clientURLs=http://10.48.12.173:4000 isLeader=true
> ffb968d2990c63f0: name=10-48-12-140 peerURLs=http://10.48.12.140:2380
> clientURLs=http://10.48.12.140:4000 isLeader=false
>
> cwbono at cwbono:~$ cw-check_cluster_state
> This script prints the status of the Cassandra, Chronos, and Memcached
> clusters.
> This node (10.48.12.173) should not be in any cluster.
>
> Describing the Cassandra cluster:
>   The cluster is stable
>     10.48.12.120 is in state normal
>
> Describing the Chronos cluster:
>   The cluster is stable
>     10.48.12.120 is in state normal
>
> Describing the Memcached cluster:
>   The cluster is stable
>     10.48.12.120 is in state normal
>
> cwbono at cwbono:~$ sudo cw-check_config_sync
>  - /etc/clearwater/dns.json is up to date
>  - /etc/clearwater/shared_config is up to date
>
> cwbono at cwbono:~$ sudo monit summary
> Monit 5.18.1 uptime: 54m
>  Service Name                     Status
> Type
>  node-cwbono                      Running
> System
>  restund_process                  Execution failed | Does...
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Wait parent
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
>
> cwbono at cwbono:~$ cat /var/log/bono/bono_current.txt
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>
> cwbono at cwbono:~$ cat /var/log/monit.log
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:03:29] error    : 'restund_process' process is not running
> [IST Jul 13 15:03:29] info     : 'restund_process' trying to restart
> [IST Jul 13 15:03:29] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul 13 15:04:00] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=10.48.12.173:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:04:10] error    : 'restund_process' process is not running
> [IST Jul 13 15:04:10] info     : 'restund_process' trying to restart
> [IST Jul 13 15:04:10] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul 13 15:04:40] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=10.48.12.173:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:04:50] error    : 'restund_process' process is not running
> [IST Jul 13 15:04:50] info     : 'restund_process' trying to restart
> [IST Jul 13 15:04:50] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul 13 15:05:20] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=10.48.12.173:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:05:30] error    : 'restund_process' process is not running
> [IST Jul 13 15:05:30] info     : 'restund_process' trying to restart
> [IST Jul 13 15:05:30] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul 13 15:06:01] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=10.48.12.173:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:06:11] error    : 'restund_process' process is not running
> [IST Jul 13 15:06:11] info     : 'restund_process' trying to restart
> [IST Jul 13 15:06:11] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul 13 15:06:41] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=10.48.12.173:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:06:51] error    : 'restund_process' process is not running
> [IST Jul 13 15:06:51] info     : 'restund_process' trying to restart
> [IST Jul 13 15:06:51] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul 13 15:07:21] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=10.48.12.173:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:07:31] error    : 'restund_process' process is not running
> [IST Jul 13 15:07:31] info     : 'restund_process' trying to restart
> [IST Jul 13 15:07:31] info     : 'restund_process' restart:
> /etc/init.d/restund
>
> ============================================================
> ===================
>
> I don't know whats going wrong here.
>
> What is icscf.cwsprout?
>
>
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
> On Tue, Jul 11, 2017 at 5:08 PM, Hrishikesh Karanjikar <
> hkaranjikar at apm.com> wrote:
>
> Hi,
>
> Thanks a lot for your reply.
>
> I am using virtualbox and host only network.
>
> The DHCP server runs inside virtualbox and I can only configure the IP
> address range.
>
> However I am not using the DHCP server and assigning static IP addresses
> to all nodes which are within the DHCP server IP address range.
>
> I added entry of cwsprout in /ets/hosts of bono node as follows,
>
> ===============================================
>
> 127.0.0.1       localhost
> 127.0.1.1       cwbono
>
> *192.168.56.103  cwsprout 192.168.56.103  icscf.cwsprout*
>
> # The following lines are desirable for IPv6 capable hosts
> ::1     localhost ip6-localhost ip6-loopback
> ff02::1 ip6-allnodes
> ff02::2 ip6-allrouters
> 192.168.56.103 cwsprout
> 192.168.56.103  icscf.cwsprout
> ::1 localhost # added by clearwater-infrastructure 1hosts script
> 192.168.56.104 cwbono #+clearwater-infrastructure
>
> ===============================================
>
> I am also able to ping cwsprout and icscf.cwsprout from bono,
>
> Here is the log,
>
> ===============================================
>
> [bono]cwbono at cwbono:~$ ping cwsprout
> PING cwsprout (192.168.56.103) 56(84) bytes of data.
> 64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.197 ms
> 64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.217 ms
> ^C
> --- cwsprout ping statistics ---
> 2 packets transmitted, 2 received, 0% packet loss, time 999ms
> rtt min/avg/max/mdev = 0.197/0.207/0.217/0.010 ms
> [bono]cwbono at cwbono:~$
> [bono]cwbono at cwbono:~$
> [bono]cwbono at cwbono:~$
> [bono]cwbono at cwbono:~$ ping icscf.cwsprout
> PING icscf.cwsprout (192.168.56.103) 56(84) bytes of data.
> 64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.112 ms
> 64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.165 ms
>
> ===============================================
>
> I am using static IP addresses as I have to specify them in local_config
> file of each node.
>
> If I use DHCP server of VirtualBox they might change. In that case I am
> not sure how do I cope up with local_config.
>
> Can I modify local_config after all nodes are up?
>
>
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
> On Tue, Jul 11, 2017 at 2:42 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
> Hi Hrishikesh,
>
>
>
> Thank you for your question and the detailed logs you have provided.
>
>
>
> The issues appears to be caused by your shared config, in the manual
> installation instructions
> <http://clearwater.readthedocs.io/en/stable/Manual_Install.html> you?ll
> see that we advise that the entries in shared config have a format like:
>
>
>
> sprout_hostname*=*sprout*.<*site_name*>.<*zone*>*
>
> sprout_registration_store*=*vellum*.<*site_name*>.<*zone*>*
>
> hs_hostname*=*hs*.<*site_name*>.<*zone*>*:8888
>
>
>
> In your shared config you have:
>
>
>
> sprout_hostname=cwsprout
> sprout_registration_store=192.168.56.107 #vellum
>
> I don?t think the sprout_hostname used here will resolve, we can see
> evidence for this in the Bono logs, Bono attempts to resolve
> icscf.<sprout_hostname> to find which location to forward SIP messages on
> you, you can see it is failing to do that here:
>
>
>
> Failed to resolve icscf.cwsprout to an IP address - Not found
> (PJ_ENOTFOUND)
>
>
>
> To resolve this issue you should change your shared config to use
> hostnames which have been configured in your DNS server to resolve to the
> appropriate location. Once you have done this run the command ?sudo
> cw-upload_shared_config?. Please let me know if this does not resolve the
> issue.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, July 6, 2017 10:22 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] restund_process Execution failed on bono
>
>
>
> Hello,
>
> I have Manually installed all 6 nodes on VMs using virtualbox.
>
> I followed the procedure given @ http://clearwater.readthedocs.
> io/en/stable/Manual_Install.html
>
> Looks like all nodes except Dime are running fine.
>
> I am getting error "restund_process Execution failed" in monit summary on
> Bono node.
>
> Here is the shared and local config file,
>
> #####################################################################
>
> [bono]cwbono at cwbono:~$ cat /etc/clearwater/shared_config
>
> home_domain=example.com
> sprout_hostname=cwsprout
> sprout_registration_store=192.168.56.107 #vellum
> hs_hostname=192.168.56.106:8888 #dime
> hs_provisioning_hostname=192.168.56.106:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.105:7888 #homer
> chronos_hostname=192.168.56.107 #vellum
> cassandra_hostname=192.168.56.107 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
> [bono]cwbono at cwbono:~$ cat /etc/clearwater/local_config
> local_ip=192.168.56.104
> public_ip=192.168.56.104
> public_hostname=cwbono
> etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,
> 192.168.56.105,192.168.56.106,192.168.56.107"
>
> #####################################################################
>
> The logs for bono node are as follows,
>
> #####################################################################
> [bono]cwbono at cwbono:~$ sudo monit summary
> [sudo] password for cwbono:
> Monit 5.18.1 uptime: 2d 16h 39m
>  Service Name                     Status
> Type
>  node-cwbono                      Running
> System
>  restund_process                  Execution failed | Does...
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Wait parent
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
>
>
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.843 UTC Status main.cpp:1358: Quiesce signal received
> 05-07-2017 11:30:15.843 UTC Status stack.cpp:125: Setting quiescing =
> PJ_TRUE
> 05-07-2017 11:30:15.851 UTC Status stack.cpp:156: Quiescing state changed
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The
> Quiescing Manager received input QUIESCE (0) when in state ACTIVE (0)
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:265: Close
> untrusted listening port
> 05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport
> for port 5060
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:273: Quiesce
> FlowTable
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The
> Quiescing Manager received input FLOWS_GONE (1) when in state
> QUIESCING_FLOWS (1)
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:290: Closing
> trusted port
> 05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport
> for port 5058
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:296: Quiescing
> all connections
> 05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:162: Start
> quiescing connections
> 05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:175: Quiescing 0
> transactions
> 05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:180: Connection
> quiescing complete
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The
> Quiescing Manager received input CONNS_GONE (2) when in state
> QUIESCING_CONNS (2)
> 05-07-2017 11:30:15.851 UTC Status main.cpp:1380: Quiesce complete
> 05-07-2017 11:30:15.853 UTC Status stack.cpp:171: PJSIP thread ended
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>
> [bono]cwbono at cwbono:~$ cat /var/log/monit.log
>
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=192.168.56.104:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in u
> [IST Jul  6 14:47:46] error    : 'restund_process' process is not running
> [IST Jul  6 14:47:46] info     : 'restund_process' trying to restart
> [IST Jul  6 14:47:46] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul  6 14:48:16] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=192.168.56.104:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in u
> [IST Jul  6 14:48:26] error    : 'restund_process' process is not running
> [IST Jul  6 14:48:26] info     : 'restund_process' trying to restart
> [IST Jul  6 14:48:26] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul  6 14:48:56] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=192.168.56.104:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in u
> [IST Jul  6 14:49:06] error    : 'restund_process' process is not running
> [IST Jul  6 14:49:06] info     : 'restund_process' trying to restart
> [IST Jul  6 14:49:06] info     : 'restund_process' restart:
> /etc/init.d/restund
>
>
> [bono]cwbono at cwbono:~$ clearwater-etcdctl cluster-health
> member 9c1928228d308a0f is healthy: got healthy result from
> http://192.168.56.107:4000
> member b0c9c017e0d47e14 is healthy: got healthy result from
> http://192.168.56.106:4000
> member d44832212a08c43f is healthy: got healthy result from
> http://192.168.56.103:4000
> member ef1a9a8a2fd05283 is healthy: got healthy result from
> http://192.168.56.104:4000
> member f63afbe816fb463d is healthy: got healthy result from
> http://192.168.56.102:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono at cwbono:~$ cw-check_cluster_state
> This script prints out the status of the Chronos, Memcached and Cassandra
> clusters.
>
> Describing the Vellum Chronos cluster:
>   The local node is *not* in this cluster
>   The cluster is stable
>     192.168.56.107 is in state normal
>
> Describing the Vellum Memcached cluster:
>   The local node is *not* in this cluster
>   The cluster is stable
>     192.168.56.107 is in state normal
>
> Describing the Vellum Cassandra cluster:
>   The local node is *not* in this cluster
>   The cluster is stable
>     192.168.56.107 is in state normal
>
> [bono]cwbono at cwbono:~$ clearwater-etcdctl member list
> 9c1928228d308a0f: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> b0c9c017e0d47e14: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=true
> d44832212a08c43f: name=192-168-56-103 peerURLs=http://192.168.56.103:2380
> clientURLs=http://192.168.56.103:4000 isLeader=false
> ef1a9a8a2fd05283: name=192-168-56-104 peerURLs=http://192.168.56.104:2380
> clientURLs=http://192.168.56.104:4000 isLeader=false
> f63afbe816fb463d: name=192-168-56-102 peerURLs=http://192.168.56.102:2380
> clientURLs=http://192.168.56.102:4000 isLeader=false
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono at cwbono:~$ sudo cw-check_config_sync
> [sudo] password for cwbono:
>  - /etc/clearwater/dns.json is up to date
>  - /etc/clearwater/shared_config is up to date
>
>
> #####################################################################
>
> The logs for other nodes are as follows
>
> #####################################################################
>
> [ellis]cwellis at cwellis:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 21h 15m
>  Service Name                     Status
> Type
>  node-cwellis                     Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>
>
> [sprout]cwsprout at cwsprout:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 20h 7m
>  Service Name                     Status
> Type
>  node-cwsprout                    Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok                   Program
>
>
> [homer]cwhomer at cwhomer:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 20h 2m
>  Service Name                     Status
> Type
>  node-cwhomer                     Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok                   Program
>
> [dime]cwdime at cwdime:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 20h 2m
>  Service Name                     Status
> Type
>  node-cwdime                      Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>
>
> [vellum]cwvellum at cwvellum:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 20h 3m
>  Service Name                     Status
> Type
>  node-cwvellum                    Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
>
>
> #####################################################################
>
> Please let me know if I am missing any configuration.
>
> Thanks
>
> Hrishikesh
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170714/36819b81/attachment.html>

From arvindas at hpe.com  Mon Jul 17 04:49:16 2017
From: arvindas at hpe.com (Shirabur, Aravind Ashok (CMS))
Date: Mon, 17 Jul 2017 08:49:16 +0000
Subject: [Project Clearwater]  Sprout S-CSCF Forking disable
Message-ID: <TU4PR84MB006447E14F30E7DD5C97EFF1DCA00@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>

Hi,

How do we disable the S-CSCF Forking functionality ? more than one UE are registered to IMS-Core network with same IMPI and IMPU, S-CSCF try to fork the all registered UE for the MT call landing on this IMPU.


Thanks
Aravind

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170717/7addf40e/attachment.html>

From michele.furlanetto at aleagames.com  Tue Jul 18 07:58:12 2017
From: michele.furlanetto at aleagames.com (Michele Furlanetto)
Date: Tue, 18 Jul 2017 13:58:12 +0200
Subject: [Project Clearwater] [Clearwater] AS Configuration and untrusted
 sources
In-Reply-To: <7296A131-7DDD-43F1-9F00-5F85FCF40FF7@aleagames.com>
References: <7296A131-7DDD-43F1-9F00-5F85FCF40FF7@aleagames.com>
Message-ID: <3ACF4837-B492-45A2-8B83-14202C9CFDEE@aleagames.com>

Hi all,
I?ve got a problem while configuring an AIO image.

In my scenario, there are is an AS, say as1.service.example.com <http://as1.service.example.com/>, which should get the third-party registration as well as being able to be addressed directly from the Sip client.

While third-party registration works, I?m unable to contact the AS directly.
Looking at Bono logs, I get Info bono.cpp:1356: Rejecting request from untrusted source.

Here is the content of /usr/share/clearwater/ellis/web-content/js/app-servers.json, indented to be more readable
{
?SERVICE?:"
<InitialFilterCriteria>
	<Priority>0</Priority>
	<TriggerPoint>
		<ConditionTypeCNF>0</ConditionTypeCNF>
		<SPT>
			<ConditionNegated>0</ConditionNegated>
			<Group>0</Group>
			<Method>REGISTER</Method>
			<Extension>
				<RegistrationType>0</RegistrationType>
				<RegistrationType>1</RegistrationType>
				<RegistrationType>2</RegistrationType>
			</Extension>
		</SPT>
		<SPT>
			<ConditionNegated>0</ConditionNegated>
			<Group>1</Group>
			<RequestURI>sip:as1.service.example.com <http://service.example.com/></RequestURI>
		</SPT>
	</TriggerPoint>
	<ApplicationServer>
		<ServerName>sip:as1.service.example.com <http://service.example.com/>:5071;transport=UDP</ServerName>
		<DefaultHandling>0</DefaultHandling>
		<Extension>
			<IncludeRegisterRequest/>
		</Extension>
	</ApplicationServer>
</InitialFilterCriteria>"
}

Some other details:
- SERVICE is checked in Ellis for test users;
- as1.service.example.com <http://as1.service.example.com/> is resolved using /etc/hosts.


Any hint on the error(s)?
Thanks,
Michele
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170718/55c68a84/attachment.html>

From Andrew.Edmonds at metaswitch.com  Thu Jul 20 06:42:05 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Thu, 20 Jul 2017 10:42:05 +0000
Subject: [Project Clearwater] restund_process Execution failed on bono
In-Reply-To: <CABmBNEZpKw62fEBwm_omy05nsOdhnQrPVhwhxn7+s_V6LJErWw@mail.gmail.com>
References: <CABmBNEafr-WsM7_E3i7cbhJujejjqfgs9aFp2dTRdr=R7Lsmww@mail.gmail.com>
	<BLUPR02MB4375A20455D0317FDCA0A97E5AE0@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEY-Saqs0REksQqk9=vfE=sOb8GtwV31JQKftBYn_y1NXQ@mail.gmail.com>
	<CABmBNEZB2HzhJZT9bFCjLFiX_W-NgigK7DCb5T64Yc7Jnj__eg@mail.gmail.com>
	<BLUPR02MB437BB7A07944A9B02AA3075E5AC0@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEZpKw62fEBwm_omy05nsOdhnQrPVhwhxn7+s_V6LJErWw@mail.gmail.com>
Message-ID: <BLUPR02MB437ACA7F2803577A6F9C8A7E5A70@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Hrishikesh,

Yes even with an entry for icscf.cwsprout in /etc/hosts I would still expect you to get the DNS error in Bono as there are still not entries for icscf.cwpsrout on your DNS server.

How to add entries to your DNS server will depend on which DNS server you have chosen to use, most of my testing has been done with a DNS server called BIND. You can find details on how to install BIND and how to configure a client to use it here<http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html?#bind>.

Please let me know if you have any further questions.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Hrishikesh Karanjikar
Sent: Friday, July 14, 2017 11:17 AM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] restund_process Execution failed on bono

Hi Andrew,
Thanks for your support.
I am not in a position to modify the current dns server nor our admins would do it.
I have added entry for icscf.cwsprout in /etc/hosts. I am able to ping icscf.cwsprout after adding this entry.
Still i get same error in the log.

===========================================================
cwbono at cwbono:~$ ping icscf.cwsprout
PING icscf.cwsprout (10.48.12.143) 56(84) bytes of data.
64 bytes from cwsprout (10.48.12.143): icmp_seq=1 ttl=64 time=0.157 ms
64 bytes from cwsprout (10.48.12.143): icmp_seq=2 ttl=64 time=0.129 ms
^C
--- icscf.cwsprout ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.129/0.143/0.157/0.014 ms

cwbono at cwbono:~$ cat /var/log/bono/bono_current.txt

14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

===========================================================
If this is not working then I might have to create a dns server locally. Then i would be able to modify or add entries
Is it possible for you to give me an example of how these entries are added?
Thanks
Hrishikesh

On Thu, Jul 13, 2017 at 9:30 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Thank you for the updated diagnostics.

We can still see the following appearing in log files:

?Failed to resolve icscf.cwsprout to an IP address?

The icscf.<sprout hostname> DNS record is used by the P-CSCF (in this case Bono) to identify which I-CSCF to forward requests on to.

Even when assigning nodes IP addresses through DHCP you must still configure your DNS with all the records that Clearwater nodes require to communicate with each other (such as icscf.<sprout hostname>). You can find a list of all the records that Clearwater requires here<http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html>.

Please could you try updating your DNS server to contain these records and restart your nodes, let me know if you still hit the issue.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, July 13, 2017 10:40 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] restund_process Execution failed on bono

Hi,
I have changed my deployment completely.
No I am not using static IP addresses any more and each node is getting IP addresses from the DHCP server.
Each node is able to ping one another using hostname cwellis, cwsprout, cwbono, cwvellum, cwhomer, cwdime.
From bono node I am able to ping, ssh to cwsprout. Check the logs,

===============================================================================

cwbono at cwbono:~$ ping cwsprout
PING cwsprout.amcc.com<http://cwsprout.amcc.com> (10.48.12.143) 56(84) bytes of data.
64 bytes from cwsprout.amcc.com<http://cwsprout.amcc.com> (10.48.12.143): icmp_seq=1 ttl=64 time=0.126 ms
64 bytes from cwsprout.amcc.com<http://cwsprout.amcc.com> (10.48.12.143): icmp_seq=2 ttl=64 time=0.203 ms
64 bytes from cwsprout.amcc.com<http://cwsprout.amcc.com> (10.48.12.143): icmp_seq=3 ttl=64 time=0.210 ms

cwbono at cwbono:~$ ssh cwsprout at cwsprout
The authenticity of host 'cwsprout (10.48.12.143)' can't be established.
ECDSA key fingerprint is 03:f8:42:81:36:ef:b1:be:7a:d8:3d:52:b5:74:f9:ba.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'cwsprout,10.48.12.143' (ECDSA) to the list of known hosts.
cwsprout at cwsprout's password:
Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-31-generic x86_64)

 * Documentation:  https://help.ubuntu.com/

  System information as of Thu Jul 13 14:08:07 IST 2017

  System load:  0.12              Processes:           115
  Usage of /:   27.7% of 7.26GB   Users logged in:     0
  Memory usage: 8%                IP address for eth0: 10.48.12.143
  Swap usage:   0%

  Graph this data and manage this system at:
    https://landscape.canonical.com/

New release '16.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Thu Jul 13 14:08:07 2017 from hdk-supermicro.amcc.com<http://hdk-supermicro.amcc.com>
[sprout]cwsprout at cwsprout:~$

[sprout]cwsprout at cwsprout:~$ exit
logout
Connection to cwsprout closed.
cwbono at cwbono:~$
cwbono at cwbono:~$
cwbono at cwbono:~$ cat /etc/clearwater/local_config
local_ip=10.48.12.173
public_ip=10.48.12.173
public_hostname=cwbono
etcd_cluster="10.48.12.142,10.48.12.143,10.48.12.173,10.48.12.140,10.48.12.139,10.48.12.120"
cwbono at cwbono:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout
sprout_registration_store=10.48.12.120 #vellum
hs_hostname=10.48.12.139:8888<http://10.48.12.139:8888> #dime
hs_provisioning_hostname=10.48.12.139:8889<http://10.48.12.139:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=10.48.12.140:7888<http://10.48.12.140:7888> #homer
chronos_hostname=vellum
cassandra_hostname=10.48.12.120 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


cwbono at cwbono:~$ clearwater-etcdctl cluster-health
member 2d821d7a0a7736b4 is healthy: got healthy result from http://10.48.12.142:4000
member 6201151ee7f99f5c is healthy: got healthy result from http://10.48.12.139:4000
member 895efb70c4b1b8b4 is healthy: got healthy result from http://10.48.12.120:4000
member c7f5d6485fb4735b is healthy: got healthy result from http://10.48.12.143:4000
member f7726a4e29ec7d3d is healthy: got healthy result from http://10.48.12.173:4000
member ffb968d2990c63f0 is healthy: got healthy result from http://10.48.12.140:4000
cluster is healthy
cwbono at cwbono:~$ clearwater-etcdctl member list
2d821d7a0a7736b4: name=10-48-12-142 peerURLs=http://10.48.12.142:2380 clientURLs=http://10.48.12.142:4000 isLeader=false
6201151ee7f99f5c: name=10-48-12-139 peerURLs=http://10.48.12.139:2380 clientURLs=http://10.48.12.139:4000 isLeader=false
895efb70c4b1b8b4: name=10-48-12-120 peerURLs=http://10.48.12.120:2380 clientURLs=http://10.48.12.120:4000 isLeader=false
c7f5d6485fb4735b: name=10-48-12-143 peerURLs=http://10.48.12.143:2380 clientURLs=http://10.48.12.143:4000 isLeader=false
f7726a4e29ec7d3d: name=10-48-12-173 peerURLs=http://10.48.12.173:2380 clientURLs=http://10.48.12.173:4000 isLeader=true
ffb968d2990c63f0: name=10-48-12-140 peerURLs=http://10.48.12.140:2380 clientURLs=http://10.48.12.140:4000 isLeader=false

cwbono at cwbono:~$ cw-check_cluster_state
This script prints the status of the Cassandra, Chronos, and Memcached clusters.
This node (10.48.12.173) should not be in any cluster.

Describing the Cassandra cluster:
  The cluster is stable
    10.48.12.120 is in state normal

Describing the Chronos cluster:
  The cluster is stable
    10.48.12.120 is in state normal

Describing the Memcached cluster:
  The cluster is stable
    10.48.12.120 is in state normal

cwbono at cwbono:~$ sudo cw-check_config_sync
 - /etc/clearwater/dns.json is up to date
 - /etc/clearwater/shared_config is up to date

cwbono at cwbono:~$ sudo monit summary
Monit 5.18.1 uptime: 54m
 Service Name                     Status                      Type
 node-cwbono                      Running                     System
 restund_process                  Execution failed | Does...  Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Wait parent                 Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program

cwbono at cwbono:~$ cat /var/log/bono/bono_current.txt
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

cwbono at cwbono:~$ cat /var/log/monit.log
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:03:29] error    : 'restund_process' process is not running
[IST Jul 13 15:03:29] info     : 'restund_process' trying to restart
[IST Jul 13 15:03:29] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:04:00] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:04:10] error    : 'restund_process' process is not running
[IST Jul 13 15:04:10] info     : 'restund_process' trying to restart
[IST Jul 13 15:04:10] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:04:40] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:04:50] error    : 'restund_process' process is not running
[IST Jul 13 15:04:50] info     : 'restund_process' trying to restart
[IST Jul 13 15:04:50] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:05:20] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:05:30] error    : 'restund_process' process is not running
[IST Jul 13 15:05:30] info     : 'restund_process' trying to restart
[IST Jul 13 15:05:30] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:06:01] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:06:11] error    : 'restund_process' process is not running
[IST Jul 13 15:06:11] info     : 'restund_process' trying to restart
[IST Jul 13 15:06:11] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:06:41] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:06:51] error    : 'restund_process' process is not running
[IST Jul 13 15:06:51] info     : 'restund_process' trying to restart
[IST Jul 13 15:06:51] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:07:21] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:07:31] error    : 'restund_process' process is not running
[IST Jul 13 15:07:31] info     : 'restund_process' trying to restart
[IST Jul 13 15:07:31] info     : 'restund_process' restart: /etc/init.d/restund

===============================================================================
I don't know whats going wrong here.
What is icscf.cwsprout?

Thanks
Hrishikesh


On Tue, Jul 11, 2017 at 5:08 PM, Hrishikesh Karanjikar <hkaranjikar at apm.com<mailto:hkaranjikar at apm.com>> wrote:
Hi,
Thanks a lot for your reply.
I am using virtualbox and host only network.
The DHCP server runs inside virtualbox and I can only configure the IP address range.
However I am not using the DHCP server and assigning static IP addresses to all nodes which are within the DHCP server IP address range.
I added entry of cwsprout in /ets/hosts of bono node as follows,

===============================================
127.0.0.1       localhost
127.0.1.1       cwbono
192.168.56.103  cwsprout
192.168.56.103  icscf.cwsprout

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
192.168.56.103 cwsprout
192.168.56.103  icscf.cwsprout
::1 localhost # added by clearwater-infrastructure 1hosts script
192.168.56.104 cwbono #+clearwater-infrastructure

===============================================
I am also able to ping cwsprout and icscf.cwsprout from bono,
Here is the log,

===============================================

[bono]cwbono at cwbono:~$ ping cwsprout
PING cwsprout (192.168.56.103) 56(84) bytes of data.
64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.197 ms
64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.217 ms
^C
--- cwsprout ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.197/0.207/0.217/0.010 ms
[bono]cwbono at cwbono:~$
[bono]cwbono at cwbono:~$
[bono]cwbono at cwbono:~$
[bono]cwbono at cwbono:~$ ping icscf.cwsprout
PING icscf.cwsprout (192.168.56.103) 56(84) bytes of data.
64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.112 ms
64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.165 ms
===============================================
I am using static IP addresses as I have to specify them in local_config file of each node.
If I use DHCP server of VirtualBox they might change. In that case I am not sure how do I cope up with local_config.
Can I modify local_config after all nodes are up?

Thanks
Hrishikesh


On Tue, Jul 11, 2017 at 2:42 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Thank you for your question and the detailed logs you have provided.

The issues appears to be caused by your shared config, in the manual installation instructions<http://clearwater.readthedocs.io/en/stable/Manual_Install.html> you?ll see that we advise that the entries in shared config have a format like:

sprout_hostname=sprout.<site_name>.<zone>
sprout_registration_store=vellum.<site_name>.<zone>
hs_hostname=hs.<site_name>.<zone>:8888

In your shared config you have:

sprout_hostname=cwsprout
sprout_registration_store=192.168.56.107 #vellum
I don?t think the sprout_hostname used here will resolve, we can see evidence for this in the Bono logs, Bono attempts to resolve icscf.<sprout_hostname> to find which location to forward SIP messages on you, you can see it is failing to do that here:

Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

To resolve this issue you should change your shared config to use hostnames which have been configured in your DNS server to resolve to the appropriate location. Once you have done this run the command ?sudo cw-upload_shared_config?. Please let me know if this does not resolve the issue.

Thanks,

Andrew


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, July 6, 2017 10:22 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] restund_process Execution failed on bono

Hello,
I have Manually installed all 6 nodes on VMs using virtualbox.
I followed the procedure given @ http://clearwater.readthedocs.io/en/stable/Manual_Install.html
Looks like all nodes except Dime are running fine.
I am getting error "restund_process Execution failed" in monit summary on Bono node.
Here is the shared and local config file,

#####################################################################

[bono]cwbono at cwbono:~$ cat /etc/clearwater/shared_config

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout
sprout_registration_store=192.168.56.107 #vellum
hs_hostname=192.168.56.106:8888<http://192.168.56.106:8888> #dime
hs_provisioning_hostname=192.168.56.106:8889<http://192.168.56.106:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.105:7888<http://192.168.56.105:7888> #homer
chronos_hostname=192.168.56.107 #vellum
cassandra_hostname=192.168.56.107 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret
[bono]cwbono at cwbono:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.104
public_ip=192.168.56.104
public_hostname=cwbono
etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,192.168.56.105,192.168.56.106,192.168.56.107"

#####################################################################
The logs for bono node are as follows,

#####################################################################
[bono]cwbono at cwbono:~$ sudo monit summary
[sudo] password for cwbono:
Monit 5.18.1 uptime: 2d 16h 39m
 Service Name                     Status                      Type
 node-cwbono                      Running                     System
 restund_process                  Execution failed | Does...  Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Wait parent                 Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program


05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.843 UTC Status main.cpp:1358: Quiesce signal received
05-07-2017 11:30:15.843 UTC Status stack.cpp:125: Setting quiescing = PJ_TRUE
05-07-2017 11:30:15.851 UTC Status stack.cpp:156: Quiescing state changed
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing Manager received input QUIESCE (0) when in state ACTIVE (0)
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:265: Close untrusted listening port
05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport for port 5060
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:273: Quiesce FlowTable
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing Manager received input FLOWS_GONE (1) when in state QUIESCING_FLOWS (1)
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:290: Closing trusted port
05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport for port 5058
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:296: Quiescing all connections
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:162: Start quiescing connections
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:175: Quiescing 0 transactions
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:180: Connection quiescing complete
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing Manager received input CONNS_GONE (2) when in state QUIESCING_CONNS (2)
05-07-2017 11:30:15.851 UTC Status main.cpp:1380: Quiesce complete
05-07-2017 11:30:15.853 UTC Status stack.cpp:171: PJSIP thread ended
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

[bono]cwbono at cwbono:~$ cat /var/log/monit.log

httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0<http://192.168.56.104:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:47:46] error    : 'restund_process' process is not running
[IST Jul  6 14:47:46] info     : 'restund_process' trying to restart
[IST Jul  6 14:47:46] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul  6 14:48:16] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0<http://192.168.56.104:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:48:26] error    : 'restund_process' process is not running
[IST Jul  6 14:48:26] info     : 'restund_process' trying to restart
[IST Jul  6 14:48:26] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul  6 14:48:56] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0<http://192.168.56.104:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:49:06] error    : 'restund_process' process is not running
[IST Jul  6 14:49:06] info     : 'restund_process' trying to restart
[IST Jul  6 14:49:06] info     : 'restund_process' restart: /etc/init.d/restund


[bono]cwbono at cwbono:~$ clearwater-etcdctl cluster-health
member 9c1928228d308a0f is healthy: got healthy result from http://192.168.56.107:4000
member b0c9c017e0d47e14 is healthy: got healthy result from http://192.168.56.106:4000
member d44832212a08c43f is healthy: got healthy result from http://192.168.56.103:4000
member ef1a9a8a2fd05283 is healthy: got healthy result from http://192.168.56.104:4000
member f63afbe816fb463d is healthy: got healthy result from http://192.168.56.102:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono at cwbono:~$ cw-check_cluster_state
This script prints out the status of the Chronos, Memcached and Cassandra clusters.

Describing the Vellum Chronos cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

Describing the Vellum Memcached cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

Describing the Vellum Cassandra cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

[bono]cwbono at cwbono:~$ clearwater-etcdctl member list
9c1928228d308a0f: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
b0c9c017e0d47e14: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=true
d44832212a08c43f: name=192-168-56-103 peerURLs=http://192.168.56.103:2380 clientURLs=http://192.168.56.103:4000 isLeader=false
ef1a9a8a2fd05283: name=192-168-56-104 peerURLs=http://192.168.56.104:2380 clientURLs=http://192.168.56.104:4000 isLeader=false
f63afbe816fb463d: name=192-168-56-102 peerURLs=http://192.168.56.102:2380 clientURLs=http://192.168.56.102:4000 isLeader=false
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono at cwbono:~$ sudo cw-check_config_sync
[sudo] password for cwbono:
 - /etc/clearwater/dns.json is up to date
 - /etc/clearwater/shared_config is up to date


#####################################################################
The logs for other nodes are as follows

#####################################################################

[ellis]cwellis at cwellis:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 21h 15m
 Service Name                     Status                      Type
 node-cwellis                     Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[sprout]cwsprout at cwsprout:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 7m
 Service Name                     Status                      Type
 node-cwsprout                    Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[homer]cwhomer at cwhomer:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 2m
 Service Name                     Status                      Type
 node-cwhomer                     Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program

[dime]cwdime at cwdime:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 2m
 Service Name                     Status                      Type
 node-cwdime                      Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[vellum]cwvellum at cwvellum:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 3m
 Service Name                     Status                      Type
 node-cwvellum                    Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program


#####################################################################
Please let me know if I am missing any configuration.
Thanks
Hrishikesh

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170720/dbb90042/attachment.html>

From hkaranjikar at apm.com  Mon Jul 24 04:08:26 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Mon, 24 Jul 2017 13:38:26 +0530
Subject: [Project Clearwater] restund_process Execution failed on bono
In-Reply-To: <BLUPR02MB437ACA7F2803577A6F9C8A7E5A70@BLUPR02MB437.namprd02.prod.outlook.com>
References: <CABmBNEafr-WsM7_E3i7cbhJujejjqfgs9aFp2dTRdr=R7Lsmww@mail.gmail.com>
	<BLUPR02MB4375A20455D0317FDCA0A97E5AE0@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEY-Saqs0REksQqk9=vfE=sOb8GtwV31JQKftBYn_y1NXQ@mail.gmail.com>
	<CABmBNEZB2HzhJZT9bFCjLFiX_W-NgigK7DCb5T64Yc7Jnj__eg@mail.gmail.com>
	<BLUPR02MB437BB7A07944A9B02AA3075E5AC0@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEZpKw62fEBwm_omy05nsOdhnQrPVhwhxn7+s_V6LJErWw@mail.gmail.com>
	<BLUPR02MB437ACA7F2803577A6F9C8A7E5A70@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <CABmBNEa2vxC_6nk8uB966JXYzP1=y=GmMGaf+xhtYC8D1JFMZQ@mail.gmail.com>

Hi Andrew,

I added the entires in /etc/local/hosts on bono node and after restart I
was able to see everything working fine.
So now monit shows all processes on all 6 nodes are running fine.
Will local configuration will have any effect on clearwater deployment?

I still have not tested any calling using Zoiper or live test suite. This
is the 1st time I have got all 6 nodes working fine.

Thanks
Hrishikesh


On Thu, Jul 20, 2017 at 4:12 PM, Andrew Edmonds <
Andrew.Edmonds at metaswitch.com> wrote:

> Hi Hrishikesh,
>
>
>
> Yes even with an entry for icscf.cwsprout in /etc/hosts I would still
> expect you to get the DNS error in Bono as there are still not entries for
> icscf.cwpsrout on your DNS server.
>
>
>
> How to add entries to your DNS server will depend on which DNS server you
> have chosen to use, most of my testing has been done with a DNS server
> called BIND. You can find details on how to install BIND and how to
> configure a client to use it here
> <http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html?#bind>
> .
>
>
>
> Please let me know if you have any further questions.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Friday, July 14, 2017 11:17 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] restund_process Execution failed on
> bono
>
>
>
> Hi Andrew,
>
> Thanks for your support.
>
> I am not in a position to modify the current dns server nor our admins
> would do it.
>
> I have added entry for icscf.cwsprout in /etc/hosts. I am able to ping
> icscf.cwsprout after adding this entry.
>
> Still i get same error in the log.
>
>
> ===========================================================
> cwbono at cwbono:~$ ping icscf.cwsprout
> PING icscf.cwsprout (10.48.12.143) 56(84) bytes of data.
> 64 bytes from cwsprout (10.48.12.143): icmp_seq=1 ttl=64 time=0.157 ms
> 64 bytes from cwsprout (10.48.12.143): icmp_seq=2 ttl=64 time=0.129 ms
> ^C
> --- icscf.cwsprout ping statistics ---
> 2 packets transmitted, 2 received, 0% packet loss, time 999ms
> rtt min/avg/max/mdev = 0.129/0.143/0.157/0.014 ms
>
> cwbono at cwbono:~$ cat /var/log/bono/bono_current.txt
>
> 14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>
> ===========================================================
>
> If this is not working then I might have to create a dns server locally.
> Then i would be able to modify or add entries
>
> Is it possible for you to give me an example of how these entries are
> added?
>
> Thanks
>
> Hrishikesh
>
>
>
> On Thu, Jul 13, 2017 at 9:30 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
> Hi Hrishikesh,
>
>
>
> Thank you for the updated diagnostics.
>
>
>
> We can still see the following appearing in log files:
>
>
>
> ?Failed to resolve icscf.cwsprout to an IP address?
>
>
>
> The icscf.<sprout hostname> DNS record is used by the P-CSCF (in this case
> Bono) to identify which I-CSCF to forward requests on to.
>
>
>
> Even when assigning nodes IP addresses through DHCP you must still
> configure your DNS with all the records that Clearwater nodes require to
> communicate with each other (such as icscf.<sprout hostname>). You can find
> a list of all the records that Clearwater requires here
> <http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html>.
>
>
>
> Please could you try updating your DNS server to contain these records and
> restart your nodes, let me know if you still hit the issue.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, July 13, 2017 10:40 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] restund_process Execution failed on
> bono
>
>
>
> Hi,
>
> I have changed my deployment completely.
>
> No I am not using static IP addresses any more and each node is getting IP
> addresses from the DHCP server.
>
> Each node is able to ping one another using hostname cwellis, cwsprout,
> cwbono, cwvellum, cwhomer, cwdime.
>
> From bono node I am able to ping, ssh to cwsprout. Check the logs,
>
> ============================================================
> ===================
>
> cwbono at cwbono:~$ ping cwsprout
> PING cwsprout.amcc.com (10.48.12.143) 56(84) bytes of data.
> 64 bytes from cwsprout.amcc.com (10.48.12.143): icmp_seq=1 ttl=64
> time=0.126 ms
> 64 bytes from cwsprout.amcc.com (10.48.12.143): icmp_seq=2 ttl=64
> time=0.203 ms
> 64 bytes from cwsprout.amcc.com (10.48.12.143): icmp_seq=3 ttl=64
> time=0.210 ms
>
> cwbono at cwbono:~$ ssh cwsprout at cwsprout
> The authenticity of host 'cwsprout (10.48.12.143)' can't be established.
> ECDSA key fingerprint is 03:f8:42:81:36:ef:b1:be:7a:d8:3d:52:b5:74:f9:ba.
> Are you sure you want to continue connecting (yes/no)? yes
> Warning: Permanently added 'cwsprout,10.48.12.143' (ECDSA) to the list of
> known hosts.
> cwsprout at cwsprout's password:
> Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-31-generic x86_64)
>
>  * Documentation:  https://help.ubuntu.com/
>
>   System information as of Thu Jul 13 14:08:07 IST 2017
>
>   System load:  0.12              Processes:           115
>   Usage of /:   27.7% of 7.26GB   Users logged in:     0
>   Memory usage: 8%                IP address for eth0: 10.48.12.143
>   Swap usage:   0%
>
>   Graph this data and manage this system at:
>     https://landscape.canonical.com/
>
> New release '16.04.2 LTS' available.
> Run 'do-release-upgrade' to upgrade to it.
>
> Last login: Thu Jul 13 14:08:07 2017 from hdk-supermicro.amcc.com
> [sprout]cwsprout at cwsprout:~$
>
> [sprout]cwsprout at cwsprout:~$ exit
> logout
> Connection to cwsprout closed.
> cwbono at cwbono:~$
> cwbono at cwbono:~$
> cwbono at cwbono:~$ cat /etc/clearwater/local_config
> local_ip=10.48.12.173
> public_ip=10.48.12.173
> public_hostname=cwbono
> etcd_cluster="10.48.12.142,10.48.12.143,10.48.12.173,10.48.
> 12.140,10.48.12.139,10.48.12.120"
> cwbono at cwbono:~$ cat /etc/clearwater/shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
>
> home_domain=example.com
> sprout_hostname=cwsprout
> sprout_registration_store=10.48.12.120 #vellum
> hs_hostname=10.48.12.139:8888 #dime
> hs_provisioning_hostname=10.48.12.139:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=10.48.12.140:7888 #homer
> chronos_hostname=vellum
> cassandra_hostname=10.48.12.120 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> cwbono at cwbono:~$ clearwater-etcdctl cluster-health
> member 2d821d7a0a7736b4 is healthy: got healthy result from
> http://10.48.12.142:4000
> member 6201151ee7f99f5c is healthy: got healthy result from
> http://10.48.12.139:4000
> member 895efb70c4b1b8b4 is healthy: got healthy result from
> http://10.48.12.120:4000
> member c7f5d6485fb4735b is healthy: got healthy result from
> http://10.48.12.143:4000
> member f7726a4e29ec7d3d is healthy: got healthy result from
> http://10.48.12.173:4000
> member ffb968d2990c63f0 is healthy: got healthy result from
> http://10.48.12.140:4000
> cluster is healthy
> cwbono at cwbono:~$ clearwater-etcdctl member list
> 2d821d7a0a7736b4: name=10-48-12-142 peerURLs=http://10.48.12.142:2380
> clientURLs=http://10.48.12.142:4000 isLeader=false
> 6201151ee7f99f5c: name=10-48-12-139 peerURLs=http://10.48.12.139:2380
> clientURLs=http://10.48.12.139:4000 isLeader=false
> 895efb70c4b1b8b4: name=10-48-12-120 peerURLs=http://10.48.12.120:2380
> clientURLs=http://10.48.12.120:4000 isLeader=false
> c7f5d6485fb4735b: name=10-48-12-143 peerURLs=http://10.48.12.143:2380
> clientURLs=http://10.48.12.143:4000 isLeader=false
> f7726a4e29ec7d3d: name=10-48-12-173 peerURLs=http://10.48.12.173:2380
> clientURLs=http://10.48.12.173:4000 isLeader=true
> ffb968d2990c63f0: name=10-48-12-140 peerURLs=http://10.48.12.140:2380
> clientURLs=http://10.48.12.140:4000 isLeader=false
>
> cwbono at cwbono:~$ cw-check_cluster_state
> This script prints the status of the Cassandra, Chronos, and Memcached
> clusters.
> This node (10.48.12.173) should not be in any cluster.
>
> Describing the Cassandra cluster:
>   The cluster is stable
>     10.48.12.120 is in state normal
>
> Describing the Chronos cluster:
>   The cluster is stable
>     10.48.12.120 is in state normal
>
> Describing the Memcached cluster:
>   The cluster is stable
>     10.48.12.120 is in state normal
>
> cwbono at cwbono:~$ sudo cw-check_config_sync
>  - /etc/clearwater/dns.json is up to date
>  - /etc/clearwater/shared_config is up to date
>
> cwbono at cwbono:~$ sudo monit summary
> Monit 5.18.1 uptime: 54m
>  Service Name                     Status
> Type
>  node-cwbono                      Running
> System
>  restund_process                  Execution failed | Does...
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Wait parent
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
>
> cwbono at cwbono:~$ cat /var/log/bono/bono_current.txt
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>
> cwbono at cwbono:~$ cat /var/log/monit.log
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:03:29] error    : 'restund_process' process is not running
> [IST Jul 13 15:03:29] info     : 'restund_process' trying to restart
> [IST Jul 13 15:03:29] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul 13 15:04:00] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=10.48.12.173:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:04:10] error    : 'restund_process' process is not running
> [IST Jul 13 15:04:10] info     : 'restund_process' trying to restart
> [IST Jul 13 15:04:10] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul 13 15:04:40] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=10.48.12.173:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:04:50] error    : 'restund_process' process is not running
> [IST Jul 13 15:04:50] info     : 'restund_process' trying to restart
> [IST Jul 13 15:04:50] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul 13 15:05:20] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=10.48.12.173:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:05:30] error    : 'restund_process' process is not running
> [IST Jul 13 15:05:30] info     : 'restund_process' trying to restart
> [IST Jul 13 15:05:30] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul 13 15:06:01] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=10.48.12.173:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:06:11] error    : 'restund_process' process is not running
> [IST Jul 13 15:06:11] info     : 'restund_process' trying to restart
> [IST Jul 13 15:06:11] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul 13 15:06:41] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=10.48.12.173:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:06:51] error    : 'restund_process' process is not running
> [IST Jul 13 15:06:51] info     : 'restund_process' trying to restart
> [IST Jul 13 15:06:51] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul 13 15:07:21] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=10.48.12.173:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in use
> [IST Jul 13 15:07:31] error    : 'restund_process' process is not running
> [IST Jul 13 15:07:31] info     : 'restund_process' trying to restart
> [IST Jul 13 15:07:31] info     : 'restund_process' restart:
> /etc/init.d/restund
>
> ============================================================
> ===================
>
> I don't know whats going wrong here.
>
> What is icscf.cwsprout?
>
>
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
> On Tue, Jul 11, 2017 at 5:08 PM, Hrishikesh Karanjikar <
> hkaranjikar at apm.com> wrote:
>
> Hi,
>
> Thanks a lot for your reply.
>
> I am using virtualbox and host only network.
>
> The DHCP server runs inside virtualbox and I can only configure the IP
> address range.
>
> However I am not using the DHCP server and assigning static IP addresses
> to all nodes which are within the DHCP server IP address range.
>
> I added entry of cwsprout in /ets/hosts of bono node as follows,
>
> ===============================================
>
> 127.0.0.1       localhost
> 127.0.1.1       cwbono
>
> *192.168.56.103  cwsprout 192.168.56.103  icscf.cwsprout*
>
> # The following lines are desirable for IPv6 capable hosts
> ::1     localhost ip6-localhost ip6-loopback
> ff02::1 ip6-allnodes
> ff02::2 ip6-allrouters
> 192.168.56.103 cwsprout
> 192.168.56.103  icscf.cwsprout
> ::1 localhost # added by clearwater-infrastructure 1hosts script
> 192.168.56.104 cwbono #+clearwater-infrastructure
>
> ===============================================
>
> I am also able to ping cwsprout and icscf.cwsprout from bono,
>
> Here is the log,
>
> ===============================================
>
> [bono]cwbono at cwbono:~$ ping cwsprout
> PING cwsprout (192.168.56.103) 56(84) bytes of data.
> 64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.197 ms
> 64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.217 ms
> ^C
> --- cwsprout ping statistics ---
> 2 packets transmitted, 2 received, 0% packet loss, time 999ms
> rtt min/avg/max/mdev = 0.197/0.207/0.217/0.010 ms
> [bono]cwbono at cwbono:~$
> [bono]cwbono at cwbono:~$
> [bono]cwbono at cwbono:~$
> [bono]cwbono at cwbono:~$ ping icscf.cwsprout
> PING icscf.cwsprout (192.168.56.103) 56(84) bytes of data.
> 64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.112 ms
> 64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.165 ms
>
> ===============================================
>
> I am using static IP addresses as I have to specify them in local_config
> file of each node.
>
> If I use DHCP server of VirtualBox they might change. In that case I am
> not sure how do I cope up with local_config.
>
> Can I modify local_config after all nodes are up?
>
>
>
> Thanks
>
> Hrishikesh
>
>
>
>
>
> On Tue, Jul 11, 2017 at 2:42 PM, Andrew Edmonds <
> Andrew.Edmonds at metaswitch.com> wrote:
>
> Hi Hrishikesh,
>
>
>
> Thank you for your question and the detailed logs you have provided.
>
>
>
> The issues appears to be caused by your shared config, in the manual
> installation instructions
> <http://clearwater.readthedocs.io/en/stable/Manual_Install.html> you?ll
> see that we advise that the entries in shared config have a format like:
>
>
>
> sprout_hostname*=*sprout*.<*site_name*>.<*zone*>*
>
> sprout_registration_store*=*vellum*.<*site_name*>.<*zone*>*
>
> hs_hostname*=*hs*.<*site_name*>.<*zone*>*:8888
>
>
>
> In your shared config you have:
>
>
>
> sprout_hostname=cwsprout
> sprout_registration_store=192.168.56.107 #vellum
>
> I don?t think the sprout_hostname used here will resolve, we can see
> evidence for this in the Bono logs, Bono attempts to resolve
> icscf.<sprout_hostname> to find which location to forward SIP messages on
> you, you can see it is failing to do that here:
>
>
>
> Failed to resolve icscf.cwsprout to an IP address - Not found
> (PJ_ENOTFOUND)
>
>
>
> To resolve this issue you should change your shared config to use
> hostnames which have been configured in your DNS server to resolve to the
> appropriate location. Once you have done this run the command ?sudo
> cw-upload_shared_config?. Please let me know if this does not resolve the
> issue.
>
>
>
> Thanks,
>
>
>
> Andrew
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Hrishikesh Karanjikar
> *Sent:* Thursday, July 6, 2017 10:22 AM
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] restund_process Execution failed on bono
>
>
>
> Hello,
>
> I have Manually installed all 6 nodes on VMs using virtualbox.
>
> I followed the procedure given @ http://clearwater.readthedocs.
> io/en/stable/Manual_Install.html
>
> Looks like all nodes except Dime are running fine.
>
> I am getting error "restund_process Execution failed" in monit summary on
> Bono node.
>
> Here is the shared and local config file,
>
> #####################################################################
>
> [bono]cwbono at cwbono:~$ cat /etc/clearwater/shared_config
>
> home_domain=example.com
> sprout_hostname=cwsprout
> sprout_registration_store=192.168.56.107 #vellum
> hs_hostname=192.168.56.106:8888 #dime
> hs_provisioning_hostname=192.168.56.106:8889 #dime
> ralf_hostname=
> ralf_session_store=
> xdms_hostname=192.168.56.105:7888 #homer
> chronos_hostname=192.168.56.107 #vellum
> cassandra_hostname=192.168.56.107 #vellum
>
> # Email server configuration
> smtp_smarthost=localhost
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
> [bono]cwbono at cwbono:~$ cat /etc/clearwater/local_config
> local_ip=192.168.56.104
> public_ip=192.168.56.104
> public_hostname=cwbono
> etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,
> 192.168.56.105,192.168.56.106,192.168.56.107"
>
> #####################################################################
>
> The logs for bono node are as follows,
>
> #####################################################################
> [bono]cwbono at cwbono:~$ sudo monit summary
> [sudo] password for cwbono:
> Monit 5.18.1 uptime: 2d 16h 39m
>  Service Name                     Status
> Type
>  node-cwbono                      Running
> System
>  restund_process                  Execution failed | Does...
> Process
>  ntp_process                      Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  bono_process                     Running
> Process
>  poll_restund                     Wait parent
> Program
>  monit_uptime                     Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  poll_bono                        Status ok
> Program
>
>
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:15.843 UTC Status main.cpp:1358: Quiesce signal received
> 05-07-2017 11:30:15.843 UTC Status stack.cpp:125: Setting quiescing =
> PJ_TRUE
> 05-07-2017 11:30:15.851 UTC Status stack.cpp:156: Quiescing state changed
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The
> Quiescing Manager received input QUIESCE (0) when in state ACTIVE (0)
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:265: Close
> untrusted listening port
> 05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport
> for port 5060
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:273: Quiesce
> FlowTable
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The
> Quiescing Manager received input FLOWS_GONE (1) when in state
> QUIESCING_FLOWS (1)
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:290: Closing
> trusted port
> 05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport
> for port 5058
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:296: Quiescing
> all connections
> 05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:162: Start
> quiescing connections
> 05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:175: Quiescing 0
> transactions
> 05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:180: Connection
> quiescing complete
> 05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The
> Quiescing Manager received input CONNS_GONE (2) when in state
> QUIESCING_CONNS (2)
> 05-07-2017 11:30:15.851 UTC Status main.cpp:1380: Quiesce complete
> 05-07-2017 11:30:15.853 UTC Status stack.cpp:171: PJSIP thread ended
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
> 05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to
> resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
>
> [bono]cwbono at cwbono:~$ cat /var/log/monit.log
>
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=192.168.56.104:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in u
> [IST Jul  6 14:47:46] error    : 'restund_process' process is not running
> [IST Jul  6 14:47:46] info     : 'restund_process' trying to restart
> [IST Jul  6 14:47:46] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul  6 14:48:16] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=192.168.56.104:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in u
> [IST Jul  6 14:48:26] error    : 'restund_process' process is not running
> [IST Jul  6 14:48:26] info     : 'restund_process' trying to restart
> [IST Jul  6 14:48:26] info     : 'restund_process' restart:
> /etc/init.d/restund
> [IST Jul  6 14:48:56] error    : 'restund_process' failed to restart (exit
> status 0) -- /etc/init.d/restund: httpdb: configured url
> http://hs.example.com:8888/impi/%s/digest
> httpd: using URI workaround
> turn: server deployed behind static NAT addr=192.168.56.104:0
> turn: extended channels enabled
> tcp: sock_bind: bind: Address already in u
> [IST Jul  6 14:49:06] error    : 'restund_process' process is not running
> [IST Jul  6 14:49:06] info     : 'restund_process' trying to restart
> [IST Jul  6 14:49:06] info     : 'restund_process' restart:
> /etc/init.d/restund
>
>
> [bono]cwbono at cwbono:~$ clearwater-etcdctl cluster-health
> member 9c1928228d308a0f is healthy: got healthy result from
> http://192.168.56.107:4000
> member b0c9c017e0d47e14 is healthy: got healthy result from
> http://192.168.56.106:4000
> member d44832212a08c43f is healthy: got healthy result from
> http://192.168.56.103:4000
> member ef1a9a8a2fd05283 is healthy: got healthy result from
> http://192.168.56.104:4000
> member f63afbe816fb463d is healthy: got healthy result from
> http://192.168.56.102:4000
> member f7132cc88f7a39fa is healthy: got healthy result from
> http://192.168.56.105:4000
> cluster is healthy
> [bono]cwbono at cwbono:~$ cw-check_cluster_state
> This script prints out the status of the Chronos, Memcached and Cassandra
> clusters.
>
> Describing the Vellum Chronos cluster:
>   The local node is *not* in this cluster
>   The cluster is stable
>     192.168.56.107 is in state normal
>
> Describing the Vellum Memcached cluster:
>   The local node is *not* in this cluster
>   The cluster is stable
>     192.168.56.107 is in state normal
>
> Describing the Vellum Cassandra cluster:
>   The local node is *not* in this cluster
>   The cluster is stable
>     192.168.56.107 is in state normal
>
> [bono]cwbono at cwbono:~$ clearwater-etcdctl member list
> 9c1928228d308a0f: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
> clientURLs=http://192.168.56.107:4000 isLeader=false
> b0c9c017e0d47e14: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
> clientURLs=http://192.168.56.106:4000 isLeader=true
> d44832212a08c43f: name=192-168-56-103 peerURLs=http://192.168.56.103:2380
> clientURLs=http://192.168.56.103:4000 isLeader=false
> ef1a9a8a2fd05283: name=192-168-56-104 peerURLs=http://192.168.56.104:2380
> clientURLs=http://192.168.56.104:4000 isLeader=false
> f63afbe816fb463d: name=192-168-56-102 peerURLs=http://192.168.56.102:2380
> clientURLs=http://192.168.56.102:4000 isLeader=false
> f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
> clientURLs=http://192.168.56.105:4000 isLeader=false
> [bono]cwbono at cwbono:~$ sudo cw-check_config_sync
> [sudo] password for cwbono:
>  - /etc/clearwater/dns.json is up to date
>  - /etc/clearwater/shared_config is up to date
>
>
> #####################################################################
>
> The logs for other nodes are as follows
>
> #####################################################################
>
> [ellis]cwellis at cwellis:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 21h 15m
>  Service Name                     Status
> Type
>  node-cwellis                     Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  mysql_process                    Running
> Process
>  ellis_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_ellis                       Status ok
> Program
>  poll_ellis_https                 Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>
>
> [sprout]cwsprout at cwsprout:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 20h 7m
>  Service Name                     Status
> Type
>  node-cwsprout                    Running
> System
>  sprout_process                   Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  memento_process                  Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  sprout_uptime                    Status ok
> Program
>  poll_sprout_sip                  Status ok
> Program
>  poll_sprout_http                 Status ok
> Program
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  memento_uptime                   Status ok
> Program
>  poll_memento                     Status ok
> Program
>  poll_memento_https               Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok                   Program
>
>
> [homer]cwhomer at cwhomer:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 20h 2m
>  Service Name                     Status
> Type
>  node-cwhomer                     Running
> System
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homer_process                    Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  poll_homer                       Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok                   Program
>
> [dime]cwdime at cwdime:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 20h 2m
>  Service Name                     Status
> Type
>  node-cwdime                      Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  nginx_process                    Running
> Process
>  homestead_process                Running
> Process
>  homestead-prov_process           Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  nginx_ping                       Status ok
> Program
>  nginx_uptime                     Status ok
> Program
>  monit_uptime                     Status ok
> Program
>  homestead_uptime                 Status ok
> Program
>  poll_homestead                   Status ok
> Program
>  check_cx_health                  Status ok
> Program
>  poll_homestead-prov              Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>
>
> [vellum]cwvellum at cwvellum:~$ sudo monit summary
> Monit 5.18.1 uptime: 1d 20h 3m
>  Service Name                     Status
> Type
>  node-cwvellum                    Running
> System
>  snmpd_process                    Running
> Process
>  ntp_process                      Running
> Process
>  memcached_process                Running
> Process
>  clearwater_queue_manager_pro...  Running
> Process
>  etcd_process                     Running
> Process
>  clearwater_diags_monitor_pro...  Running
> Process
>  clearwater_config_manager_pr...  Running
> Process
>  clearwater_cluster_manager_p...  Running
> Process
>  cassandra_process                Running
> Process
>  chronos_process                  Running
> Process
>  astaire_process                  Running
> Process
>  monit_uptime                     Status ok
> Program
>  memcached_uptime                 Status ok
> Program
>  poll_memcached                   Status ok
> Program
>  clearwater_queue_manager_uptime  Status ok
> Program
>  etcd_uptime                      Status ok
> Program
>  poll_etcd_cluster                Status ok
> Program
>  poll_etcd                        Status ok
> Program
>  cassandra_uptime                 Status ok
> Program
>  poll_cassandra                   Status ok
> Program
>  poll_cqlsh                       Status ok
> Program
>  chronos_uptime                   Status ok
> Program
>  poll_chronos                     Status ok
> Program
>  astaire_uptime                   Status ok
> Program
>
>
> #####################################################################
>
> Please let me know if I am missing any configuration.
>
> Thanks
>
> Hrishikesh
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170724/457432f7/attachment.html>

From Andrew.Edmonds at metaswitch.com  Mon Jul 24 05:53:20 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Mon, 24 Jul 2017 09:53:20 +0000
Subject: [Project Clearwater] [Clearwater] AS Configuration and
 untrusted sources
In-Reply-To: <3ACF4837-B492-45A2-8B83-14202C9CFDEE@aleagames.com>
References: <7296A131-7DDD-43F1-9F00-5F85FCF40FF7@aleagames.com>
	<3ACF4837-B492-45A2-8B83-14202C9CFDEE@aleagames.com>
Message-ID: <BLUPR02MB43730770DB57A115A4B287BE5BB0@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Michele,

Thank you for your question.

It may be the case that you need to update your firewall rules to allow traffic from your SIP client to reach the Application Server. However in order to verify this I will need the Bono logs from the time which you attempted to contact the AS. Could you please provide these logs?

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Michele Furlanetto
Sent: Tuesday, July 18, 2017 12:58 PM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources

Hi all,
I?ve got a problem while configuring an AIO image.

In my scenario, there are is an AS, say as1.service.example.com<http://as1.service.example.com>, which should get the third-party registration as well as being able to be addressed directly from the Sip client.

While third-party registration works, I?m unable to contact the AS directly.
Looking at Bono logs, I get Info bono.cpp:1356: Rejecting request from untrusted source.

Here is the content of /usr/share/clearwater/ellis/web-content/js/app-servers.json, indented to be more readable
{
?SERVICE?:"
<InitialFilterCriteria>
            <Priority>0</Priority>
            <TriggerPoint>
                        <ConditionTypeCNF>0</ConditionTypeCNF>
                        <SPT>
                                    <ConditionNegated>0</ConditionNegated>
                                    <Group>0</Group>
                                    <Method>REGISTER</Method>
                                    <Extension>
                                                <RegistrationType>0</RegistrationType>
                                                <RegistrationType>1</RegistrationType>
                                                <RegistrationType>2</RegistrationType>
                                    </Extension>
                        </SPT>
                        <SPT>
                                    <ConditionNegated>0</ConditionNegated>
                                    <Group>1</Group>
                                    <RequestURI>sip:as1.service.example.com<http://service.example.com></RequestURI>
                        </SPT>
            </TriggerPoint>
            <ApplicationServer>
                        <ServerName>sip:as1.service.example.com<http://service.example.com>:5071;transport=UDP</ServerName>
                        <DefaultHandling>0</DefaultHandling>
                        <Extension>
                                    <IncludeRegisterRequest/>
                        </Extension>
            </ApplicationServer>
</InitialFilterCriteria>"
}

Some other details:
- SERVICE is checked in Ellis for test users;
- as1.service.example.com<http://as1.service.example.com> is resolved using /etc/hosts.


Any hint on the error(s)?
Thanks,
Michele
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170724/7f69e541/attachment.html>

From Andrew.Edmonds at metaswitch.com  Mon Jul 24 06:18:19 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Mon, 24 Jul 2017 10:18:19 +0000
Subject: [Project Clearwater] Sprout S-CSCF Forking disable
In-Reply-To: <TU4PR84MB006447E14F30E7DD5C97EFF1DCA00@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>
References: <TU4PR84MB006447E14F30E7DD5C97EFF1DCA00@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>
Message-ID: <BLUPR02MB4375769948F002026F849F0E5BB0@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Aravind,

Thank you for your question.

If you do have more than one UE registered to an IMS-Core network with the same IMPI and IMPU the S-CSCF needs some way to determine which endpoint to send the call to. The only way to do this with the standard Project Clearwater deployment is to have the S-CSCF fork the calls. This means that you cannot disable this functionality.

I believe the only other way to implement this in compliance with IMS standards would be to have an Application Server invoked which targets the call at a specific SIP instance. That AS would need logic to decide which endpoint to choose (e.g. based on pre-configured call manager rules such as 'ring my desk phone between 8am and 4pm, otherwise ring my cell phone').

Please let me know if you have any further questions about this.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shirabur, Aravind Ashok (CMS)
Sent: Monday, July 17, 2017 9:49 AM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Sprout S-CSCF Forking disable

Hi,

How do we disable the S-CSCF Forking functionality ? more than one UE are registered to IMS-Core network with same IMPI and IMPU, S-CSCF try to fork the all registered UE for the MT call landing on this IMPU.


Thanks
Aravind

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170724/3b38a430/attachment.html>

From badrlamti at gmail.com  Fri Jul 21 09:39:10 2017
From: badrlamti at gmail.com (Bader LAMTI)
Date: Fri, 21 Jul 2017 15:39:10 +0200
Subject: [Project Clearwater] [Clearwater] call problem 200OK
Message-ID: <CAFwW5BHKDNwc-=gypEZ6OnFs5+UR0imZBuDGny2pcxrceGzcfA@mail.gmail.com>

Hi,



I have installed Clearwater on Openstack with 7 vms with this configuration:


Ellis         private ip: 10.0.0.6          floating ip: 192.168.1.103

Bono        private ip: 10.0.0.13        floating ip: 192.168.1.101

Sprout      private ip: 10.0.0.15        floating ip: 192.168.1.109

Homer      private ip: 10.0.0.3          floating ip: 192.168.1.110

Dime        private ip: 10.0.0.10        floating ip: 192.168.1.107

Vellum      private ip: 10.0.0.17        floating ip: 192.168.1.105

Dns          private ip: 10.0.0.12         floating ip: 192.168.1.114


I have a problem when I make a call.

The problem is the following : when I answer on the second softphone the
first softphone continues ringing.


I see on the capture Wireshark that the 200 OK of the INVITE does not
arrive at bono.

Please find attached the logs of bono and  wireshark trace.


I would appreciate if you can help me to solve this issue.


Thanking you + best regards,
-- 
*Bader **LAMTI *
*+33 6 99 57 88 21*
*?l?ve ing?nieur ? l'Ensimag GRENOBLE-INP *
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170721/eae401a4/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: trace_wireshark_logs_bono.zip
Type: application/zip
Size: 3565316 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170721/eae401a4/attachment.zip>

From Jochen.Kappel at de.ibm.com  Mon Jul 24 08:43:18 2017
From: Jochen.Kappel at de.ibm.com (Jochen Kappel)
Date: Mon, 24 Jul 2017 12:43:18 +0000
Subject: [Project Clearwater] Ellis UI not installed
Message-ID: <OF1FB7DF81.E8F66329-ON00258167.00454F96-00258167.0045E214@notes.na.collabserv.com>

An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170724/eb535b7e/attachment.html>

From michele.furlanetto at aleagames.com  Mon Jul 24 10:07:45 2017
From: michele.furlanetto at aleagames.com (Michele Furlanetto)
Date: Mon, 24 Jul 2017 16:07:45 +0200
Subject: [Project Clearwater] [Clearwater] AS Configuration and
 untrusted sources
In-Reply-To: <BLUPR02MB43730770DB57A115A4B287BE5BB0@BLUPR02MB437.namprd02.prod.outlook.com>
References: <7296A131-7DDD-43F1-9F00-5F85FCF40FF7@aleagames.com>
	<3ACF4837-B492-45A2-8B83-14202C9CFDEE@aleagames.com>
	<BLUPR02MB43730770DB57A115A4B287BE5BB0@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <AAB4BECC-919D-43AB-A00A-78C1110C3244@aleagames.com>

Hi Andrew,
Thanks for your response.

In attachment you can find what you asked me.
It?s about 15 minutes of logs, beginning with the service start.
An example of message being rejected can be found at line 72104, while the rejection is at 72399.

I had to censor part of contents and some headers of the messages, so there may be some inconsistencies.


Thanks,
Michele



> Il giorno 24 lug 2017, alle ore 11:53, Andrew Edmonds <Andrew.Edmonds at metaswitch.com> ha scritto:
> 
> Hi Michele,
>  
> Thank you for your question.
>  
> It may be the case that you need to update your firewall rules to allow traffic from your SIP client to reach the Application Server. However in order to verify this I will need the Bono logs from the time which you attempted to contact the AS. Could you please provide these logs?
>  
> Thanks,
>  
> Andrew
>  
> From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Michele Furlanetto
> Sent: Tuesday, July 18, 2017 12:58 PM
> To: clearwater at lists.projectclearwater.org
> Subject: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources
>  
> Hi all,
> I?ve got a problem while configuring an AIO image.
> 
> In my scenario, there are is an AS, say as1.service.example.com <http://as1.service.example.com/>, which should get the third-party registration as well as being able to be addressed directly from the Sip client.
> 
> While third-party registration works, I?m unable to contact the AS directly.
> Looking at Bono logs, I get Info bono.cpp:1356: Rejecting request from untrusted source.
> 
> Here is the content of /usr/share/clearwater/ellis/web-content/js/app-servers.json, indented to be more readable
> {
> ?SERVICE?:"
> <InitialFilterCriteria>
>             <Priority>0</Priority>
>             <TriggerPoint>
>                         <ConditionTypeCNF>0</ConditionTypeCNF>
>                         <SPT>
>                                     <ConditionNegated>0</ConditionNegated>
>                                     <Group>0</Group>
>                                     <Method>REGISTER</Method>
>                                     <Extension>
>                                                 <RegistrationType>0</RegistrationType>
>                                                 <RegistrationType>1</RegistrationType>
>                                                 <RegistrationType>2</RegistrationType>
>                                     </Extension>
>                         </SPT>
>                         <SPT>
>                                     <ConditionNegated>0</ConditionNegated>
>                                     <Group>1</Group>
>                                     <RequestURI>sip:as1.service.example.com <http://service.example.com/></RequestURI>
>                         </SPT>
>             </TriggerPoint>
>             <ApplicationServer>
>                         <ServerName>sip:as1.service.example.com <http://service.example.com/>:5071;transport=UDP</ServerName>
>                         <DefaultHandling>0</DefaultHandling>
>                         <Extension>
>                                     <IncludeRegisterRequest/>
>                         </Extension>
>             </ApplicationServer>
> </InitialFilterCriteria>"
> }
> 
> Some other details:
> - SERVICE is checked in Ellis for test users;
> - as1.service.example.com <http://as1.service.example.com/> is resolved using /etc/hosts.
> 
> 
> Any hint on the error(s)?
> Thanks,
> Michele
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org <mailto:Clearwater at lists.projectclearwater.org>
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org <http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170724/78ece878/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: bono_20170724T100000Z.txt.zip
Type: application/zip
Size: 314981 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170724/78ece878/attachment.zip>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170724/78ece878/attachment-0001.html>

From Andrew.Edmonds at metaswitch.com  Tue Jul 25 06:59:01 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Tue, 25 Jul 2017 10:59:01 +0000
Subject: [Project Clearwater] [Clearwater] AS Configuration and
 untrusted sources
In-Reply-To: <AAB4BECC-919D-43AB-A00A-78C1110C3244@aleagames.com>
References: <7296A131-7DDD-43F1-9F00-5F85FCF40FF7@aleagames.com>
	<3ACF4837-B492-45A2-8B83-14202C9CFDEE@aleagames.com>
	<BLUPR02MB43730770DB57A115A4B287BE5BB0@BLUPR02MB437.namprd02.prod.outlook.com>
	<AAB4BECC-919D-43AB-A00A-78C1110C3244@aleagames.com>
Message-ID: <BLUPR02MB437897854AEF6F573BCE5E3E5B80@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Michele,

Thank you for the extra diagnostics.

To help understand why Bono is rejecting the SUBSCRIBE to the Application Server it will help to know what kind of security checks Bono applies to incoming messages. When you successfully register a subscriber Bono stores the following details about the device used to register the subscriber:


?         IP address

?         Transport (i.e. TCP or UDP)

?         The port that the requests were sent from

Then if Bono receives an INVITE for a call from a registered subscriber whose device matches Bono?s record of the above three details then the call is allowed to proceed through to the IMS Core.

What appears to be happening here from looking at this line:

24-07-2017 10:38:25.417 UTC Debug bono.cpp:1166: Message received on non-trusted port 5060

is that the port which the SUBSRIBE message is being sent from does not match with the port that the device used to register the subscriber and hence Bono regards it as an untrusted port.

Exactly why different ports are being used for REGISTERS and SUBSCRIBES will depend on your SIP client. If do not know how to modify these I may be able to help if you give me details of your SIP client and a log file from Bono collected at a time when your made a registration.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Michele Furlanetto
Sent: Monday, July 24, 2017 3:08 PM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources

Hi Andrew,
Thanks for your response.

In attachment you can find what you asked me.
It?s about 15 minutes of logs, beginning with the service start.
An example of message being rejected can be found at line 72104, while the rejection is at 72399.

I had to censor part of contents and some headers of the messages, so there may be some inconsistencies.


Thanks,
Michele


Il giorno 24 lug 2017, alle ore 11:53, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> ha scritto:

Hi Michele,

Thank you for your question.

It may be the case that you need to update your firewall rules to allow traffic from your SIP client to reach the Application Server. However in order to verify this I will need the Bono logs from the time which you attempted to contact the AS. Could you please provide these logs?

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Michele Furlanetto
Sent: Tuesday, July 18, 2017 12:58 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources

Hi all,
I?ve got a problem while configuring an AIO image.

In my scenario, there are is an AS, say as1.service.example.com<http://as1.service.example.com/>, which should get the third-party registration as well as being able to be addressed directly from the Sip client.

While third-party registration works, I?m unable to contact the AS directly.
Looking at Bono logs, I get Info bono.cpp:1356: Rejecting request from untrusted source.

Here is the content of /usr/share/clearwater/ellis/web-content/js/app-servers.json, indented to be more readable
{
?SERVICE?:"
<InitialFilterCriteria>
            <Priority>0</Priority>
            <TriggerPoint>
                        <ConditionTypeCNF>0</ConditionTypeCNF>
                        <SPT>
                                    <ConditionNegated>0</ConditionNegated>
                                    <Group>0</Group>
                                    <Method>REGISTER</Method>
                                    <Extension>
                                                <RegistrationType>0</RegistrationType>
                                                <RegistrationType>1</RegistrationType>
                                                <RegistrationType>2</RegistrationType>
                                    </Extension>
                        </SPT>
                        <SPT>
                                    <ConditionNegated>0</ConditionNegated>
                                    <Group>1</Group>
                                    <RequestURI>sip:as1.service.example.com<http://service.example.com/></RequestURI>
                        </SPT>
            </TriggerPoint>
            <ApplicationServer>
                        <ServerName>sip:as1.service.example.com<http://service.example.com/>:5071;transport=UDP</ServerName>
                        <DefaultHandling>0</DefaultHandling>
                        <Extension>
                                    <IncludeRegisterRequest/>
                        </Extension>
            </ApplicationServer>
</InitialFilterCriteria>"
}

Some other details:
- SERVICE is checked in Ellis for test users;
- as1.service.example.com<http://as1.service.example.com/> is resolved using /etc/hosts.


Any hint on the error(s)?
Thanks,
Michele
_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170725/2a924642/attachment.html>

From Andrew.Edmonds at metaswitch.com  Tue Jul 25 09:40:54 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Tue, 25 Jul 2017 13:40:54 +0000
Subject: [Project Clearwater] [Clearwater] AS Configuration and
 untrusted sources
In-Reply-To: <AAB4BECC-919D-43AB-A00A-78C1110C3244@aleagames.com>
References: <7296A131-7DDD-43F1-9F00-5F85FCF40FF7@aleagames.com>
	<3ACF4837-B492-45A2-8B83-14202C9CFDEE@aleagames.com>
	<BLUPR02MB43730770DB57A115A4B287BE5BB0@BLUPR02MB437.namprd02.prod.outlook.com>
	<AAB4BECC-919D-43AB-A00A-78C1110C3244@aleagames.com>
Message-ID: <BLUPR02MB437901136FF4E64F7DC3D14E5B80@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Michele,

I?ve spoken to a colleague about this issue and have a new idea now as to why the SUBSCRIBE message is being rejected.

After Bono receives the SUBSCRIBE from the SIP device we can see the log:

24-07-2017 10:38:25.414 UTC Verbose pjsip: tcpc0x7f094c03 TCP transport 10.2.0.127:5058 is connecting to 10.2.0.127:5060..

10.2.0.127 is the Bono node?s IP address. So this is telling us that Bono is forwarding the SUBSCRIBE to itself from its trusted port 5058 (used to communicate with the IMS core) to its untrusted port 5060 (which SIP devices send their requests to). We then see in the logs Bono receiving this SUBSCRIBE and then rejecting it.

So the question we need to resolve now becomes why is Bono routing the SUBSCRIBE to itself? To answer this we need to understand how SIP messages are routed. Messages containing SIP methods (like INVITEs, SUBSCRIBEs, REGISTERs etc.) are routed based off of the Route-header. If we look at the route headers for one of the SUBSCRIBERs we see:

Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690

Here the port of the S-CSCF (5054) has not been included in the SIP address of the S-CSCF. We can also see from the logs that we do not retrieve a port number via SRV DNS lookup:

24-07-2017 10:38:25.414 UTC Debug dnscachedresolver.cpp:686: Received DNS response for _sip._udp.scscf.cw-aio type SRV
24-07-2017 10:38:25.414 UTC Warning dnscachedresolver.cpp:828: Failed to retrieve record for _sip._udp.scscf.cw-aio: Domain name not found

This means that Bono does not know which port to forward the message to and so it chooses 5060 which routes the message back to itself.

The next thing to consider then is why does the Route-Header not contain the port number? The Route header which is to be applied to future requests for a subscriber is communicated to Bono by the S-CSCF in a Service-Route header after it successfully authenticates a subscriber. If we have a look at the 200 OK in response to the registration we see:

Service-Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690

This means that it is the S-CSCF that is causing Bono to attach a Route header to the SUBSCRIBE which does not contain a port number. To figure out why the S-CSCF is doing this could you please send me a copy of /etc/clearwater/shared_config and /etc/clearwater/local_config from the SPN and a copy of the SPN log file during a REGISTRATION and SUBSCRIBE.

Thanks,

Andrew


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Michele Furlanetto
Sent: Monday, July 24, 2017 3:08 PM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources

Hi Andrew,
Thanks for your response.

In attachment you can find what you asked me.
It?s about 15 minutes of logs, beginning with the service start.
An example of message being rejected can be found at line 72104, while the rejection is at 72399.

I had to censor part of contents and some headers of the messages, so there may be some inconsistencies.


Thanks,
Michele


Il giorno 24 lug 2017, alle ore 11:53, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> ha scritto:

Hi Michele,

Thank you for your question.

It may be the case that you need to update your firewall rules to allow traffic from your SIP client to reach the Application Server. However in order to verify this I will need the Bono logs from the time which you attempted to contact the AS. Could you please provide these logs?

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Michele Furlanetto
Sent: Tuesday, July 18, 2017 12:58 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources

Hi all,
I?ve got a problem while configuring an AIO image.

In my scenario, there are is an AS, say as1.service.example.com<http://as1.service.example.com/>, which should get the third-party registration as well as being able to be addressed directly from the Sip client.

While third-party registration works, I?m unable to contact the AS directly.
Looking at Bono logs, I get Info bono.cpp:1356: Rejecting request from untrusted source.

Here is the content of /usr/share/clearwater/ellis/web-content/js/app-servers.json, indented to be more readable
{
?SERVICE?:"
<InitialFilterCriteria>
            <Priority>0</Priority>
            <TriggerPoint>
                        <ConditionTypeCNF>0</ConditionTypeCNF>
                        <SPT>
                                    <ConditionNegated>0</ConditionNegated>
                                    <Group>0</Group>
                                    <Method>REGISTER</Method>
                                    <Extension>
                                                <RegistrationType>0</RegistrationType>
                                                <RegistrationType>1</RegistrationType>
                                                <RegistrationType>2</RegistrationType>
                                    </Extension>
                        </SPT>
                        <SPT>
                                    <ConditionNegated>0</ConditionNegated>
                                    <Group>1</Group>
                                    <RequestURI>sip:as1.service.example.com<http://service.example.com/></RequestURI>
                        </SPT>
            </TriggerPoint>
            <ApplicationServer>
                        <ServerName>sip:as1.service.example.com<http://service.example.com/>:5071;transport=UDP</ServerName>
                        <DefaultHandling>0</DefaultHandling>
                        <Extension>
                                    <IncludeRegisterRequest/>
                        </Extension>
            </ApplicationServer>
</InitialFilterCriteria>"
}

Some other details:
- SERVICE is checked in Ellis for test users;
- as1.service.example.com<http://as1.service.example.com/> is resolved using /etc/hosts.


Any hint on the error(s)?
Thanks,
Michele
_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170725/73acd41c/attachment.html>

From Jochen.Kappel at de.ibm.com  Tue Jul 25 09:48:08 2017
From: Jochen.Kappel at de.ibm.com (Jochen Kappel)
Date: Tue, 25 Jul 2017 13:48:08 +0000
Subject: [Project Clearwater] Ellis UI not installed
In-Reply-To: <OFA00DA5D9.1E66E227-ON00258167.0045FE97@LocalDomain>
References: <OFA00DA5D9.1E66E227-ON00258167.0045FE97@LocalDomain>
Message-ID: <OF76DC950F.E9029897-ON00258168.004BC2ED-00258168.004BD17E@notes.na.collabserv.com>

An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170725/df478a2a/attachment.html>

From Andrew.Edmonds at metaswitch.com  Wed Jul 26 05:11:46 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Wed, 26 Jul 2017 09:11:46 +0000
Subject: [Project Clearwater] restund_process Execution failed on bono
In-Reply-To: <CABmBNEa2vxC_6nk8uB966JXYzP1=y=GmMGaf+xhtYC8D1JFMZQ@mail.gmail.com>
References: <CABmBNEafr-WsM7_E3i7cbhJujejjqfgs9aFp2dTRdr=R7Lsmww@mail.gmail.com>
	<BLUPR02MB4375A20455D0317FDCA0A97E5AE0@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEY-Saqs0REksQqk9=vfE=sOb8GtwV31JQKftBYn_y1NXQ@mail.gmail.com>
	<CABmBNEZB2HzhJZT9bFCjLFiX_W-NgigK7DCb5T64Yc7Jnj__eg@mail.gmail.com>
	<BLUPR02MB437BB7A07944A9B02AA3075E5AC0@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEZpKw62fEBwm_omy05nsOdhnQrPVhwhxn7+s_V6LJErWw@mail.gmail.com>
	<BLUPR02MB437ACA7F2803577A6F9C8A7E5A70@BLUPR02MB437.namprd02.prod.outlook.com>
	<CABmBNEa2vxC_6nk8uB966JXYzP1=y=GmMGaf+xhtYC8D1JFMZQ@mail.gmail.com>
Message-ID: <BLUPR02MB437D886332BCAC2E52F9378E5B90@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Hrishikesh,

Yes changing local configuration can have an effect on your Clearwater deployment. What settings are you considering changing?

I?m glad to hear your nodes all looks healthy, you should try a live call now using Zoiper and see if it works.

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Hrishikesh Karanjikar
Sent: Monday, July 24, 2017 9:08 AM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] restund_process Execution failed on bono

Hi Andrew,
I added the entires in /etc/local/hosts on bono node and after restart I was able to see everything working fine.
So now monit shows all processes on all 6 nodes are running fine.
Will local configuration will have any effect on clearwater deployment?
I still have not tested any calling using Zoiper or live test suite. This is the 1st time I have got all 6 nodes working fine.

Thanks
Hrishikesh


On Thu, Jul 20, 2017 at 4:12 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Yes even with an entry for icscf.cwsprout in /etc/hosts I would still expect you to get the DNS error in Bono as there are still not entries for icscf.cwpsrout on your DNS server.

How to add entries to your DNS server will depend on which DNS server you have chosen to use, most of my testing has been done with a DNS server called BIND. You can find details on how to install BIND and how to configure a client to use it here<http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html?#bind>.

Please let me know if you have any further questions.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Friday, July 14, 2017 11:17 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] restund_process Execution failed on bono

Hi Andrew,
Thanks for your support.
I am not in a position to modify the current dns server nor our admins would do it.
I have added entry for icscf.cwsprout in /etc/hosts. I am able to ping icscf.cwsprout after adding this entry.
Still i get same error in the log.

===========================================================
cwbono at cwbono:~$ ping icscf.cwsprout
PING icscf.cwsprout (10.48.12.143) 56(84) bytes of data.
64 bytes from cwsprout (10.48.12.143): icmp_seq=1 ttl=64 time=0.157 ms
64 bytes from cwsprout (10.48.12.143): icmp_seq=2 ttl=64 time=0.129 ms
^C
--- icscf.cwsprout ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.129/0.143/0.157/0.014 ms

cwbono at cwbono:~$ cat /var/log/bono/bono_current.txt

14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
14-07-2017 10:10:19.440 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

===========================================================
If this is not working then I might have to create a dns server locally. Then i would be able to modify or add entries
Is it possible for you to give me an example of how these entries are added?
Thanks
Hrishikesh

On Thu, Jul 13, 2017 at 9:30 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Thank you for the updated diagnostics.

We can still see the following appearing in log files:

?Failed to resolve icscf.cwsprout to an IP address?

The icscf.<sprout hostname> DNS record is used by the P-CSCF (in this case Bono) to identify which I-CSCF to forward requests on to.

Even when assigning nodes IP addresses through DHCP you must still configure your DNS with all the records that Clearwater nodes require to communicate with each other (such as icscf.<sprout hostname>). You can find a list of all the records that Clearwater requires here<http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html>.

Please could you try updating your DNS server to contain these records and restart your nodes, let me know if you still hit the issue.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, July 13, 2017 10:40 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] restund_process Execution failed on bono

Hi,
I have changed my deployment completely.
No I am not using static IP addresses any more and each node is getting IP addresses from the DHCP server.
Each node is able to ping one another using hostname cwellis, cwsprout, cwbono, cwvellum, cwhomer, cwdime.
From bono node I am able to ping, ssh to cwsprout. Check the logs,

===============================================================================

cwbono at cwbono:~$ ping cwsprout
PING cwsprout.amcc.com<http://cwsprout.amcc.com> (10.48.12.143) 56(84) bytes of data.
64 bytes from cwsprout.amcc.com<http://cwsprout.amcc.com> (10.48.12.143): icmp_seq=1 ttl=64 time=0.126 ms
64 bytes from cwsprout.amcc.com<http://cwsprout.amcc.com> (10.48.12.143): icmp_seq=2 ttl=64 time=0.203 ms
64 bytes from cwsprout.amcc.com<http://cwsprout.amcc.com> (10.48.12.143): icmp_seq=3 ttl=64 time=0.210 ms

cwbono at cwbono:~$ ssh cwsprout at cwsprout
The authenticity of host 'cwsprout (10.48.12.143)' can't be established.
ECDSA key fingerprint is 03:f8:42:81:36:ef:b1:be:7a:d8:3d:52:b5:74:f9:ba.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'cwsprout,10.48.12.143' (ECDSA) to the list of known hosts.
cwsprout at cwsprout's password:
Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-31-generic x86_64)

 * Documentation:  https://help.ubuntu.com/

  System information as of Thu Jul 13 14:08:07 IST 2017

  System load:  0.12              Processes:           115
  Usage of /:   27.7% of 7.26GB   Users logged in:     0
  Memory usage: 8%                IP address for eth0: 10.48.12.143
  Swap usage:   0%

  Graph this data and manage this system at:
    https://landscape.canonical.com/

New release '16.04.2 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Thu Jul 13 14:08:07 2017 from hdk-supermicro.amcc.com<http://hdk-supermicro.amcc.com>
[sprout]cwsprout at cwsprout:~$

[sprout]cwsprout at cwsprout:~$ exit
logout
Connection to cwsprout closed.
cwbono at cwbono:~$
cwbono at cwbono:~$
cwbono at cwbono:~$ cat /etc/clearwater/local_config
local_ip=10.48.12.173
public_ip=10.48.12.173
public_hostname=cwbono
etcd_cluster="10.48.12.142,10.48.12.143,10.48.12.173,10.48.12.140,10.48.12.139,10.48.12.120"
cwbono at cwbono:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout
sprout_registration_store=10.48.12.120 #vellum
hs_hostname=10.48.12.139:8888<http://10.48.12.139:8888> #dime
hs_provisioning_hostname=10.48.12.139:8889<http://10.48.12.139:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=10.48.12.140:7888<http://10.48.12.140:7888> #homer
chronos_hostname=vellum
cassandra_hostname=10.48.12.120 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


cwbono at cwbono:~$ clearwater-etcdctl cluster-health
member 2d821d7a0a7736b4 is healthy: got healthy result from http://10.48.12.142:4000
member 6201151ee7f99f5c is healthy: got healthy result from http://10.48.12.139:4000
member 895efb70c4b1b8b4 is healthy: got healthy result from http://10.48.12.120:4000
member c7f5d6485fb4735b is healthy: got healthy result from http://10.48.12.143:4000
member f7726a4e29ec7d3d is healthy: got healthy result from http://10.48.12.173:4000
member ffb968d2990c63f0 is healthy: got healthy result from http://10.48.12.140:4000
cluster is healthy
cwbono at cwbono:~$ clearwater-etcdctl member list
2d821d7a0a7736b4: name=10-48-12-142 peerURLs=http://10.48.12.142:2380 clientURLs=http://10.48.12.142:4000 isLeader=false
6201151ee7f99f5c: name=10-48-12-139 peerURLs=http://10.48.12.139:2380 clientURLs=http://10.48.12.139:4000 isLeader=false
895efb70c4b1b8b4: name=10-48-12-120 peerURLs=http://10.48.12.120:2380 clientURLs=http://10.48.12.120:4000 isLeader=false
c7f5d6485fb4735b: name=10-48-12-143 peerURLs=http://10.48.12.143:2380 clientURLs=http://10.48.12.143:4000 isLeader=false
f7726a4e29ec7d3d: name=10-48-12-173 peerURLs=http://10.48.12.173:2380 clientURLs=http://10.48.12.173:4000 isLeader=true
ffb968d2990c63f0: name=10-48-12-140 peerURLs=http://10.48.12.140:2380 clientURLs=http://10.48.12.140:4000 isLeader=false

cwbono at cwbono:~$ cw-check_cluster_state
This script prints the status of the Cassandra, Chronos, and Memcached clusters.
This node (10.48.12.173) should not be in any cluster.

Describing the Cassandra cluster:
  The cluster is stable
    10.48.12.120 is in state normal

Describing the Chronos cluster:
  The cluster is stable
    10.48.12.120 is in state normal

Describing the Memcached cluster:
  The cluster is stable
    10.48.12.120 is in state normal

cwbono at cwbono:~$ sudo cw-check_config_sync
 - /etc/clearwater/dns.json is up to date
 - /etc/clearwater/shared_config is up to date

cwbono at cwbono:~$ sudo monit summary
Monit 5.18.1 uptime: 54m
 Service Name                     Status                      Type
 node-cwbono                      Running                     System
 restund_process                  Execution failed | Does...  Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Wait parent                 Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program

cwbono at cwbono:~$ cat /var/log/bono/bono_current.txt
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
13-07-2017 09:34:34.680 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

cwbono at cwbono:~$ cat /var/log/monit.log
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:03:29] error    : 'restund_process' process is not running
[IST Jul 13 15:03:29] info     : 'restund_process' trying to restart
[IST Jul 13 15:03:29] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:04:00] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:04:10] error    : 'restund_process' process is not running
[IST Jul 13 15:04:10] info     : 'restund_process' trying to restart
[IST Jul 13 15:04:10] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:04:40] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:04:50] error    : 'restund_process' process is not running
[IST Jul 13 15:04:50] info     : 'restund_process' trying to restart
[IST Jul 13 15:04:50] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:05:20] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:05:30] error    : 'restund_process' process is not running
[IST Jul 13 15:05:30] info     : 'restund_process' trying to restart
[IST Jul 13 15:05:30] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:06:01] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:06:11] error    : 'restund_process' process is not running
[IST Jul 13 15:06:11] info     : 'restund_process' trying to restart
[IST Jul 13 15:06:11] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:06:41] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:06:51] error    : 'restund_process' process is not running
[IST Jul 13 15:06:51] info     : 'restund_process' trying to restart
[IST Jul 13 15:06:51] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul 13 15:07:21] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=10.48.12.173:0<http://10.48.12.173:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in use
[IST Jul 13 15:07:31] error    : 'restund_process' process is not running
[IST Jul 13 15:07:31] info     : 'restund_process' trying to restart
[IST Jul 13 15:07:31] info     : 'restund_process' restart: /etc/init.d/restund

===============================================================================
I don't know whats going wrong here.
What is icscf.cwsprout?

Thanks
Hrishikesh


On Tue, Jul 11, 2017 at 5:08 PM, Hrishikesh Karanjikar <hkaranjikar at apm.com<mailto:hkaranjikar at apm.com>> wrote:
Hi,
Thanks a lot for your reply.
I am using virtualbox and host only network.
The DHCP server runs inside virtualbox and I can only configure the IP address range.
However I am not using the DHCP server and assigning static IP addresses to all nodes which are within the DHCP server IP address range.
I added entry of cwsprout in /ets/hosts of bono node as follows,

===============================================
127.0.0.1       localhost
127.0.1.1       cwbono
192.168.56.103  cwsprout
192.168.56.103  icscf.cwsprout

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
192.168.56.103 cwsprout
192.168.56.103  icscf.cwsprout
::1 localhost # added by clearwater-infrastructure 1hosts script
192.168.56.104 cwbono #+clearwater-infrastructure

===============================================
I am also able to ping cwsprout and icscf.cwsprout from bono,
Here is the log,

===============================================

[bono]cwbono at cwbono:~$ ping cwsprout
PING cwsprout (192.168.56.103) 56(84) bytes of data.
64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.197 ms
64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.217 ms
^C
--- cwsprout ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.197/0.207/0.217/0.010 ms
[bono]cwbono at cwbono:~$
[bono]cwbono at cwbono:~$
[bono]cwbono at cwbono:~$
[bono]cwbono at cwbono:~$ ping icscf.cwsprout
PING icscf.cwsprout (192.168.56.103) 56(84) bytes of data.
64 bytes from cwsprout (192.168.56.103): icmp_seq=1 ttl=64 time=0.112 ms
64 bytes from cwsprout (192.168.56.103): icmp_seq=2 ttl=64 time=0.165 ms
===============================================
I am using static IP addresses as I have to specify them in local_config file of each node.
If I use DHCP server of VirtualBox they might change. In that case I am not sure how do I cope up with local_config.
Can I modify local_config after all nodes are up?

Thanks
Hrishikesh


On Tue, Jul 11, 2017 at 2:42 PM, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> wrote:
Hi Hrishikesh,

Thank you for your question and the detailed logs you have provided.

The issues appears to be caused by your shared config, in the manual installation instructions<http://clearwater.readthedocs.io/en/stable/Manual_Install.html> you?ll see that we advise that the entries in shared config have a format like:

sprout_hostname=sprout.<site_name>.<zone>
sprout_registration_store=vellum.<site_name>.<zone>
hs_hostname=hs.<site_name>.<zone>:8888

In your shared config you have:

sprout_hostname=cwsprout
sprout_registration_store=192.168.56.107 #vellum
I don?t think the sprout_hostname used here will resolve, we can see evidence for this in the Bono logs, Bono attempts to resolve icscf.<sprout_hostname> to find which location to forward SIP messages on you, you can see it is failing to do that here:

Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

To resolve this issue you should change your shared config to use hostnames which have been configured in your DNS server to resolve to the appropriate location. Once you have done this run the command ?sudo cw-upload_shared_config?. Please let me know if this does not resolve the issue.

Thanks,

Andrew


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, July 6, 2017 10:22 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] restund_process Execution failed on bono

Hello,
I have Manually installed all 6 nodes on VMs using virtualbox.
I followed the procedure given @ http://clearwater.readthedocs.io/en/stable/Manual_Install.html
Looks like all nodes except Dime are running fine.
I am getting error "restund_process Execution failed" in monit summary on Bono node.
Here is the shared and local config file,

#####################################################################

[bono]cwbono at cwbono:~$ cat /etc/clearwater/shared_config

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout
sprout_registration_store=192.168.56.107 #vellum
hs_hostname=192.168.56.106:8888<http://192.168.56.106:8888> #dime
hs_provisioning_hostname=192.168.56.106:8889<http://192.168.56.106:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.105:7888<http://192.168.56.105:7888> #homer
chronos_hostname=192.168.56.107 #vellum
cassandra_hostname=192.168.56.107 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret
[bono]cwbono at cwbono:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.104
public_ip=192.168.56.104
public_hostname=cwbono
etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,192.168.56.105,192.168.56.106,192.168.56.107"

#####################################################################
The logs for bono node are as follows,

#####################################################################
[bono]cwbono at cwbono:~$ sudo monit summary
[sudo] password for cwbono:
Monit 5.18.1 uptime: 2d 16h 39m
 Service Name                     Status                      Type
 node-cwbono                      Running                     System
 restund_process                  Execution failed | Does...  Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Wait parent                 Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program


05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.748 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:15.843 UTC Status main.cpp:1358: Quiesce signal received
05-07-2017 11:30:15.843 UTC Status stack.cpp:125: Setting quiescing = PJ_TRUE
05-07-2017 11:30:15.851 UTC Status stack.cpp:156: Quiescing state changed
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing Manager received input QUIESCE (0) when in state ACTIVE (0)
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:265: Close untrusted listening port
05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport for port 5060
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:273: Quiesce FlowTable
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing Manager received input FLOWS_GONE (1) when in state QUIESCING_FLOWS (1)
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:290: Closing trusted port
05-07-2017 11:30:15.851 UTC Status stack.cpp:368: Destroyed TCP transport for port 5058
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:296: Quiescing all connections
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:162: Start quiescing connections
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:175: Quiescing 0 transactions
05-07-2017 11:30:15.851 UTC Status connection_tracker.cpp:180: Connection quiescing complete
05-07-2017 11:30:15.851 UTC Status quiescing_manager.cpp:139: The Quiescing Manager received input CONNS_GONE (2) when in state QUIESCING_CONNS (2)
05-07-2017 11:30:15.851 UTC Status main.cpp:1380: Quiesce complete
05-07-2017 11:30:15.853 UTC Status stack.cpp:171: PJSIP thread ended
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)
05-07-2017 11:30:16.749 UTC Error sip_connection_pool.cpp:189: Failed to resolve icscf.cwsprout to an IP address - Not found (PJ_ENOTFOUND)

[bono]cwbono at cwbono:~$ cat /var/log/monit.log

httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0<http://192.168.56.104:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:47:46] error    : 'restund_process' process is not running
[IST Jul  6 14:47:46] info     : 'restund_process' trying to restart
[IST Jul  6 14:47:46] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul  6 14:48:16] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0<http://192.168.56.104:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:48:26] error    : 'restund_process' process is not running
[IST Jul  6 14:48:26] info     : 'restund_process' trying to restart
[IST Jul  6 14:48:26] info     : 'restund_process' restart: /etc/init.d/restund
[IST Jul  6 14:48:56] error    : 'restund_process' failed to restart (exit status 0) -- /etc/init.d/restund: httpdb: configured url http://hs.example.com:8888/impi/%s/digest<http://hs.example.com:8888/impi/%25s/digest>
httpd: using URI workaround
turn: server deployed behind static NAT addr=192.168.56.104:0<http://192.168.56.104:0>
turn: extended channels enabled
tcp: sock_bind: bind: Address already in u
[IST Jul  6 14:49:06] error    : 'restund_process' process is not running
[IST Jul  6 14:49:06] info     : 'restund_process' trying to restart
[IST Jul  6 14:49:06] info     : 'restund_process' restart: /etc/init.d/restund


[bono]cwbono at cwbono:~$ clearwater-etcdctl cluster-health
member 9c1928228d308a0f is healthy: got healthy result from http://192.168.56.107:4000
member b0c9c017e0d47e14 is healthy: got healthy result from http://192.168.56.106:4000
member d44832212a08c43f is healthy: got healthy result from http://192.168.56.103:4000
member ef1a9a8a2fd05283 is healthy: got healthy result from http://192.168.56.104:4000
member f63afbe816fb463d is healthy: got healthy result from http://192.168.56.102:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono at cwbono:~$ cw-check_cluster_state
This script prints out the status of the Chronos, Memcached and Cassandra clusters.

Describing the Vellum Chronos cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

Describing the Vellum Memcached cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

Describing the Vellum Cassandra cluster:
  The local node is *not* in this cluster
  The cluster is stable
    192.168.56.107 is in state normal

[bono]cwbono at cwbono:~$ clearwater-etcdctl member list
9c1928228d308a0f: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
b0c9c017e0d47e14: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=true
d44832212a08c43f: name=192-168-56-103 peerURLs=http://192.168.56.103:2380 clientURLs=http://192.168.56.103:4000 isLeader=false
ef1a9a8a2fd05283: name=192-168-56-104 peerURLs=http://192.168.56.104:2380 clientURLs=http://192.168.56.104:4000 isLeader=false
f63afbe816fb463d: name=192-168-56-102 peerURLs=http://192.168.56.102:2380 clientURLs=http://192.168.56.102:4000 isLeader=false
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono at cwbono:~$ sudo cw-check_config_sync
[sudo] password for cwbono:
 - /etc/clearwater/dns.json is up to date
 - /etc/clearwater/shared_config is up to date


#####################################################################
The logs for other nodes are as follows

#####################################################################

[ellis]cwellis at cwellis:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 21h 15m
 Service Name                     Status                      Type
 node-cwellis                     Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[sprout]cwsprout at cwsprout:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 7m
 Service Name                     Status                      Type
 node-cwsprout                    Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[homer]cwhomer at cwhomer:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 2m
 Service Name                     Status                      Type
 node-cwhomer                     Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program

[dime]cwdime at cwdime:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 2m
 Service Name                     Status                      Type
 node-cwdime                      Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


[vellum]cwvellum at cwvellum:~$ sudo monit summary
Monit 5.18.1 uptime: 1d 20h 3m
 Service Name                     Status                      Type
 node-cwvellum                    Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program


#####################################################################
Please let me know if I am missing any configuration.
Thanks
Hrishikesh

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170726/7f80aa82/attachment.html>

From michele.furlanetto at aleagames.com  Wed Jul 26 05:27:43 2017
From: michele.furlanetto at aleagames.com (Michele Furlanetto)
Date: Wed, 26 Jul 2017 11:27:43 +0200
Subject: [Project Clearwater] [Clearwater] AS Configuration and
 untrusted sources
In-Reply-To: <BLUPR02MB437901136FF4E64F7DC3D14E5B80@BLUPR02MB437.namprd02.prod.outlook.com>
References: <7296A131-7DDD-43F1-9F00-5F85FCF40FF7@aleagames.com>
	<3ACF4837-B492-45A2-8B83-14202C9CFDEE@aleagames.com>
	<BLUPR02MB43730770DB57A115A4B287BE5BB0@BLUPR02MB437.namprd02.prod.outlook.com>
	<AAB4BECC-919D-43AB-A00A-78C1110C3244@aleagames.com>
	<BLUPR02MB437901136FF4E64F7DC3D14E5B80@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <4AACAE0B-0199-49B4-9234-1FF4FFE2AEFC@aleagames.com>

Hi Andrew,

As I said, I didn?t configure any DNS service to support our AS, 
all I did in this way was adding the lines to /etc/hosts

10.2.0.91 participant.mcptt.example.com
10.2.0.127 example.com <http://example.com/>

In attachment you can find the local_config and shared_config files.

Referring the previous message, my SIP client does use the same port for both REGISTER and SUBSCRIBE.

Thank you for your support,
Michele




> Il giorno 25 lug 2017, alle ore 15:40, Andrew Edmonds <Andrew.Edmonds at metaswitch.com> ha scritto:
> 
> Hi Michele,
>  
> I?ve spoken to a colleague about this issue and have a new idea now as to why the SUBSCRIBE message is being rejected.
>  
> After Bono receives the SUBSCRIBE from the SIP device we can see the log:
>  
> 24-07-2017 10:38:25.414 UTC Verbose pjsip: tcpc0x7f094c03 TCP transport 10.2.0.127:5058 is connecting to 10.2.0.127:5060..
>  
> 10.2.0.127 is the Bono node?s IP address. So this is telling us that Bono is forwarding the SUBSCRIBE to itself from its trusted port 5058 (used to communicate with the IMS core) to its untrusted port 5060 (which SIP devices send their requests to). We then see in the logs Bono receiving this SUBSCRIBE and then rejecting it.
>  
> So the question we need to resolve now becomes why is Bono routing the SUBSCRIBE to itself? To answer this we need to understand how SIP messages are routed. Messages containing SIP methods (like INVITEs, SUBSCRIBEs, REGISTERs etc.) are routed based off of the Route-header. If we look at the route headers for one of the SUBSCRIBERs we see:
>  
> Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690 <sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690>
>  
> Here the port of the S-CSCF (5054) has not been included in the SIP address of the S-CSCF. We can also see from the logs that we do not retrieve a port number via SRV DNS lookup:
>  
> 24-07-2017 10:38:25.414 UTC Debug dnscachedresolver.cpp:686: Received DNS response for _sip._udp.scscf.cw-aio type SRV
> 24-07-2017 10:38:25.414 UTC Warning dnscachedresolver.cpp:828: Failed to retrieve record for _sip._udp.scscf.cw-aio: Domain name not found
>  
> This means that Bono does not know which port to forward the message to and so it chooses 5060 which routes the message back to itself.
>  
> The next thing to consider then is why does the Route-Header not contain the port number? The Route header which is to be applied to future requests for a subscriber is communicated to Bono by the S-CSCF in a Service-Route header after it successfully authenticates a subscriber. If we have a look at the 200 OK in response to the registration we see:
>  
> Service-Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690 <sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690>
>  
> This means that it is the S-CSCF that is causing Bono to attach a Route header to the SUBSCRIBE which does not contain a port number. To figure out why the S-CSCF is doing this could you please send me a copy of /etc/clearwater/shared_config and /etc/clearwater/local_config from the SPN and a copy of the SPN log file during a REGISTRATION and SUBSCRIBE.
>  
> Thanks,
>  
> Andrew
>  
>  
> From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Michele Furlanetto
> Sent: Monday, July 24, 2017 3:08 PM
> To: clearwater at lists.projectclearwater.org
> Subject: Re: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources
>  
> Hi Andrew, 
> Thanks for your response.
>  
> In attachment you can find what you asked me.
> It?s about 15 minutes of logs, beginning with the service start.
> An example of message being rejected can be found at line 72104, while the rejection is at 72399.
>  
> I had to censor part of contents and some headers of the messages, so there may be some inconsistencies.
>  
>  
> Thanks,
> Michele
>  
>  
> Il giorno 24 lug 2017, alle ore 11:53, Andrew Edmonds <Andrew.Edmonds at metaswitch.com <mailto:Andrew.Edmonds at metaswitch.com>> ha scritto:
>  
> Hi Michele,
>  
> Thank you for your question.
>  
> It may be the case that you need to update your firewall rules to allow traffic from your SIP client to reach the Application Server. However in order to verify this I will need the Bono logs from the time which you attempted to contact the AS. Could you please provide these logs?
>  
> Thanks,
>  
> Andrew
>  
> From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org <mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Michele Furlanetto
> Sent: Tuesday, July 18, 2017 12:58 PM
> To: clearwater at lists.projectclearwater.org <mailto:clearwater at lists.projectclearwater.org>
> Subject: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources
>  
> Hi all,
> I?ve got a problem while configuring an AIO image.
> 
> In my scenario, there are is an AS, say as1.service.example.com <http://as1.service.example.com/>, which should get the third-party registration as well as being able to be addressed directly from the Sip client.
> 
> While third-party registration works, I?m unable to contact the AS directly.
> Looking at Bono logs, I get Info bono.cpp:1356: Rejecting request from untrusted source.
> 
> Here is the content of /usr/share/clearwater/ellis/web-content/js/app-servers.json, indented to be more readable
> {
> ?SERVICE?:"
> <InitialFilterCriteria>
>             <Priority>0</Priority>
>             <TriggerPoint>
>                         <ConditionTypeCNF>0</ConditionTypeCNF>
>                         <SPT>
>                                     <ConditionNegated>0</ConditionNegated>
>                                     <Group>0</Group>
>                                     <Method>REGISTER</Method>
>                                     <Extension>
>                                                 <RegistrationType>0</RegistrationType>
>                                                 <RegistrationType>1</RegistrationType>
>                                                 <RegistrationType>2</RegistrationType>
>                                     </Extension>
>                         </SPT>
>                         <SPT>
>                                     <ConditionNegated>0</ConditionNegated>
>                                     <Group>1</Group>
>                                     <RequestURI>sip:as1.service.example.com <http://service.example.com/></RequestURI>
>                         </SPT>
>             </TriggerPoint>
>             <ApplicationServer>
>                         <ServerName>sip:as1.service.example.com <http://service.example.com/>:5071;transport=UDP</ServerName>
>                         <DefaultHandling>0</DefaultHandling>
>                         <Extension>
>                                     <IncludeRegisterRequest/>
>                         </Extension>
>             </ApplicationServer>
> </InitialFilterCriteria>"
> }
> 
> Some other details:
> - SERVICE is checked in Ellis for test users;
> - as1.service.example.com <http://as1.service.example.com/> is resolved using /etc/hosts.
> 
> 
> Any hint on the error(s)?
> Thanks,
> Michele
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org <mailto:Clearwater at lists.projectclearwater.org>
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org <http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org>
>  
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org <mailto:Clearwater at lists.projectclearwater.org>
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org <http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170726/2f246a02/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: local and shared configs.zip
Type: application/zip
Size: 1498 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170726/2f246a02/attachment.zip>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170726/2f246a02/attachment-0001.html>

From michele.furlanetto at aleagames.com  Wed Jul 26 11:40:10 2017
From: michele.furlanetto at aleagames.com (Michele Furlanetto)
Date: Wed, 26 Jul 2017 17:40:10 +0200
Subject: [Project Clearwater] [Clearwater] AS Configuration and
 untrusted sources
In-Reply-To: <4AACAE0B-0199-49B4-9234-1FF4FFE2AEFC@aleagames.com>
References: <7296A131-7DDD-43F1-9F00-5F85FCF40FF7@aleagames.com>
	<3ACF4837-B492-45A2-8B83-14202C9CFDEE@aleagames.com>
	<BLUPR02MB43730770DB57A115A4B287BE5BB0@BLUPR02MB437.namprd02.prod.outlook.com>
	<AAB4BECC-919D-43AB-A00A-78C1110C3244@aleagames.com>
	<BLUPR02MB437901136FF4E64F7DC3D14E5B80@BLUPR02MB437.namprd02.prod.outlook.com>
	<4AACAE0B-0199-49B4-9234-1FF4FFE2AEFC@aleagames.com>
Message-ID: <6E2AAAE8-F6FB-4E4A-97B0-0480FC01EAA1@aleagames.com>

I went a little forward, removing the line relative to as1.service.example.com <http://as1.service.example.com/> in file /etc/hosts and adding 

srv-host=_sip._udp.scscf.cw-aio,scscf.cw-aio,5054
srv-host=_sip._udp.as1.service.example.com,example.com,5071

to /etc/dnsmasq.conf.

now the SUBSCRIBEs (and other messages) are delivered to as1, but I?ve lost the working third-party registration.

After fixing this, the next step for me will be adding another AS, as2, which will dialog only with as1.
The simplest think I could try is adding 
srv-host=_sip._udp.as2.service.example.com,example.com,5072
but is not enough. What configuration I?m missing?

Thank you,
Michele


> Il giorno 26 lug 2017, alle ore 11:27, Michele Furlanetto <michele.furlanetto at aleagames.com> ha scritto:
> 
> Hi Andrew,
> 
> As I said, I didn?t configure any DNS service to support our AS, 
> all I did in this way was adding the lines to /etc/hosts
> 
> 10.2.0.91 as1.service.example.com <http://participant.mcptt.example.com/>
> 10.2.0.127 example.com <http://example.com/>
> 
> In attachment you can find the local_config and shared_config files.
> 
> Referring the previous message, my SIP client does use the same port for both REGISTER and SUBSCRIBE.
> 
> Thank you for your support,
> Michele
> 
> <local and shared configs.zip>
> 
> 
>> Il giorno 25 lug 2017, alle ore 15:40, Andrew Edmonds <Andrew.Edmonds at metaswitch.com <mailto:Andrew.Edmonds at metaswitch.com>> ha scritto:
>> 
>> Hi Michele,
>>  
>> I?ve spoken to a colleague about this issue and have a new idea now as to why the SUBSCRIBE message is being rejected.
>>  
>> After Bono receives the SUBSCRIBE from the SIP device we can see the log:
>>  
>> 24-07-2017 10:38:25.414 UTC Verbose pjsip: tcpc0x7f094c03 TCP transport 10.2.0.127:5058 is connecting to 10.2.0.127:5060..
>>  
>> 10.2.0.127 is the Bono node?s IP address. So this is telling us that Bono is forwarding the SUBSCRIBE to itself from its trusted port 5058 (used to communicate with the IMS core) to its untrusted port 5060 (which SIP devices send their requests to). We then see in the logs Bono receiving this SUBSCRIBE and then rejecting it.
>>  
>> So the question we need to resolve now becomes why is Bono routing the SUBSCRIBE to itself? To answer this we need to understand how SIP messages are routed. Messages containing SIP methods (like INVITEs, SUBSCRIBEs, REGISTERs etc.) are routed based off of the Route-header. If we look at the route headers for one of the SUBSCRIBERs we see:
>>  
>> Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690 <sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690>
>>  
>> Here the port of the S-CSCF (5054) has not been included in the SIP address of the S-CSCF. We can also see from the logs that we do not retrieve a port number via SRV DNS lookup:
>>  
>> 24-07-2017 10:38:25.414 UTC Debug dnscachedresolver.cpp:686: Received DNS response for _sip._udp.scscf.cw-aio type SRV
>> 24-07-2017 10:38:25.414 UTC Warning dnscachedresolver.cpp:828: Failed to retrieve record for _sip._udp.scscf.cw-aio: Domain name not found
>>  
>> This means that Bono does not know which port to forward the message to and so it chooses 5060 which routes the message back to itself.
>>  
>> The next thing to consider then is why does the Route-Header not contain the port number? The Route header which is to be applied to future requests for a subscriber is communicated to Bono by the S-CSCF in a Service-Route header after it successfully authenticates a subscriber. If we have a look at the 200 OK in response to the registration we see:
>>  
>> Service-Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690 <sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690>
>>  
>> This means that it is the S-CSCF that is causing Bono to attach a Route header to the SUBSCRIBE which does not contain a port number. To figure out why the S-CSCF is doing this could you please send me a copy of /etc/clearwater/shared_config and /etc/clearwater/local_config from the SPN and a copy of the SPN log file during a REGISTRATION and SUBSCRIBE.
>>  
>> Thanks,
>>  
>> Andrew
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170726/0437d73c/attachment.html>

From hkaranjikar at apm.com  Thu Jul 27 07:21:48 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Thu, 27 Jul 2017 16:51:48 +0530
Subject: [Project Clearwater] SIP 408 - Request Timeout
Message-ID: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>

Hi,

I have used manual installation for clearwater.
I have created 6 VMs using Virtualbox and using host only network.
All 6 nodes are running fine as per monit logs.
I have installed Zoiper client and tried to add new account as follows,

===================================================
Zoiper Preferences

Domain        -    example.com
Username    -    6505550708
Password    -
Auth. Uname    -    6505550708 at example.com
Outbound Proxy    -    192.168.56.105:8060

Error        -     SIP 408 - Request Timeout


Private Identity Generated by Ellis

Private Identity:

6505550028 at example.com
Password:Na5ZWdQj4

===================================================

How ever I am getting error mentioned in subject line.
Here are other logs for each node,

===================================================

[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888 #dime
hs_provisioning_hostname=192.168.56.109:8889 #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888 #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from
http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from
http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from
http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from
http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from
http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from
http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from
http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from
http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from
http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from
http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat
/etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888 #dime
hs_provisioning_hostname=192.168.56.109:8889 #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888 #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from
http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from
http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from
http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from
http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from
http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from
http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from
http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from
http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from
http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from
http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from
http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from
http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from
http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from
http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from
http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from
http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from
http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from
http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from
http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from
http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from
http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from
http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from
http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from
http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from
http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from
http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from
http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from
http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from
http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from
http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$
192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from
http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from
http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from
http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from
http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from
http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from
http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from
http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from
http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from
http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from
http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from
http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from
http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from
http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from
http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from
http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from
http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from
http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from
http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from
http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from
http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from
http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380
clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380
clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380
clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380
clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380
clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380
clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$


===================================================

Please let me know what is wrong with the setup?

Thanks
Hrishikesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170727/75029682/attachment.html>

From Andrew.Edmonds at metaswitch.com  Mon Jul 31 05:07:52 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Mon, 31 Jul 2017 09:07:52 +0000
Subject: [Project Clearwater] [Clearwater] AS Configuration and
 untrusted sources
In-Reply-To: <6E2AAAE8-F6FB-4E4A-97B0-0480FC01EAA1@aleagames.com>
References: <7296A131-7DDD-43F1-9F00-5F85FCF40FF7@aleagames.com>
	<3ACF4837-B492-45A2-8B83-14202C9CFDEE@aleagames.com>
	<BLUPR02MB43730770DB57A115A4B287BE5BB0@BLUPR02MB437.namprd02.prod.outlook.com>
	<AAB4BECC-919D-43AB-A00A-78C1110C3244@aleagames.com>
	<BLUPR02MB437901136FF4E64F7DC3D14E5B80@BLUPR02MB437.namprd02.prod.outlook.com>
	<4AACAE0B-0199-49B4-9234-1FF4FFE2AEFC@aleagames.com>
	<6E2AAAE8-F6FB-4E4A-97B0-0480FC01EAA1@aleagames.com>
Message-ID: <BLUPR02MB4371DC95390C2D4E830905AE5B20@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Michele,

Could I get some more diagnostics to try and work out why third-party registration is no longer working. The Sprout logs (which can be found on SPNs under /var/log/sprout/sprout_current.txt) for a registration may be helpful here.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Michele Furlanetto
Sent: Wednesday, July 26, 2017 4:40 PM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] [Clearwater] AS Configuration and untrusted sources

I went a little forward, removing the line relative to as1.service.example.com<http://as1.service.example.com> in file /etc/hosts and adding

srv-host=_sip._udp.scscf.cw-aio,scscf.cw-aio,5054
srv-host=_sip._udp.as1.service.example.com<http://udp.as1.service.example.com>,example.com<http://example.com>,5071

to /etc/dnsmasq.conf.

now the SUBSCRIBEs (and other messages) are delivered to as1, but I?ve lost the working third-party registration.

After fixing this, the next step for me will be adding another AS, as2, which will dialog only with as1.
The simplest think I could try is adding
srv-host=_sip._udp.as2.service.example.com<http://udp.as2.service.example.com>,example.com<http://example.com>,5072
but is not enough. What configuration I?m missing?

Thank you,
Michele


Il giorno 26 lug 2017, alle ore 11:27, Michele Furlanetto <michele.furlanetto at aleagames.com<mailto:michele.furlanetto at aleagames.com>> ha scritto:

Hi Andrew,

As I said, I didn?t configure any DNS service to support our AS,
all I did in this way was adding the lines to /etc/hosts

10.2.0.91 as1.service.example.com<http://participant.mcptt.example.com/>
10.2.0.127 example.com<http://example.com/>

In attachment you can find the local_config and shared_config files.

Referring the previous message, my SIP client does use the same port for both REGISTER and SUBSCRIBE.

Thank you for your support,
Michele

<local and shared configs.zip>


Il giorno 25 lug 2017, alle ore 15:40, Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>> ha scritto:

Hi Michele,

I?ve spoken to a colleague about this issue and have a new idea now as to why the SUBSCRIBE message is being rejected.

After Bono receives the SUBSCRIBE from the SIP device we can see the log:

24-07-2017 10:38:25.414 UTC Verbose pjsip: tcpc0x7f094c03 TCP transport 10.2.0.127:5058 is connecting to 10.2.0.127:5060..

10.2.0.127 is the Bono node?s IP address. So this is telling us that Bono is forwarding the SUBSCRIBE to itself from its trusted port 5058 (used to communicate with the IMS core) to its untrusted port 5060 (which SIP devices send their requests to). We then see in the logs Bono receiving this SUBSCRIBE and then rejecting it.

So the question we need to resolve now becomes why is Bono routing the SUBSCRIBE to itself? To answer this we need to understand how SIP messages are routed. Messages containing SIP methods (like INVITEs, SUBSCRIBEs, REGISTERs etc.) are routed based off of the Route-header. If we look at the route headers for one of the SUBSCRIBERs we see:

Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690

Here the port of the S-CSCF (5054) has not been included in the SIP address of the S-CSCF. We can also see from the logs that we do not retrieve a port number via SRV DNS lookup:

24-07-2017 10:38:25.414 UTC Debug dnscachedresolver.cpp:686: Received DNS response for _sip._udp.scscf.cw-aio type SRV
24-07-2017 10:38:25.414 UTC Warning dnscachedresolver.cpp:828: Failed to retrieve record for _sip._udp.scscf.cw-aio: Domain name not found

This means that Bono does not know which port to forward the message to and so it chooses 5060 which routes the message back to itself.

The next thing to consider then is why does the Route-Header not contain the port number? The Route header which is to be applied to future requests for a subscriber is communicated to Bono by the S-CSCF in a Service-Route header after it successfully authenticates a subscriber. If we have a look at the 200 OK in response to the registration we see:

Service-Route: sip:scscf.cw-aio;transport=udp;lr;orig;username=6505550751%40example.com;nonce=716970855dbe8690

This means that it is the S-CSCF that is causing Bono to attach a Route header to the SUBSCRIBE which does not contain a port number. To figure out why the S-CSCF is doing this could you please send me a copy of /etc/clearwater/shared_config and /etc/clearwater/local_config from the SPN and a copy of the SPN log file during a REGISTRATION and SUBSCRIBE.

Thanks,

Andrew
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170731/5ba750f6/attachment.html>

From rob at projectclearwater.org  Mon Jul 31 05:13:40 2017
From: rob at projectclearwater.org (Robert Day (projectclearwater.org))
Date: Mon, 31 Jul 2017 09:13:40 +0000
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
Message-ID: <BY2PR02MB2149BFFB2D1407E8078AD0FAF4B20@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Hrishikesh,

I think there could be one of two problems here:

?         The request could be failing to reach your Bono node at all, due to incorrect port forwarding, local firewall rules, etc. ? so Zoiper is generating a 408 Request Timeout locally

?         Or the request could be reaching Bono, but it could be failing to pass it on, so Bono is generating the 408 Request Timeout

To see which of these is happening, could you do one of the following (whatever?s easiest)?

?         Take packet capture on your local PC (e.g. with Wireshark), to see if Zoiper is successfully sending messages to Bono and getting the 408 response (or if it?s just retransmitting them until a timeout)

?         Turn on debug logging on Bono (see http://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html#bono), and check the Bono logs to see if there are records of it receiving the SIP messages

I saw from your other emails to the mailing list that you?re using /etc/hosts for DNS records, not an external DNS server. Is this still the case? Bono will attempt to look up SRV records (which /etc/hosts doesn?t provide) in order to reach Sprout nodes by default, which could be the problem here, and setting scscf_uri=?sip: cwsprout1:5054;transport=tcp? should resolve this (as explicitly specifying the port removes the need for an A record lookup).

Thanks,
Rob


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Hrishikesh Karanjikar
Sent: 27 July 2017 12:22
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP 408 - Request Timeout

Hi,
I have used manual installation for clearwater.
I have created 6 VMs using Virtualbox and using host only network.
All 6 nodes are running fine as per monit logs.
I have installed Zoiper client and tried to add new account as follows,

===================================================
Zoiper Preferences

Domain        -    example.com<http://example.com>
Username    -    6505550708
Password    -
Auth. Uname    -    6505550708 at example.com<mailto:6505550708 at example.com>
Outbound Proxy    -    192.168.56.105:8060<http://192.168.56.105:8060>

Error        -     SIP 408 - Request Timeout


Private Identity Generated by Ellis

Private Identity:

6505550028 at example.com<mailto:6505550028 at example.com>
Password:Na5ZWdQj4

===================================================
How ever I am getting error mentioned in subject line.
Here are other logs for each node,

===================================================

[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888<http://192.168.56.109:8888> #dime
hs_provisioning_hostname=192.168.56.109:8889<http://192.168.56.109:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888<http://192.168.56.108:7888> #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888<http://192.168.56.109:8888> #dime
hs_provisioning_hostname=192.168.56.109:8889<http://192.168.56.109:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888<http://192.168.56.108:7888> #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$
192.168.56.109:4000<http://192.168.56.109:4000> isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$


===================================================
Please let me know what is wrong with the setup?
Thanks
Hrishikesh



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170731/0040c182/attachment.html>

From rob at projectclearwater.org  Mon Jul 31 05:13:42 2017
From: rob at projectclearwater.org (Robert Day (projectclearwater.org))
Date: Mon, 31 Jul 2017 09:13:42 +0000
Subject: [Project Clearwater] [Clearwater] call problem 200OK
In-Reply-To: <CAFwW5BHKDNwc-=gypEZ6OnFs5+UR0imZBuDGny2pcxrceGzcfA@mail.gmail.com>
References: <CAFwW5BHKDNwc-=gypEZ6OnFs5+UR0imZBuDGny2pcxrceGzcfA@mail.gmail.com>
Message-ID: <BY2PR02MB2149569920BA97B039CD325CF4B20@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Bader,

I?ve taken a look at your Wireshark traces and I agree ? Bono isn?t receiving the 200 OK.

Looking at the difference between the 180 Ringing and the 200 OK, I can see that:

?         The 180 Ringing goes from 192.168.1.27 to 192.168.1.101 (packet 8896 in trace_all_traffic.pcap) and then is rewritten (by OpenStack floating IP handling) to go from 192.168.1.27 to 10.0.0.13 (packet 8897)

?         However, the 200 OK (frame 14917) is not rewritten in the same way ? I don?t see a 200 OK going to 10.0.0.13

One thing that might be worth trying is using TCP, not UDP, which is a configuration option in most softphones ? TCP can sometimes work better in environments with firewalls/NAT/floating IPs.

If that doesn?t help, though, this looks like an OpenStack problem in handling floating IPs, not a Clearwater problem, so I?d suggest looking at your OpenStack logs and configuration.

Best,
Rob


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Bader LAMTI
Sent: 21 July 2017 14:39
To: clearwater at lists.projectclearwater.org; Andrew Edmonds <Andrew.Edmonds at metaswitch.com>
Subject: [Project Clearwater] [Clearwater] call problem 200OK

Hi,

I have installed Clearwater on Openstack with 7 vms with this configuration:

Ellis         private ip: 10.0.0.6          floating ip: 192.168.1.103
Bono        private ip: 10.0.0.13        floating ip: 192.168.1.101
Sprout      private ip: 10.0.0.15        floating ip: 192.168.1.109
Homer      private ip: 10.0.0.3          floating ip: 192.168.1.110
Dime        private ip: 10.0.0.10        floating ip: 192.168.1.107
Vellum      private ip: 10.0.0.17        floating ip: 192.168.1.105
Dns          private ip: 10.0.0.12         floating ip: 192.168.1.114

I have a problem when I make a call.
The problem is the following : when I answer on the second softphone the first softphone continues ringing.

I see on the capture Wireshark that the 200 OK of the INVITE does not arrive at bono.
Please find attached the logs of bono and  wireshark trace.

I would appreciate if you can help me to solve this issue.

Thanking you + best regards,
--
Bader LAMTI
+33 6 99 57 88 21
?l?ve ing?nieur ? l'Ensimag GRENOBLE-INP

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170731/c1efbcbb/attachment.html>

From Andrew.Edmonds at metaswitch.com  Mon Jul 31 05:14:21 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Mon, 31 Jul 2017 09:14:21 +0000
Subject: [Project Clearwater] SIP 408 - Request Timeout
In-Reply-To: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
References: <CABmBNEZE612nguPbg8-dLhvRDBB0g7VDo8cuHZ87BrPURT-UHg@mail.gmail.com>
Message-ID: <BLUPR02MB437E06C13B70FCC491E0A27E5B20@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Hrishikesh,

Thank you for your question.

I?m not sure whether this is deliberate but it looks like you haven?t included the password in your Zoiper configuration. You must make sure you use the same password the Ellis client gives you.

We have this guide<https://clearwater.readthedocs.io/en/stable/Making_your_first_call.html> that might help you with the configuration you need to use with Zoiper.

If this guide goes not help could I please have Bono and Sprout logs during the time of the registration.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Hrishikesh Karanjikar
Sent: Thursday, July 27, 2017 12:22 PM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP 408 - Request Timeout

Hi,
I have used manual installation for clearwater.
I have created 6 VMs using Virtualbox and using host only network.
All 6 nodes are running fine as per monit logs.
I have installed Zoiper client and tried to add new account as follows,

===================================================
Zoiper Preferences

Domain        -    example.com<http://example.com>
Username    -    6505550708
Password    -
Auth. Uname    -    6505550708 at example.com<mailto:6505550708 at example.com>
Outbound Proxy    -    192.168.56.105:8060<http://192.168.56.105:8060>

Error        -     SIP 408 - Request Timeout


Private Identity Generated by Ellis

Private Identity:

6505550028 at example.com<mailto:6505550028 at example.com>
Password:Na5ZWdQj4

===================================================
How ever I am getting error mentioned in subject line.
Here are other logs for each node,

===================================================

[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888<http://192.168.56.109:8888> #dime
hs_provisioning_hostname=192.168.56.109:8889<http://192.168.56.109:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888<http://192.168.56.108:7888> #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/local_config
local_ip=192.168.56.105
public_ip=192.168.56.105
public_hostname=cwellis1
etcd_cluster="192.168.56.105,192.168.56.106,192.168.56.107,192.168.56.108,192.168.56.109,192.168.56.110"

[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ cat /etc/clearwater/shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################

home_domain=example.com<http://example.com>
sprout_hostname=cwsprout1
sprout_registration_store=192.168.56.110 #vellum
hs_hostname=192.168.56.109:8888<http://192.168.56.109:8888> #dime
hs_provisioning_hostname=192.168.56.109:8889<http://192.168.56.109:8889> #dime
ralf_hostname=
ralf_session_store=
xdms_hostname=192.168.56.108:7888<http://192.168.56.108:7888> #homer
chronos_hostname=vellum
cassandra_hostname=192.168.56.110 #vellum

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


[ellis]cwellis1 at cwellis1:~$ sudo monit summary
[sudo] password for cwellis1:
Monit 5.18.1 uptime: 3h 33m
 Service Name                     Status                      Type
 node-cwellis1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 mysql_process                    Running                     Process
 ellis_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_ellis                       Status ok                   Program
 poll_ellis_https                 Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[ellis]cwellis1 at cwellis1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[ellis]cwellis1 at cwellis1:~$


[bono]cwbono1 at cwbono1:~$ sudo monit summary
[sudo] password for cwbono1:
Monit 5.18.1 uptime: 23m
 Service Name                     Status                      Type
 node-cwbono1                     Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[bono]cwbono1 at cwbono1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$
192.168.56.109:4000<http://192.168.56.109:4000> isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[bono]cwbono1 at cwbono1:~$



[sprout]cwsprout1 at cwsprout1:~$ sudo monit summary
[sudo] password for cwsprout1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwsprout1                   Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memento_process                  Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memento_uptime                   Status ok                   Program
 poll_memento                     Status ok                   Program
 poll_memento_https               Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[sprout]cwsprout1 at cwsprout1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false




[homer]cwhomer1 at cwhomer1:~$ sudo monit summary
[sudo] password for cwhomer1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwhomer1                    Running                     System
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homer_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 poll_homer                       Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[homer]cwhomer1 at cwhomer1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[homer]cwhomer1 at cwhomer1:~$


[dime]cwdime1 at cwdime1:~$ sudo monit summary
[sudo] password for cwdime1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwdime1                     Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[dime]cwdime1 at cwdime1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[dime]cwdime1 at cwdime1:~$



[vellum]cwvellum1 at cwvellum1:~$ sudo monit summary
[sudo] password for cwvellum1:
Monit 5.18.1 uptime: 22m
 Service Name                     Status                      Type
 node-cwvellum1                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 cassandra_uptime                 Status ok                   Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Program
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl cluster-health
member 275f7358be9c56b9 is healthy: got healthy result from http://192.168.56.107:4000
member 37a3412cc7457712 is healthy: got healthy result from http://192.168.56.109:4000
member 59ee11d785e88e75 is healthy: got healthy result from http://192.168.56.110:4000
member d0eda43d23f9dc26 is healthy: got healthy result from http://192.168.56.106:4000
member ed17a7546df2174c is healthy: got healthy result from http://192.168.56.108:4000
member f7132cc88f7a39fa is healthy: got healthy result from http://192.168.56.105:4000
cluster is healthy
[vellum]cwvellum1 at cwvellum1:~$ clearwater-etcdctl member list
275f7358be9c56b9: name=192-168-56-107 peerURLs=http://192.168.56.107:2380 clientURLs=http://192.168.56.107:4000 isLeader=false
37a3412cc7457712: name=192-168-56-109 peerURLs=http://192.168.56.109:2380 clientURLs=http://192.168.56.109:4000 isLeader=false
59ee11d785e88e75: name=192-168-56-110 peerURLs=http://192.168.56.110:2380 clientURLs=http://192.168.56.110:4000 isLeader=false
d0eda43d23f9dc26: name=192-168-56-106 peerURLs=http://192.168.56.106:2380 clientURLs=http://192.168.56.106:4000 isLeader=false
ed17a7546df2174c: name=192-168-56-108 peerURLs=http://192.168.56.108:2380 clientURLs=http://192.168.56.108:4000 isLeader=true
f7132cc88f7a39fa: name=192-168-56-105 peerURLs=http://192.168.56.105:2380 clientURLs=http://192.168.56.105:4000 isLeader=false
[vellum]cwvellum1 at cwvellum1:~$


===================================================
Please let me know what is wrong with the setup?
Thanks
Hrishikesh



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170731/2161bdf2/attachment.html>

