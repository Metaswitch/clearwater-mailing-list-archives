From richard.whitehouse at projectclearwater.org  Thu Mar  1 04:58:11 2018
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Thu, 1 Mar 2018 09:58:11 +0000
Subject: [Project Clearwater] Problems in manual installation of
 clearwater
In-Reply-To: <B89A1E0D519F0B44A3E67F67A33D3AE472DA93@BGSMSX108.gar.corp.intel.com>
References: <B89A1E0D519F0B44A3E67F67A33D3AE472D9CA@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB3362B779BC0E5BEC0D206659F0C70@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DA93@BGSMSX108.gar.corp.intel.com>
Message-ID: <BN6PR02MB336207B283339956ABE9573BF0C60@BN6PR02MB3362.namprd02.prod.outlook.com>

Pushpendra,

I think there are two reasons you are hitting problems installing Clearwater:


1)      The ellis node that you've created has a user called ellis, which as I described below is required to be a system user under which the ellis processes run.

I'd suggest you create the node with a different username (e.g. ubuntu or clearwater).


2)      The ellis server has bind9 installed on it, prior to installing Clearwater, which is conflicting with dnsmasq which Clearwater uses for caching DNS queries. You'll need to uninstall this before installing the Clearwater software.

At a guess, you selected the 'DNS server' task selection when installing the Ubuntu VM. We'd recommend that the only task selection you make as part of installing the Ubuntu VM is 'OpenSSH server', as that's useful to log into the node remotely, which allows you to copy and paste commands, and upload files easily.

We'll make a change to Clearwater so it detects this misconfiguration and requires it to be corrected prior to installing Clearwater.


Hope this helps!


Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 03:41
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com>
Subject: RE: Problems in manual installation of clearwater

Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox, but same errors (dpkg).

thanks

From: Kumar, Pushpendra
Sent: Thursday, March 1, 2018 9:07 AM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Hi Richard,
Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox).

This is the output of netstat -pltun:

[ellis]ellis at Ellis:~$ sudo netstat -pltun
[sudo] password for ellis:
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 10.224.61.25:53         0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:53            0.0.0.0:*               LISTEN      25075/named
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      28512/sshd
tcp        0      0 127.0.0.1:953           0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:2812          0.0.0.0:*               LISTEN      9667/monit
tcp        0      0 127.0.0.1:8000          0.0.0.0:*               LISTEN      2019/nginx
tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      8998/mysqld
tcp        0      0 10.224.61.25:2380       0.0.0.0:*               LISTEN      12039/etcd
tcp6       0      0 :::53                   :::*                    LISTEN      25075/named
tcp6       0      0 :::22                   :::*                    LISTEN      28512/sshd
tcp6       0      0 ::1:953                 :::*                    LISTEN      25075/named
tcp6       0      0 :::4000                 :::*                    LISTEN      12039/etcd
tcp6       0      0 :::80                   :::*                    LISTEN      2019/nginx
udp        0      0 10.224.61.25:53         0.0.0.0:*                           25075/named
udp        0      0 127.0.0.1:53            0.0.0.0:*                           25075/named
udp        0      0 0.0.0.0:68              0.0.0.0:*                           703/dhclient
udp        0      0 10.224.61.25:123        0.0.0.0:*                           9178/ntpd
udp        0      0 127.0.0.1:123           0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:123             0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:49694           0.0.0.0:*                           703/dhclient
udp6       0      0 :::29724                :::*                                703/dhclient
udp6       0      0 :::53                   :::*                                25075/named
udp6       0      0 fe80::a00:27ff:fe11:123 :::*                                9178/ntpd
udp6       0      0 ::1:123                 :::*                                9178/ntpd
udp6       0      0 :::123                  :::*                                9178/ntpd

I am also getting 502 Bad Gateway when I trying to connect ellis using http://ellis.iind.intel.com

Thanks
Pushpendra

From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Sent: Thursday, March 1, 2018 4:21 AM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

It sounds like you attempted to install ellis on a node was installed on a node in which there already was an ellis user - is that correct?

Ellis requires a user to run the components as for security so that we aren't running components as root which don't require root privileges, and this is required to be ellis.

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

>From these logs, and the similar  logs regarding bono, sprout and clearwater-management it looks like there's already a service running on the nodes which is bound to port 53 before you install Clearwater.

We've only regularly tested performing the manual install on a clean Ubuntu box, and if I create a new Ubuntu VM (e.g. the basic Ubuntu 14.0.4 VM in Amazon EC2 - ami-a22323d8 ) I don't see anything already running on Port 53 before I install Clearwater.

ubuntu at ip-10-0-162-214:~$ sudo netstat -pltun
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1133/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      1133/sshd
udp        0      0 0.0.0.0:5254            0.0.0.0:*                           582/dhclient
udp        0      0 0.0.0.0:68              0.0.0.0:*                           582/dhclient
udp6       0      0 :::21727                :::*                                582/dhclient

Can you clarify what image you are attempting to install ellis on where you are seeing these errors, and what's already installed on the box? Can you run the above netstat command?

For the error:

reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

Can you provide the complete output of the vellum build log? It'd be useful to know in what context this error was output - had something in the build already failed? It sounds like it failed to install memcached on the vellum node.

Regarding smtp_smarthost - this needs to be an SMTP server which can send mail so that Ellis can send password recovery emails. If it's not configured then password recovery emails won't work. See the entry in http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options for details about the SMTP options.

Regarding home_domain, it's described in Clearwater Options reference - see http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options  - it needs to be a domain which will resolve to the P-CSCFs (e.g. the bono nodes in the deployment). It's usually also the root domain for all of the domains used.

iind.intel.com might be a good choice if you can configure DNS entries under that domain - you'll need to configure the DNS entries listed in http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html in this domain

You will need to complete the DNS configuration before Ellis will work - it needs a DNS entry to exist in order to communicate with the other nodes in the deployment.



Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 28 February 2018 19:00
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: Problems in manual installation of clearwater

Hi Richards,
I need your help in Clearwater project manual installation, Its on high priority so please consider that.

I am installing the clearwater usingg manual installation. I have created the 6 VMs on virtualbox (using bridge adapter in network setting, used the same IP as public_ip and local_ip in local.conf) as I follow http://clearwater.readthedocs.io/en/stable/Manual_Install.html. while installing I have faced some errors (mentioned below), it will be your great help if u guide some solutions for them:

One more thing as I am using bridge adapter in network, I have not did any port forwarding as mention in document (I am able to ping vm from one to another i.e. they are are communicating)

1.in installtion of ellis:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install ellis --yes

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

Note: First time I install ellis I got this error, then I installed again in new node from scratch then also got the same error.


dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)



2.in installation of bono:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install bono restund --yes

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use



3.in installation of sprout:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install sprout --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use



4.in installtion of homer:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install homer --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
                                                                                                                                                                                                                                      [fail]
invoke-rc.d: initscript dnsmasq, action "start" failed.
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use


5.in installtion of dime:
$sudo DEBIAN_FRONTEND=noninteractive apt-get install dime clearwater-prov-tools --yes


dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use




6.in installtion of vellum:

* Starting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for port 53: Address already in use

* Restarting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

                                                                                                                                                                                                                                      [fail]
reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

-->I ignore above errors and move on to next step i.e. editing shared_config file and uploading


-->shared_config file:

[bono]bono at Bono:/etc/clearwater$ cat shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################
# Deployment definitions
home_domain=iind.intel.com
sprout_hostname=sprout.iind.intel.com
sprout_registration_store=vellum.iind.intel.com
hs_hostname=hs.iind.intel.com:8888
hs_provisioning_hostname=hs.iind.intel.com:8889
homestead_impu_store=vellum.iind.intel.com
ralf_hostname=ralf.iind.intel.com:10888
ralf_session_store=vellum.iind.intel.com
xdms_hostname=homer.iind.intel.com:7888
chronos_hostname=vellum.iind.intel.com
cassandra_hostname=vellum.iin.intel.com

# Email server configuration
smtp_smarthost=
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


?  What would I use in smtp_smarthost=   (is it localhost?)

ques: Is home_domain =iind.intel.com is right? (when I ping using #ping ellis it automatically takes like ellis.iind.intel.com) , it basically the intel's domain.


-->local_config : (IP ans hostname changed in every node)

local_ip=10.224.61.25
public_ip=10.224.61.25
public_hostname=Ellis
etcd_cluster="10.224.61.20,10.224.61.21,10.224.61.22,10.224.61.25,10.224.61.48,10.224.61.50"



?  After that when I am trying to connect to ellis using http://ellis.iind.intel.com or http://10.224.61.25 it is giving like 502 Bad Gateway nginx /1.4.6 (Ubuntu). I have not did any DNS configuration yet (because I am using the intel domain or do I need to do it... in which node and how).

?  I will love to hear your response


Thanks
Pushpendra


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180301/3bcfea1b/attachment.html>

From richard.whitehouse at projectclearwater.org  Thu Mar  1 05:42:57 2018
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Thu, 1 Mar 2018 10:42:57 +0000
Subject: [Project Clearwater] Problems in manual installation of
 clearwater
In-Reply-To: <B89A1E0D519F0B44A3E67F67A33D3AE472DB4E@BGSMSX108.gar.corp.intel.com>
References: <B89A1E0D519F0B44A3E67F67A33D3AE472D9CA@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB3362B779BC0E5BEC0D206659F0C70@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DA93@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB336207B283339956ABE9573BF0C60@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DB4E@BGSMSX108.gar.corp.intel.com>
Message-ID: <BN6PR02MB3362F4F8BC5057D7092F1C46F0C60@BN6PR02MB3362.namprd02.prod.outlook.com>

Pushpendra,

Responses inline.


Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 10:34
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Thanks Richards for replying,

1.       Yes, I have installed the ellis from root only. (while installing Ubuntu VM I have given hostname as Ellis, username as ellis) Should hostname Ellis is fine or I give different name, I will change the username to clearwater or Ubuntu(should every node have different username or username can be same).

[Richard] It's fine for the hostname. We'd normally name the box "ellis-1", but if you only plan on having one node, just using ellis should be fine for a hostname.

The username can be the same on every node.


2.       Yes, I have selected the openSSH server and DNS server at the time of installing VM. So I have to create VM again from scratch :). Is bind9 necessary to uninstall?

[Richard] Yes, otherwise dnsmasq won't be correctly configured.


3.       Can I use any home_domain (like example.com or anything or only that <zone> in which I am installing the deployment)

[Richard] You need to use a DNS zone for which you can configure DNS records in. So long as you can do that, you can use anything.



4.       As I follow manual document, after editing and uploading shared_config file they move on to logon to ellis. Is there any other steps before move on to log on to ellis (DNS configuration etc).

[Richard] You need to configure DNS records before logging into Ellis will work - it needs the DNS records set up .



5.       One more thing, should I use NAT or Bridge adapter in network setting in virtualbox as NAT will private IP only and Bridge adapter will give public IP. Is port forwarding need to do in bridge adapter also?

[Richard] If your IT networking allows the Bridge adapter to work, then that's probably the easier way to go. (In Bridge adapter mode, your PC will act as a network switch, and the MAC addresses from the virtual machines will be visible to your network).


So I have to installed all the VM again or can I modifies existing one.

Thanks,
Pushpendra

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Thursday, March 1, 2018 3:28 PM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

I think there are two reasons you are hitting problems installing Clearwater:


1)      The ellis node that you've created has a user called ellis, which as I described below is required to be a system user under which the ellis processes run.

I'd suggest you create the node with a different username (e.g. ubuntu or clearwater).

2)      The ellis server has bind9 installed on it, prior to installing Clearwater, which is conflicting with dnsmasq which Clearwater uses for caching DNS queries. You'll need to uninstall this before installing the Clearwater software.

At a guess, you selected the 'DNS server' task selection when installing the Ubuntu VM. We'd recommend that the only task selection you make as part of installing the Ubuntu VM is 'OpenSSH server', as that's useful to log into the node remotely, which allows you to copy and paste commands, and upload files easily.

We'll make a change to Clearwater so it detects this misconfiguration and requires it to be corrected prior to installing Clearwater.


Hope this helps!


Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 03:41
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox, but same errors (dpkg).

thanks

From: Kumar, Pushpendra
Sent: Thursday, March 1, 2018 9:07 AM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Hi Richard,
Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox).

This is the output of netstat -pltun:

[ellis]ellis at Ellis:~$ sudo netstat -pltun
[sudo] password for ellis:
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 10.224.61.25:53         0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:53            0.0.0.0:*               LISTEN      25075/named
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      28512/sshd
tcp        0      0 127.0.0.1:953           0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:2812          0.0.0.0:*               LISTEN      9667/monit
tcp        0      0 127.0.0.1:8000          0.0.0.0:*               LISTEN      2019/nginx
tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      8998/mysqld
tcp        0      0 10.224.61.25:2380       0.0.0.0:*               LISTEN      12039/etcd
tcp6       0      0 :::53                   :::*                    LISTEN      25075/named
tcp6       0      0 :::22                   :::*                    LISTEN      28512/sshd
tcp6       0      0 ::1:953                 :::*                    LISTEN      25075/named
tcp6       0      0 :::4000                 :::*                    LISTEN      12039/etcd
tcp6       0      0 :::80                   :::*                    LISTEN      2019/nginx
udp        0      0 10.224.61.25:53         0.0.0.0:*                           25075/named
udp        0      0 127.0.0.1:53            0.0.0.0:*                           25075/named
udp        0      0 0.0.0.0:68              0.0.0.0:*                           703/dhclient
udp        0      0 10.224.61.25:123        0.0.0.0:*                           9178/ntpd
udp        0      0 127.0.0.1:123           0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:123             0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:49694           0.0.0.0:*                           703/dhclient
udp6       0      0 :::29724                :::*                                703/dhclient
udp6       0      0 :::53                   :::*                                25075/named
udp6       0      0 fe80::a00:27ff:fe11:123 :::*                                9178/ntpd
udp6       0      0 ::1:123                 :::*                                9178/ntpd
udp6       0      0 :::123                  :::*                                9178/ntpd

I am also getting 502 Bad Gateway when I trying to connect ellis using http://ellis.iind.intel.com

Thanks
Pushpendra

From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Sent: Thursday, March 1, 2018 4:21 AM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

It sounds like you attempted to install ellis on a node was installed on a node in which there already was an ellis user - is that correct?

Ellis requires a user to run the components as for security so that we aren't running components as root which don't require root privileges, and this is required to be ellis.

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

>From these logs, and the similar  logs regarding bono, sprout and clearwater-management it looks like there's already a service running on the nodes which is bound to port 53 before you install Clearwater.

We've only regularly tested performing the manual install on a clean Ubuntu box, and if I create a new Ubuntu VM (e.g. the basic Ubuntu 14.0.4 VM in -a22323d8 ) I don't see anything already running on Port 53 before I install Clearwater.

ubuntu at ip-10-0-162-214:~$ sudo netstat -pltun
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1133/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      1133/sshd
udp        0      0 0.0.0.0:5254            0.0.0.0:*                           582/dhclient
udp        0      0 0.0.0.0:68              0.0.0.0:*                           582/dhclient
udp6       0      0 :::21727                :::*                                582/dhclient

Can you clarify what image you are attempting to install ellis on where you are seeing these errors, and what's already installed on the box? Can you run the above netstat command?

For the error:

reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

Can you provide the complete output of the vellum build log? It'd be useful to know in what context this error was output - had something in the build already failed? It sounds like it failed to install memcached on the vellum node.

Regarding smtp_smarthost - this needs to be an SMTP server which can send mail so that Ellis can send password recovery emails. If it's not configured then password recovery emails won't work. See the entry in http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options for details about the SMTP options.

Regarding home_domain, it's described in Clearwater Options reference - see http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options  - it needs to be a domain which will resolve to the P-CSCFs (e.g. the bono nodes in the deployment). It's usually also the root domain for all of the domains used.

iind.intel.com might be a good choice if you can configure DNS entries under that domain - you'll need to configure the DNS entries listed in http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html in this domain

You will need to complete the DNS configuration before Ellis will work - it needs a DNS entry to exist in order to communicate with the other nodes in the deployment.



Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 28 February 2018 19:00
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: Problems in manual installation of clearwater

Hi Richards,
I need your help in Clearwater project manual installation, Its on high priority so please consider that.

I am installing the clearwater usingg manual installation. I have created the 6 VMs on virtualbox (using bridge adapter in network setting, used the same IP as public_ip and local_ip in local.conf) as I follow http://clearwater.readthedocs.io/en/stable/Manual_Install.html. while installing I have faced some errors (mentioned below), it will be your great help if u guide some solutions for them:

One more thing as I am using bridge adapter in network, I have not did any port forwarding as mention in document (I am able to ping vm from one to another i.e. they are are communicating)

1.in installtion of ellis:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install ellis --yes

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

Note: First time I install ellis I got this error, then I installed again in new node from scratch then also got the same error.


dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)



2.in installation of bono:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install bono restund --yes

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use



3.in installation of sprout:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install sprout --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use



4.in installtion of homer:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install homer --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
                                                                                                                                                                                                                                      [fail]
invoke-rc.d: initscript dnsmasq, action "start" failed.
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use


5.in installtion of dime:
$sudo DEBIAN_FRONTEND=noninteractive apt-get install dime clearwater-prov-tools --yes


dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use




6.in installtion of vellum:

* Starting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for port 53: Address already in use

* Restarting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

                                                                                                                                                                                                                                      [fail]
reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

-->I ignore above errors and move on to next step i.e. editing shared_config file and uploading


-->shared_config file:

[bono]bono at Bono:/etc/clearwater$ cat shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################
# Deployment definitions
home_domain=iind.intel.com
sprout_hostname=sprout.iind.intel.com
sprout_registration_store=vellum.iind.intel.com
hs_hostname=hs.iind.intel.com:8888
hs_provisioning_hostname=hs.iind.intel.com:8889
homestead_impu_store=vellum.iind.intel.com
ralf_hostname=ralf.iind.intel.com:10888
ralf_session_store=vellum.iind.intel.com
xdms_hostname=homer.iind.intel.com:7888
chronos_hostname=vellum.iind.intel.com
cassandra_hostname=vellum.iin.intel.com

# Email server configuration
smtp_smarthost=
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


?  What would I use in smtp_smarthost=   (is it localhost?)

ques: Is home_domain =iind.intel.com is right? (when I ping using #ping ellis it automatically takes like ellis.iind.intel.com) , it basically the intel's domain.


-->local_config : (IP ans hostname changed in every node)

local_ip=10.224.61.25
public_ip=10.224.61.25
public_hostname=Ellis
etcd_cluster="10.224.61.20,10.224.61.21,10.224.61.22,10.224.61.25,10.224.61.48,10.224.61.50"



?  After that when I am trying to connect to ellis using http://ellis.iind.intel.com or http://10.224.61.25 it is giving like 502 Bad Gateway nginx /1.4.6 (Ubuntu). I have not did any DNS configuration yet (because I am using the intel domain or do I need to do it... in which node and how).

?  I will love to hear your response


Thanks
Pushpendra


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180301/675c0e6c/attachment.html>

From richard.whitehouse at projectclearwater.org  Thu Mar  1 06:00:54 2018
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Thu, 1 Mar 2018 11:00:54 +0000
Subject: [Project Clearwater] Problems in manual installation of
 clearwater
In-Reply-To: <B89A1E0D519F0B44A3E67F67A33D3AE472DB6C@BGSMSX108.gar.corp.intel.com>
References: <B89A1E0D519F0B44A3E67F67A33D3AE472D9CA@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB3362B779BC0E5BEC0D206659F0C70@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DA93@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB336207B283339956ABE9573BF0C60@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DB6C@BGSMSX108.gar.corp.intel.com>
Message-ID: <BN6PR02MB33629EC2131660C8D1E8FE86F0C60@BN6PR02MB3362.namprd02.prod.outlook.com>

What user are you running this as? Can you post the full log of the install process?

Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 10:46
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Hi, I have created the new clean ubuntu vm(without openSSH and DNSserver), still I am getting this error while installing ellis:

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)


Note: On the same machine there is other vm which has installed ellis. Should there only be one ellis on one machine or it doesn't matter?

Thanks




From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Thursday, March 1, 2018 3:28 PM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

I think there are two reasons you are hitting problems installing Clearwater:


1)      The ellis node that you've created has a user called ellis, which as I described below is required to be a system user under which the ellis processes run.

I'd suggest you create the node with a different username (e.g. ubuntu or clearwater).

2)      The ellis server has bind9 installed on it, prior to installing Clearwater, which is conflicting with dnsmasq which Clearwater uses for caching DNS queries. You'll need to uninstall this before installing the Clearwater software.

At a guess, you selected the 'DNS server' task selection when installing the Ubuntu VM. We'd recommend that the only task selection you make as part of installing the Ubuntu VM is 'OpenSSH server', as that's useful to log into the node remotely, which allows you to copy and paste commands, and upload files easily.

We'll make a change to Clearwater so it detects this misconfiguration and requires it to be corrected prior to installing Clearwater.


Hope this helps!


Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 03:41
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox, but same errors (dpkg).

thanks

From: Kumar, Pushpendra
Sent: Thursday, March 1, 2018 9:07 AM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Hi Richard,
Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox).

This is the output of netstat -pltun:

[ellis]ellis at Ellis:~$ sudo netstat -pltun
[sudo] password for ellis:
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 10.224.61.25:53         0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:53            0.0.0.0:*               LISTEN      25075/named
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      28512/sshd
tcp        0      0 127.0.0.1:953           0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:2812          0.0.0.0:*               LISTEN      9667/monit
tcp        0      0 127.0.0.1:8000          0.0.0.0:*               LISTEN      2019/nginx
tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      8998/mysqld
tcp        0      0 10.224.61.25:2380       0.0.0.0:*               LISTEN      12039/etcd
tcp6       0      0 :::53                   :::*                    LISTEN      25075/named
tcp6       0      0 :::22                   :::*                    LISTEN      28512/sshd
tcp6       0      0 ::1:953                 :::*                    LISTEN      25075/named
tcp6       0      0 :::4000                 :::*                    LISTEN      12039/etcd
tcp6       0      0 :::80                   :::*                    LISTEN      2019/nginx
udp        0      0 10.224.61.25:53         0.0.0.0:*                           25075/named
udp        0      0 127.0.0.1:53            0.0.0.0:*                           25075/named
udp        0      0 0.0.0.0:68              0.0.0.0:*                           703/dhclient
udp        0      0 10.224.61.25:123        0.0.0.0:*                           9178/ntpd
udp        0      0 127.0.0.1:123           0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:123             0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:49694           0.0.0.0:*                           703/dhclient
udp6       0      0 :::29724                :::*                                703/dhclient
udp6       0      0 :::53                   :::*                                25075/named
udp6       0      0 fe80::a00:27ff:fe11:123 :::*                                9178/ntpd
udp6       0      0 ::1:123                 :::*                                9178/ntpd
udp6       0      0 :::123                  :::*                                9178/ntpd

I am also getting 502 Bad Gateway when I trying to connect ellis using http://ellis.iind.intel.com

Thanks
Pushpendra

From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Sent: Thursday, March 1, 2018 4:21 AM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

It sounds like you attempted to install ellis on a node was installed on a node in which there already was an ellis user - is that correct?

Ellis requires a user to run the components as for security so that we aren't running components as root which don't require root privileges, and this is required to be ellis.

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

>From these logs, and the similar  logs regarding bono, sprout and clearwater-management it looks like there's already a service running on the nodes which is bound to port 53 before you install Clearwater.

We've only regularly tested performing the manual install on a clean Ubuntu box, and if I create a new Ubuntu VM (e.g. the basic Ubuntu 14.0.4 VM ) I don't see anything already running on Port 53 before I install Clearwater.

ubuntu at ip-10-0-162-214:~$ sudo netstat -pltun
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1133/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      1133/sshd
udp        0      0 0.0.0.0:5254            0.0.0.0:*                           582/dhclient
udp        0      0 0.0.0.0:68              0.0.0.0:*                           582/dhclient
udp6       0      0 :::21727                :::*                                582/dhclient

Can you clarify what image you are attempting to install ellis on where you are seeing these errors, and what's already installed on the box? Can you run the above netstat command?

For the error:

reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

Can you provide the complete output of the vellum build log? It'd be useful to know in what context this error was output - had something in the build already failed? It sounds like it failed to install memcached on the vellum node.

Regarding smtp_smarthost - this needs to be an SMTP server which can send mail so that Ellis can send password recovery emails. If it's not configured then password recovery emails won't work. See the entry in http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options for details about the SMTP options.

Regarding home_domain, it's described in Clearwater Options reference - see http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options  - it needs to be a domain which will resolve to the P-CSCFs (e.g. the bono nodes in the deployment). It's usually also the root domain for all of the domains used.

iind.intel.com might be a good choice if you can configure DNS entries under that domain - you'll need to configure the DNS entries listed in http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html in this domain

You will need to complete the DNS configuration before Ellis will work - it needs a DNS entry to exist in order to communicate with the other nodes in the deployment.



Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 28 February 2018 19:00
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: Problems in manual installation of clearwater

Hi Richards,
I need your help in Clearwater project manual installation, Its on high priority so please consider that.

I am installing the clearwater usingg manual installation. I have created the 6 VMs on virtualbox (using bridge adapter in network setting, used the same IP as public_ip and local_ip in local.conf) as I follow http://clearwater.readthedocs.io/en/stable/Manual_Install.html. while installing I have faced some errors (mentioned below), it will be your great help if u guide some solutions for them:

One more thing as I am using bridge adapter in network, I have not did any port forwarding as mention in document (I am able to ping vm from one to another i.e. they are are communicating)

1.in installtion of ellis:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install ellis --yes

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

Note: First time I install ellis I got this error, then I installed again in new node from scratch then also got the same error.


dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)



2.in installation of bono:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install bono restund --yes

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use



3.in installation of sprout:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install sprout --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use



4.in installtion of homer:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install homer --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
                                                                                                                                                                                                                                      [fail]
invoke-rc.d: initscript dnsmasq, action "start" failed.
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use


5.in installtion of dime:
$sudo DEBIAN_FRONTEND=noninteractive apt-get install dime clearwater-prov-tools --yes


dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use




6.in installtion of vellum:

* Starting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for port 53: Address already in use

* Restarting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

                                                                                                                                                                                                                                      [fail]
reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

-->I ignore above errors and move on to next step i.e. editing shared_config file and uploading


-->shared_config file:

[bono]bono at Bono:/etc/clearwater$ cat shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################
# Deployment definitions
home_domain=iind.intel.com
sprout_hostname=sprout.iind.intel.com
sprout_registration_store=vellum.iind.intel.com
hs_hostname=hs.iind.intel.com:8888
hs_provisioning_hostname=hs.iind.intel.com:8889
homestead_impu_store=vellum.iind.intel.com
ralf_hostname=ralf.iind.intel.com:10888
ralf_session_store=vellum.iind.intel.com
xdms_hostname=homer.iind.intel.com:7888
chronos_hostname=vellum.iind.intel.com
cassandra_hostname=vellum.iin.intel.com

# Email server configuration
smtp_smarthost=
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


?  What would I use in smtp_smarthost=   (is it localhost?)

ques: Is home_domain =iind.intel.com is right? (when I ping using #ping ellis it automatically takes like ellis.iind.intel.com) , it basically the intel's domain.


-->local_config : (IP ans hostname changed in every node)

local_ip=10.224.61.25
public_ip=10.224.61.25
public_hostname=Ellis
etcd_cluster="10.224.61.20,10.224.61.21,10.224.61.22,10.224.61.25,10.224.61.48,10.224.61.50"



?  After that when I am trying to connect to ellis using http://ellis.iind.intel.com or http://10.224.61.25 it is giving like 502 Bad Gateway nginx /1.4.6 (Ubuntu). I have not did any DNS configuration yet (because I am using the intel domain or do I need to do it... in which node and how).

?  I will love to hear your response


Thanks
Pushpendra


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180301/964e49b0/attachment.html>

From msc.jaber at gmail.com  Thu Mar  1 08:38:08 2018
From: msc.jaber at gmail.com (jaber daneshamooz)
Date: Thu, 1 Mar 2018 17:08:08 +0330
Subject: [Project Clearwater] SIP registeration cw-aio virtualbox
Message-ID: <423BFBC8-9655-4E77-A809-D73E08390375@gmail.com>

hi, I have trouble in registering client with zipper to my cw-aio installed in a virtual box. the virtual box itself is installed on an ubuntu. by localhost, do you mean the ip address of my linux or what? and please illustrate the registration of clients with informative details


From richard.whitehouse at projectclearwater.org  Fri Mar  2 04:18:48 2018
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Fri, 2 Mar 2018 09:18:48 +0000
Subject: [Project Clearwater] Problems in manual installation of
 clearwater
In-Reply-To: <B89A1E0D519F0B44A3E67F67A33D3AE472DB96@BGSMSX108.gar.corp.intel.com>
References: <B89A1E0D519F0B44A3E67F67A33D3AE472D9CA@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB3362B779BC0E5BEC0D206659F0C70@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DA93@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB336207B283339956ABE9573BF0C60@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DB6C@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB33629EC2131660C8D1E8FE86F0C60@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DB96@BGSMSX108.gar.corp.intel.com>
Message-ID: <BN6PR02MB336209987D05D9DE46BCF9CEF0C50@BN6PR02MB3362.namprd02.prod.outlook.com>

Kumar,

You can ignore those - as I said my previous email the important ones for Clearwater packages are:

Get:1 http://repo.cw-ngv.com binary/ Release.gpg [819 B]
Get:2 http://repo.cw-ngv.com binary/ Release [1,219 B]
Get:4 http://repo.cw-ngv.com binary/ Packages [23.0 kB]

(InRelease is a newer version of Release - you either need Release and Release.gpg, or InRelease, and we provide the older earlier mechanism. We don't provide translated package repository descriptions, hence the lack of Translation-en and Translation-en_IN).


Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 13:04
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Thanks Richard, Now that dpkg problem is solved in ellis. One thing, When I update using sudo apt-get update after after setting the debian package in clearwater.list, it give likes :

ubuntu at ellis-1:~$ sudo apt-get update >> update1.txt
ubuntu at ellis-1:~$ cat update1.txt
Hit http://security.ubuntu.com trusty-security InRelease
Ign http://repo.cw-ngv.com binary/ InRelease
Ign http://in.archive.ubuntu.com trusty InRelease
Hit http://security.ubuntu.com trusty-security/main Sources
Get:1 http://repo.cw-ngv.com binary/ Release.gpg [819 B]
Get:2 http://repo.cw-ngv.com binary/ Release [1,219 B]
Get:3 http://in.archive.ubuntu.com trusty-updates InRelease [65.9 kB]
Hit http://security.ubuntu.com trusty-security/restricted Sources
Get:4 http://repo.cw-ngv.com binary/ Packages [23.0 kB]
Hit http://security.ubuntu.com trusty-security/universe Sources
Hit http://security.ubuntu.com trusty-security/multiverse Sources
Get:5 http://in.archive.ubuntu.com trusty-backports InRelease [65.9 kB]
Hit http://security.ubuntu.com trusty-security/main amd64 Packages
Hit http://security.ubuntu.com trusty-security/restricted amd64 Packages
Get:6 http://in.archive.ubuntu.com trusty Release.gpg [933 B]
Hit http://security.ubuntu.com trusty-security/universe amd64 Packages
Get:7 http://in.archive.ubuntu.com trusty-updates/main Sources [412 kB]
Hit http://security.ubuntu.com trusty-security/multiverse amd64 Packages
Ign http://repo.cw-ngv.com binary/ Translation-en_IN
Ign http://repo.cw-ngv.com binary/ Translation-en
Hit http://security.ubuntu.com trusty-security/main i386 Packages

->Will it be create problem later or its fine?

Thanks,
Pushpendra

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Thursday, March 1, 2018 4:31 PM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

What user are you running this as? Can you post the full log of the install process?

Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 10:46
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: Problems in manual installation of clearwater

Hi, I have created the new clean ubuntu vm(without openSSH and DNSserver), still I am getting this error while installing ellis:

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)


Note: On the same machine there is other vm which has installed ellis. Should there only be one ellis on one machine or it doesn't matter?

Thanks




From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Thursday, March 1, 2018 3:28 PM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

I think there are two reasons you are hitting problems installing Clearwater:


1)      The ellis node that you've created has a user called ellis, which as I described below is required to be a system user under which the ellis processes run.

I'd suggest you create the node with a different username (e.g. ubuntu or clearwater).

2)      The ellis server has bind9 installed on it, prior to installing Clearwater, which is conflicting with dnsmasq which Clearwater uses for caching DNS queries. You'll need to uninstall this before installing the Clearwater software.

At a guess, you selected the 'DNS server' task selection when installing the Ubuntu VM. We'd recommend that the only task selection you make as part of installing the Ubuntu VM is 'OpenSSH server', as that's useful to log into the node remotely, which allows you to copy and paste commands, and upload files easily.

We'll make a change to Clearwater so it detects this misconfiguration and requires it to be corrected prior to installing Clearwater.


Hope this helps!


Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 03:41
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox, but same errors (dpkg).

thanks

From: Kumar, Pushpendra
Sent: Thursday, March 1, 2018 9:07 AM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Hi Richard,
Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox).

This is the output of netstat -pltun:

[ellis]ellis at Ellis:~$ sudo netstat -pltun
[sudo] password for ellis:
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 10.224.61.25:53         0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:53            0.0.0.0:*               LISTEN      25075/named
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      28512/sshd
tcp        0      0 127.0.0.1:953           0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:2812          0.0.0.0:*               LISTEN      9667/monit
tcp        0      0 127.0.0.1:8000          0.0.0.0:*               LISTEN      2019/nginx
tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      8998/mysqld
tcp        0      0 10.224.61.25:2380       0.0.0.0:*               LISTEN      12039/etcd
tcp6       0      0 :::53                   :::*                    LISTEN      25075/named
tcp6       0      0 :::22                   :::*                    LISTEN      28512/sshd
tcp6       0      0 ::1:953                 :::*                    LISTEN      25075/named
tcp6       0      0 :::4000                 :::*                    LISTEN      12039/etcd
tcp6       0      0 :::80                   :::*                    LISTEN      2019/nginx
udp        0      0 10.224.61.25:53         0.0.0.0:*                           25075/named
udp        0      0 127.0.0.1:53            0.0.0.0:*                           25075/named
udp        0      0 0.0.0.0:68              0.0.0.0:*                           703/dhclient
udp        0      0 10.224.61.25:123        0.0.0.0:*                           9178/ntpd
udp        0      0 127.0.0.1:123           0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:123             0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:49694           0.0.0.0:*                           703/dhclient
udp6       0      0 :::29724                :::*                                703/dhclient
udp6       0      0 :::53                   :::*                                25075/named
udp6       0      0 fe80::a00:27ff:fe11:123 :::*                                9178/ntpd
udp6       0      0 ::1:123                 :::*                                9178/ntpd
udp6       0      0 :::123                  :::*                                9178/ntpd

I am also getting 502 Bad Gateway when I trying to connect ellis using http://ellis.iind.intel.com

Thanks
Pushpendra

From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Sent: Thursday, March 1, 2018 4:21 AM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

It sounds like you attempted to install ellis on a node was installed on a node in which there already was an ellis user - is that correct?

Ellis requires a user to run the components as for security so that we aren't running components as root which don't require root privileges, and this is required to be ellis.

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

>From these logs, and the similar  logs regarding bono, sprout and clearwater-management it looks like there's already a service running on the nodes which is bound to port 53 before you install Clearwater.

We've only regularly tested performing the manual install on a clean Ubuntu box, and if I create a new Ubuntu VM (e.g. the basic Ubuntu 14.0.4 VM ) I don't see anything already running on Port 53 before I install Clearwater.

ubuntu at ip-10-0-162-214:~$ sudo netstat -pltun
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1133/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      1133/sshd
udp        0      0 0.0.0.0:5254            0.0.0.0:*                           582/dhclient
udp        0      0 0.0.0.0:68              0.0.0.0:*                           582/dhclient
udp6       0      0 :::21727                :::*                                582/dhclient

Can you clarify what image you are attempting to install ellis on where you are seeing these errors, and what's already installed on the box? Can you run the above netstat command?

For the error:

reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

Can you provide the complete output of the vellum build log? It'd be useful to know in what context this error was output - had something in the build already failed? It sounds like it failed to install memcached on the vellum node.

Regarding smtp_smarthost - this needs to be an SMTP server which can send mail so that Ellis can send password recovery emails. If it's not configured then password recovery emails won't work. See the entry in http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options for details about the SMTP options.

Regarding home_domain, it's described in Clearwater Options reference - see http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options  - it needs to be a domain which will resolve to the P-CSCFs (e.g. the bono nodes in the deployment). It's usually also the root domain for all of the domains used.

iind.intel.com might be a good choice if you can configure DNS entries under that domain - you'll need to configure the DNS entries listed in http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html in this domain

You will need to complete the DNS configuration before Ellis will work - it needs a DNS entry to exist in order to communicate with the other nodes in the deployment.



Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 28 February 2018 19:00
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: Problems in manual installation of clearwater

Hi Richards,
I need your help in Clearwater project manual installation, Its on high priority so please consider that.

I am installing the clearwater usingg manual installation. I have created the 6 VMs on virtualbox (using bridge adapter in network setting, used the same IP as public_ip and local_ip in local.conf) as I follow http://clearwater.readthedocs.io/en/stable/Manual_Install.html. while installing I have faced some errors (mentioned below), it will be your great help if u guide some solutions for them:

One more thing as I am using bridge adapter in network, I have not did any port forwarding as mention in document (I am able to ping vm from one to another i.e. they are are communicating)

1.in installtion of ellis:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install ellis --yes

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

Note: First time I install ellis I got this error, then I installed again in new node from scratch then also got the same error.


dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)



2.in installation of bono:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install bono restund --yes

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use



3.in installation of sprout:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install sprout --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use



4.in installtion of homer:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install homer --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
                                                                                                                                                                                                                                      [fail]
invoke-rc.d: initscript dnsmasq, action "start" failed.
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use


5.in installtion of dime:
$sudo DEBIAN_FRONTEND=noninteractive apt-get install dime clearwater-prov-tools --yes


dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use




6.in installtion of vellum:

* Starting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for port 53: Address already in use

* Restarting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

                                                                                                                                                                                                                                      [fail]
reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

-->I ignore above errors and move on to next step i.e. editing shared_config file and uploading


-->shared_config file:

[bono]bono at Bono:/etc/clearwater$ cat shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################
# Deployment definitions
home_domain=iind.intel.com
sprout_hostname=sprout.iind.intel.com
sprout_registration_store=vellum.iind.intel.com
hs_hostname=hs.iind.intel.com:8888
hs_provisioning_hostname=hs.iind.intel.com:8889
homestead_impu_store=vellum.iind.intel.com
ralf_hostname=ralf.iind.intel.com:10888
ralf_session_store=vellum.iind.intel.com
xdms_hostname=homer.iind.intel.com:7888
chronos_hostname=vellum.iind.intel.com
cassandra_hostname=vellum.iin.intel.com

# Email server configuration
smtp_smarthost=
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


?  What would I use in smtp_smarthost=   (is it localhost?)

ques: Is home_domain =iind.intel.com is right? (when I ping using #ping ellis it automatically takes like ellis.iind.intel.com) , it basically the intel's domain.


-->local_config : (IP ans hostname changed in every node)

local_ip=10.224.61.25
public_ip=10.224.61.25
public_hostname=Ellis
etcd_cluster="10.224.61.20,10.224.61.21,10.224.61.22,10.224.61.25,10.224.61.48,10.224.61.50"



?  After that when I am trying to connect to ellis using http://ellis.iind.intel.com or http://10.224.61.25 it is giving like 502 Bad Gateway nginx /1.4.6 (Ubuntu). I have not did any DNS configuration yet (because I am using the intel domain or do I need to do it... in which node and how).

?  I will love to hear your response


Thanks
Pushpendra


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180302/29b82d2b/attachment.html>

From richard.whitehouse at projectclearwater.org  Fri Mar  2 04:31:59 2018
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Fri, 2 Mar 2018 09:31:59 +0000
Subject: [Project Clearwater] Problems in manual installation of
 clearwater
In-Reply-To: <B89A1E0D519F0B44A3E67F67A33D3AE472DC88@BGSMSX108.gar.corp.intel.com>
References: <B89A1E0D519F0B44A3E67F67A33D3AE472D9CA@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB3362B779BC0E5BEC0D206659F0C70@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DA93@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB336207B283339956ABE9573BF0C60@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DB6C@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB33629EC2131660C8D1E8FE86F0C60@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DB96@BGSMSX108.gar.corp.intel.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DC88@BGSMSX108.gar.corp.intel.com>
Message-ID: <BN6PR02MB3362B93819922C8BF68D28DFF0C50@BN6PR02MB3362.namprd02.prod.outlook.com>

Pushpendra,

Are all of the nodes whose IPs are named in the etcd cluster setting running, or is just the Ellis node running?

Does the ellis node have IP connectivity to the other nodes?

If at least half of them aren't running then the cluster won't have quorum and won't be able to start.

Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 02 March 2018 07:50
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Hi Richard,
I have successfully installed the ellis, bono etc, but when I am trying to download the shared_config file on ellis-1, it gives like-

ubuntu at ellis-1:~$ cw-config download shared_config
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint http://10.224.61.19:4000 exceeded header timeout

error #0: client: endpoint http://10.224.61.19:4000 exceeded header timeout

No changes to configuration can be made while the configuration database does not have quorum. Restore connectivity to the uncontactable nodes in the deployment and try again.


? Local_config file is-



ubuntu at ellis-1:/etc/clearwater$ cat local_config

local_ip=10.224.61.19

public_ip=10.224.61.19

public_hostname=ellis-1

etcd_cluster="10.224.61.19,10.224.61.20,10.224.61.27,10.224.61.34,10.224.61.39,10.224.61.48"



Thanks,

Pushpendra

From: Kumar, Pushpendra
Sent: Thursday, March 1, 2018 6:34 PM
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: Problems in manual installation of clearwater

Thanks Richard, Now that dpkg problem is solved in ellis. One thing, When I update using sudo apt-get update after after setting the debian package in clearwater.list, it give likes :

ubuntu at ellis-1:~$ sudo apt-get update >> update1.txt
ubuntu at ellis-1:~$ cat update1.txt
Hit http://security.ubuntu.com trusty-security InRelease
Ign http://repo.cw-ngv.com binary/ InRelease
Ign http://in.archive.ubuntu.com trusty InRelease
Hit http://security.ubuntu.com trusty-security/main Sources
Get:1 http://repo.cw-ngv.com binary/ Release.gpg [819 B]
Get:2 http://repo.cw-ngv.com binary/ Release [1,219 B]
Get:3 http://in.archive.ubuntu.com trusty-updates InRelease [65.9 kB]
Hit http://security.ubuntu.com trusty-security/restricted Sources
Get:4 http://repo.cw-ngv.com binary/ Packages [23.0 kB]
Hit http://security.ubuntu.com trusty-security/universe Sources
Hit http://security.ubuntu.com trusty-security/multiverse Sources
Get:5 http://in.archive.ubuntu.com trusty-backports InRelease [65.9 kB]
Hit http://security.ubuntu.com trusty-security/main amd64 Packages
Hit http://security.ubuntu.com trusty-security/restricted amd64 Packages
Get:6 http://in.archive.ubuntu.com trusty Release.gpg [933 B]
Hit http://security.ubuntu.com trusty-security/universe amd64 Packages
Get:7 http://in.archive.ubuntu.com trusty-updates/main Sources [412 kB]
Hit http://security.ubuntu.com trusty-security/multiverse amd64 Packages
Ign http://repo.cw-ngv.com binary/ Translation-en_IN
Ign http://repo.cw-ngv.com binary/ Translation-en
Hit http://security.ubuntu.com trusty-security/main i386 Packages

->Will it be create problem later or its fine?

Thanks,
Pushpendra

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Thursday, March 1, 2018 4:31 PM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

What user are you running this as? Can you post the full log of the install process?

Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 10:46
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: Problems in manual installation of clearwater

Hi, I have created the new clean ubuntu vm(without openSSH and DNSserver), still I am getting this error while installing ellis:

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)


Note: On the same machine there is other vm which has installed ellis. Should there only be one ellis on one machine or it doesn't matter?

Thanks




From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Thursday, March 1, 2018 3:28 PM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

I think there are two reasons you are hitting problems installing Clearwater:


1)      The ellis node that you've created has a user called ellis, which as I described below is required to be a system user under which the ellis processes run.

I'd suggest you create the node with a different username (e.g. ubuntu or clearwater).

2)      The ellis server has bind9 installed on it, prior to installing Clearwater, which is conflicting with dnsmasq which Clearwater uses for caching DNS queries. You'll need to uninstall this before installing the Clearwater software.

At a guess, you selected the 'DNS server' task selection when installing the Ubuntu VM. We'd recommend that the only task selection you make as part of installing the Ubuntu VM is 'OpenSSH server', as that's useful to log into the node remotely, which allows you to copy and paste commands, and upload files easily.

We'll make a change to Clearwater so it detects this misconfiguration and requires it to be corrected prior to installing Clearwater.


Hope this helps!


Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 03:41
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox, but same errors (dpkg).

thanks

From: Kumar, Pushpendra
Sent: Thursday, March 1, 2018 9:07 AM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Hi Richard,
Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox).

This is the output of netstat -pltun:

[ellis]ellis at Ellis:~$ sudo netstat -pltun
[sudo] password for ellis:
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 10.224.61.25:53         0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:53            0.0.0.0:*               LISTEN      25075/named
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      28512/sshd
tcp        0      0 127.0.0.1:953           0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:2812          0.0.0.0:*               LISTEN      9667/monit
tcp        0      0 127.0.0.1:8000          0.0.0.0:*               LISTEN      2019/nginx
tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      8998/mysqld
tcp        0      0 10.224.61.25:2380       0.0.0.0:*               LISTEN      12039/etcd
tcp6       0      0 :::53                   :::*                    LISTEN      25075/named
tcp6       0      0 :::22                   :::*                    LISTEN      28512/sshd
tcp6       0      0 ::1:953                 :::*                    LISTEN      25075/named
tcp6       0      0 :::4000                 :::*                    LISTEN      12039/etcd
tcp6       0      0 :::80                   :::*                    LISTEN      2019/nginx
udp        0      0 10.224.61.25:53         0.0.0.0:*                           25075/named
udp        0      0 127.0.0.1:53            0.0.0.0:*                           25075/named
udp        0      0 0.0.0.0:68              0.0.0.0:*                           703/dhclient
udp        0      0 10.224.61.25:123        0.0.0.0:*                           9178/ntpd
udp        0      0 127.0.0.1:123           0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:123             0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:49694           0.0.0.0:*                           703/dhclient
udp6       0      0 :::29724                :::*                                703/dhclient
udp6       0      0 :::53                   :::*                                25075/named
udp6       0      0 fe80::a00:27ff:fe11:123 :::*                                9178/ntpd
udp6       0      0 ::1:123                 :::*                                9178/ntpd
udp6       0      0 :::123                  :::*                                9178/ntpd

I am also getting 502 Bad Gateway when I trying to connect ellis using http://ellis.iind.intel.com

Thanks
Pushpendra

From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Sent: Thursday, March 1, 2018 4:21 AM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

It sounds like you attempted to install ellis on a node was installed on a node in which there already was an ellis user - is that correct?

Ellis requires a user to run the components as for security so that we aren't running components as root which don't require root privileges, and this is required to be ellis.

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

>From these logs, and the similar  logs regarding bono, sprout and clearwater-management it looks like there's already a service running on the nodes which is bound to port 53 before you install Clearwater.

We've only regularly tested performing the manual install on a clean Ubuntu box, and if I create a new Ubuntu VM (e.g. the basic Ubuntu 14.0.4 VM ) I don't see anything already running on Port 53 before I install Clearwater.

ubuntu at ip-10-0-162-214:~$ sudo netstat -pltun
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1133/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      1133/sshd
udp        0      0 0.0.0.0:5254            0.0.0.0:*                           582/dhclient
udp        0      0 0.0.0.0:68              0.0.0.0:*                           582/dhclient
udp6       0      0 :::21727                :::*                                582/dhclient

Can you clarify what image you are attempting to install ellis on where you are seeing these errors, and what's already installed on the box? Can you run the above netstat command?

For the error:

reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

Can you provide the complete output of the vellum build log? It'd be useful to know in what context this error was output - had something in the build already failed? It sounds like it failed to install memcached on the vellum node.

Regarding smtp_smarthost - this needs to be an SMTP server which can send mail so that Ellis can send password recovery emails. If it's not configured then password recovery emails won't work. See the entry in http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options for details about the SMTP options.

Regarding home_domain, it's described in Clearwater Options reference - see http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options  - it needs to be a domain which will resolve to the P-CSCFs (e.g. the bono nodes in the deployment). It's usually also the root domain for all of the domains used.

iind.intel.com might be a good choice if you can configure DNS entries under that domain - you'll need to configure the DNS entries listed in http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html in this domain

You will need to complete the DNS configuration before Ellis will work - it needs a DNS entry to exist in order to communicate with the other nodes in the deployment.



Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 28 February 2018 19:00
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: Problems in manual installation of clearwater

Hi Richards,
I need your help in Clearwater project manual installation, Its on high priority so please consider that.

I am installing the clearwater usingg manual installation. I have created the 6 VMs on virtualbox (using bridge adapter in network setting, used the same IP as public_ip and local_ip in local.conf) as I follow http://clearwater.readthedocs.io/en/stable/Manual_Install.html. while installing I have faced some errors (mentioned below), it will be your great help if u guide some solutions for them:

One more thing as I am using bridge adapter in network, I have not did any port forwarding as mention in document (I am able to ping vm from one to another i.e. they are are communicating)

1.in installtion of ellis:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install ellis --yes

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

Note: First time I install ellis I got this error, then I installed again in new node from scratch then also got the same error.


dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)



2.in installation of bono:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install bono restund --yes

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use



3.in installation of sprout:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install sprout --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use



4.in installtion of homer:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install homer --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
                                                                                                                                                                                                                                      [fail]
invoke-rc.d: initscript dnsmasq, action "start" failed.
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use


5.in installtion of dime:
$sudo DEBIAN_FRONTEND=noninteractive apt-get install dime clearwater-prov-tools --yes


dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use




6.in installtion of vellum:

* Starting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for port 53: Address already in use

* Restarting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

                                                                                                                                                                                                                                      [fail]
reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

-->I ignore above errors and move on to next step i.e. editing shared_config file and uploading


-->shared_config file:

[bono]bono at Bono:/etc/clearwater$ cat shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################
# Deployment definitions
home_domain=iind.intel.com
sprout_hostname=sprout.iind.intel.com
sprout_registration_store=vellum.iind.intel.com
hs_hostname=hs.iind.intel.com:8888
hs_provisioning_hostname=hs.iind.intel.com:8889
homestead_impu_store=vellum.iind.intel.com
ralf_hostname=ralf.iind.intel.com:10888
ralf_session_store=vellum.iind.intel.com
xdms_hostname=homer.iind.intel.com:7888
chronos_hostname=vellum.iind.intel.com
cassandra_hostname=vellum.iin.intel.com

# Email server configuration
smtp_smarthost=
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


?  What would I use in smtp_smarthost=   (is it localhost?)

ques: Is home_domain =iind.intel.com is right? (when I ping using #ping ellis it automatically takes like ellis.iind.intel.com) , it basically the intel's domain.


-->local_config : (IP ans hostname changed in every node)

local_ip=10.224.61.25
public_ip=10.224.61.25
public_hostname=Ellis
etcd_cluster="10.224.61.20,10.224.61.21,10.224.61.22,10.224.61.25,10.224.61.48,10.224.61.50"



?  After that when I am trying to connect to ellis using http://ellis.iind.intel.com or http://10.224.61.25 it is giving like 502 Bad Gateway nginx /1.4.6 (Ubuntu). I have not did any DNS configuration yet (because I am using the intel domain or do I need to do it... in which node and how).

?  I will love to hear your response


Thanks
Pushpendra


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180302/2acd363d/attachment.html>

From Adam.Lindley at metaswitch.com  Fri Mar  2 12:46:26 2018
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Fri, 2 Mar 2018 17:46:26 +0000
Subject: [Project Clearwater] SIP registeration cw-aio virtualbox
In-Reply-To: <423BFBC8-9655-4E77-A809-D73E08390375@gmail.com>
References: <423BFBC8-9655-4E77-A809-D73E08390375@gmail.com>
Message-ID: <BY2PR02MB203795FB7F580F42FBBC6B08E2C50@BY2PR02MB2037.namprd02.prod.outlook.com>

Hi Jaber,

I'd recommend taking a look at our documentation on setting this up, over at http://clearwater.readthedocs.io/en/stable/Making_your_first_call.html 
If you've followed those steps, you should be able to make a first pass at diagnosing your problems with some help from http://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html 

If you've already been following those, can you provide some more detail on the errors you're seeing, as well as the exact steps you've followed, details on your setup, and any diagnostics you can get.

Cheers,
Adam

-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of jaber daneshamooz
Sent: 01 March 2018 13:38
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP registeration cw-aio virtualbox

hi, I have trouble in registering client with zipper to my cw-aio installed in a virtual box. the virtual box itself is installed on an ubuntu. by localhost, do you mean the ip address of my linux or what? and please illustrate the registration of clients with informative details _______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



From wangwulin at hotmail.com  Fri Mar  2 02:38:19 2018
From: wangwulin at hotmail.com (wang wulin)
Date: Fri, 2 Mar 2018 07:38:19 +0000
Subject: [Project Clearwater] [Clearwater] Could not get subscriber data
 from HSS
Message-ID: <KL1PR0301MB2120B6181E0CA61719D6BFC9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>

Hi Clearwater Team,


I deployed a stress node according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html, and tried to run stress via "/usr/share/clearwater/bin/run_stress clearwater.opnfv 10 10 ".

"Register" works now, but Call step still failed:
[cid:29209895-4920-4757-a1c0-5756bb653935]
 I got the error from /var/log/sprout: "Error hssconnection.cpp:704: Could not get subscriber data from HSS"
[cid:3b29aefa-363d-4be8-98af-0fff4b0d0c52]

I only executed the 4 commands below on Vellum Node:
1)  . /etc/clearwater/config; for DN in {2440000000..2440000099} ; do echo sip:$DN@$home_domain,$DN at clearwater.opnf<mailto:DN at clearwater.opnf>v,clearwater.opnfv,7kkzTyGW ;done > users.csv
2) cd /usr/share/clearwater/crest-prov/src/metaswitch/crest/tools/ && python bulk_create.py users.csv
3) ./users.create_xdm.sh
4) ./users.create_homestead.sh

Did I miss some other steps?
Do you know if we "HSS" node is also required?


2)

I also got the error from Dime node:


root at dime-5y29tl:/var/log/ralf# vim /var/log/syslog
Mar  2 06:37:01 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument
Mar  2 06:37:02 dime-5y29tl config-manager[10778]: dropped request: 'issue-alarm config-manager 8500.3'
Mar  2 06:37:08 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument
Mar  2 06:37:18 dime-5y29tl issue-alarm: message repeated 12 times: [ zmq_msg_recv: Invalid argument]
Mar  2 06:37:18 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9001.1'
Mar  2 06:37:21 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9002.1'
Mar  2 06:37:22 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument

root at dime-5y29tl:/var/log/ralf# vim /var/log/monit.log
[UTC Mar  2 02:31:43] error    : 'poll_etcd_cluster' '/usr/share/clearwater/bin/poll_etcd_cluster.sh' failed with exit status (1) -- 1
[UTC Mar  2 02:31:43] info     : 'poll_etcd_cluster' exec: /bin/bash
[UTC Mar  2 02:31:53] info     : 'poll_etcd_cluster' status succeeded [status=0] -- zmq_msg_recv: Resource temporarily unavailable


Any help would be much appreciated!



Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180302/39d306c3/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pastedImage.png
Type: image/png
Size: 26935 bytes
Desc: pastedImage.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180302/39d306c3/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pastedImage.png
Type: image/png
Size: 97367 bytes
Desc: pastedImage.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180302/39d306c3/attachment-0001.png>

From wangwulin at hotmail.com  Fri Mar  2 03:39:12 2018
From: wangwulin at hotmail.com (wang wulin)
Date: Fri, 2 Mar 2018 08:39:12 +0000
Subject: [Project Clearwater] =?gb2312?b?tPC4tDogW0NsZWFyd2F0ZXJdIENvdWxk?=
 =?gb2312?b?IG5vdCBnZXQgc3Vic2NyaWJlciBkYXRhIGZyb20gSFNT?=
In-Reply-To: <KL1PR0301MB2120B6181E0CA61719D6BFC9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>
References: <KL1PR0301MB2120B6181E0CA61719D6BFC9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>
Message-ID: <KL1PR0301MB2120FFFF26D295D0A10CE5A9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>

Hi Clearwater Team,


Here is the result after running ""/usr/share/clearwater/bin/run_stress clearwater.opnfv 16 10 "."


2018-03-02      08:25:12.757782 1519979112.757782: Aborting call on unexpected message for Call-Id '1-21165 at 10.67.79.16': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:54572;received=10.67.79.16;branch=z9hG4bK-21165-1-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:/bDGU121V2 at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 1-21165 at 10.67.79.16
From: <sip:2770000012 at clearwater.opnfv>;tag=21165SIPpTag001
To: <sip:2770000015 at clearwater.opnfv>;tag=z9hG4bKPjtk6L-d0QjzkZB8rbq-CMdJGW9zpHhMbt
CSeq: 1 INVITE
Content-Length:  0


Total calls: 1
Successful calls: 0 (0.0%)
Failed calls: 1 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 5
Successful re-REGISTERs: 5 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 12.0ms


Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?2? 15:38
???: clearwater at lists.projectclearwater.org
??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,


I deployed a stress node according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html, and tried to run stress via "/usr/share/clearwater/bin/run_stress clearwater.opnfv 10 10 ".

"Register" works now, but Call step still failed:
[cid:29209895-4920-4757-a1c0-5756bb653935]
 I got the error from /var/log/sprout: "Error hssconnection.cpp:704: Could not get subscriber data from HSS"
[cid:3b29aefa-363d-4be8-98af-0fff4b0d0c52]

I only executed the 4 commands below on Vellum Node:
1)  . /etc/clearwater/config; for DN in {2770000000..2770000099} ; do echo sip:$DN@$home_domain,$DN at clearwater.opnf<mailto:DN at clearwater.opnf>v,clearwater.opnfv,7kkzTyGW ;done > users.csv
2) cd /usr/share/clearwater/crest-prov/src/metaswitch/crest/tools/ && python bulk_create.py users.csv
3) ./users.create_xdm.sh
4) ./users.create_homestead.sh

Did I miss some other steps?
Do you know if we "HSS" node is also required?


2)

I also got the error from Dime node:


root at dime-5y29tl:/var/log/ralf# vim /var/log/syslog
Mar  2 06:37:01 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument
Mar  2 06:37:02 dime-5y29tl config-manager[10778]: dropped request: 'issue-alarm config-manager 8500.3'
Mar  2 06:37:08 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument
Mar  2 06:37:18 dime-5y29tl issue-alarm: message repeated 12 times: [ zmq_msg_recv: Invalid argument]
Mar  2 06:37:18 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9001.1'
Mar  2 06:37:21 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9002.1'
Mar  2 06:37:22 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument

root at dime-5y29tl:/var/log/ralf# vim /var/log/monit.log
[UTC Mar  2 02:31:43] error    : 'poll_etcd_cluster' '/usr/share/clearwater/bin/poll_etcd_cluster.sh' failed with exit status (1) -- 1
[UTC Mar  2 02:31:43] info     : 'poll_etcd_cluster' exec: /bin/bash
[UTC Mar  2 02:31:53] info     : 'poll_etcd_cluster' status succeeded [status=0] -- zmq_msg_recv: Resource temporarily unavailable


Any help would be much appreciated!



Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180302/070969e5/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pastedImage.png
Type: image/png
Size: 26935 bytes
Desc: pastedImage.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180302/070969e5/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pastedImage.png
Type: image/png
Size: 97367 bytes
Desc: pastedImage.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180302/070969e5/attachment-0001.png>

From Adam.Lindley at metaswitch.com  Fri Mar  2 13:13:41 2018
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Fri, 2 Mar 2018 18:13:41 +0000
Subject: [Project Clearwater] [Clearwater] Could not get subscriber data
 from HSS
In-Reply-To: <KL1PR0301MB2120FFFF26D295D0A10CE5A9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>
References: <KL1PR0301MB2120B6181E0CA61719D6BFC9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>
	<KL1PR0301MB2120FFFF26D295D0A10CE5A9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>
Message-ID: <BY2PR02MB203710A4805B4BAB8F9F76DCE2C50@BY2PR02MB2037.namprd02.prod.outlook.com>

Hi Linda,

You shouldn?t need an HSS to run a simple Clearwater system; Homestead-prov provides the basic functionality needed to run a few calls.
It looks like there?s an issue with the provisioned subscriber information, as you can see Homestead returning a 404 in the Httpclient logs. I think the next place for you to look in diagnosing this issue is the homestead logs on the Dime node, potentially increasing the log level if the default doesn?t give you what you need. You should be looking  for errors in it reading from the cassandra subscriber store.

If you?re unable to see anything there, send over the homestead logs, and any other diagnostics/details about your system. If you?re able to provide the full log, rather than just screenshots, that would be helpful as well.
It might also help to look at the contents of your config files, such as /etc/clearwater/shared_config, and the output of `sudo monit summary`.

The zmq_msg error you are seeing is not going to be related to  your call failure, so I suggest we focus on the first issue for now, but I think it may be down to not having the clearwater-snmp-alarm-agent correctly installed/configured, so you could look into that as well.

Let us know how it goes,

Adam



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of wang wulin
Sent: 02 March 2018 08:39
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] ??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,



Here is the result after running ""/usr/share/clearwater/bin/run_stress clearwater.opnfv 16 10 "."


2018-03-02      08:25:12.757782 1519979112.757782: Aborting call on unexpected message for Call-Id '1-21165 at 10.67.79.16': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:54572;received=10.67.79.16;branch=z9hG4bK-21165-1-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:/bDGU121V2 at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr<sip://bDGU121V2 at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>>
Call-ID: 1-21165 at 10.67.79.16<mailto:1-21165 at 10.67.79.16>
From: <sip:2770000012 at clearwater.opnfv>;tag=21165SIPpTag001
To: <sip:2770000015 at clearwater.opnfv>;tag=z9hG4bKPjtk6L-d0QjzkZB8rbq-CMdJGW9zpHhMbt
CSeq: 1 INVITE
Content-Length:  0

Total calls: 1
Successful calls: 0 (0.0%)
Failed calls: 1 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 5
Successful re-REGISTERs: 5 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 12.0ms


Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com<mailto:wangwulin at hotmail.com>>
????: 2018?3?2? 15:38
???: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,



I deployed a stress node according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html, and tried to run stress via "/usr/share/clearwater/bin/run_stress clearwater.opnfv 10 10 ".

"Register" works now, but Call step still failed:
[cid:image001.png at 01D3B252.2F1A46F0]
 I got the error from /var/log/sprout: "Error hssconnection.cpp:704: Could not get subscriber data from HSS"
[cid:image002.png at 01D3B252.2F1A46F0]

I only executed the 4 commands below on Vellum Node:
1)  . /etc/clearwater/config; for DN in {2770000000..2770000099} ; do echo sip:$DN@$home_domain,$DN at clearwater.opnf<mailto:DN at clearwater.opnf>v,clearwater.opnfv,7kkzTyGW ;done > users.csv
2) cd /usr/share/clearwater/crest-prov/src/metaswitch/crest/tools/ && python bulk_create.py users.csv
3) ./users.create_xdm.sh
4) ./users.create_homestead.sh

Did I miss some other steps?
Do you know if we "HSS" node is also required?



2)

I also got the error from Dime node:


root at dime-5y29tl:/var/log/ralf# vim /var/log/syslog
Mar  2 06:37:01 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument
Mar  2 06:37:02 dime-5y29tl config-manager[10778]: dropped request: 'issue-alarm config-manager 8500.3'
Mar  2 06:37:08 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument
Mar  2 06:37:18 dime-5y29tl issue-alarm: message repeated 12 times: [ zmq_msg_recv: Invalid argument]
Mar  2 06:37:18 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9001.1'
Mar  2 06:37:21 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9002.1'
Mar  2 06:37:22 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument

root at dime-5y29tl:/var/log/ralf# vim /var/log/monit.log
[UTC Mar  2 02:31:43] error    : 'poll_etcd_cluster' '/usr/share/clearwater/bin/poll_etcd_cluster.sh' failed with exit status (1) -- 1
[UTC Mar  2 02:31:43] info     : 'poll_etcd_cluster' exec: /bin/bash
[UTC Mar  2 02:31:53] info     : 'poll_etcd_cluster' status succeeded [status=0] -- zmq_msg_recv: Resource temporarily unavailable


Any help would be much appreciated!





Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180302/f1001b84/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 26935 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180302/f1001b84/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 97367 bytes
Desc: image002.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180302/f1001b84/attachment-0001.png>

From anthonynlee at gmail.com  Mon Mar  5 12:06:40 2018
From: anthonynlee at gmail.com (Anthony Lee)
Date: Mon, 5 Mar 2018 12:06:40 -0500
Subject: [Project Clearwater] Is it a bug that Clearwater invalidate the
 ODI once it receives 200OK response?
In-Reply-To: <BN6PR02MB33628546C8BDA1C3FE63A1F6F0C70@BN6PR02MB3362.namprd02.prod.outlook.com>
References: <CA+pBo5GYDWB6riVQhiB9fLH82jfbgnXiBYr5v6vq2qi94xsvtw@mail.gmail.com>
	<CA+pBo5F3CT=DCu45DVCFgFG0BuoYFgG0aO5So2Mtw8OeAHAXgg@mail.gmail.com>
	<BN6PR02MB33628546C8BDA1C3FE63A1F6F0C70@BN6PR02MB3362.namprd02.prod.outlook.com>
Message-ID: <CA+pBo5EF_dn4rytFyBoR-ZAjhZcY4TncH2fh_Rj+u4miRKp8jA@mail.gmail.com>

Hi Richard,

My application server is doing RCS Message Store and Forward, it have two
roles: Originating and Terminating.
When it act as terminating role(let's called it TPF) it provides
Store-and-Forward: it accepts the Invite request from the network
and act as below:
     1. if the user is in registered state,
        sends 200OK response to the network, sends a new Invite request to
the user;

     2. if the user is not in registered state,
        sends 200OK response to the network.

After the Sip session is established TPF store the MSRP messages it
received when the receiver is in unregistered state and
will send the message(s) to the receiver once he registers.

Now when I tested my application server with Clearwater the 200OK from TPF
let Clearwater believes that the transaction is finished and
Clearwater invalidats OID for the service chain and this makes my
application server can't execute the rest service after it sends 200OK back
to Clearwater's scscf.

In the TS spec I don't find any suggestion that this behavior should be
supported or should not be supported.
My understanding is that while the 200OK response does mean the SIP
transaction is done but  it doesn't mean the service chain is done.

About when or what should trigger the invalidation of OID, maybe invalidate
OID once there is no more iFC is matched with the request for the
terminating session case?

Currently I'm using a walk around to make Clearwater continue to check the
rest iFCs:

     <SPT>
        <ConditionNegated>0</ConditionNegated>
        <Group>52</Group>
        <SIPHeader>
          <Header>P-Served-User</Header>
          <Content>.+\;sescase=orig\;.+</Content>
       </SIPHeader>
     </SPT>

The first time the request hits the terminating side the P-Served-User is
there, the second time this header is not there so this works.

But I'm hoping to have better solution for this issue.


Thanks
Anthony











On Wed, Feb 28, 2018 at 5:24 PM, Richard Whitehouse (projectclearwater.org)
<richard.whitehouse at projectclearwater.org> wrote:

> Anthony,
>
>
>
> Can you explain more about what your application server is doing, and why
> it?s responding on the ISC interface in this fashion?
>
>
>
> Can you point to anything in the TS specs which suggests that it?s
> supported for an AS to behave in this fashion?
>
>
>
> From Clearwater?s perspective, we need to invalidate the Original Dialog
> Identifier information at some point, and once we?ve received a 200 OK on
> the transaction, we don?t expect to hear anything more the Application
> Server as the 200 OK represents a final response for that SIP transaction.
>
>
>
> If the Application Server is allowed to send a SIP INVITE with a
> correlating ODI token to Clearwater at any point after we?ve sent it the
> request, we may need to keep that state around for an arbitrarily long
> period of time, which isn?t tenable.
>
>
>
>
>
> Richard
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Anthony Lee
> *Sent:* 24 February 2018 01:53
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Is it a bug that Clearwater
> invalidate the ODI once it receives 200OK response?
>
>
>
> From TS 124.229 V12.6.0, the spec doesn't say anything about the 200OK
> response from the request.
>
> It only talks about the subsequent request should be co-related with the
> previous request by using ODI in route header.
>
> To me it looks like a bug.
>
>
>
> On Fri, Feb 23, 2018 at 2:21 PM, Anthony Lee <anthonynlee at gmail.com>
> wrote:
>
> In my case, there is a application service in terminating side doing
> message Store-And-Forward.
>
> So when the service receives an Invite it replies 200OK response
> immiediately and then it create a new Invite  to the user.
>
>
>
> Since scscf invalidated the AS chain when it receives 200OK response the
> Invite request is matched with iFC again from the beginning instead just
> match with the rest iFCs. So it fail to send to the user.
>
>
>
> Is it a bug?
>
>
>
>
>
> Anthony
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180305/0dceb95b/attachment.html>

From richard.whitehouse at projectclearwater.org  Mon Mar  5 16:54:00 2018
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Mon, 5 Mar 2018 21:54:00 +0000
Subject: [Project Clearwater] Problems in manual installation of
 clearwater
In-Reply-To: <B89A1E0D519F0B44A3E67F67A33D3AE472DCF0@BGSMSX108.gar.corp.intel.com>
References: <B89A1E0D519F0B44A3E67F67A33D3AE472D9CA@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB3362B779BC0E5BEC0D206659F0C70@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DA93@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB336207B283339956ABE9573BF0C60@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DB6C@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB33629EC2131660C8D1E8FE86F0C60@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DB96@BGSMSX108.gar.corp.intel.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DC88@BGSMSX108.gar.corp.intel.com>
	<BN6PR02MB3362B93819922C8BF68D28DFF0C50@BN6PR02MB3362.namprd02.prod.outlook.com>
	<B89A1E0D519F0B44A3E67F67A33D3AE472DCF0@BGSMSX108.gar.corp.intel.com>
Message-ID: <BN6PR02MB33628C34C91E28423F719D43F0DA0@BN6PR02MB3362.namprd02.prod.outlook.com>

Pushpendra,

No, you shouldn't install bind on any node.

You need to have a DNS server, separate to Project Clearwater. If you need help configuring your DNS server, you should consult it's documentation.


Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 02 March 2018 13:03
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Hi Richard,
I am confusing about DNS configuration, is it need to configure on every node (bind need to install on every node?).  How to configure DNS records. Can you provide a guideline.
When I am trying to create ID on ellis it says like -
Failed to update the server (see detailed diagnostics in developer console). Please refresh the page. I think its because of DNS configuration.

Thanks,
Pushpendra
From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Friday, March 2, 2018 3:02 PM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

Are all of the nodes whose IPs are named in the etcd cluster setting running, or is just the Ellis node running?

Does the ellis node have IP connectivity to the other nodes?

If at least half of them aren't running then the cluster won't have quorum and won't be able to start.

Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 02 March 2018 07:50
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: Problems in manual installation of clearwater

Hi Richard,
I have successfully installed the ellis, bono etc, but when I am trying to download the shared_config file on ellis-1, it gives like-

ubuntu at ellis-1:~$ cw-config download shared_config
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint http://10.224.61.19:4000 exceeded header timeout

error #0: client: endpoint http://10.224.61.19:4000 exceeded header timeout

No changes to configuration can be made while the configuration database does not have quorum. Restore connectivity to the uncontactable nodes in the deployment and try again.


? Local_config file is-



ubuntu at ellis-1:/etc/clearwater$ cat local_config

local_ip=10.224.61.19

public_ip=10.224.61.19

public_hostname=ellis-1

etcd_cluster="10.224.61.19,10.224.61.20,10.224.61.27,10.224.61.34,10.224.61.39,10.224.61.48"



Thanks,

Pushpendra

From: Kumar, Pushpendra
Sent: Thursday, March 1, 2018 6:34 PM
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: Problems in manual installation of clearwater

Thanks Richard, Now that dpkg problem is solved in ellis. One thing, When I update using sudo apt-get update after after setting the debian package in clearwater.list, it give likes :

ubuntu at ellis-1:~$ sudo apt-get update >> update1.txt
ubuntu at ellis-1:~$ cat update1.txt
Hit http://security.ubuntu.com trusty-security InRelease
Ign http://repo.cw-ngv.com binary/ InRelease
Ign http://in.archive.ubuntu.com trusty InRelease
Hit http://security.ubuntu.com trusty-security/main Sources
Get:1 http://repo.cw-ngv.com binary/ Release.gpg [819 B]
Get:2 http://repo.cw-ngv.com binary/ Release [1,219 B]
Get:3 http://in.archive.ubuntu.com trusty-updates InRelease [65.9 kB]
Hit http://security.ubuntu.com trusty-security/restricted Sources
Get:4 http://repo.cw-ngv.com binary/ Packages [23.0 kB]
Hit http://security.ubuntu.com trusty-security/universe Sources
Hit http://security.ubuntu.com trusty-security/multiverse Sources
Get:5 http://in.archive.ubuntu.com trusty-backports InRelease [65.9 kB]
Hit http://security.ubuntu.com trusty-security/main amd64 Packages
Hit http://security.ubuntu.com trusty-security/restricted amd64 Packages
Get:6 http://in.archive.ubuntu.com trusty Release.gpg [933 B]
Hit http://security.ubuntu.com trusty-security/universe amd64 Packages
Get:7 http://in.archive.ubuntu.com trusty-updates/main Sources [412 kB]
Hit http://security.ubuntu.com trusty-security/multiverse amd64 Packages
Ign http://repo.cw-ngv.com binary/ Translation-en_IN
Ign http://repo.cw-ngv.com binary/ Translation-en
Hit http://security.ubuntu.com trusty-security/main i386 Packages

->Will it be create problem later or its fine?

Thanks,
Pushpendra

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Thursday, March 1, 2018 4:31 PM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

What user are you running this as? Can you post the full log of the install process?

Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 10:46
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: Problems in manual installation of clearwater

Hi, I have created the new clean ubuntu vm(without openSSH and DNSserver), still I am getting this error while installing ellis:

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)


Note: On the same machine there is other vm which has installed ellis. Should there only be one ellis on one machine or it doesn't matter?

Thanks




From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Thursday, March 1, 2018 3:28 PM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

I think there are two reasons you are hitting problems installing Clearwater:


1)      The ellis node that you've created has a user called ellis, which as I described below is required to be a system user under which the ellis processes run.

I'd suggest you create the node with a different username (e.g. ubuntu or clearwater).

2)      The ellis server has bind9 installed on it, prior to installing Clearwater, which is conflicting with dnsmasq which Clearwater uses for caching DNS queries. You'll need to uninstall this before installing the Clearwater software.

At a guess, you selected the 'DNS server' task selection when installing the Ubuntu VM. We'd recommend that the only task selection you make as part of installing the Ubuntu VM is 'OpenSSH server', as that's useful to log into the node remotely, which allows you to copy and paste commands, and upload files easily.

We'll make a change to Clearwater so it detects this misconfiguration and requires it to be corrected prior to installing Clearwater.


Hope this helps!


Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 03:41
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox, but same errors (dpkg).

thanks

From: Kumar, Pushpendra
Sent: Thursday, March 1, 2018 9:07 AM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Hi Richard,
Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox).

This is the output of netstat -pltun:

[ellis]ellis at Ellis:~$ sudo netstat -pltun
[sudo] password for ellis:
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 10.224.61.25:53         0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:53            0.0.0.0:*               LISTEN      25075/named
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      28512/sshd
tcp        0      0 127.0.0.1:953           0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:2812          0.0.0.0:*               LISTEN      9667/monit
tcp        0      0 127.0.0.1:8000          0.0.0.0:*               LISTEN      2019/nginx
tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      8998/mysqld
tcp        0      0 10.224.61.25:2380       0.0.0.0:*               LISTEN      12039/etcd
tcp6       0      0 :::53                   :::*                    LISTEN      25075/named
tcp6       0      0 :::22                   :::*                    LISTEN      28512/sshd
tcp6       0      0 ::1:953                 :::*                    LISTEN      25075/named
tcp6       0      0 :::4000                 :::*                    LISTEN      12039/etcd
tcp6       0      0 :::80                   :::*                    LISTEN      2019/nginx
udp        0      0 10.224.61.25:53         0.0.0.0:*                           25075/named
udp        0      0 127.0.0.1:53            0.0.0.0:*                           25075/named
udp        0      0 0.0.0.0:68              0.0.0.0:*                           703/dhclient
udp        0      0 10.224.61.25:123        0.0.0.0:*                           9178/ntpd
udp        0      0 127.0.0.1:123           0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:123             0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:49694           0.0.0.0:*                           703/dhclient
udp6       0      0 :::29724                :::*                                703/dhclient
udp6       0      0 :::53                   :::*                                25075/named
udp6       0      0 fe80::a00:27ff:fe11:123 :::*                                9178/ntpd
udp6       0      0 ::1:123                 :::*                                9178/ntpd
udp6       0      0 :::123                  :::*                                9178/ntpd

I am also getting 502 Bad Gateway when I trying to connect ellis using http://ellis.iind.intel.com

Thanks
Pushpendra

From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Sent: Thursday, March 1, 2018 4:21 AM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

It sounds like you attempted to install ellis on a node was installed on a node in which there already was an ellis user - is that correct?

Ellis requires a user to run the components as for security so that we aren't running components as root which don't require root privileges, and this is required to be ellis.

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

>From these logs, and the similar  logs regarding bono, sprout and clearwater-management it looks like there's already a service running on the nodes which is bound to port 53 before you install Clearwater.

We've only regularly tested performing the manual install on a clean Ubuntu box, and if I create a new Ubuntu VM (e.g. the basic Ubuntu 14.0.4 VM ) I don't see anything already running on Port 53 before I install Clearwater.

ubuntu at ip-10-0-162-214:~$ sudo netstat -pltun
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1133/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      1133/sshd
udp        0      0 0.0.0.0:5254            0.0.0.0:*                           582/dhclient
udp        0      0 0.0.0.0:68              0.0.0.0:*                           582/dhclient
udp6       0      0 :::21727                :::*                                582/dhclient

Can you clarify what image you are attempting to install ellis on where you are seeing these errors, and what's already installed on the box? Can you run the above netstat command?

For the error:

reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

Can you provide the complete output of the vellum build log? It'd be useful to know in what context this error was output - had something in the build already failed? It sounds like it failed to install memcached on the vellum node.

Regarding smtp_smarthost - this needs to be an SMTP server which can send mail so that Ellis can send password recovery emails. If it's not configured then password recovery emails won't work. See the entry in http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options for details about the SMTP options.

Regarding home_domain, it's described in Clearwater Options reference - see http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options  - it needs to be a domain which will resolve to the P-CSCFs (e.g. the bono nodes in the deployment). It's usually also the root domain for all of the domains used.

iind.intel.com might be a good choice if you can configure DNS entries under that domain - you'll need to configure the DNS entries listed in http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html in this domain

You will need to complete the DNS configuration before Ellis will work - it needs a DNS entry to exist in order to communicate with the other nodes in the deployment.



Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 28 February 2018 19:00
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: Problems in manual installation of clearwater

Hi Richards,
I need your help in Clearwater project manual installation, Its on high priority so please consider that.

I am installing the clearwater usingg manual installation. I have created the 6 VMs on virtualbox (using bridge adapter in network setting, used the same IP as public_ip and local_ip in local.conf) as I follow http://clearwater.readthedocs.io/en/stable/Manual_Install.html. while installing I have faced some errors (mentioned below), it will be your great help if u guide some solutions for them:

One more thing as I am using bridge adapter in network, I have not did any port forwarding as mention in document (I am able to ping vm from one to another i.e. they are are communicating)

1.in installtion of ellis:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install ellis --yes

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8
Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

Note: First time I install ellis I got this error, then I installed again in new node from scratch then also got the same error.


dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)



2.in installation of bono:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install bono restund --yes

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use



3.in installation of sprout:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install sprout --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use



4.in installtion of homer:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install homer --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
                                                                                                                                                                                                                                      [fail]
invoke-rc.d: initscript dnsmasq, action "start" failed.
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use


5.in installtion of dime:
$sudo DEBIAN_FRONTEND=noninteractive apt-get install dime clearwater-prov-tools --yes


dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use




6.in installtion of vellum:

* Starting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for port 53: Address already in use

* Restarting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

                                                                                                                                                                                                                                      [fail]
reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

-->I ignore above errors and move on to next step i.e. editing shared_config file and uploading


-->shared_config file:

[bono]bono at Bono:/etc/clearwater$ cat shared_config
#####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment
#####################################################################
# Deployment definitions
home_domain=iind.intel.com
sprout_hostname=sprout.iind.intel.com
sprout_registration_store=vellum.iind.intel.com
hs_hostname=hs.iind.intel.com:8888
hs_provisioning_hostname=hs.iind.intel.com:8889
homestead_impu_store=vellum.iind.intel.com
ralf_hostname=ralf.iind.intel.com:10888
ralf_session_store=vellum.iind.intel.com
xdms_hostname=homer.iind.intel.com:7888
chronos_hostname=vellum.iind.intel.com
cassandra_hostname=vellum.iin.intel.com

# Email server configuration
smtp_smarthost=
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


?  What would I use in smtp_smarthost=   (is it localhost?)

ques: Is home_domain =iind.intel.com is right? (when I ping using #ping ellis it automatically takes like ellis.iind.intel.com) , it basically the intel's domain.


-->local_config : (IP ans hostname changed in every node)

local_ip=10.224.61.25
public_ip=10.224.61.25
public_hostname=Ellis
etcd_cluster="10.224.61.20,10.224.61.21,10.224.61.22,10.224.61.25,10.224.61.48,10.224.61.50"



?  After that when I am trying to connect to ellis using http://ellis.iind.intel.com or http://10.224.61.25 it is giving like 502 Bad Gateway nginx /1.4.6 (Ubuntu). I have not did any DNS configuration yet (because I am using the intel domain or do I need to do it... in which node and how).

?  I will love to hear your response


Thanks
Pushpendra


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180305/957ab057/attachment.html>

From wangwulin at hotmail.com  Mon Mar  5 20:23:05 2018
From: wangwulin at hotmail.com (wang wulin)
Date: Tue, 6 Mar 2018 01:23:05 +0000
Subject: [Project Clearwater] [Clearwater] 480 Temporarily Unavailable
In-Reply-To: <KL1PR0301MB2120FFFF26D295D0A10CE5A9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>
References: <KL1PR0301MB2120B6181E0CA61719D6BFC9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>,
	<KL1PR0301MB2120FFFF26D295D0A10CE5A9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>
Message-ID: <KL1PR0301MB2120E99F14638671BECFD0E2B9D90@KL1PR0301MB2120.apcprd03.prod.outlook.com>

Hi Clearwater Team,


I tried with "nice -n-20 /usr/share/clearwater/bin/sipp -i 10.67.79.16 -sf ./sip-stress.xml 10.67.79.17 -t tn -s clearwater.opnfv -inf ./users.csv.1 -users 1000 -m 1000 -default_behaviors all,-bye -max_socket 65000 -max_reconnect -1 -reconnect_sleep 0 -reconnect_close 0 -send_timeout 4000 -recv_timeout 12000"


and the statistics shows here: https://hastebin.com/inekabihin.sql


2018-03-05      22:55:36:131    1520290536.131506: Aborting call on unexpected message for Call-Id '679-28944 at 10.67.79.16': while expecting 'INVITE' (index 23), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:8412;rport=8412;received=10.67.79.16;branch=z9hG4bK-2010000642-679-30.000000-1
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:n409qMON4B at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 2010000642-30.000000///679-28944 at 10.67.79.16
From: <sip:2010000642 at clearwater.opnfv>;tag=28944SIPpTag006791234
To: <sip:2010000643 at clearwater.opnfv>;tag=z9hG4bKPj33yF8tC.WrHnJKCwT2P.NJawKhULQPY1
CSeq: 546 INVITE
Content-Length:  0

Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?2? 16:39
???: clearwater at lists.projectclearwater.org
??: ??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,


Here is the result after running ""/usr/share/clearwater/bin/run_stress clearwater.opnfv 16 10 "."


2018-03-02      08:25:12.757782 1519979112.757782: Aborting call on unexpected message for Call-Id '1-21165 at 10.67.79.16': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:54572;received=10.67.79.16;branch=z9hG4bK-21165-1-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:/bDGU121V2 at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 1-21165 at 10.67.79.16
From: <sip:2770000012 at clearwater.opnfv>;tag=21165SIPpTag001
To: <sip:2770000015 at clearwater.opnfv>;tag=z9hG4bKPjtk6L-d0QjzkZB8rbq-CMdJGW9zpHhMbt
CSeq: 1 INVITE
Content-Length:  0


Total calls: 1
Successful calls: 0 (0.0%)
Failed calls: 1 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 5
Successful re-REGISTERs: 5 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 12.0ms


Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?2? 15:38
???: clearwater at lists.projectclearwater.org
??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,


I deployed a stress node according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html, and tried to run stress via "nice -n-20 /usr/share/clearwater/bin/sipp -i 10.67.79.16 -sf ./sip-stress.xml 10.67.79.17 -t tn -s clearwater.opnfv -inf ./users.csv.1 -users 1000 -m 1000 -default_behaviors all,-bye -max_socket 65000 -max_reconnect -1 -reconnect_sleep 0 -reconnect_close 0 -send_timeout 4000 -recv_timeout 12000 ".

"Register" works now, but Call step still failed, please see below for detailed info: https://hastebin.com/inekabihin.sql

2018-03-05      22:55:36:131    1520290536.131506: Aborting call on unexpected message for Call-Id '679-28944 at 10.67.79.16': while expecting 'INVITE' (index 23), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:8412;rport=8412;received=10.67.79.16;branch=z9hG4bK-2010000642-679-30.000000-1
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:n409qMON4B at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 2010000642-30.000000///679-28944 at 10.67.79.16
From: <sip:2010000642 at clearwater.opnfv>;tag=28944SIPpTag006791234
To: <sip:2010000643 at clearwater.opnfv>;tag=z9hG4bKPj33yF8tC.WrHnJKCwT2P.NJawKhULQPY1
CSeq: 546 INVITE
Content-Length:  0


 I got the error from /var/log/sprout: "Error hssconnection.cpp:704: Could not get subscriber data from HSS"


I only executed the 4 commands below on Vellum Node:
1)  . /etc/clearwater/config; for DN in {2770000000..2770000099} ; do echo sip:$DN@$home_domain,$DN at clearwater.opnf<mailto:DN at clearwater.opnf>v,clearwater.opnfv,7kkzTyGW ;done > users.csv
2) cd /usr/share/clearwater/crest-prov/src/metaswitch/crest/tools/ && python bulk_create.py users.csv
3) ./users.create_xdm.sh
4) ./users.create_homestead.sh

Did I miss some other steps?
Do you know if we "HSS" node is also required?


2)

I also got the error from Dime node:


root at dime-5y29tl:/var/log/ralf# vim /var/log/syslog
Mar  2 06:37:01 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument
Mar  2 06:37:02 dime-5y29tl config-manager[10778]: dropped request: 'issue-alarm config-manager 8500.3'
Mar  2 06:37:08 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument
Mar  2 06:37:18 dime-5y29tl issue-alarm: message repeated 12 times: [ zmq_msg_recv: Invalid argument]
Mar  2 06:37:18 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9001.1'
Mar  2 06:37:21 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9002.1'
Mar  2 06:37:22 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument

root at dime-5y29tl:/var/log/ralf# vim /var/log/monit.log
[UTC Mar  2 02:31:43] error    : 'poll_etcd_cluster' '/usr/share/clearwater/bin/poll_etcd_cluster.sh' failed with exit status (1) -- 1
[UTC Mar  2 02:31:43] info     : 'poll_etcd_cluster' exec: /bin/bash
[UTC Mar  2 02:31:53] info     : 'poll_etcd_cluster' status succeeded [status=0] -- zmq_msg_recv: Resource temporarily unavailable


Any help would be much appreciated!



Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180306/ec98f65e/attachment.html>

From wangwulin at hotmail.com  Mon Mar  5 21:05:30 2018
From: wangwulin at hotmail.com (wang wulin)
Date: Tue, 6 Mar 2018 02:05:30 +0000
Subject: [Project Clearwater] =?gb2312?b?tPC4tDogW0NsZWFyd2F0ZXJdIDQ4MCBU?=
 =?gb2312?b?ZW1wb3JhcmlseSBVbmF2YWlsYWJsZQ==?=
In-Reply-To: <KL1PR0301MB2120E99F14638671BECFD0E2B9D90@KL1PR0301MB2120.apcprd03.prod.outlook.com>
References: <KL1PR0301MB2120B6181E0CA61719D6BFC9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>,
	<KL1PR0301MB2120FFFF26D295D0A10CE5A9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>,
	<KL1PR0301MB2120E99F14638671BECFD0E2B9D90@KL1PR0301MB2120.apcprd03.prod.outlook.com>
Message-ID: <KL1PR0301MB21207021C492FA72DDB45037B9D90@KL1PR0301MB2120.apcprd03.prod.outlook.com>

I checked the syslog on Bind Node, it says:

Description: clearwater-cluster-manager is exiting due to missing configuration. @@Cause: clearwater-cluster-manager was started without the mandatory etcd_cluster_key parameter. @@Effect: Datastore cluster management services are no longer available. @@Action: Verify that the configuration file /etc/clearwater/local_config is correct according to the documentation.

Anyone would give some instructions about how to config etcd_cluster_key ?

Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?6? 9:23
???: Clearwater at lists.projectclearwater.org
??: [Clearwater] 480 Temporarily Unavailable


Hi Clearwater Team,


I tried with "nice -n-20 /usr/share/clearwater/bin/sipp -i 10.67.79.16 -sf ./sip-stress.xml 10.67.79.17 -t tn -s clearwater.opnfv -inf ./users.csv.1 -users 1000 -m 1000 -default_behaviors all,-bye -max_socket 65000 -max_reconnect -1 -reconnect_sleep 0 -reconnect_close 0 -send_timeout 4000 -recv_timeout 12000"


and the statistics shows here: https://hastebin.com/inekabihin.sql


2018-03-05      22:55:36:131    1520290536.131506: Aborting call on unexpected message for Call-Id '679-28944 at 10.67.79.16': while expecting 'INVITE' (index 23), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:8412;rport=8412;received=10.67.79.16;branch=z9hG4bK-2010000642-679-30.000000-1
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:n409qMON4B at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 2010000642-30.000000///679-28944 at 10.67.79.16
From: <sip:2010000642 at clearwater.opnfv>;tag=28944SIPpTag006791234
To: <sip:2010000643 at clearwater.opnfv>;tag=z9hG4bKPj33yF8tC.WrHnJKCwT2P.NJawKhULQPY1
CSeq: 546 INVITE
Content-Length:  0

Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?2? 16:39
???: clearwater at lists.projectclearwater.org
??: ??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,


Here is the result after running ""/usr/share/clearwater/bin/run_stress clearwater.opnfv 16 10 "."


2018-03-02      08:25:12.757782 1519979112.757782: Aborting call on unexpected message for Call-Id '1-21165 at 10.67.79.16': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:54572;received=10.67.79.16;branch=z9hG4bK-21165-1-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:/bDGU121V2 at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 1-21165 at 10.67.79.16
From: <sip:2770000012 at clearwater.opnfv>;tag=21165SIPpTag001
To: <sip:2770000015 at clearwater.opnfv>;tag=z9hG4bKPjtk6L-d0QjzkZB8rbq-CMdJGW9zpHhMbt
CSeq: 1 INVITE
Content-Length:  0


Total calls: 1
Successful calls: 0 (0.0%)
Failed calls: 1 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 5
Successful re-REGISTERs: 5 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 12.0ms


Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?2? 15:38
???: clearwater at lists.projectclearwater.org
??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,


I deployed a stress node according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html, and tried to run stress via "nice -n-20 /usr/share/clearwater/bin/sipp -i 10.67.79.16 -sf ./sip-stress.xml 10.67.79.17 -t tn -s clearwater.opnfv -inf ./users.csv.1 -users 1000 -m 1000 -default_behaviors all,-bye -max_socket 65000 -max_reconnect -1 -reconnect_sleep 0 -reconnect_close 0 -send_timeout 4000 -recv_timeout 12000 ".

"Register" works now, but Call step still failed, please see below for detailed info: https://hastebin.com/inekabihin.sql

2018-03-05      22:55:36:131    1520290536.131506: Aborting call on unexpected message for Call-Id '679-28944 at 10.67.79.16': while expecting 'INVITE' (index 23), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:8412;rport=8412;received=10.67.79.16;branch=z9hG4bK-2010000642-679-30.000000-1
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:n409qMON4B at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 2010000642-30.000000///679-28944 at 10.67.79.16
From: <sip:2010000642 at clearwater.opnfv>;tag=28944SIPpTag006791234
To: <sip:2010000643 at clearwater.opnfv>;tag=z9hG4bKPj33yF8tC.WrHnJKCwT2P.NJawKhULQPY1
CSeq: 546 INVITE
Content-Length:  0


 I got the error from /var/log/sprout: "Error hssconnection.cpp:704: Could not get subscriber data from HSS"


I only executed the 4 commands below on Vellum Node:
1)  . /etc/clearwater/config; for DN in {2770000000..2770000099} ; do echo sip:$DN@$home_domain,$DN at clearwater.opnf<mailto:DN at clearwater.opnf>v,clearwater.opnfv,7kkzTyGW ;done > users.csv
2) cd /usr/share/clearwater/crest-prov/src/metaswitch/crest/tools/ && python bulk_create.py users.csv
3) ./users.create_xdm.sh
4) ./users.create_homestead.sh

Did I miss some other steps?
Do you know if we "HSS" node is also required?


2)

I also got the error from Dime node:


root at dime-5y29tl:/var/log/ralf# vim /var/log/syslog
Mar  2 06:37:01 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument
Mar  2 06:37:02 dime-5y29tl config-manager[10778]: dropped request: 'issue-alarm config-manager 8500.3'
Mar  2 06:37:08 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument
Mar  2 06:37:18 dime-5y29tl issue-alarm: message repeated 12 times: [ zmq_msg_recv: Invalid argument]
Mar  2 06:37:18 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9001.1'
Mar  2 06:37:21 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9002.1'
Mar  2 06:37:22 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument

root at dime-5y29tl:/var/log/ralf# vim /var/log/monit.log
[UTC Mar  2 02:31:43] error    : 'poll_etcd_cluster' '/usr/share/clearwater/bin/poll_etcd_cluster.sh' failed with exit status (1) -- 1
[UTC Mar  2 02:31:43] info     : 'poll_etcd_cluster' exec: /bin/bash
[UTC Mar  2 02:31:53] info     : 'poll_etcd_cluster' status succeeded [status=0] -- zmq_msg_recv: Resource temporarily unavailable


Any help would be much appreciated!



Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180306/77191103/attachment.html>

From Bennett.Allen at metaswitch.com  Tue Mar  6 09:13:28 2018
From: Bennett.Allen at metaswitch.com (Bennett Allen)
Date: Tue, 6 Mar 2018 14:13:28 +0000
Subject: [Project Clearwater] Could not get subscriber data from HSS
Message-ID: <CY4PR02MB2517F3D443564A17EE4BB1CF9CD90@CY4PR02MB2517.namprd02.prod.outlook.com>

Hi Linda,

Did you receive Adam's email, asking for additional logs and config which we'd need to debug this? 

He asked for:
*	The contents of /var/log/homestead/ on the Dime node, with potentially increased logging level, so we can spot errors when it reads from the Cassandra subscriber store.
*	The /etc/clearwater/shared_config config file.
*	The output of `sudo monit summary`.

Also, you mentioned a 'Bind Node' - this isn't part of Project Clearwater (you can see what is part of Project Clearwater at http://www.projectclearwater.org/technical/clearwater-architecture/). Could you confirm how you created this deployment - for example, was it from OpenStack Heat templates? 

Thanks,
Ben

-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 06 March 2018 02:06
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 59, Issue 12

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. ??: [Clearwater] 480 Temporarily Unavailable (wang wulin)


----------------------------------------------------------------------

Message: 1
Date: Tue, 6 Mar 2018 02:05:30 +0000
From: wang wulin <wangwulin at hotmail.com>
To: "Clearwater at lists.projectclearwater.org"
	<Clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] ??: [Clearwater] 480 Temporarily
	Unavailable
Message-ID:
	<KL1PR0301MB21207021C492FA72DDB45037B9D90 at KL1PR0301MB2120.apcprd03.prod.outlook.com>
	
Content-Type: text/plain; charset="gb2312"

I checked the syslog on Bind Node, it says:

Description: clearwater-cluster-manager is exiting due to missing configuration. @@Cause: clearwater-cluster-manager was started without the mandatory etcd_cluster_key parameter. @@Effect: Datastore cluster management services are no longer available. @@Action: Verify that the configuration file /etc/clearwater/local_config is correct according to the documentation.

Anyone would give some instructions about how to config etcd_cluster_key ?

Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?6? 9:23
???: Clearwater at lists.projectclearwater.org
??: [Clearwater] 480 Temporarily Unavailable


Hi Clearwater Team,


I tried with "nice -n-20 /usr/share/clearwater/bin/sipp -i 10.67.79.16 -sf ./sip-stress.xml 10.67.79.17 -t tn -s clearwater.opnfv -inf ./users.csv.1 -users 1000 -m 1000 -default_behaviors all,-bye -max_socket 65000 -max_reconnect -1 -reconnect_sleep 0 -reconnect_close 0 -send_timeout 4000 -recv_timeout 12000"


and the statistics shows here: https://hastebin.com/inekabihin.sql


2018-03-05      22:55:36:131    1520290536.131506: Aborting call on unexpected message for Call-Id '679-28944 at 10.67.79.16': while expecting 'INVITE' (index 23), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:8412;rport=8412;received=10.67.79.16;branch=z9hG4bK-2010000642-679-30.000000-1
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:n409qMON4B at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 2010000642-30.000000///679-28944 at 10.67.79.16
From: <sip:2010000642 at clearwater.opnfv>;tag=28944SIPpTag006791234
To: <sip:2010000643 at clearwater.opnfv>;tag=z9hG4bKPj33yF8tC.WrHnJKCwT2P.NJawKhULQPY1
CSeq: 546 INVITE
Content-Length:  0

Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?2? 16:39
???: clearwater at lists.projectclearwater.org
??: ??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,


Here is the result after running ""/usr/share/clearwater/bin/run_stress clearwater.opnfv 16 10 "."


2018-03-02      08:25:12.757782 1519979112.757782: Aborting call on unexpected message for Call-Id '1-21165 at 10.67.79.16': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:54572;received=10.67.79.16;branch=z9hG4bK-21165-1-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:/bDGU121V2 at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 1-21165 at 10.67.79.16
From: <sip:2770000012 at clearwater.opnfv>;tag=21165SIPpTag001
To: <sip:2770000015 at clearwater.opnfv>;tag=z9hG4bKPjtk6L-d0QjzkZB8rbq-CMdJGW9zpHhMbt
CSeq: 1 INVITE
Content-Length:  0


Total calls: 1
Successful calls: 0 (0.0%)
Failed calls: 1 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 5
Successful re-REGISTERs: 5 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 12.0ms


Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?2? 15:38
???: clearwater at lists.projectclearwater.org
??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,


I deployed a stress node according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html, and tried to run stress via "nice -n-20 /usr/share/clearwater/bin/sipp -i 10.67.79.16 -sf ./sip-stress.xml 10.67.79.17 -t tn -s clearwater.opnfv -inf ./users.csv.1 -users 1000 -m 1000 -default_behaviors all,-bye -max_socket 65000 -max_reconnect -1 -reconnect_sleep 0 -reconnect_close 0 -send_timeout 4000 -recv_timeout 12000 ".

"Register" works now, but Call step still failed, please see below for detailed info: https://hastebin.com/inekabihin.sql

2018-03-05      22:55:36:131    1520290536.131506: Aborting call on unexpected message for Call-Id '679-28944 at 10.67.79.16': while expecting 'INVITE' (index 23), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:8412;rport=8412;received=10.67.79.16;branch=z9hG4bK-2010000642-679-30.000000-1
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:n409qMON4B at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 2010000642-30.000000///679-28944 at 10.67.79.16
From: <sip:2010000642 at clearwater.opnfv>;tag=28944SIPpTag006791234
To: <sip:2010000643 at clearwater.opnfv>;tag=z9hG4bKPj33yF8tC.WrHnJKCwT2P.NJawKhULQPY1
CSeq: 546 INVITE
Content-Length:  0


 I got the error from /var/log/sprout: "Error hssconnection.cpp:704: Could not get subscriber data from HSS"


I only executed the 4 commands below on Vellum Node:
1)  . /etc/clearwater/config; for DN in {2770000000..2770000099} ; do echo sip:$DN@$home_domain,$DN at clearwater.opnf<mailto:DN at clearwater.opnf>v,clearwater.opnfv,7kkzTyGW ;done > users.csv
2) cd /usr/share/clearwater/crest-prov/src/metaswitch/crest/tools/ && python bulk_create.py users.csv
3) ./users.create_xdm.sh
4) ./users.create_homestead.sh

Did I miss some other steps?
Do you know if we "HSS" node is also required?


2)

I also got the error from Dime node:


root at dime-5y29tl:/var/log/ralf# vim /var/log/syslog Mar  2 06:37:01 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument Mar  2 06:37:02 dime-5y29tl config-manager[10778]: dropped request: 'issue-alarm config-manager 8500.3'
Mar  2 06:37:08 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument Mar  2 06:37:18 dime-5y29tl issue-alarm: message repeated 12 times: [ zmq_msg_recv: Invalid argument] Mar  2 06:37:18 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9001.1'
Mar  2 06:37:21 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9002.1'
Mar  2 06:37:22 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument

root at dime-5y29tl:/var/log/ralf# vim /var/log/monit.log
[UTC Mar  2 02:31:43] error    : 'poll_etcd_cluster' '/usr/share/clearwater/bin/poll_etcd_cluster.sh' failed with exit status (1) -- 1
[UTC Mar  2 02:31:43] info     : 'poll_etcd_cluster' exec: /bin/bash
[UTC Mar  2 02:31:53] info     : 'poll_etcd_cluster' status succeeded [status=0] -- zmq_msg_recv: Resource temporarily unavailable


Any help would be much appreciated!



Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180306/77191103/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 59, Issue 12
******************************************



From Bennett.Allen at metaswitch.com  Tue Mar  6 09:15:31 2018
From: Bennett.Allen at metaswitch.com (Bennett Allen)
Date: Tue, 6 Mar 2018 14:15:31 +0000
Subject: [Project Clearwater] Is it a bug that Clearwater invalidates
 the ODI once it receives 200 ok response?
Message-ID: <CY4PR02MB2517DE75D8ACFD36931F75149CD90@CY4PR02MB2517.namprd02.prod.outlook.com>

Hi Anthony, 

We've had a look in the 3GPP specs to confirm whether Clearwater is doing the right thing here, and TS 23.218 section 5.2.3 states:

"If an Application Server decides to locally terminate a request and sends back a final response for that request via the ISC interface to the S-CSCF, the S-CSCF shall abandon verification of the matching of the triggers of lower priority in the list.
NOTE 4:  If AS has service logic whereby it wishes to send a request to the S-CSCF to continue with filter criteria evaluation from where it left off with the final response to the previous request, then a new request must be sent with data that can be used by the S-CSCF to determine where it left off with filter criteria evaluation.  For example, a parameter can be included in the request that is also defined in a service point trigger."
I think this is describing the situation you're in - your AS has sent a final response (the 200 OK), and now "wishes to send a request to the S-CSCF to continue with filter criteria evaluation from where it left off with the final response to the previous request". The specs suggest that what you're currently doing, checking a parameter in the service point trigger, is the right approach.

Instead of checking the P-Served-User header, one thing we've seen that works well is to have the application server add an extra header when it re-originates the request, checking for that header in the IFCs, and skipping the application server if it's present. For example:

            <SPT>
                <ConditionNegated>1</ConditionNegated>
                <Group>2</Group>
                <SIPHeader>
                    <Header>X-ContinueFC</Header>
                    <Content>orig</Content>
                </SIPHeader>
                <Extension/>
            </SPT>

Let us know how it goes,
Ben



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 05 March 2018 21:55
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 59, Issue 10

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. Re: Is it a bug that Clearwater invalidate the ODI once it
      receives 200OK response? (Anthony Lee)
   2. Re: Problems in manual installation of clearwater
      (Richard Whitehouse (projectclearwater.org))


----------------------------------------------------------------------

Message: 1
Date: Mon, 5 Mar 2018 12:06:40 -0500
From: Anthony Lee <anthonynlee at gmail.com>
To: "Richard Whitehouse (projectclearwater.org)"
	<richard.whitehouse at projectclearwater.org>
Cc: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Is it a bug that Clearwater
	invalidate the ODI once it receives 200OK response?
Message-ID:
	<CA+pBo5EF_dn4rytFyBoR-ZAjhZcY4TncH2fh_Rj+u4miRKp8jA at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Hi Richard,

My application server is doing RCS Message Store and Forward, it have two
roles: Originating and Terminating.
When it act as terminating role(let's called it TPF) it provides
Store-and-Forward: it accepts the Invite request from the network and act as below:
     1. if the user is in registered state,
        sends 200OK response to the network, sends a new Invite request to the user;

     2. if the user is not in registered state,
        sends 200OK response to the network.

After the Sip session is established TPF store the MSRP messages it received when the receiver is in unregistered state and will send the message(s) to the receiver once he registers.

Now when I tested my application server with Clearwater the 200OK from TPF let Clearwater believes that the transaction is finished and Clearwater invalidats OID for the service chain and this makes my application server can't execute the rest service after it sends 200OK back to Clearwater's scscf.

In the TS spec I don't find any suggestion that this behavior should be supported or should not be supported.
My understanding is that while the 200OK response does mean the SIP transaction is done but  it doesn't mean the service chain is done.

About when or what should trigger the invalidation of OID, maybe invalidate OID once there is no more iFC is matched with the request for the terminating session case?

Currently I'm using a walk around to make Clearwater continue to check the rest iFCs:

     <SPT>
        <ConditionNegated>0</ConditionNegated>
        <Group>52</Group>
        <SIPHeader>
          <Header>P-Served-User</Header>
          <Content>.+\;sescase=orig\;.+</Content>
       </SIPHeader>
     </SPT>

The first time the request hits the terminating side the P-Served-User is there, the second time this header is not there so this works.

But I'm hoping to have better solution for this issue.


Thanks
Anthony











On Wed, Feb 28, 2018 at 5:24 PM, Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org> wrote:

> Anthony,
>
>
>
> Can you explain more about what your application server is doing, and 
> why it?s responding on the ISC interface in this fashion?
>
>
>
> Can you point to anything in the TS specs which suggests that it?s 
> supported for an AS to behave in this fashion?
>
>
>
> From Clearwater?s perspective, we need to invalidate the Original 
> Dialog Identifier information at some point, and once we?ve received a 
> 200 OK on the transaction, we don?t expect to hear anything more the 
> Application Server as the 200 OK represents a final response for that SIP transaction.
>
>
>
> If the Application Server is allowed to send a SIP INVITE with a 
> correlating ODI token to Clearwater at any point after we?ve sent it 
> the request, we may need to keep that state around for an arbitrarily 
> long period of time, which isn?t tenable.
>
>
>
>
>
> Richard
>
>
>
> *From:* Clearwater 
> [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Anthony Lee
> *Sent:* 24 February 2018 01:53
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Is it a bug that Clearwater 
> invalidate the ODI once it receives 200OK response?
>
>
>
> From TS 124.229 V12.6.0, the spec doesn't say anything about the 200OK 
> response from the request.
>
> It only talks about the subsequent request should be co-related with 
> the previous request by using ODI in route header.
>
> To me it looks like a bug.
>
>
>
> On Fri, Feb 23, 2018 at 2:21 PM, Anthony Lee <anthonynlee at gmail.com>
> wrote:
>
> In my case, there is a application service in terminating side doing 
> message Store-And-Forward.
>
> So when the service receives an Invite it replies 200OK response 
> immiediately and then it create a new Invite  to the user.
>
>
>
> Since scscf invalidated the AS chain when it receives 200OK response 
> the Invite request is matched with iFC again from the beginning 
> instead just match with the rest iFCs. So it fail to send to the user.
>
>
>
> Is it a bug?
>
>
>
>
>
> Anthony
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180305/0dceb95b/attachment-0001.html>

------------------------------

Message: 2
Date: Mon, 5 Mar 2018 21:54:00 +0000
From: "Richard Whitehouse (projectclearwater.org)"
	<richard.whitehouse at projectclearwater.org>
To: "Kumar, Pushpendra" <pushpendra.kumar at intel.com>
Cc: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Problems in manual installation of
	clearwater
Message-ID:
	<BN6PR02MB33628C34C91E28423F719D43F0DA0 at BN6PR02MB3362.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="us-ascii"

Pushpendra,

No, you shouldn't install bind on any node.

You need to have a DNS server, separate to Project Clearwater. If you need help configuring your DNS server, you should consult it's documentation.


Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 02 March 2018 13:03
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Hi Richard,
I am confusing about DNS configuration, is it need to configure on every node (bind need to install on every node?).  How to configure DNS records. Can you provide a guideline.
When I am trying to create ID on ellis it says like - Failed to update the server (see detailed diagnostics in developer console). Please refresh the page. I think its because of DNS configuration.

Thanks,
Pushpendra
From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Friday, March 2, 2018 3:02 PM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

Are all of the nodes whose IPs are named in the etcd cluster setting running, or is just the Ellis node running?

Does the ellis node have IP connectivity to the other nodes?

If at least half of them aren't running then the cluster won't have quorum and won't be able to start.

Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 02 March 2018 07:50
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: Problems in manual installation of clearwater

Hi Richard,
I have successfully installed the ellis, bono etc, but when I am trying to download the shared_config file on ellis-1, it gives like-

ubuntu at ellis-1:~$ cw-config download shared_config
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint http://10.224.61.19:4000 exceeded header timeout

error #0: client: endpoint http://10.224.61.19:4000 exceeded header timeout

No changes to configuration can be made while the configuration database does not have quorum. Restore connectivity to the uncontactable nodes in the deployment and try again.


? Local_config file is-



ubuntu at ellis-1:/etc/clearwater$ cat local_config

local_ip=10.224.61.19

public_ip=10.224.61.19

public_hostname=ellis-1

etcd_cluster="10.224.61.19,10.224.61.20,10.224.61.27,10.224.61.34,10.224.61.39,10.224.61.48"



Thanks,

Pushpendra

From: Kumar, Pushpendra
Sent: Thursday, March 1, 2018 6:34 PM
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: Problems in manual installation of clearwater

Thanks Richard, Now that dpkg problem is solved in ellis. One thing, When I update using sudo apt-get update after after setting the debian package in clearwater.list, it give likes :

ubuntu at ellis-1:~$ sudo apt-get update >> update1.txt ubuntu at ellis-1:~$ cat update1.txt Hit http://security.ubuntu.com trusty-security InRelease Ign http://repo.cw-ngv.com binary/ InRelease Ign http://in.archive.ubuntu.com trusty InRelease Hit http://security.ubuntu.com trusty-security/main Sources
Get:1 http://repo.cw-ngv.com binary/ Release.gpg [819 B]
Get:2 http://repo.cw-ngv.com binary/ Release [1,219 B]
Get:3 http://in.archive.ubuntu.com trusty-updates InRelease [65.9 kB] Hit http://security.ubuntu.com trusty-security/restricted Sources
Get:4 http://repo.cw-ngv.com binary/ Packages [23.0 kB] Hit http://security.ubuntu.com trusty-security/universe Sources Hit http://security.ubuntu.com trusty-security/multiverse Sources
Get:5 http://in.archive.ubuntu.com trusty-backports InRelease [65.9 kB] Hit http://security.ubuntu.com trusty-security/main amd64 Packages Hit http://security.ubuntu.com trusty-security/restricted amd64 Packages
Get:6 http://in.archive.ubuntu.com trusty Release.gpg [933 B] Hit http://security.ubuntu.com trusty-security/universe amd64 Packages
Get:7 http://in.archive.ubuntu.com trusty-updates/main Sources [412 kB] Hit http://security.ubuntu.com trusty-security/multiverse amd64 Packages Ign http://repo.cw-ngv.com binary/ Translation-en_IN Ign http://repo.cw-ngv.com binary/ Translation-en Hit http://security.ubuntu.com trusty-security/main i386 Packages

->Will it be create problem later or its fine?

Thanks,
Pushpendra

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Thursday, March 1, 2018 4:31 PM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

What user are you running this as? Can you post the full log of the install process?

Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 10:46
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: Problems in manual installation of clearwater

Hi, I have created the new clean ubuntu vm(without openSSH and DNSserver), still I am getting this error while installing ellis:

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8 Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)


Note: On the same machine there is other vm which has installed ellis. Should there only be one ellis on one machine or it doesn't matter?

Thanks




From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Thursday, March 1, 2018 3:28 PM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

I think there are two reasons you are hitting problems installing Clearwater:


1)      The ellis node that you've created has a user called ellis, which as I described below is required to be a system user under which the ellis processes run.

I'd suggest you create the node with a different username (e.g. ubuntu or clearwater).

2)      The ellis server has bind9 installed on it, prior to installing Clearwater, which is conflicting with dnsmasq which Clearwater uses for caching DNS queries. You'll need to uninstall this before installing the Clearwater software.

At a guess, you selected the 'DNS server' task selection when installing the Ubuntu VM. We'd recommend that the only task selection you make as part of installing the Ubuntu VM is 'OpenSSH server', as that's useful to log into the node remotely, which allows you to copy and paste commands, and upload files easily.

We'll make a change to Clearwater so it detects this misconfiguration and requires it to be corrected prior to installing Clearwater.


Hope this helps!


Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 01 March 2018 03:41
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox, but same errors (dpkg).

thanks

From: Kumar, Pushpendra
Sent: Thursday, March 1, 2018 9:07 AM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: RE: Problems in manual installation of clearwater

Hi Richard,
Thanks for replying. I am using virtualbox for installing nodes, I have installed ellis first time on that node (for reconfirm, I installed all the nodes again on virtualbox).

This is the output of netstat -pltun:

[ellis]ellis at Ellis:~$ sudo netstat -pltun [sudo] password for ellis:
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 10.224.61.25:53         0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:53            0.0.0.0:*               LISTEN      25075/named
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      28512/sshd
tcp        0      0 127.0.0.1:953           0.0.0.0:*               LISTEN      25075/named
tcp        0      0 127.0.0.1:2812          0.0.0.0:*               LISTEN      9667/monit
tcp        0      0 127.0.0.1:8000          0.0.0.0:*               LISTEN      2019/nginx
tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      8998/mysqld
tcp        0      0 10.224.61.25:2380       0.0.0.0:*               LISTEN      12039/etcd
tcp6       0      0 :::53                   :::*                    LISTEN      25075/named
tcp6       0      0 :::22                   :::*                    LISTEN      28512/sshd
tcp6       0      0 ::1:953                 :::*                    LISTEN      25075/named
tcp6       0      0 :::4000                 :::*                    LISTEN      12039/etcd
tcp6       0      0 :::80                   :::*                    LISTEN      2019/nginx
udp        0      0 10.224.61.25:53         0.0.0.0:*                           25075/named
udp        0      0 127.0.0.1:53            0.0.0.0:*                           25075/named
udp        0      0 0.0.0.0:68              0.0.0.0:*                           703/dhclient
udp        0      0 10.224.61.25:123        0.0.0.0:*                           9178/ntpd
udp        0      0 127.0.0.1:123           0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:123             0.0.0.0:*                           9178/ntpd
udp        0      0 0.0.0.0:49694           0.0.0.0:*                           703/dhclient
udp6       0      0 :::29724                :::*                                703/dhclient
udp6       0      0 :::53                   :::*                                25075/named
udp6       0      0 fe80::a00:27ff:fe11:123 :::*                                9178/ntpd
udp6       0      0 ::1:123                 :::*                                9178/ntpd
udp6       0      0 :::123                  :::*                                9178/ntpd

I am also getting 502 Bad Gateway when I trying to connect ellis using http://ellis.iind.intel.com

Thanks
Pushpendra

From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Sent: Thursday, March 1, 2018 4:21 AM
To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Problems in manual installation of clearwater

Pushpendra,

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8 Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

It sounds like you attempted to install ellis on a node was installed on a node in which there already was an ellis user - is that correct?

Ellis requires a user to run the components as for security so that we aren't running components as root which don't require root privileges, and this is required to be ellis.

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

>From these logs, and the similar  logs regarding bono, sprout and clearwater-management it looks like there's already a service running on the nodes which is bound to port 53 before you install Clearwater.

We've only regularly tested performing the manual install on a clean Ubuntu box, and if I create a new Ubuntu VM (e.g. the basic Ubuntu 14.0.4 VM ) I don't see anything already running on Port 53 before I install Clearwater.

ubuntu at ip-10-0-162-214:~$ sudo netstat -pltun Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1133/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      1133/sshd
udp        0      0 0.0.0.0:5254            0.0.0.0:*                           582/dhclient
udp        0      0 0.0.0.0:68              0.0.0.0:*                           582/dhclient
udp6       0      0 :::21727                :::*                                582/dhclient

Can you clarify what image you are attempting to install ellis on where you are seeing these errors, and what's already installed on the box? Can you run the above netstat command?

For the error:

reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

Can you provide the complete output of the vellum build log? It'd be useful to know in what context this error was output - had something in the build already failed? It sounds like it failed to install memcached on the vellum node.

Regarding smtp_smarthost - this needs to be an SMTP server which can send mail so that Ellis can send password recovery emails. If it's not configured then password recovery emails won't work. See the entry in http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options for details about the SMTP options.

Regarding home_domain, it's described in Clearwater Options reference - see http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options  - it needs to be a domain which will resolve to the P-CSCFs (e.g. the bono nodes in the deployment). It's usually also the root domain for all of the domains used.

iind.intel.com might be a good choice if you can configure DNS entries under that domain - you'll need to configure the DNS entries listed in http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html in this domain

You will need to complete the DNS configuration before Ellis will work - it needs a DNS entry to exist in order to communicate with the other nodes in the deployment.



Richard

From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
Sent: 28 February 2018 19:00
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>
Subject: Problems in manual installation of clearwater

Hi Richards,
I need your help in Clearwater project manual installation, Its on high priority so please consider that.

I am installing the clearwater usingg manual installation. I have created the 6 VMs on virtualbox (using bridge adapter in network setting, used the same IP as public_ip and local_ip in local.conf) as I follow http://clearwater.readthedocs.io/en/stable/Manual_Install.html. while installing I have faced some errors (mentioned below), it will be your great help if u guide some solutions for them:

One more thing as I am using bridge adapter in network, I have not did any port forwarding as mention in document (I am able to ping vm from one to another i.e. they are are communicating)

1.in installtion of ellis:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install ellis --yes

usermod: user ellis is currently used by process 1343
dpkg: error processing package ellis (--configure):
subprocess installed post-installation script returned error exit status 8 Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
Processing triggers for ureadahead (0.100.0-16) ...
Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)

Note: First time I install ellis I got this error, then I installed again in new node from scratch then also got the same error.


dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use


$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

Errors were encountered while processing:
ellis
E: Sub-process /usr/bin/dpkg returned an error code (1)



2.in installation of bono:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install bono restund --yes

dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use



3.in installation of sprout:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install sprout --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

$sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use



4.in installtion of homer:

$sudo DEBIAN_FRONTEND=noninteractive apt-get install homer --yes

dnsmasq: failed to create listening socket for port 53: Address already in use
                                                                                                                                                                                                                                      [fail]
invoke-rc.d: initscript dnsmasq, action "start" failed.
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use


5.in installtion of dime:
$sudo DEBIAN_FRONTEND=noninteractive apt-get install dime clearwater-prov-tools --yes


dnsmasq: failed to create listening socket for port 53: Address already in use
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use




6.in installtion of vellum:

* Starting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for port 53: Address already in use

* Restarting DNS forwarder and DHCP server dnsmasq
dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use

                                                                                                                                                                                                                                      [fail]
reload: Job is not running: clearwater-monit
/usr/share/clearwater/infrastructure/scripts/memcached: line 43: /etc/memcached.conf: No such file or directory
reload: Job is not running: clearwater-monit

-->I ignore above errors and move on to next step i.e. editing 
-->shared_config file and uploading


-->shared_config file:

[bono]bono at Bono:/etc/clearwater$ cat shared_config #####################################################################
# No Shared Config has been provided
# Replace this file with the Shared Configuration for your deployment #####################################################################
# Deployment definitions
home_domain=iind.intel.com
sprout_hostname=sprout.iind.intel.com
sprout_registration_store=vellum.iind.intel.com
hs_hostname=hs.iind.intel.com:8888
hs_provisioning_hostname=hs.iind.intel.com:8889
homestead_impu_store=vellum.iind.intel.com
ralf_hostname=ralf.iind.intel.com:10888
ralf_session_store=vellum.iind.intel.com
xdms_hostname=homer.iind.intel.com:7888
chronos_hostname=vellum.iind.intel.com
cassandra_hostname=vellum.iin.intel.com

# Email server configuration
smtp_smarthost=
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


?  What would I use in smtp_smarthost=   (is it localhost?)

ques: Is home_domain =iind.intel.com is right? (when I ping using #ping ellis it automatically takes like ellis.iind.intel.com) , it basically the intel's domain.


-->local_config : (IP ans hostname changed in every node)

local_ip=10.224.61.25
public_ip=10.224.61.25
public_hostname=Ellis
etcd_cluster="10.224.61.20,10.224.61.21,10.224.61.22,10.224.61.25,10.224.61.48,10.224.61.50"



?  After that when I am trying to connect to ellis using http://ellis.iind.intel.com or http://10.224.61.25 it is giving like 502 Bad Gateway nginx /1.4.6 (Ubuntu). I have not did any DNS configuration yet (because I am using the intel domain or do I need to do it... in which node and how).

?  I will love to hear your response


Thanks
Pushpendra


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180305/957ab057/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 59, Issue 10
******************************************



From anthonynlee at gmail.com  Tue Mar  6 09:26:39 2018
From: anthonynlee at gmail.com (Anthony Lee)
Date: Tue, 6 Mar 2018 09:26:39 -0500
Subject: [Project Clearwater] Is it a bug that Clearwater invalidates
 the ODI once it receives 200 ok response?
In-Reply-To: <CY4PR02MB2517DE75D8ACFD36931F75149CD90@CY4PR02MB2517.namprd02.prod.outlook.com>
References: <CY4PR02MB2517DE75D8ACFD36931F75149CD90@CY4PR02MB2517.namprd02.prod.outlook.com>
Message-ID: <CA+pBo5Ex37XPxO1hxHuZQbh-5Y84A5sAjsKfSPXd+gg_tEMZ3g@mail.gmail.com>

I see, thanks for the  clarification and the suggestion.
I'll try it.

On Mar 6, 2018 09:16, "Bennett Allen" <Bennett.Allen at metaswitch.com> wrote:

> Hi Anthony,
>
> We've had a look in the 3GPP specs to confirm whether Clearwater is doing
> the right thing here, and TS 23.218 section 5.2.3 states:
>
> "If an Application Server decides to locally terminate a request and sends
> back a final response for that request via the ISC interface to the S-CSCF,
> the S-CSCF shall abandon verification of the matching of the triggers of
> lower priority in the list.
> NOTE 4:  If AS has service logic whereby it wishes to send a request to
> the S-CSCF to continue with filter criteria evaluation from where it left
> off with the final response to the previous request, then a new request
> must be sent with data that can be used by the S-CSCF to determine where it
> left off with filter criteria evaluation.  For example, a parameter can be
> included in the request that is also defined in a service point trigger."
> I think this is describing the situation you're in - your AS has sent a
> final response (the 200 OK), and now "wishes to send a request to the
> S-CSCF to continue with filter criteria evaluation from where it left off
> with the final response to the previous request". The specs suggest that
> what you're currently doing, checking a parameter in the service point
> trigger, is the right approach.
>
> Instead of checking the P-Served-User header, one thing we've seen that
> works well is to have the application server add an extra header when it
> re-originates the request, checking for that header in the IFCs, and
> skipping the application server if it's present. For example:
>
>             <SPT>
>                 <ConditionNegated>1</ConditionNegated>
>                 <Group>2</Group>
>                 <SIPHeader>
>                     <Header>X-ContinueFC</Header>
>                     <Content>orig</Content>
>                 </SIPHeader>
>                 <Extension/>
>             </SPT>
>
> Let us know how it goes,
> Ben
>
>
>
> -----Original Message-----
> From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> On Behalf Of clearwater-request at lists.projectclearwater.org
> Sent: 05 March 2018 21:55
> To: clearwater at lists.projectclearwater.org
> Subject: Clearwater Digest, Vol 59, Issue 10
>
> Send Clearwater mailing list submissions to
>         clearwater at lists.projectclearwater.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.projectclearwater.org/mailman/
> listinfo/clearwater_lists.projectclearwater.org
>
> or, via email, send a message with subject or body 'help' to
>         clearwater-request at lists.projectclearwater.org
>
> You can reach the person managing the list at
>         clearwater-owner at lists.projectclearwater.org
>
> When replying, please edit your Subject line so it is more specific than
> "Re: Contents of Clearwater digest..."
>
>
> Today's Topics:
>
>    1. Re: Is it a bug that Clearwater invalidate the ODI once it
>       receives 200OK response? (Anthony Lee)
>    2. Re: Problems in manual installation of clearwater
>       (Richard Whitehouse (projectclearwater.org))
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 5 Mar 2018 12:06:40 -0500
> From: Anthony Lee <anthonynlee at gmail.com>
> To: "Richard Whitehouse (projectclearwater.org)"
>         <richard.whitehouse at projectclearwater.org>
> Cc: "clearwater at lists.projectclearwater.org"
>         <clearwater at lists.projectclearwater.org>
> Subject: Re: [Project Clearwater] Is it a bug that Clearwater
>         invalidate the ODI once it receives 200OK response?
> Message-ID:
>         <CA+pBo5EF_dn4rytFyBoR-ZAjhZcY4TncH2fh_Rj+u4miRKp8jA@
> mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Hi Richard,
>
> My application server is doing RCS Message Store and Forward, it have two
> roles: Originating and Terminating.
> When it act as terminating role(let's called it TPF) it provides
> Store-and-Forward: it accepts the Invite request from the network and act
> as below:
>      1. if the user is in registered state,
>         sends 200OK response to the network, sends a new Invite request to
> the user;
>
>      2. if the user is not in registered state,
>         sends 200OK response to the network.
>
> After the Sip session is established TPF store the MSRP messages it
> received when the receiver is in unregistered state and will send the
> message(s) to the receiver once he registers.
>
> Now when I tested my application server with Clearwater the 200OK from TPF
> let Clearwater believes that the transaction is finished and Clearwater
> invalidats OID for the service chain and this makes my application server
> can't execute the rest service after it sends 200OK back to Clearwater's
> scscf.
>
> In the TS spec I don't find any suggestion that this behavior should be
> supported or should not be supported.
> My understanding is that while the 200OK response does mean the SIP
> transaction is done but  it doesn't mean the service chain is done.
>
> About when or what should trigger the invalidation of OID, maybe
> invalidate OID once there is no more iFC is matched with the request for
> the terminating session case?
>
> Currently I'm using a walk around to make Clearwater continue to check the
> rest iFCs:
>
>      <SPT>
>         <ConditionNegated>0</ConditionNegated>
>         <Group>52</Group>
>         <SIPHeader>
>           <Header>P-Served-User</Header>
>           <Content>.+\;sescase=orig\;.+</Content>
>        </SIPHeader>
>      </SPT>
>
> The first time the request hits the terminating side the P-Served-User is
> there, the second time this header is not there so this works.
>
> But I'm hoping to have better solution for this issue.
>
>
> Thanks
> Anthony
>
>
>
>
>
>
>
>
>
>
>
> On Wed, Feb 28, 2018 at 5:24 PM, Richard Whitehouse (projectclearwater.org)
> <richard.whitehouse at projectclearwater.org> wrote:
>
> > Anthony,
> >
> >
> >
> > Can you explain more about what your application server is doing, and
> > why it?s responding on the ISC interface in this fashion?
> >
> >
> >
> > Can you point to anything in the TS specs which suggests that it?s
> > supported for an AS to behave in this fashion?
> >
> >
> >
> > From Clearwater?s perspective, we need to invalidate the Original
> > Dialog Identifier information at some point, and once we?ve received a
> > 200 OK on the transaction, we don?t expect to hear anything more the
> > Application Server as the 200 OK represents a final response for that
> SIP transaction.
> >
> >
> >
> > If the Application Server is allowed to send a SIP INVITE with a
> > correlating ODI token to Clearwater at any point after we?ve sent it
> > the request, we may need to keep that state around for an arbitrarily
> > long period of time, which isn?t tenable.
> >
> >
> >
> >
> >
> > Richard
> >
> >
> >
> > *From:* Clearwater
> > [mailto:clearwater-bounces at lists.projectclearwater.org]
> > *On Behalf Of *Anthony Lee
> > *Sent:* 24 February 2018 01:53
> > *To:* clearwater at lists.projectclearwater.org
> > *Subject:* Re: [Project Clearwater] Is it a bug that Clearwater
> > invalidate the ODI once it receives 200OK response?
> >
> >
> >
> > From TS 124.229 V12.6.0, the spec doesn't say anything about the 200OK
> > response from the request.
> >
> > It only talks about the subsequent request should be co-related with
> > the previous request by using ODI in route header.
> >
> > To me it looks like a bug.
> >
> >
> >
> > On Fri, Feb 23, 2018 at 2:21 PM, Anthony Lee <anthonynlee at gmail.com>
> > wrote:
> >
> > In my case, there is a application service in terminating side doing
> > message Store-And-Forward.
> >
> > So when the service receives an Invite it replies 200OK response
> > immiediately and then it create a new Invite  to the user.
> >
> >
> >
> > Since scscf invalidated the AS chain when it receives 200OK response
> > the Invite request is matched with iFC again from the beginning
> > instead just match with the rest iFCs. So it fail to send to the user.
> >
> >
> >
> > Is it a bug?
> >
> >
> >
> >
> >
> > Anthony
> >
> >
> >
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.
> projectclearwater.org/attachments/20180305/0dceb95b/attachment-0001.html>
>
> ------------------------------
>
> Message: 2
> Date: Mon, 5 Mar 2018 21:54:00 +0000
> From: "Richard Whitehouse (projectclearwater.org)"
>         <richard.whitehouse at projectclearwater.org>
> To: "Kumar, Pushpendra" <pushpendra.kumar at intel.com>
> Cc: "clearwater at lists.projectclearwater.org"
>         <clearwater at lists.projectclearwater.org>
> Subject: Re: [Project Clearwater] Problems in manual installation of
>         clearwater
> Message-ID:
>         <BN6PR02MB33628C34C91E28423F719D43F0DA0 at BN6PR02MB3362.
> namprd02.prod.outlook.com>
>
> Content-Type: text/plain; charset="us-ascii"
>
> Pushpendra,
>
> No, you shouldn't install bind on any node.
>
> You need to have a DNS server, separate to Project Clearwater. If you need
> help configuring your DNS server, you should consult it's documentation.
>
>
> Richard
>
> From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
> Sent: 02 March 2018 13:03
> To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse@
> projectclearwater.org>
> Subject: RE: Problems in manual installation of clearwater
>
> Hi Richard,
> I am confusing about DNS configuration, is it need to configure on every
> node (bind need to install on every node?).  How to configure DNS records.
> Can you provide a guideline.
> When I am trying to create ID on ellis it says like - Failed to update the
> server (see detailed diagnostics in developer console). Please refresh the
> page. I think its because of DNS configuration.
>
> Thanks,
> Pushpendra
> From: Richard Whitehouse (projectclearwater.org) [mailto:
> richard.whitehouse at projectclearwater.org]
> Sent: Friday, March 2, 2018 3:02 PM
> To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:
> pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org
> <mailto:clearwater at lists.projectclearwater.org>
> Subject: RE: Problems in manual installation of clearwater
>
> Pushpendra,
>
> Are all of the nodes whose IPs are named in the etcd cluster setting
> running, or is just the Ellis node running?
>
> Does the ellis node have IP connectivity to the other nodes?
>
> If at least half of them aren't running then the cluster won't have quorum
> and won't be able to start.
>
> Richard
>
> From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
> Sent: 02 March 2018 07:50
> To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse@
> projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
> Subject: RE: Problems in manual installation of clearwater
>
> Hi Richard,
> I have successfully installed the ellis, bono etc, but when I am trying to
> download the shared_config file on ellis-1, it gives like-
>
> ubuntu at ellis-1:~$ cw-config download shared_config
> Error:  client: etcd cluster is unavailable or misconfigured; error #0:
> client: endpoint http://10.224.61.19:4000 exceeded header timeout
>
> error #0: client: endpoint http://10.224.61.19:4000 exceeded header
> timeout
>
> No changes to configuration can be made while the configuration database
> does not have quorum. Restore connectivity to the uncontactable nodes in
> the deployment and try again.
>
>
> ? Local_config file is-
>
>
>
> ubuntu at ellis-1:/etc/clearwater$ cat local_config
>
> local_ip=10.224.61.19
>
> public_ip=10.224.61.19
>
> public_hostname=ellis-1
>
> etcd_cluster="10.224.61.19,10.224.61.20,10.224.61.27,10.224.
> 61.34,10.224.61.39,10.224.61.48"
>
>
>
> Thanks,
>
> Pushpendra
>
> From: Kumar, Pushpendra
> Sent: Thursday, March 1, 2018 6:34 PM
> To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse@
> projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
> Subject: RE: Problems in manual installation of clearwater
>
> Thanks Richard, Now that dpkg problem is solved in ellis. One thing, When
> I update using sudo apt-get update after after setting the debian package
> in clearwater.list, it give likes :
>
> ubuntu at ellis-1:~$ sudo apt-get update >> update1.txt ubuntu at ellis-1:~$
> cat update1.txt Hit http://security.ubuntu.com trusty-security InRelease
> Ign http://repo.cw-ngv.com binary/ InRelease Ign
> http://in.archive.ubuntu.com trusty InRelease Hit
> http://security.ubuntu.com trusty-security/main Sources
> Get:1 http://repo.cw-ngv.com binary/ Release.gpg [819 B]
> Get:2 http://repo.cw-ngv.com binary/ Release [1,219 B]
> Get:3 http://in.archive.ubuntu.com trusty-updates InRelease [65.9 kB] Hit
> http://security.ubuntu.com trusty-security/restricted Sources
> Get:4 http://repo.cw-ngv.com binary/ Packages [23.0 kB] Hit
> http://security.ubuntu.com trusty-security/universe Sources Hit
> http://security.ubuntu.com trusty-security/multiverse Sources
> Get:5 http://in.archive.ubuntu.com trusty-backports InRelease [65.9 kB]
> Hit http://security.ubuntu.com trusty-security/main amd64 Packages Hit
> http://security.ubuntu.com trusty-security/restricted amd64 Packages
> Get:6 http://in.archive.ubuntu.com trusty Release.gpg [933 B] Hit
> http://security.ubuntu.com trusty-security/universe amd64 Packages
> Get:7 http://in.archive.ubuntu.com trusty-updates/main Sources [412 kB]
> Hit http://security.ubuntu.com trusty-security/multiverse amd64 Packages
> Ign http://repo.cw-ngv.com binary/ Translation-en_IN Ign
> http://repo.cw-ngv.com binary/ Translation-en Hit
> http://security.ubuntu.com trusty-security/main i386 Packages
>
> ->Will it be create problem later or its fine?
>
> Thanks,
> Pushpendra
>
> From: Richard Whitehouse (projectclearwater.org) [mailto:
> richard.whitehouse at projectclearwater.org]
> Sent: Thursday, March 1, 2018 4:31 PM
> To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:
> pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org
> <mailto:clearwater at lists.projectclearwater.org>
> Subject: RE: Problems in manual installation of clearwater
>
> What user are you running this as? Can you post the full log of the
> install process?
>
> Richard
>
> From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
> Sent: 01 March 2018 10:46
> To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse@
> projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
> Subject: RE: Problems in manual installation of clearwater
>
> Hi, I have created the new clean ubuntu vm(without openSSH and DNSserver),
> still I am getting this error while installing ellis:
>
> usermod: user ellis is currently used by process 1343
> dpkg: error processing package ellis (--configure):
> subprocess installed post-installation script returned error exit status 8
> Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
> Processing triggers for ureadahead (0.100.0-16) ...
> Errors were encountered while processing:
> ellis
> E: Sub-process /usr/bin/dpkg returned an error code (1)
>
>
> Note: On the same machine there is other vm which has installed ellis.
> Should there only be one ellis on one machine or it doesn't matter?
>
> Thanks
>
>
>
>
> From: Richard Whitehouse (projectclearwater.org) [mailto:
> richard.whitehouse at projectclearwater.org]
> Sent: Thursday, March 1, 2018 3:28 PM
> To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:
> pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org
> <mailto:clearwater at lists.projectclearwater.org>
> Subject: RE: Problems in manual installation of clearwater
>
> Pushpendra,
>
> I think there are two reasons you are hitting problems installing
> Clearwater:
>
>
> 1)      The ellis node that you've created has a user called ellis, which
> as I described below is required to be a system user under which the ellis
> processes run.
>
> I'd suggest you create the node with a different username (e.g. ubuntu or
> clearwater).
>
> 2)      The ellis server has bind9 installed on it, prior to installing
> Clearwater, which is conflicting with dnsmasq which Clearwater uses for
> caching DNS queries. You'll need to uninstall this before installing the
> Clearwater software.
>
> At a guess, you selected the 'DNS server' task selection when installing
> the Ubuntu VM. We'd recommend that the only task selection you make as part
> of installing the Ubuntu VM is 'OpenSSH server', as that's useful to log
> into the node remotely, which allows you to copy and paste commands, and
> upload files easily.
>
> We'll make a change to Clearwater so it detects this misconfiguration and
> requires it to be corrected prior to installing Clearwater.
>
>
> Hope this helps!
>
>
> Richard
>
> From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
> Sent: 01 March 2018 03:41
> To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.
> Whitehouse at metaswitch.com>>
> Subject: RE: Problems in manual installation of clearwater
>
> Thanks for replying. I am using virtualbox for installing nodes, I have
> installed ellis first time on that node (for reconfirm, I installed all the
> nodes again on virtualbox, but same errors (dpkg).
>
> thanks
>
> From: Kumar, Pushpendra
> Sent: Thursday, March 1, 2018 9:07 AM
> To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:
> Richard.Whitehouse at metaswitch.com>>
> Subject: RE: Problems in manual installation of clearwater
>
> Hi Richard,
> Thanks for replying. I am using virtualbox for installing nodes, I have
> installed ellis first time on that node (for reconfirm, I installed all the
> nodes again on virtualbox).
>
> This is the output of netstat -pltun:
>
> [ellis]ellis at Ellis:~$ sudo netstat -pltun [sudo] password for ellis:
> Active Internet connections (only servers)
> Proto Recv-Q Send-Q Local Address           Foreign Address         State
>      PID/Program name
> tcp        0      0 10.224.61.25:53         0.0.0.0:*
>  LISTEN      25075/named
> tcp        0      0 127.0.0.1:53            0.0.0.0:*
>  LISTEN      25075/named
> tcp        0      0 0.0.0.0:22              0.0.0.0:*
>  LISTEN      28512/sshd
> tcp        0      0 127.0.0.1:953           0.0.0.0:*
>  LISTEN      25075/named
> tcp        0      0 127.0.0.1:2812          0.0.0.0:*
>  LISTEN      9667/monit
> tcp        0      0 127.0.0.1:8000          0.0.0.0:*
>  LISTEN      2019/nginx
> tcp        0      0 127.0.0.1:3306          0.0.0.0:*
>  LISTEN      8998/mysqld
> tcp        0      0 10.224.61.25:2380       0.0.0.0:*
>  LISTEN      12039/etcd
> tcp6       0      0 :::53                   :::*
> LISTEN      25075/named
> tcp6       0      0 :::22                   :::*
> LISTEN      28512/sshd
> tcp6       0      0 ::1:953                 :::*
> LISTEN      25075/named
> tcp6       0      0 :::4000                 :::*
> LISTEN      12039/etcd
> tcp6       0      0 :::80                   :::*
> LISTEN      2019/nginx
> udp        0      0 10.224.61.25:53         0.0.0.0:*
>        25075/named
> udp        0      0 127.0.0.1:53            0.0.0.0:*
>        25075/named
> udp        0      0 0.0.0.0:68              0.0.0.0:*
>        703/dhclient
> udp        0      0 10.224.61.25:123        0.0.0.0:*
>        9178/ntpd
> udp        0      0 127.0.0.1:123           0.0.0.0:*
>        9178/ntpd
> udp        0      0 0.0.0.0:123             0.0.0.0:*
>        9178/ntpd
> udp        0      0 0.0.0.0:49694           0.0.0.0:*
>        703/dhclient
> udp6       0      0 :::29724                :::*
>       703/dhclient
> udp6       0      0 :::53                   :::*
>       25075/named
> udp6       0      0 fe80::a00:27ff:fe11:123 :::*
>       9178/ntpd
> udp6       0      0 ::1:123                 :::*
>       9178/ntpd
> udp6       0      0 :::123                  :::*
>       9178/ntpd
>
> I am also getting 502 Bad Gateway when I trying to connect ellis using
> http://ellis.iind.intel.com
>
> Thanks
> Pushpendra
>
> From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
> Sent: Thursday, March 1, 2018 4:21 AM
> To: Kumar, Pushpendra <pushpendra.kumar at intel.com<mailto:
> pushpendra.kumar at intel.com>>; clearwater at lists.projectclearwater.org
> <mailto:clearwater at lists.projectclearwater.org>
> Subject: RE: Problems in manual installation of clearwater
>
> Pushpendra,
>
> usermod: user ellis is currently used by process 1343
> dpkg: error processing package ellis (--configure):
> subprocess installed post-installation script returned error exit status 8
> Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
> Processing triggers for ureadahead (0.100.0-16) ...
> Errors were encountered while processing:
> ellis
> E: Sub-process /usr/bin/dpkg returned an error code (1)
>
> It sounds like you attempted to install ellis on a node was installed on a
> node in which there already was an ellis user - is that correct?
>
> Ellis requires a user to run the components as for security so that we
> aren't running components as root which don't require root privileges, and
> this is required to be ellis.
>
> dnsmasq: failed to create listening socket for 127.0.0.1: Address already
> in use
> dnsmasq: failed to create listening socket for port 53: Address already in
> use
>
>
> $sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management
> --yes
>
> Errors were encountered while processing:
> ellis
> E: Sub-process /usr/bin/dpkg returned an error code (1)
>
> >From these logs, and the similar  logs regarding bono, sprout and
> clearwater-management it looks like there's already a service running on
> the nodes which is bound to port 53 before you install Clearwater.
>
> We've only regularly tested performing the manual install on a clean
> Ubuntu box, and if I create a new Ubuntu VM (e.g. the basic Ubuntu 14.0.4
> VM ) I don't see anything already running on Port 53 before I install
> Clearwater.
>
> ubuntu at ip-10-0-162-214:~$ sudo netstat -pltun Active Internet connections
> (only servers)
> Proto Recv-Q Send-Q Local Address           Foreign Address         State
>      PID/Program name
> tcp        0      0 0.0.0.0:22              0.0.0.0:*
>  LISTEN      1133/sshd
> tcp6       0      0 :::22                   :::*
> LISTEN      1133/sshd
> udp        0      0 0.0.0.0:5254            0.0.0.0:*
>        582/dhclient
> udp        0      0 0.0.0.0:68              0.0.0.0:*
>        582/dhclient
> udp6       0      0 :::21727                :::*
>       582/dhclient
>
> Can you clarify what image you are attempting to install ellis on where
> you are seeing these errors, and what's already installed on the box? Can
> you run the above netstat command?
>
> For the error:
>
> reload: Job is not running: clearwater-monit
> /usr/share/clearwater/infrastructure/scripts/memcached: line 43:
> /etc/memcached.conf: No such file or directory
> reload: Job is not running: clearwater-monit
>
> Can you provide the complete output of the vellum build log? It'd be
> useful to know in what context this error was output - had something in the
> build already failed? It sounds like it failed to install memcached on the
> vellum node.
>
> Regarding smtp_smarthost - this needs to be an SMTP server which can send
> mail so that Ellis can send password recovery emails. If it's not
> configured then password recovery emails won't work. See the entry in
> http://clearwater.readthedocs.io/en/stable/Clearwater_
> Configuration_Options_Reference.html#core-options for details about the
> SMTP options.
>
> Regarding home_domain, it's described in Clearwater Options reference -
> see http://clearwater.readthedocs.io/en/stable/Clearwater_
> Configuration_Options_Reference.html#core-options  - it needs to be a
> domain which will resolve to the P-CSCFs (e.g. the bono nodes in the
> deployment). It's usually also the root domain for all of the domains used.
>
> iind.intel.com might be a good choice if you can configure DNS entries
> under that domain - you'll need to configure the DNS entries listed in
> http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html in
> this domain
>
> You will need to complete the DNS configuration before Ellis will work -
> it needs a DNS entry to exist in order to communicate with the other nodes
> in the deployment.
>
>
>
> Richard
>
> From: Kumar, Pushpendra [mailto:pushpendra.kumar at intel.com]
> Sent: 28 February 2018 19:00
> To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.
> Whitehouse at metaswitch.com>>
> Subject: Problems in manual installation of clearwater
>
> Hi Richards,
> I need your help in Clearwater project manual installation, Its on high
> priority so please consider that.
>
> I am installing the clearwater usingg manual installation. I have created
> the 6 VMs on virtualbox (using bridge adapter in network setting, used the
> same IP as public_ip and local_ip in local.conf) as I follow
> http://clearwater.readthedocs.io/en/stable/Manual_Install.html. while
> installing I have faced some errors (mentioned below), it will be your
> great help if u guide some solutions for them:
>
> One more thing as I am using bridge adapter in network, I have not did any
> port forwarding as mention in document (I am able to ping vm from one to
> another i.e. they are are communicating)
>
> 1.in installtion of ellis:
>
> $sudo DEBIAN_FRONTEND=noninteractive apt-get install ellis --yes
>
> usermod: user ellis is currently used by process 1343
> dpkg: error processing package ellis (--configure):
> subprocess installed post-installation script returned error exit status 8
> Processing triggers for libc-bin (2.19-0ubuntu6.9) ...
> Processing triggers for ureadahead (0.100.0-16) ...
> Errors were encountered while processing:
> ellis
> E: Sub-process /usr/bin/dpkg returned an error code (1)
>
> Note: First time I install ellis I got this error, then I installed again
> in new node from scratch then also got the same error.
>
>
> dnsmasq: failed to create listening socket for 127.0.0.1: Address already
> in use
> dnsmasq: failed to create listening socket for port 53: Address already in
> use
>
>
> $sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management
> --yes
>
> Errors were encountered while processing:
> ellis
> E: Sub-process /usr/bin/dpkg returned an error code (1)
>
>
>
> 2.in installation of bono:
>
> $sudo DEBIAN_FRONTEND=noninteractive apt-get install bono restund --yes
>
> dnsmasq: failed to create listening socket for 127.0.0.1: Address already
> in use
> dnsmasq: failed to create listening socket for port 53: Address already in
> use
>
>
>
> 3.in installation of sprout:
>
> $sudo DEBIAN_FRONTEND=noninteractive apt-get install sprout --yes
>
> dnsmasq: failed to create listening socket for port 53: Address already in
> use
> dnsmasq: failed to create listening socket for port 53: Address already in
> use
> dnsmasq: failed to create listening socket for 127.0.0.1: Address already
> in use
> dnsmasq: failed to create listening socket for 127.0.0.1: Address already
> in use
>
> $sudo DEBIAN_FRONTEND=noninteractive apt-get install clearwater-management
> --yes
>
> dnsmasq: failed to create listening socket for port 53: Address already in
> use
> dnsmasq: failed to create listening socket for 127.0.0.1: Address already
> in use
>
>
>
> 4.in installtion of homer:
>
> $sudo DEBIAN_FRONTEND=noninteractive apt-get install homer --yes
>
> dnsmasq: failed to create listening socket for port 53: Address already in
> use
>
>
>
>     [fail]
> invoke-rc.d: initscript dnsmasq, action "start" failed.
> dnsmasq: failed to create listening socket for 127.0.0.1: Address already
> in use
>
>
> 5.in installtion of dime:
> $sudo DEBIAN_FRONTEND=noninteractive apt-get install dime
> clearwater-prov-tools --yes
>
>
> dnsmasq: failed to create listening socket for port 53: Address already in
> use
> dnsmasq: failed to create listening socket for 127.0.0.1: Address already
> in use
>
>
>
>
> 6.in installtion of vellum:
>
> * Starting DNS forwarder and DHCP server dnsmasq
> dnsmasq: failed to create listening socket for port 53: Address already in
> use
>
> * Restarting DNS forwarder and DHCP server dnsmasq
> dnsmasq: failed to create listening socket for 127.0.0.1: Address already
> in use
>
>
>
>
>     [fail]
> reload: Job is not running: clearwater-monit
> /usr/share/clearwater/infrastructure/scripts/memcached: line 43:
> /etc/memcached.conf: No such file or directory
> reload: Job is not running: clearwater-monit
>
> -->I ignore above errors and move on to next step i.e. editing
> -->shared_config file and uploading
>
>
> -->shared_config file:
>
> [bono]bono at Bono:/etc/clearwater$ cat shared_config
> #####################################################################
> # No Shared Config has been provided
> # Replace this file with the Shared Configuration for your deployment
> #####################################################################
> # Deployment definitions
> home_domain=iind.intel.com
> sprout_hostname=sprout.iind.intel.com
> sprout_registration_store=vellum.iind.intel.com
> hs_hostname=hs.iind.intel.com:8888
> hs_provisioning_hostname=hs.iind.intel.com:8889
> homestead_impu_store=vellum.iind.intel.com
> ralf_hostname=ralf.iind.intel.com:10888
> ralf_session_store=vellum.iind.intel.com
> xdms_hostname=homer.iind.intel.com:7888
> chronos_hostname=vellum.iind.intel.com
> cassandra_hostname=vellum.iin.intel.com
>
> # Email server configuration
> smtp_smarthost=
> smtp_username=username
> smtp_password=password
> email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=
> clearwater at example.org>
>
> # Keys
> signup_key=secret
> turn_workaround=secret
> ellis_api_key=secret
> ellis_cookie_key=secret
>
>
> ?  What would I use in smtp_smarthost=   (is it localhost?)
>
> ques: Is home_domain =iind.intel.com is right? (when I ping using #ping
> ellis it automatically takes like ellis.iind.intel.com) , it basically
> the intel's domain.
>
>
> -->local_config : (IP ans hostname changed in every node)
>
> local_ip=10.224.61.25
> public_ip=10.224.61.25
> public_hostname=Ellis
> etcd_cluster="10.224.61.20,10.224.61.21,10.224.61.22,10.224.
> 61.25,10.224.61.48,10.224.61.50"
>
>
>
> ?  After that when I am trying to connect to ellis using
> http://ellis.iind.intel.com or http://10.224.61.25 it is giving like 502
> Bad Gateway nginx /1.4.6 (Ubuntu). I have not did any DNS configuration yet
> (because I am using the intel domain or do I need to do it... in which node
> and how).
>
> ?  I will love to hear your response
>
>
> Thanks
> Pushpendra
>
>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.
> projectclearwater.org/attachments/20180305/957ab057/attachment.html>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
> ------------------------------
>
> End of Clearwater Digest, Vol 59, Issue 10
> ******************************************
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180306/e9b4da02/attachment.html>

From wangwulin at hotmail.com  Tue Mar  6 22:35:15 2018
From: wangwulin at hotmail.com (wang wulin)
Date: Wed, 7 Mar 2018 03:35:15 +0000
Subject: [Project Clearwater] [Clearwater] Could not get subscriber data
 from HSS
In-Reply-To: <KL1PR0301MB2120FFFF26D295D0A10CE5A9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>
References: <KL1PR0301MB2120B6181E0CA61719D6BFC9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>,
	<KL1PR0301MB2120FFFF26D295D0A10CE5A9B9C50@KL1PR0301MB2120.apcprd03.prod.outlook.com>
Message-ID: <HK0PR03MB27555AE520E6302A7FBC93FBB9D80@HK0PR03MB2755.apcprd03.prod.outlook.com>

Hi Ben,


Sorry, I did not receive neither Adm's answer earlier nor your answer.  There seems something issue with my hotmail.

1) I deoloyed clearwater via one testcase named "cloudify_ims" from opnfv/functest project, where 3 steps are run:

   * deploy a VNF orchestrator (Cloudify)

   * deploy a Clearwater vIMS (IP Multimedia Subsystem) VNF from this orchestrator based on a TOSCA blueprint defined in [1]

   * run suite of signaling tests on top of this VNF

[1]: https://github.com/Orange-OpenSource/opnfv-cloudify-clearwater/archive/master.zip

8 instances are created and I also created a new instance named "stress_node" according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html
Please see https://hastebin.com/edixewanip.rb for detailed instance info or below
bash-4.3# openstack server list
+--------------------------------------+--------------------------------------------+--------+--------------------------------------------------+----------------------+-----------+
| ID                                   | Name                                       | Status | Networks                                         | Image                | Flavor    |
+--------------------------------------+--------------------------------------------+--------+--------------------------------------------------+----------------------+-----------+
| 50b934e1-131f-4599-a7aa-85d744fda72c | stress_node                                | ACTIVE | cloudify_ims_network=10.67.79.14                 | ubuntu_14.04         | m1.small  |
| bc7da755-b58b-48a5-8258-dda0bdb2b92d | server_clearwater-opnfv_bono_host_zk2kcg   | ACTIVE | cloudify_ims_network=10.67.79.17, 192.168.33.211 | ubuntu_14.04         | m1.small  |
| 3fe2b0bf-4b12-42a1-8be7-a9222c46428f | server_clearwater-opnfv_sprout_host_s6a4tr | ACTIVE | cloudify_ims_network=10.67.79.20                 | ubuntu_14.04         | m1.small  |
| c032dacd-eb8b-48a8-b314-9520f7ff64da | server_clearwater-opnfv_dime_host_kag04s   | ACTIVE | cloudify_ims_network=10.67.79.18                 | ubuntu_14.04         | m1.small  |
| 7d09aa0f-bbeb-4b62-be56-95bfd097def0 | server_clearwater-opnfv_vellum_host_ldyhbh | ACTIVE | cloudify_ims_network=10.67.79.6                  | ubuntu_14.04         | m1.small  |
| 3005c542-0ee5-4430-9f56-f221ccb1f104 | server_clearwater-opnfv_ellis_host_s7cahy  | ACTIVE | cloudify_ims_network=10.67.79.15, 192.168.33.201 | ubuntu_14.04         | m1.small  |
| 04b894ff-1c07-428a-9a8b-72db5335e843 | server_clearwater-opnfv_homer_host_m88cq7  | ACTIVE | cloudify_ims_network=10.67.79.9                  | ubuntu_14.04         | m1.small  |
| 4614747e-2cb7-4450-837e-aa9c51af8e53 | server_clearwater-opnfv_bind_host_643c4w   | ACTIVE | cloudify_ims_network=10.67.79.10, 192.168.33.208 | ubuntu_14.04         | m1.small  |
| 8f26ed2c-8095-43b1-8c40-13ddc080fbc9 | server_clearwater-opnfv_proxy_host_nocslx  | ACTIVE | cloudify_ims_network=10.67.79.5                  | ubuntu_14.04         | m1.small  |
| 7ae3612f-67a5-42c6-ab1b-94dbd067272a | cloudify_manager                           | ACTIVE | cloudify_ims_network=10.67.79.11, 192.168.33.207 | cloudify_manager_4.0 | m1.medium |
+--------------------------------------+--------------------------------------------+--------+--------------------------------------------------+----------------------+-----------+

2) root at dime-5y29tl:/var/log/homestead# cat homestead_current.txt
........
07-03-2018 03:30:03.145 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 21 requests in last 5644ms, minimum threshold for a change is 4495.530273
07-03-2018 03:30:05.674 UTC Status alarm.cpp:62: homestead issued 1501.1 alarm
07-03-2018 03:30:12.148 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 21 requests in last 9005ms, minimum threshold for a change is 7172.617188
07-03-2018 03:30:28.567 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 21 requests in last 16420ms, minimum threshold for a change is 13078.776367
07-03-2018 03:30:33.027 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 21 requests in last 4460ms, minimum threshold for a change is 3552.456787
07-03-2018 03:30:35.674 UTC Status alarm.cpp:62: homestead issued 1501.1 alarm
07-03-2018 03:30:45.640 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 22 requests in last 12613ms, minimum threshold for a change is 10046.443359



3)
root at dime-5y29tl:/var/log/homestead# cat /etc/clearwater/shared_config
# Deployment definitions
home_domain=clearwater.opnfv
sprout_hostname=sprout.clearwater.local
chronos_hostname=10.67.79.10:7253
hs_hostname=hs.clearwater.local:8888
hs_provisioning_hostname=hs-prov.clearwater.local:8889
sprout_impi_store=vellum.clearwater.local
sprout_registration_store=vellum.clearwater.local
cassandra_hostname=vellum.clearwater.local
chronos_hostname=vellum.clearwater.local
ralf_session_store=vellum.clearwater.local
ralf_hostname=ralf.clearwater.local:10888
xdms_hostname=homer.clearwater.local:7888
signaling_dns_server=10.67.79.10

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

4)
root at dime-5y29tl:/var/log/homestead# cat /etc/clearwater/local_config
local_ip=10.67.79.18
public_ip=
public_hostname=dime-5y29tl.clearwater.local
etcd_cluster=10.67.79.10
etcd_cluster_key=bind



5)
root at dime-5y29tl:/var/log/homestead# monit summary
Monit 5.18.1 uptime: 35d 17h 57m
 Service Name                     Status                      Type
 node-dime-5y29tl.clearwater....  Running                     System
 snmpd_process                    Running                     Process
 ralf_process                     Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 ralf_uptime                      Status ok                   Program
 poll_ralf                        Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program

6) It works well when I make a call via XLite? but failed when I run via SIPp, see here: https://hastebin.com/ujilusimik.hs

root at stress-linda:/usr/share/clearwater/sip-stress# nice -n-20 /usr/share/clearwater/bin/sipp -i 10.67.79.16 -sf ./sip-stress.xml 10.67.79.17 -t tn -s clearwater.opnfv -inf ./users.csv.2 -users 50 -m 50 -default_behaviors all,-bye -max_socket 65000 -max_reconnect -1 -reconnect_sleep 0 -reconnect_close 0 -send_timeout 4000 -recv_timeout 12000

------------------------------ Scenario Screen -------- [1-9]: Change Screen --
     Users (length)   Port   Total-time  Total-calls  Remote-host
          50 (0 ms)   5060   40633.25 s           50  10.67.79.17:5060(TCP)

  Call limit reached (-m 50), 0.000 s period  0 ms scheduler resolution
  0 calls (limit 50)                     Peak was 50 calls, after 0 s
  0 Running, 3 Paused, 0 Woken up
  0 dead call msg (discarded)            14 out-of-call msg (discarded)
  1 open sockets

                                 Messages  Retrans   Timeout   Unexpected-Msg
       Pause [0ms/10:00]         50                            0
    REGISTER ---------->         50        0
         401 <----------         50        0         0         0
    REGISTER ---------->         50        0
         200 <----------         50        0         0         0
    REGISTER ---------->         50        0
         401 <----------         50        0         0         0
    REGISTER ---------->         50        0
         200 <----------         50        0         0         0
       Pause [    10.0s]         50                            0
    REGISTER ---------->  B-RTD1 1072      0
         200 <----------  E-RTD1 1072      0         0         0
    REGISTER ---------->  B-RTD1 1072      0
         200 <----------  E-RTD1 1072      0         0         0
       Pause [$reg_pause]        895                           0
       Pause [$pre_call_delay]   177                           0
      INVITE ---------->  B-RTD2 177       0
         100 <----------         176       0         0         0
      INVITE <----------         1         0         0         0
         100 <----------         1         0         0         0
      INVITE <----------         126       0         23        27
         100 ---------->         127       0
         180 ---------->         127       0
         180 <----------         127       0         0         0
       Pause [$call_answer]      127                           0
         200 ---------->         127       0
         200 <----------         127       0         0         0
         ACK ---------->         127       0
         ACK <----------         127       0         0         0
      UPDATE ---------->         127       0
      UPDATE <----------         127       0         0         0
         200 ---------->         127       0
         200 <----------  E-RTD2 127       0         0         0
       Pause [$call_length]      127                           0
         BYE ---------->  B-RTD3 127       0
         BYE <----------         127       0         0         0
         200 ---------->         127       0
         200 <----------  E-RTD3 127       0         0         0
       Pause [$post_call_delay]  127                           0
------------------------------ Test Terminated --------------------------------


----------------------------- Statistics Screen ------- [1-9]: Change Screen --
  Start Time             | 2018-03-06   06:34:20:871    1520318060.871621
  Last Reset Time        | 2018-03-06   17:51:34:128    1520358694.128324
  Current Time           | 2018-03-06   17:51:34:131    1520358694.131933
-------------------------+---------------------------+--------------------------
  Counter Name           | Periodic value            | Cumulative value
-------------------------+---------------------------+--------------------------
  Elapsed Time           | 00:00:00:003              | 11:17:13:260
  Call Rate              |    0.000 cps              |    0.001 cps
-------------------------+---------------------------+--------------------------
  Incoming call created  |        0                  |        0
  OutGoing call created  |        0                  |       50
  Total Call created     |                           |       50
  Current Call           |        0                  |
-------------------------+---------------------------+--------------------------
  Successful call        |        0                  |        0
  Failed call            |        0                  |       50
-------------------------+---------------------------+--------------------------
  Response Time register | 00:00:00:000              | 00:00:00:004
  Response Time call-set | 00:00:00:000              | 00:00:06:032
  Response Time call-tea | 00:00:00:000              | 00:00:00:003
  Call Length            | 00:00:00:000              | 01:49:29:242
------------------------------ Test Terminated --------------------------------


2018-03-06      17:51:34:125    1520358694.125924: Aborting call on unexpected message for Call-Id '35-18929 at 10.67.79.16': while expecting 'INVITE' (index 23), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:23879;rport=23879;received=10.67.79.16;branch=z9hG4bK-2010000030-35-21.000000-1
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:zad374VHyb at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 2010000030-21.000000///35-18929 at 10.67.79.16
From: <sip:2010000030 at clearwater.opnfv>;tag=18929SIPpTag00351234
To: <sip:2010000031 at clearwater.opnfv>;tag=z9hG4bKPjtsB4PSC2qg1mvZI4DWvb3yPmB6pp-3j-
CSeq: 333 INVITE
Content-Length:  0

Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?2? 16:39
???: clearwater at lists.projectclearwater.org
??: ??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,


Here is the result after running ""/usr/share/clearwater/bin/run_stress clearwater.opnfv 16 10 "."


2018-03-02      08:25:12.757782 1519979112.757782: Aborting call on unexpected message for Call-Id '1-21165 at 10.67.79.16': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:54572;received=10.67.79.16;branch=z9hG4bK-21165-1-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:/bDGU121V2 at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 1-21165 at 10.67.79.16
From: <sip:2770000012 at clearwater.opnfv>;tag=21165SIPpTag001
To: <sip:2770000015 at clearwater.opnfv>;tag=z9hG4bKPjtk6L-d0QjzkZB8rbq-CMdJGW9zpHhMbt
CSeq: 1 INVITE
Content-Length:  0


Total calls: 1
Successful calls: 0 (0.0%)
Failed calls: 1 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 5
Successful re-REGISTERs: 5 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 12.0ms


Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?2? 15:38
???: clearwater at lists.projectclearwater.org
??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,


I deployed a stress node according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html, and tried to run stress via "/usr/share/clearwater/bin/run_stress clearwater.opnfv 10 10 ".

"Register" works now, but Call step still failed:
[cid:29209895-4920-4757-a1c0-5756bb653935]
 I got the error from /var/log/sprout: "Error hssconnection.cpp:704: Could not get subscriber data from HSS"
[cid:3b29aefa-363d-4be8-98af-0fff4b0d0c52]

I only executed the 4 commands below on Vellum Node:
1)  . /etc/clearwater/config; for DN in {2770000000..2770000099} ; do echo sip:$DN@$home_domain,$DN at clearwater.opnf<mailto:DN at clearwater.opnf>v,clearwater.opnfv,7kkzTyGW ;done > users.csv
2) cd /usr/share/clearwater/crest-prov/src/metaswitch/crest/tools/ && python bulk_create.py users.csv
3) ./users.create_xdm.sh
4) ./users.create_homestead.sh

Did I miss some other steps?
Do you know if we "HSS" node is also required?


2)

I also got the error from Dime node:


root at dime-5y29tl:/var/log/ralf# vim /var/log/syslog
Mar  2 06:37:01 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument
Mar  2 06:37:02 dime-5y29tl config-manager[10778]: dropped request: 'issue-alarm config-manager 8500.3'
Mar  2 06:37:08 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument
Mar  2 06:37:18 dime-5y29tl issue-alarm: message repeated 12 times: [ zmq_msg_recv: Invalid argument]
Mar  2 06:37:18 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9001.1'
Mar  2 06:37:21 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9002.1'
Mar  2 06:37:22 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument

root at dime-5y29tl:/var/log/ralf# vim /var/log/monit.log
[UTC Mar  2 02:31:43] error    : 'poll_etcd_cluster' '/usr/share/clearwater/bin/poll_etcd_cluster.sh' failed with exit status (1) -- 1
[UTC Mar  2 02:31:43] info     : 'poll_etcd_cluster' exec: /bin/bash
[UTC Mar  2 02:31:53] info     : 'poll_etcd_cluster' status succeeded [status=0] -- zmq_msg_recv: Resource temporarily unavailable


Any help would be much appreciated!



Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180307/67b024b5/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pastedImage.png
Type: image/png
Size: 26935 bytes
Desc: pastedImage.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180307/67b024b5/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pastedImage.png
Type: image/png
Size: 97367 bytes
Desc: pastedImage.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180307/67b024b5/attachment-0001.png>

From Bennett.Allen at metaswitch.com  Fri Mar  9 06:41:57 2018
From: Bennett.Allen at metaswitch.com (Bennett Allen)
Date: Fri, 9 Mar 2018 11:41:57 +0000
Subject: [Project Clearwater] [Clearwater] Could not get subscriber data
 from HSS
Message-ID: <CY4PR02MB2517F39043DEA50069E3B28C9CDE0@CY4PR02MB2517.namprd02.prod.outlook.com>

Hi Linda,
Thank you for all the information. It's interesting that it works with XLite but then not when you use SIPp. We would suggest having a look at the SIP invite messages for both methods (using a packet capture tool such as tcpdump) and seeing if you can spot any differences between them? If you can spot any differences this would give an idea of what needs changing in SIPp.
Let us know how it goes.
Thank you,
Ben



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 07 March 2018 03:36
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 59, Issue 15

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. Re: [Clearwater] Could not get subscriber data from HSS
      (wang wulin)


----------------------------------------------------------------------

Message: 1
Date: Wed, 7 Mar 2018 03:35:15 +0000
From: wang wulin <wangwulin at hotmail.com>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] [Clearwater] Could not get
	subscriber data from HSS
Message-ID:
	<HK0PR03MB27555AE520E6302A7FBC93FBB9D80 at HK0PR03MB2755.apcprd03.prod.outlook.com>
	
Content-Type: text/plain; charset="gb2312"

Hi Ben,


Sorry, I did not receive neither Adm's answer earlier nor your answer.  There seems something issue with my hotmail.

1) I deoloyed clearwater via one testcase named "cloudify_ims" from opnfv/functest project, where 3 steps are run:

   * deploy a VNF orchestrator (Cloudify)

   * deploy a Clearwater vIMS (IP Multimedia Subsystem) VNF from this orchestrator based on a TOSCA blueprint defined in [1]

   * run suite of signaling tests on top of this VNF

[1]: https://github.com/Orange-OpenSource/opnfv-cloudify-clearwater/archive/master.zip

8 instances are created and I also created a new instance named "stress_node" according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html
Please see https://hastebin.com/edixewanip.rb for detailed instance info or below bash-4.3# openstack server list
+--------------------------------------+--------------------------------------------+--------+--------------------------------------------------+----------------------+-----------+
| ID                                   | Name                                       | Status | Networks                                         | Image                | Flavor    |
+--------------------------------------+--------------------------------------------+--------+--------------------------------------------------+----------------------+-----------+
| 50b934e1-131f-4599-a7aa-85d744fda72c | stress_node                                | ACTIVE | cloudify_ims_network=10.67.79.14                 | ubuntu_14.04         | m1.small  |
| bc7da755-b58b-48a5-8258-dda0bdb2b92d | server_clearwater-opnfv_bono_host_zk2kcg   | ACTIVE | cloudify_ims_network=10.67.79.17, 192.168.33.211 | ubuntu_14.04         | m1.small  |
| 3fe2b0bf-4b12-42a1-8be7-a9222c46428f | server_clearwater-opnfv_sprout_host_s6a4tr | ACTIVE | cloudify_ims_network=10.67.79.20                 | ubuntu_14.04         | m1.small  |
| c032dacd-eb8b-48a8-b314-9520f7ff64da | server_clearwater-opnfv_dime_host_kag04s   | ACTIVE | cloudify_ims_network=10.67.79.18                 | ubuntu_14.04         | m1.small  |
| 7d09aa0f-bbeb-4b62-be56-95bfd097def0 | server_clearwater-opnfv_vellum_host_ldyhbh | ACTIVE | cloudify_ims_network=10.67.79.6                  | ubuntu_14.04         | m1.small  |
| 3005c542-0ee5-4430-9f56-f221ccb1f104 | server_clearwater-opnfv_ellis_host_s7cahy  | ACTIVE | cloudify_ims_network=10.67.79.15, 192.168.33.201 | ubuntu_14.04         | m1.small  |
| 04b894ff-1c07-428a-9a8b-72db5335e843 | server_clearwater-opnfv_homer_host_m88cq7  | ACTIVE | cloudify_ims_network=10.67.79.9                  | ubuntu_14.04         | m1.small  |
| 4614747e-2cb7-4450-837e-aa9c51af8e53 | server_clearwater-opnfv_bind_host_643c4w   | ACTIVE | cloudify_ims_network=10.67.79.10, 192.168.33.208 | ubuntu_14.04         | m1.small  |
| 8f26ed2c-8095-43b1-8c40-13ddc080fbc9 | server_clearwater-opnfv_proxy_host_nocslx  | ACTIVE | cloudify_ims_network=10.67.79.5                  | ubuntu_14.04         | m1.small  |
| 7ae3612f-67a5-42c6-ab1b-94dbd067272a | cloudify_manager                           | ACTIVE | cloudify_ims_network=10.67.79.11, 192.168.33.207 | cloudify_manager_4.0 | m1.medium |
+--------------------------------------+--------------------------------------------+--------+--------------------------------------------------+----------------------+-----------+

2) root at dime-5y29tl:/var/log/homestead# cat homestead_current.txt ........
07-03-2018 03:30:03.145 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 21 requests in last 5644ms, minimum threshold for a change is 4495.530273
07-03-2018 03:30:05.674 UTC Status alarm.cpp:62: homestead issued 1501.1 alarm
07-03-2018 03:30:12.148 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 21 requests in last 9005ms, minimum threshold for a change is 7172.617188
07-03-2018 03:30:28.567 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 21 requests in last 16420ms, minimum threshold for a change is 13078.776367
07-03-2018 03:30:33.027 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 21 requests in last 4460ms, minimum threshold for a change is 3552.456787
07-03-2018 03:30:35.674 UTC Status alarm.cpp:62: homestead issued 1501.1 alarm
07-03-2018 03:30:45.640 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 22 requests in last 12613ms, minimum threshold for a change is 10046.443359



3)
root at dime-5y29tl:/var/log/homestead# cat /etc/clearwater/shared_config # Deployment definitions home_domain=clearwater.opnfv sprout_hostname=sprout.clearwater.local
chronos_hostname=10.67.79.10:7253
hs_hostname=hs.clearwater.local:8888
hs_provisioning_hostname=hs-prov.clearwater.local:8889
sprout_impi_store=vellum.clearwater.local
sprout_registration_store=vellum.clearwater.local
cassandra_hostname=vellum.clearwater.local
chronos_hostname=vellum.clearwater.local
ralf_session_store=vellum.clearwater.local
ralf_hostname=ralf.clearwater.local:10888
xdms_hostname=homer.clearwater.local:7888
signaling_dns_server=10.67.79.10

# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret

4)
root at dime-5y29tl:/var/log/homestead# cat /etc/clearwater/local_config
local_ip=10.67.79.18
public_ip=
public_hostname=dime-5y29tl.clearwater.local
etcd_cluster=10.67.79.10
etcd_cluster_key=bind



5)
root at dime-5y29tl:/var/log/homestead# monit summary Monit 5.18.1 uptime: 35d 17h 57m
 Service Name                     Status                      Type
 node-dime-5y29tl.clearwater....  Running                     System
 snmpd_process                    Running                     Process
 ralf_process                     Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 ralf_uptime                      Status ok                   Program
 poll_ralf                        Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program

6) It works well when I make a call via XLite? but failed when I run via SIPp, see here: https://hastebin.com/ujilusimik.hs

root at stress-linda:/usr/share/clearwater/sip-stress# nice -n-20 /usr/share/clearwater/bin/sipp -i 10.67.79.16 -sf ./sip-stress.xml 10.67.79.17 -t tn -s clearwater.opnfv -inf ./users.csv.2 -users 50 -m 50 -default_behaviors all,-bye -max_socket 65000 -max_reconnect -1 -reconnect_sleep 0 -reconnect_close 0 -send_timeout 4000 -recv_timeout 12000

------------------------------ Scenario Screen -------- [1-9]: Change Screen --
     Users (length)   Port   Total-time  Total-calls  Remote-host
          50 (0 ms)   5060   40633.25 s           50  10.67.79.17:5060(TCP)

  Call limit reached (-m 50), 0.000 s period  0 ms scheduler resolution
  0 calls (limit 50)                     Peak was 50 calls, after 0 s
  0 Running, 3 Paused, 0 Woken up
  0 dead call msg (discarded)            14 out-of-call msg (discarded)
  1 open sockets

                                 Messages  Retrans   Timeout   Unexpected-Msg
       Pause [0ms/10:00]         50                            0
    REGISTER ---------->         50        0
         401 <----------         50        0         0         0
    REGISTER ---------->         50        0
         200 <----------         50        0         0         0
    REGISTER ---------->         50        0
         401 <----------         50        0         0         0
    REGISTER ---------->         50        0
         200 <----------         50        0         0         0
       Pause [    10.0s]         50                            0
    REGISTER ---------->  B-RTD1 1072      0
         200 <----------  E-RTD1 1072      0         0         0
    REGISTER ---------->  B-RTD1 1072      0
         200 <----------  E-RTD1 1072      0         0         0
       Pause [$reg_pause]        895                           0
       Pause [$pre_call_delay]   177                           0
      INVITE ---------->  B-RTD2 177       0
         100 <----------         176       0         0         0
      INVITE <----------         1         0         0         0
         100 <----------         1         0         0         0
      INVITE <----------         126       0         23        27
         100 ---------->         127       0
         180 ---------->         127       0
         180 <----------         127       0         0         0
       Pause [$call_answer]      127                           0
         200 ---------->         127       0
         200 <----------         127       0         0         0
         ACK ---------->         127       0
         ACK <----------         127       0         0         0
      UPDATE ---------->         127       0
      UPDATE <----------         127       0         0         0
         200 ---------->         127       0
         200 <----------  E-RTD2 127       0         0         0
       Pause [$call_length]      127                           0
         BYE ---------->  B-RTD3 127       0
         BYE <----------         127       0         0         0
         200 ---------->         127       0
         200 <----------  E-RTD3 127       0         0         0
       Pause [$post_call_delay]  127                           0
------------------------------ Test Terminated --------------------------------


----------------------------- Statistics Screen ------- [1-9]: Change Screen --
  Start Time             | 2018-03-06   06:34:20:871    1520318060.871621
  Last Reset Time        | 2018-03-06   17:51:34:128    1520358694.128324
  Current Time           | 2018-03-06   17:51:34:131    1520358694.131933
-------------------------+---------------------------+------------------
-------------------------+---------------------------+--------
  Counter Name           | Periodic value            | Cumulative value
-------------------------+---------------------------+------------------
-------------------------+---------------------------+--------
  Elapsed Time           | 00:00:00:003              | 11:17:13:260
  Call Rate              |    0.000 cps              |    0.001 cps
-------------------------+---------------------------+------------------
-------------------------+---------------------------+--------
  Incoming call created  |        0                  |        0
  OutGoing call created  |        0                  |       50
  Total Call created     |                           |       50
  Current Call           |        0                  |
-------------------------+---------------------------+------------------
-------------------------+---------------------------+--------
  Successful call        |        0                  |        0
  Failed call            |        0                  |       50
-------------------------+---------------------------+------------------
-------------------------+---------------------------+--------
  Response Time register | 00:00:00:000              | 00:00:00:004
  Response Time call-set | 00:00:00:000              | 00:00:06:032
  Response Time call-tea | 00:00:00:000              | 00:00:00:003
  Call Length            | 00:00:00:000              | 01:49:29:242
------------------------------ Test Terminated --------------------------------


2018-03-06      17:51:34:125    1520358694.125924: Aborting call on unexpected message for Call-Id '35-18929 at 10.67.79.16': while expecting 'INVITE' (index 23), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:23879;rport=23879;received=10.67.79.16;branch=z9hG4bK-2010000030-35-21.000000-1
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:zad374VHyb at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 2010000030-21.000000///35-18929 at 10.67.79.16
From: <sip:2010000030 at clearwater.opnfv>;tag=18929SIPpTag00351234
To: <sip:2010000031 at clearwater.opnfv>;tag=z9hG4bKPjtsB4PSC2qg1mvZI4DWvb3yPmB6pp-3j-
CSeq: 333 INVITE
Content-Length:  0

Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?2? 16:39
???: clearwater at lists.projectclearwater.org
??: ??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,


Here is the result after running ""/usr/share/clearwater/bin/run_stress clearwater.opnfv 16 10 "."


2018-03-02      08:25:12.757782 1519979112.757782: Aborting call on unexpected message for Call-Id '1-21165 at 10.67.79.16': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.16:54572;received=10.67.79.16;branch=z9hG4bK-21165-1-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Record-Route: <sip:10.67.79.17:5058;transport=TCP;lr>
Record-Route: <sip:/bDGU121V2 at bono-i2sn7d.clearwater.local:5060;transport=TCP;lr>
Call-ID: 1-21165 at 10.67.79.16
From: <sip:2770000012 at clearwater.opnfv>;tag=21165SIPpTag001
To: <sip:2770000015 at clearwater.opnfv>;tag=z9hG4bKPjtk6L-d0QjzkZB8rbq-CMdJGW9zpHhMbt
CSeq: 1 INVITE
Content-Length:  0


Total calls: 1
Successful calls: 0 (0.0%)
Failed calls: 1 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%) # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 5
Successful re-REGISTERs: 5 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 12.0ms


Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com>
????: 2018?3?2? 15:38
???: clearwater at lists.projectclearwater.org
??: [Clearwater] Could not get subscriber data from HSS


Hi Clearwater Team,


I deployed a stress node according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html, and tried to run stress via "/usr/share/clearwater/bin/run_stress clearwater.opnfv 10 10 ".

"Register" works now, but Call step still failed:
[cid:29209895-4920-4757-a1c0-5756bb653935]
 I got the error from /var/log/sprout: "Error hssconnection.cpp:704: Could not get subscriber data from HSS"
[cid:3b29aefa-363d-4be8-98af-0fff4b0d0c52]

I only executed the 4 commands below on Vellum Node:
1)  . /etc/clearwater/config; for DN in {2770000000..2770000099} ; do echo sip:$DN@$home_domain,$DN at clearwater.opnf<mailto:DN at clearwater.opnf>v,clearwater.opnfv,7kkzTyGW ;done > users.csv
2) cd /usr/share/clearwater/crest-prov/src/metaswitch/crest/tools/ && python bulk_create.py users.csv
3) ./users.create_xdm.sh
4) ./users.create_homestead.sh

Did I miss some other steps?
Do you know if we "HSS" node is also required?


2)

I also got the error from Dime node:


root at dime-5y29tl:/var/log/ralf# vim /var/log/syslog Mar  2 06:37:01 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument Mar  2 06:37:02 dime-5y29tl config-manager[10778]: dropped request: 'issue-alarm config-manager 8500.3'
Mar  2 06:37:08 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument Mar  2 06:37:18 dime-5y29tl issue-alarm: message repeated 12 times: [ zmq_msg_recv: Invalid argument] Mar  2 06:37:18 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9001.1'
Mar  2 06:37:21 dime-5y29tl queue-manager[10648]: dropped request: 'issue-alarm queue-manager 9002.1'
Mar  2 06:37:22 dime-5y29tl issue-alarm: zmq_msg_recv: Invalid argument

root at dime-5y29tl:/var/log/ralf# vim /var/log/monit.log
[UTC Mar  2 02:31:43] error    : 'poll_etcd_cluster' '/usr/share/clearwater/bin/poll_etcd_cluster.sh' failed with exit status (1) -- 1
[UTC Mar  2 02:31:43] info     : 'poll_etcd_cluster' exec: /bin/bash
[UTC Mar  2 02:31:53] info     : 'poll_etcd_cluster' status succeeded [status=0] -- zmq_msg_recv: Resource temporarily unavailable


Any help would be much appreciated!



Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180307/67b024b5/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pastedImage.png
Type: image/png
Size: 26935 bytes
Desc: pastedImage.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180307/67b024b5/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pastedImage.png
Type: image/png
Size: 97367 bytes
Desc: pastedImage.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180307/67b024b5/attachment-0001.png>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 59, Issue 15
******************************************



From anthonynlee at gmail.com  Fri Mar  9 22:54:10 2018
From: anthonynlee at gmail.com (Anthony Lee)
Date: Fri, 9 Mar 2018 22:54:10 -0500
Subject: [Project Clearwater] Does Clearwater support to provision
 wildcarded PSI?
Message-ID: <CA+pBo5GQ2N6A8pMr7FfeH-9dvGYR6S967fZwSHWq5eOuO_5rzA@mail.gmail.com>

I need to provision wildcarded PSI, does clearwater support to do that?
If yes, could you provide an example?

Thanks

Anthony
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180309/bc37d731/attachment.html>

From anthonynlee at gmail.com  Sat Mar 10 10:44:54 2018
From: anthonynlee at gmail.com (Anthony Lee)
Date: Sat, 10 Mar 2018 10:44:54 -0500
Subject: [Project Clearwater] Does Clearwater support to provision
 wildcarded PSI?
In-Reply-To: <CA+pBo5GQ2N6A8pMr7FfeH-9dvGYR6S967fZwSHWq5eOuO_5rzA@mail.gmail.com>
References: <CA+pBo5GQ2N6A8pMr7FfeH-9dvGYR6S967fZwSHWq5eOuO_5rzA@mail.gmail.com>
Message-ID: <CA+pBo5E+qago6L-TOU8Z60LfgBvXApZpCWCLvfehM9YvXAq_Dw@mail.gmail.com>

My wildcard PSI is  sip:.+ at list-service.example.com


I found some information in Homestead_current.txt:

Debug http_handlers.cpp:695: Did not receive valid JSON with a
'wildcard_identity' element

Does it mean scscf need to add "wildcard_identity" into the JSON request?
Is there any configuration need to be done in Sprout?



On Fri, Mar 9, 2018 at 10:54 PM, Anthony Lee <anthonynlee at gmail.com> wrote:

> I need to provision wildcarded PSI, does clearwater support to do that?
> If yes, could you provide an example?
>
> Thanks
>
> Anthony
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180310/73cf3f3a/attachment.html>

From anthonynlee at gmail.com  Sat Mar 10 10:48:14 2018
From: anthonynlee at gmail.com (Anthony Lee)
Date: Sat, 10 Mar 2018 10:48:14 -0500
Subject: [Project Clearwater] Does Clearwater support to provision
 wildcarded PSI?
In-Reply-To: <CA+pBo5E+qago6L-TOU8Z60LfgBvXApZpCWCLvfehM9YvXAq_Dw@mail.gmail.com>
References: <CA+pBo5GQ2N6A8pMr7FfeH-9dvGYR6S967fZwSHWq5eOuO_5rzA@mail.gmail.com>
	<CA+pBo5E+qago6L-TOU8Z60LfgBvXApZpCWCLvfehM9YvXAq_Dw@mail.gmail.com>
Message-ID: <CA+pBo5FdjOaTxVManL+QAQYZ4OS3pHbVSVHUT7kXxEBph6=YUQ@mail.gmail.com>

Sorry, forgot to mention that  Sprout's log  shows that 404 not found for
that wildcard PSI return from Homestead. .

On Sat, Mar 10, 2018 at 10:44 AM, Anthony Lee <anthonynlee at gmail.com> wrote:

> My wildcard PSI is  sip:.+ at list-service.example.com
>
>
> I found some information in Homestead_current.txt:
>
> Debug http_handlers.cpp:695: Did not receive valid JSON with a
> 'wildcard_identity' element
>
> Does it mean scscf need to add "wildcard_identity" into the JSON request?
> Is there any configuration need to be done in Sprout?
>
>
>
> On Fri, Mar 9, 2018 at 10:54 PM, Anthony Lee <anthonynlee at gmail.com>
> wrote:
>
>> I need to provision wildcarded PSI, does clearwater support to do that?
>> If yes, could you provide an example?
>>
>> Thanks
>>
>> Anthony
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180310/73fe17cf/attachment.html>

From anthonynlee at gmail.com  Sat Mar 10 15:02:12 2018
From: anthonynlee at gmail.com (Anthony Lee)
Date: Sat, 10 Mar 2018 15:02:12 -0500
Subject: [Project Clearwater] Does Clearwater support to provision
 wildcarded PSI?
In-Reply-To: <CA+pBo5FdjOaTxVManL+QAQYZ4OS3pHbVSVHUT7kXxEBph6=YUQ@mail.gmail.com>
References: <CA+pBo5GQ2N6A8pMr7FfeH-9dvGYR6S967fZwSHWq5eOuO_5rzA@mail.gmail.com>
	<CA+pBo5E+qago6L-TOU8Z60LfgBvXApZpCWCLvfehM9YvXAq_Dw@mail.gmail.com>
	<CA+pBo5FdjOaTxVManL+QAQYZ4OS3pHbVSVHUT7kXxEBph6=YUQ@mail.gmail.com>
Message-ID: <CA+pBo5EgQubf=DPzZxaoBJG4p7JikhzsjeE4a2pJ_mdoS4B+WA@mail.gmail.com>

Now I added P-Profile-Key header into the request, the header value is my
wildcard PSI uri( sip:.+ at list-service.example.com ).

Homestead  log:
10-03-2018 19:39:50.073 UTC [7f22987f8700] Debug http_handlers.cpp:619:
Determining request type from '{"reqtype": "call", "server_name":
"sip:scscf.192.168.1.10:5054;transport=TCP", "wildcard_identity": "
sip:.+ at list-service.example.com"}'

10-03-2018 19:39:50.074 UTC [7f22c4fb1700] Debug memcached_cache.cpp:27:
Pausing stopwatch due to Memcached GET fetch result for
impu\\sip:.+ at list-service.example.com
10-03-2018 19:39:50.075 UTC [7f22c4fb1700] Debug memcached_cache.cpp:33:
Resuming stopwatch due to Memcached GET fetch result for impu\\
sip:.+ at list-service.example.com

10-03-2018 19:39:50.075 UTC [7f22c4fb1700] Debug memcachedstore.cpp:414:
Key not found

10-03-2018 19:39:50.075 UTC [7f229affd700] Debug hsprov_store.cpp:78:
Issuing get for key
sip:7gbcen27b7lqvh3noa3nt8nftc7gbcen27b7lqvh3noa3nt8nftc at list-service.example.com

10-03-2018 19:39:50.078 UTC [7f229affd700] Debug cassandra_store.cpp:536:
Cassandra request failed: rc=2, Row
sip:7gbcen27b7lqvh3noa3nt8nftc7gbcen27b7lqvh3noa3nt8nftc at list-service.
example.com not present in column_family impu


Looks like Homestead uses P-Profile-Key's value to search in cache but uses
R-URI to search in database.

Is it expected behavior?








On Sat, Mar 10, 2018 at 10:48 AM, Anthony Lee <anthonynlee at gmail.com> wrote:

> Sorry, forgot to mention that  Sprout's log  shows that 404 not found for
> that wildcard PSI return from Homestead. .
>
> On Sat, Mar 10, 2018 at 10:44 AM, Anthony Lee <anthonynlee at gmail.com>
> wrote:
>
>> My wildcard PSI is  sip:.+ at list-service.example.com
>>
>>
>> I found some information in Homestead_current.txt:
>>
>> Debug http_handlers.cpp:695: Did not receive valid JSON with a
>> 'wildcard_identity' element
>>
>> Does it mean scscf need to add "wildcard_identity" into the JSON request?
>> Is there any configuration need to be done in Sprout?
>>
>>
>>
>> On Fri, Mar 9, 2018 at 10:54 PM, Anthony Lee <anthonynlee at gmail.com>
>> wrote:
>>
>>> I need to provision wildcarded PSI, does clearwater support to do that?
>>> If yes, could you provide an example?
>>>
>>> Thanks
>>>
>>> Anthony
>>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180310/ededaeea/attachment.html>

From msc.jaber at gmail.com  Sat Mar 10 09:18:43 2018
From: msc.jaber at gmail.com (jaber daneshamooz)
Date: Sat, 10 Mar 2018 17:48:43 +0330
Subject: [Project Clearwater] SIP registeration cw-aio virtualbox
Message-ID: <CABJGTs5Ki1Q8eQ5V1kXyURO8hqAAHCtgyi0r6vDeMu2K2fpOVw@mail.gmail.com>

Hi. tanx for your help. I could overcome the problem by changing the
advanced setting of client application(zoiper)




-- 
* Jaber Daneshamooz  *
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180310/88192924/attachment.html>

From Bennett.Allen at metaswitch.com  Tue Mar 13 13:17:47 2018
From: Bennett.Allen at metaswitch.com (Bennett Allen)
Date: Tue, 13 Mar 2018 17:17:47 +0000
Subject: [Project Clearwater] Does Clearwater support to provision
 wildcarded PSI?
In-Reply-To: <CA+pBo5EgQubf=DPzZxaoBJG4p7JikhzsjeE4a2pJ_mdoS4B+WA@mail.gmail.com>
References: <CA+pBo5GQ2N6A8pMr7FfeH-9dvGYR6S967fZwSHWq5eOuO_5rzA@mail.gmail.com>
	<CA+pBo5E+qago6L-TOU8Z60LfgBvXApZpCWCLvfehM9YvXAq_Dw@mail.gmail.com>
	<CA+pBo5FdjOaTxVManL+QAQYZ4OS3pHbVSVHUT7kXxEBph6=YUQ@mail.gmail.com>
	<CA+pBo5EgQubf=DPzZxaoBJG4p7JikhzsjeE4a2pJ_mdoS4B+WA@mail.gmail.com>
Message-ID: <CY4PR02MB2517ADB82CBAD0472608BAFB9CD20@CY4PR02MB2517.namprd02.prod.outlook.com>

Hi Anthony,
From the logs you have sent I see that you are using homestead-prov, we only support wildcarded entries with a HSS, which would explain the issues you are hitting. Here is a link to the docs about a HSS - http://clearwater.readthedocs.io/en/stable/External_HSS_Integration.html .
Let us know how it goes,
Ben


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Anthony Lee
Sent: 10 March 2018 20:02
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Does Clearwater support to provision wildcarded PSI?

Now I added P-Profile-Key header into the request, the header value is my wildcard PSI uri( sip:.+ at list-service.example.com<mailto:sip%3A.%2B at list-service.example.com> ).

Homestead  log:
10-03-2018 19:39:50.073 UTC [7f22987f8700] Debug http_handlers.cpp:619: Determining request type from '{"reqtype": "call", "server_name": "sip:scscf.192.168.1.10:5054;transport=TCP", "wildcard_identity": "sip:.+ at list-service.example.com<mailto:sip%3A.%2B at list-service.example.com>"}'

10-03-2018 19:39:50.074 UTC [7f22c4fb1700] Debug memcached_cache.cpp:27: Pausing stopwatch due to Memcached GET fetch result for impu\\sip:.+ at list-service.example.com<http://example.com>
10-03-2018 19:39:50.075 UTC [7f22c4fb1700] Debug memcached_cache.cpp:33: Resuming stopwatch due to Memcached GET fetch result for impu\\sip:.+ at list-service.example.com<mailto:sip%3A.%2B at list-service.example.com>

10-03-2018 19:39:50.075 UTC [7f22c4fb1700] Debug memcachedstore.cpp:414: Key not found

10-03-2018 19:39:50.075 UTC [7f229affd700] Debug hsprov_store.cpp:78: Issuing get for key sip:7gbcen27b7lqvh3noa3nt8nftc7gbcen27b7lqvh3noa3nt8nftc at list-service.example.com<mailto:sip%3A7gbcen27b7lqvh3noa3nt8nftc7gbcen27b7lqvh3noa3nt8nftc at list-service.example.com>

10-03-2018 19:39:50.078 UTC [7f229affd700] Debug cassandra_store.cpp:536: Cassandra request failed: rc=2, Row sip:7gbcen27b7lqvh3noa3nt8nftc7gbcen27b7lqvh3noa3nt8nftc at list-service.example.com<http://example.com> not present in column_family impu


Looks like Homestead uses P-Profile-Key's value to search in cache but uses R-URI to search in database.

Is it expected behavior?








On Sat, Mar 10, 2018 at 10:48 AM, Anthony Lee <anthonynlee at gmail.com<mailto:anthonynlee at gmail.com>> wrote:
Sorry, forgot to mention that  Sprout's log  shows that 404 not found for that wildcard PSI return from Homestead. .

On Sat, Mar 10, 2018 at 10:44 AM, Anthony Lee <anthonynlee at gmail.com<mailto:anthonynlee at gmail.com>> wrote:
My wildcard PSI is  sip:.+ at list-service.example.com<mailto:sip%3A.%2B at list-service.example.com>


I found some information in Homestead_current.txt:

Debug http_handlers.cpp:695: Did not receive valid JSON with a 'wildcard_identity' element

Does it mean scscf need to add "wildcard_identity" into the JSON request?
Is there any configuration need to be done in Sprout?



On Fri, Mar 9, 2018 at 10:54 PM, Anthony Lee <anthonynlee at gmail.com<mailto:anthonynlee at gmail.com>> wrote:
I need to provision wildcarded PSI, does clearwater support to do that?
If yes, could you provide an example?

Thanks

Anthony



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180313/16db0230/attachment.html>

From Bennett.Allen at metaswitch.com  Tue Mar 13 13:20:33 2018
From: Bennett.Allen at metaswitch.com (Bennett Allen)
Date: Tue, 13 Mar 2018 17:20:33 +0000
Subject: [Project Clearwater] Failed to register to client (Zoiper5 and
 XLite)
In-Reply-To: <CAPqjCcEF3k_OAM+r36MBdzb0WwAEjuu04YMWnDi8JjBMSee0WQ@mail.gmail.com>
References: <CAPqjCcEF3k_OAM+r36MBdzb0WwAEjuu04YMWnDi8JjBMSee0WQ@mail.gmail.com>
Message-ID: <CY4PR02MB2517D7714D64E18B4A5679519CD20@CY4PR02MB2517.namprd02.prod.outlook.com>

Hi Pushpendra,
Firstly I have CC?d this onto the clearwater mailing list and please send all replies via this as otherwise your emails may not be seen by my colleagues or me!
In answer to your questions I don?t think what the ns value is matters so what you?ve got should be fine, and the second value using your bind IP looks good. It also doesn?t look like the DNS server/queries is the problem in your situation, as homestead is trying to contact the vellum node and failing there.
If you have a look at the logs on the vellum node for Cassandra, Astaire and Rogers, these are at /var/log/Cassandra/, /var/log/Astaire and /var/log/rogers. This should give a clue as to what is happening, as based on the monit.log they don?t look like they?re working properly.
Let us know how it goes,
Ben


From: Pushpendra [mailto:pushpendra16mnnit at gmail.com]
Sent: 09 March 2018 11:06
To: Bennett Allen <Bennett.Allen at metaswitch.com>
Subject: Failed to register to client (Zoiper5 and XLite)

Hi Ben,
I am able to login and create private identity on ellis, but when I am trying to register these identity on client app (zoiper5 an XLite), its not registered. Its in an endless loop. I am using <zone> as domain while registering to client. Please find the log detail below:
Please let me what is wrong and how can I fix, thanks in advance ?

[vellum]ubuntu at vellum:/var/log$ tail -40 monit.log
[IST Mar  7 23:54:05] error    : 'cassandra_process' process is not running
[IST Mar  7 23:54:05] info     : 'cassandra_process' trying to restart
[IST Mar  7 23:54:05] info     : 'cassandra_process' restart: /bin/bash
[IST Mar  7 23:54:06] info     : 'cassandra_process' process is running with pid 31746
[IST Mar  7 23:54:06] error    : 'cassandra_uptime' '/usr/share/clearwater/infrastructure/monit_uptime/check-cassandra-uptime' failed with exit status (1) -- no output
[IST Mar  7 23:54:06] error    : 'astaire_uptime' '/usr/share/clearwater/infrastructure/monit_uptime/check-astaire-uptime' failed with exit status (1) -- no output
[IST Mar  7 23:54:16] error    : 'rogers_process' process is not running
[IST Mar  7 23:54:16] info     : 'rogers_process' trying to restart
[IST Mar  7 23:54:16] info     : 'rogers_process' restart: /bin/bash
[IST Mar  7 23:54:16] info     : 'rogers_process' process is running with pid 31827
[IST Mar  7 23:54:16] error    : 'clearwater_cluster_manager_process' process is not running
[IST Mar  7 23:54:16] info     : 'clearwater_cluster_manager_process' trying to restart
[IST Mar  7 23:54:16] info     : 'clearwater_cluster_manager_process' restart: /bin/bash
[IST Mar  7 23:54:17] error    : 'cassandra_process' process is not running
[IST Mar  7 23:54:17] info     : 'cassandra_process' trying to restart
[IST Mar  7 23:54:17] info     : 'cassandra_process' restart: /bin/bash
[IST Mar  7 23:54:17] info     : 'cassandra_process' process is running with pid 32022
[IST Mar  7 23:54:17] error    : 'cassandra_uptime' '/usr/share/clearwater/infrastructure/monit_uptime/check-cassandra-uptime' failed with exit status (1) -- no output
[IST Mar  7 23:54:17] error    : 'astaire_uptime' '/usr/share/clearwater/infrastructure/monit_uptime/check-astaire-uptime' failed with exit status (1) -- no output
[IST Mar  7 23:54:27] error    : 'rogers_uptime' '/usr/share/clearwater/infrastructure/monit_uptime/check-rogers-uptime' failed with exit status (1) -- no output
[IST Mar  7 23:54:27] info     : 'clearwater_cluster_manager_process' process is running with pid 31936
[IST Mar  7 23:54:27] error    : 'cassandra_process' process is not running
[IST Mar  7 23:54:27] info     : 'cassandra_process' trying to restart
[IST Mar  7 23:54:27] info     : 'cassandra_process' restart: /bin/bash
[IST Mar  7 23:54:28] info     : 'cassandra_process' process is running with pid 32610
[IST Mar  7 23:54:28] error    : 'cassandra_uptime' '/usr/share/clearwater/infrastructure/monit_uptime/check-cassandra-uptime' failed with exit status (1) -- no output
[IST Mar  7 23:54:28] error    : 'astaire_uptime' '/usr/share/clearwater/infrastructure/monit_uptime/check-astaire-uptime' failed with exit status (1) -- no output
[IST Mar  7 23:54:38] error    : 'rogers_uptime' '/usr/share/clearwater/infrastructure/monit_uptime/check-rogers-uptime' failed with exit status (1) -- no output
[IST Mar  7 23:54:38] error    : 'cassandra_uptime' '/usr/share/clearwater/infrastructure/monit_uptime/check-cassandra-uptime' failed with exit status (1) -- no output
[IST Mar  7 23:54:39] info     : 'astaire_uptime' status succeeded [status=0] -- zmq_msg_recv: Resource temporarily unavailable
[IST Mar  7 23:54:46] info     : Awakened by the SIGHUP signal
Reinitializing Monit - Control file '/etc/monit/monitrc'
[IST Mar  7 23:54:47] info     : 'node-vellum' Monit reloaded
[IST Mar  7 23:54:47] error    : Cannot create socket to [localhost]:2812 -- Connection refused
[IST Mar  7 23:54:57] info     : Awakened by the SIGHUP signal
Reinitializing Monit - Control file '/etc/monit/monitrc'
[IST Mar  7 23:54:57] info     : 'node-vellum' Monit reloaded
[IST Mar  7 23:54:57] error    : Cannot create socket to [localhost]:2812 -- Connection refused
[IST Mar  7 23:55:07] error    : 'cassandra_uptime' '/usr/share/clearwater/infrastructure/monit_uptime/check-cassandra-uptime' failed with exit status (1) -- no output
[IST Mar  7 23:55:17] info     : 'cassandra_uptime' status succeeded [status=0] -- zmq_msg_recv: Resource temporarily unavailable



[dime]ubuntu at dime:/var/log/homestead$ tail -30 homestead_err.log
Thrift: Wed Mar  7 23:54:03 2018 TSocket::open() error on socket (after THRIFT_POLL) <Host: 10.224.61.24 Port: 9160>Connection refused
Thrift: Wed Mar  7 23:54:03 2018 TSocket::open() error on socket (after THRIFT_POLL) <Host: 10.224.61.24 Port: 9160>Connection refused


[dime]ubuntu at dime:/var/log/ralf$ tail -40 ralf_current.txt
08-03-2018 14:00:02.739 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:00:07.740 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:00:12.740 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:00:17.744 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:00:22.745 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:00:27.745 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:00:31.591 UTC [7f0837fff700] Status alarm.cpp:244: Reraising all alarms with a known state
08-03-2018 14:00:32.745 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:00:37.746 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:00:42.746 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:00:47.747 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:00:52.747 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:00:57.747 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:01:01.591 UTC [7f0837fff700] Status alarm.cpp:244: Reraising all alarms with a known state
08-03-2018 14:01:02.748 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:01:07.748 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:01:12.749 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:01:17.749 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:01:22.749 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:01:27.750 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:01:31.591 UTC [7f0837fff700] Status alarm.cpp:244: Reraising all alarms with a known state
08-03-2018 14:03:57.762 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No Diameter peers have been found
08-03-2018 14:04:01.599 UTC [7f0837fff700] Status alarm.cpp:244: Reraising all alarms with a known state
08-03-2018 14:04:02.764 UTC [7f07ee7bc700] Warning dnscachedresolver.cpp:836: Failed to retrieve record for _diameter._tcp.iind.intel.com<http://tcp.iind.intel.com>: Domain name not found
08-03-2018 14:04:02.764 UTC [7f07ee7bc700] Warning dnscachedresolver.cpp:836: Failed to retrieve record for _diameter._sctp.iind.intel.com<http://sctp.iind.intel.com>: Domain name not found



[dime]ubuntu at dime:/var/log/homestead$ tail -30 homestead_current.txt
08-03-2018 16:00:25.983 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising all alarms with a known state
08-03-2018 16:00:55.984 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising all alarms with a known state
08-03-2018 16:01:25.984 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising all alarms with a known state
08-03-2018 16:01:55.984 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising all alarms with a known state
08-03-2018 16:02:25.989 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising all alarms with a known state
08-03-2018 16:02:55.989 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising all alarms with a known state
08-03-2018 16:03:25.989 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising all alarms with a known state




[sprout]ubuntu at sprout:/var/log/sprout$ tail -30 sprout_current.txt
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=97208 (rdata0x7fd04c092988)
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug uri_classifier.cpp:172: Classified URI as 3
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=97208 (tdta0x7fd0d40829d0) created
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Verbose common_sip_processing.cpp:103: TX 277 bytes Response msg 200/OPTIONS/cseq=97208 (tdta0x7fd0d40829d0) to TCP 10.224.61.22:52668<http://10.224.61.22:52668>:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=52668;received=10.224.61.22;branch=z9hG4bK-97208
Call-ID: poll-sip-97208
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=97208
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-97208
CSeq: 97208 OPTIONS
Content-Length:  0


--end msg--
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug pjsip: tdta0x7fd0d408 Destroying txdata Response msg 200/OPTIONS/cseq=97208 (tdta0x7fd0d40829d0)
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fd04c092988
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug thread_dispatcher.cpp:284: Request latency = 292us
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug event_statistic_accumulator.cpp:32: Accumulate 292 for 0x187c788
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug event_statistic_accumulator.cpp:32: Accumulate 292 for 0x187c7d0
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 10).
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug utils.cpp:878: Removed IOHook 0x7fd0b37bde30 to stack. There are now 0 hooks
08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
08-03-2018 17:45:39.463 UTC [7fd0d17fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
08-03-2018 17:45:40.955 UTC [7fd053eff700] Verbose pjsip: tcps0x7fd04c09 TCP connection closed
08-03-2018 17:45:40.955 UTC [7fd053eff700] Debug connection_tracker.cpp:67: Connection 0x7fd04c0910f8 has been destroyed
08-03-2018 17:45:40.955 UTC [7fd053eff700] Verbose pjsip: tcps0x7fd04c09 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)


$TTL 5m ; Default TTL

; SOA, NS and A record for DNS server itself
@                 3600 IN SOA  ns admin ( 2014010800 ; Serial
                                          3600       ; Refresh
                                          3600       ; Retry
                                          3600       ; Expire
                                          300 )      ; Minimum TTL
@                 3600 IN NS   ns
ns                3600 IN A    10.224.61.82 ; IPv4 address of BIND server


; bono
; ====
;
; Per-node records - not required to have both IPv4 and IPv6 records

bono-1                 IN A     10.224.61.8
.
.
.
.

 In zone file infront of ns (you can see above) I am using the IP address of machine where i installed the bind9, but in that machine i checked in /etc/resolv.conf file nameserver is 127.0.0.53. (what should I use in place of ns?)

-> In clearwater node also i have write in  /etc/dnsmasq.resolv.conf
naeserver as  10.224.61.82 (that machine's IP where i installed bind9), what should i write?

Thanks,
Pushpendra



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180313/0e2ad018/attachment.html>

From anthonynlee at gmail.com  Tue Mar 13 14:49:48 2018
From: anthonynlee at gmail.com (Anthony Lee)
Date: Tue, 13 Mar 2018 14:49:48 -0400
Subject: [Project Clearwater] Does Clearwater support to provision
 wildcarded PSI?
In-Reply-To: <CY4PR02MB2517ADB82CBAD0472608BAFB9CD20@CY4PR02MB2517.namprd02.prod.outlook.com>
References: <CA+pBo5GQ2N6A8pMr7FfeH-9dvGYR6S967fZwSHWq5eOuO_5rzA@mail.gmail.com>
	<CA+pBo5E+qago6L-TOU8Z60LfgBvXApZpCWCLvfehM9YvXAq_Dw@mail.gmail.com>
	<CA+pBo5FdjOaTxVManL+QAQYZ4OS3pHbVSVHUT7kXxEBph6=YUQ@mail.gmail.com>
	<CA+pBo5EgQubf=DPzZxaoBJG4p7JikhzsjeE4a2pJ_mdoS4B+WA@mail.gmail.com>
	<CY4PR02MB2517ADB82CBAD0472608BAFB9CD20@CY4PR02MB2517.namprd02.prod.outlook.com>
Message-ID: <CA+pBo5E80Oa5qLdYoAfd573cJFngifKScLqDfUJ7hVVzDVPc8w@mail.gmail.com>

Thanks, Ben.


I wonder what the difference is between using DNS to map the (w)PSI domain
to my application server
and using (w)PSI in HSS profile.


Anthony


On Tue, Mar 13, 2018 at 1:17 PM, Bennett Allen <Bennett.Allen at metaswitch.com
> wrote:

> Hi Anthony,
>
> From the logs you have sent I see that you are using homestead-prov, we
> only support wildcarded entries with a HSS, which would explain the issues
> you are hitting. Here is a link to the docs about a HSS -
> http://clearwater.readthedocs.io/en/stable/External_HSS_Integration.html .
>
> Let us know how it goes,
>
> Ben
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Anthony Lee
> *Sent:* 10 March 2018 20:02
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Does Clearwater support to provision
> wildcarded PSI?
>
>
>
> Now I added P-Profile-Key header into the request, the header value is my
> wildcard PSI uri( sip:.+ at list-service.example.com ).
>
>
>
> Homestead  log:
>
> 10-03-2018 19:39:50.073 UTC [7f22987f8700] Debug http_handlers.cpp:619:
> Determining request type from '{"reqtype": "call", "server_name": "
> sip:scscf.192.168.1.10:5054;transport=TCP", "wildcard_identity": "
> sip:.+ at list-service.example.com"}'
>
>
>
> 10-03-2018 19:39:50.074 UTC [7f22c4fb1700] Debug memcached_cache.cpp:27:
> Pausing stopwatch due to Memcached GET fetch result for
> impu\\sip:.+ at list-service.example.com
>
> 10-03-2018 19:39:50.075 UTC [7f22c4fb1700] Debug memcached_cache.cpp:33:
> Resuming stopwatch due to Memcached GET fetch result for impu\\
> sip:.+ at list-service.example.com
>
>
>
> 10-03-2018 19:39:50.075 UTC [7f22c4fb1700] Debug memcachedstore.cpp:414:
> Key not found
>
>
>
> 10-03-2018 19:39:50.075 UTC [7f229affd700] Debug hsprov_store.cpp:78:
> Issuing get for key sip:7gbcen27b7lqvh3noa3nt8nftc7gbc
> en27b7lqvh3noa3nt8nftc at list-service.example.com
>
>
>
> 10-03-2018 19:39:50.078 UTC [7f229affd700] Debug cassandra_store.cpp:536:
> Cassandra request failed: rc=2, Row sip:7gbcen27b7lqvh3noa3nt8nftc7gbc
> en27b7lqvh3noa3nt8nftc at list-service.example.com not present in
> column_family impu
>
>
>
>
>
> Looks like Homestead uses P-Profile-Key's value to search in cache but
> uses R-URI to search in database.
>
>
>
> Is it expected behavior?
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> On Sat, Mar 10, 2018 at 10:48 AM, Anthony Lee <anthonynlee at gmail.com>
> wrote:
>
> Sorry, forgot to mention that  Sprout's log  shows that 404 not found for
> that wildcard PSI return from Homestead. .
>
>
>
> On Sat, Mar 10, 2018 at 10:44 AM, Anthony Lee <anthonynlee at gmail.com>
> wrote:
>
> My wildcard PSI is  sip:.+ at list-service.example.com
>
>
>
>
>
> I found some information in Homestead_current.txt:
>
>
>
> Debug http_handlers.cpp:695: Did not receive valid JSON with a
> 'wildcard_identity' element
>
>
>
> Does it mean scscf need to add "wildcard_identity" into the JSON request?
>
> Is there any configuration need to be done in Sprout?
>
>
>
>
>
>
>
> On Fri, Mar 9, 2018 at 10:54 PM, Anthony Lee <anthonynlee at gmail.com>
> wrote:
>
> I need to provision wildcarded PSI, does clearwater support to do that?
>
> If yes, could you provide an example?
>
>
>
> Thanks
>
>
>
> Anthony
>
>
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180313/171f3a47/attachment.html>

From pushpendra16mnnit at gmail.com  Tue Mar 13 15:40:26 2018
From: pushpendra16mnnit at gmail.com (Pushpendra)
Date: Wed, 14 Mar 2018 01:10:26 +0530
Subject: [Project Clearwater] Failed to register to client (Zoiper5 and
 XLite)
In-Reply-To: <CY4PR02MB2517D7714D64E18B4A5679519CD20@CY4PR02MB2517.namprd02.prod.outlook.com>
References: <CAPqjCcEF3k_OAM+r36MBdzb0WwAEjuu04YMWnDi8JjBMSee0WQ@mail.gmail.com>
	<CY4PR02MB2517D7714D64E18B4A5679519CD20@CY4PR02MB2517.namprd02.prod.outlook.com>
Message-ID: <CAPqjCcH7bfFPiSvUzc1Fjn80uGwja2ZZYF1fFhXKE7ZGcffRDg@mail.gmail.com>

Hi Ben,
Thanks for replying. I want to do stress testing, I havr created new node
and create 50000 no. Using that script given in document. But all call are
failed. Do I need to register these no. to any client before running
command of stress testing?

Thanks

On 13-Mar-2018 10:50 PM, "Bennett Allen" <Bennett.Allen at metaswitch.com>
wrote:

> Hi Pushpendra,
>
> Firstly I have CC?d this onto the clearwater mailing list and please send
> all replies via this as otherwise your emails may not be seen by my
> colleagues or me!
>
> In answer to your questions I don?t think what the ns value is matters so
> what you?ve got should be fine, and the second value using your bind IP
> looks good. It also doesn?t look like the DNS server/queries is the problem
> in your situation, as homestead is trying to contact the vellum node and
> failing there.
>
> If you have a look at the logs on the vellum node for Cassandra, Astaire
> and Rogers, these are at /var/log/Cassandra/, /var/log/Astaire and
> /var/log/rogers. This should give a clue as to what is happening, as based
> on the monit.log they don?t look like they?re working properly.
>
> Let us know how it goes,
>
> Ben
>
>
>
>
>
> *From:* Pushpendra [mailto:pushpendra16mnnit at gmail.com]
> *Sent:* 09 March 2018 11:06
> *To:* Bennett Allen <Bennett.Allen at metaswitch.com>
> *Subject:* Failed to register to client (Zoiper5 and XLite)
>
>
>
> Hi Ben,
>
> I am able to login and create private identity on ellis, but when I am
> trying to register these identity on client app (zoiper5 an XLite), its not
> registered. Its in an endless loop. I am using <zone> as domain while
> registering to client. Please find the log detail below:
>
> Please let me what is wrong and how can I fix, thanks in advance J
>
>
>
> [vellum]ubuntu at vellum:/var/log$ tail -40 monit.log
>
> [IST Mar  7 23:54:05] error    : 'cassandra_process' process is not running
>
> [IST Mar  7 23:54:05] info     : 'cassandra_process' trying to restart
>
> [IST Mar  7 23:54:05] info     : 'cassandra_process' restart: /bin/bash
>
> [IST Mar  7 23:54:06] info     : 'cassandra_process' process is running
> with pid 31746
>
> [IST Mar  7 23:54:06] error    : 'cassandra_uptime' '/usr/share/clearwater/
> infrastructure/monit_uptime/check-cassandra-uptime' failed with exit
> status (1) -- no output
>
> [IST Mar  7 23:54:06] error    : 'astaire_uptime' '/usr/share/clearwater/
> infrastructure/monit_uptime/check-astaire-uptime' failed with exit status
> (1) -- no output
>
> [IST Mar  7 23:54:16] error    : 'rogers_process' process is not running
>
> [IST Mar  7 23:54:16] info     : 'rogers_process' trying to restart
>
> [IST Mar  7 23:54:16] info     : 'rogers_process' restart: /bin/bash
>
> [IST Mar  7 23:54:16] info     : 'rogers_process' process is running with
> pid 31827
>
> [IST Mar  7 23:54:16] error    : 'clearwater_cluster_manager_process'
> process is not running
>
> [IST Mar  7 23:54:16] info     : 'clearwater_cluster_manager_process'
> trying to restart
>
> [IST Mar  7 23:54:16] info     : 'clearwater_cluster_manager_process'
> restart: /bin/bash
>
> [IST Mar  7 23:54:17] error    : 'cassandra_process' process is not running
>
> [IST Mar  7 23:54:17] info     : 'cassandra_process' trying to restart
>
> [IST Mar  7 23:54:17] info     : 'cassandra_process' restart: /bin/bash
>
> [IST Mar  7 23:54:17] info     : 'cassandra_process' process is running
> with pid 32022
>
> [IST Mar  7 23:54:17] error    : 'cassandra_uptime' '/usr/share/clearwater/
> infrastructure/monit_uptime/check-cassandra-uptime' failed with exit
> status (1) -- no output
>
> [IST Mar  7 23:54:17] error    : 'astaire_uptime' '/usr/share/clearwater/
> infrastructure/monit_uptime/check-astaire-uptime' failed with exit status
> (1) -- no output
>
> [IST Mar  7 23:54:27] error    : 'rogers_uptime' '/usr/share/clearwater/
> infrastructure/monit_uptime/check-rogers-uptime' failed with exit status
> (1) -- no output
>
> [IST Mar  7 23:54:27] info     : 'clearwater_cluster_manager_process'
> process is running with pid 31936
>
> [IST Mar  7 23:54:27] error    : 'cassandra_process' process is not running
>
> [IST Mar  7 23:54:27] info     : 'cassandra_process' trying to restart
>
> [IST Mar  7 23:54:27] info     : 'cassandra_process' restart: /bin/bash
>
> [IST Mar  7 23:54:28] info     : 'cassandra_process' process is running
> with pid 32610
>
> [IST Mar  7 23:54:28] error    : 'cassandra_uptime' '/usr/share/clearwater/
> infrastructure/monit_uptime/check-cassandra-uptime' failed with exit
> status (1) -- no output
>
> [IST Mar  7 23:54:28] error    : 'astaire_uptime' '/usr/share/clearwater/
> infrastructure/monit_uptime/check-astaire-uptime' failed with exit status
> (1) -- no output
>
> [IST Mar  7 23:54:38] error    : 'rogers_uptime' '/usr/share/clearwater/
> infrastructure/monit_uptime/check-rogers-uptime' failed with exit status
> (1) -- no output
>
> [IST Mar  7 23:54:38] error    : 'cassandra_uptime' '/usr/share/clearwater/
> infrastructure/monit_uptime/check-cassandra-uptime' failed with exit
> status (1) -- no output
>
> [IST Mar  7 23:54:39] info     : 'astaire_uptime' status succeeded
> [status=0] -- zmq_msg_recv: Resource temporarily unavailable
>
> [IST Mar  7 23:54:46] info     : Awakened by the SIGHUP signal
>
> Reinitializing Monit - Control file '/etc/monit/monitrc'
>
> [IST Mar  7 23:54:47] info     : 'node-vellum' Monit reloaded
>
> [IST Mar  7 23:54:47] error    : Cannot create socket to [localhost]:2812
> -- Connection refused
>
> [IST Mar  7 23:54:57] info     : Awakened by the SIGHUP signal
>
> Reinitializing Monit - Control file '/etc/monit/monitrc'
>
> [IST Mar  7 23:54:57] info     : 'node-vellum' Monit reloaded
>
> [IST Mar  7 23:54:57] error    : Cannot create socket to [localhost]:2812
> -- Connection refused
>
> [IST Mar  7 23:55:07] error    : 'cassandra_uptime' '/usr/share/clearwater/
> infrastructure/monit_uptime/check-cassandra-uptime' failed with exit
> status (1) -- no output
>
> [IST Mar  7 23:55:17] info     : 'cassandra_uptime' status succeeded
> [status=0] -- zmq_msg_recv: Resource temporarily unavailable
>
>
>
>
>
>
>
> [dime]ubuntu at dime:/var/log/homestead$ tail -30 homestead_err.log
>
> Thrift: Wed Mar  7 23:54:03 2018 TSocket::open() error on socket (after
> THRIFT_POLL) <Host: 10.224.61.24 Port: 9160>Connection refused
>
> Thrift: Wed Mar  7 23:54:03 2018 TSocket::open() error on socket (after
> THRIFT_POLL) <Host: 10.224.61.24 Port: 9160>Connection refused
>
>
>
>
>
> [dime]ubuntu at dime:/var/log/ralf$ tail -40 ralf_current.txt
>
> 08-03-2018 14:00:02.739 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:00:07.740 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:00:12.740 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:00:17.744 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:00:22.745 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:00:27.745 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:00:31.591 UTC [7f0837fff700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
> 08-03-2018 14:00:32.745 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:00:37.746 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:00:42.746 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:00:47.747 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:00:52.747 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:00:57.747 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:01:01.591 UTC [7f0837fff700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
> 08-03-2018 14:01:02.748 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:01:07.748 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:01:12.749 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:01:17.749 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:01:22.749 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:01:27.750 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:01:31.591 UTC [7f0837fff700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
> 08-03-2018 14:03:57.762 UTC [7f07ee7bc700] Error diameterstack.cpp:853: No
> Diameter peers have been found
>
> 08-03-2018 14:04:01.599 UTC [7f0837fff700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
> 08-03-2018 14:04:02.764 UTC [7f07ee7bc700] Warning
> dnscachedresolver.cpp:836: Failed to retrieve record for _diameter._
> tcp.iind.intel.com: Domain name not found
>
> 08-03-2018 14:04:02.764 UTC [7f07ee7bc700] Warning
> dnscachedresolver.cpp:836: Failed to retrieve record for _diameter._
> sctp.iind.intel.com: Domain name not found
>
>
>
>
>
>
>
> [dime]ubuntu at dime:/var/log/homestead$ tail -30 homestead_current.txt
>
> 08-03-2018 16:00:25.983 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
> 08-03-2018 16:00:55.984 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
> 08-03-2018 16:01:25.984 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
> 08-03-2018 16:01:55.984 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
> 08-03-2018 16:02:25.989 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
> 08-03-2018 16:02:55.989 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
> 08-03-2018 16:03:25.989 UTC [7fd5977fe700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
>
>
>
>
>
>
>
>
> [sprout]ubuntu at sprout:/var/log/sprout$ tail -30 sprout_current.txt
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug pjsip: sip_endpoint.c
> Distributing rdata to modules: Request msg OPTIONS/cseq=97208
> (rdata0x7fd04c092988)
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug uri_classifier.cpp:172:
> Classified URI as 3
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug pjsip:       endpoint
> Response msg 200/OPTIONS/cseq=97208 (tdta0x7fd0d40829d0) created
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Verbose
> common_sip_processing.cpp:103: TX 277 bytes Response msg
> 200/OPTIONS/cseq=97208 (tdta0x7fd0d40829d0) to TCP 10.224.61.22:52668:
>
> --start msg--
>
>
>
> SIP/2.0 200 OK
>
> Via: SIP/2.0/TCP 10.224.61.22;rport=52668;received=10.224.61.22;branch=
> z9hG4bK-97208
>
> Call-ID: poll-sip-97208
>
> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=97208
>
> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-97208
>
> CSeq: 97208 OPTIONS
>
> Content-Length:  0
>
>
>
>
>
> --end msg--
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug
> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug pjsip: tdta0x7fd0d408
> Destroying txdata Response msg 200/OPTIONS/cseq=97208 (tdta0x7fd0d40829d0)
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug
> thread_dispatcher.cpp:270: Worker thread completed processing message
> 0x7fd04c092988
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug
> thread_dispatcher.cpp:284: Request latency = 292us
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 292 for 0x187c788
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 292 for 0x187c7d0
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug load_monitor.cpp:341: Not
> recalculating rate as we haven't processed 20 requests yet (only 10).
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug utils.cpp:878: Removed
> IOHook 0x7fd0b37bde30 to stack. There are now 0 hooks
>
> 08-03-2018 17:45:38.954 UTC [7fd0b37be700] Debug
> thread_dispatcher.cpp:158: Attempting to process queue element
>
> 08-03-2018 17:45:39.463 UTC [7fd0d17fa700] Warning (Net-SNMP): Warning:
> Failed to connect to the agentx master agent ([NIL]):
>
> 08-03-2018 17:45:40.955 UTC [7fd053eff700] Verbose pjsip: tcps0x7fd04c09
> TCP connection closed
>
> 08-03-2018 17:45:40.955 UTC [7fd053eff700] Debug
> connection_tracker.cpp:67: Connection 0x7fd04c0910f8 has been destroyed
>
> 08-03-2018 17:45:40.955 UTC [7fd053eff700] Verbose pjsip: tcps0x7fd04c09
> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>
>
>
>
>
> $TTL 5m ; Default TTL
>
>
>
> ; SOA, NS and A record for DNS server itself
>
> @                 3600 IN SOA  ns admin ( 2014010800 ; Serial
>
>                                           3600       ; Refresh
>
>                                           3600       ; Retry
>
>                                           3600       ; Expire
>
>                                           300 )      ; Minimum TTL
>
> @                 3600 IN NS   ns
>
> *ns                3600 IN A    10.224.61.82* ; IPv4 address of BIND
> server
>
>
>
>
>
> ; bono
>
> ; ====
>
> ;
>
> ; Per-node records - not required to have both IPv4 and IPv6 records
>
>
>
> bono-1                 IN A     10.224.61.8
>
> .
>
> .
>
> .
>
> .
>
>
>
>  In zone file infront of ns (you can see above) I am using the IP address
> of machine where i installed the bind9, but in that machine i checked in
> /etc/resolv.conf file nameserver is 127.0.0.53. (*what should I use in
> place of ns?*)
>
>
>
> -> In clearwater node also i have write in  /etc/dnsmasq.resolv.conf
>
> naeserver as  10.224.61.82 (that machine's IP where i installed bind9),
> what should i write?
>
>
>
> Thanks,
>
> Pushpendra
>
>
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180314/b2b3b0df/attachment.html>

From pushpendra16mnnit at gmail.com  Wed Mar 14 08:04:57 2018
From: pushpendra16mnnit at gmail.com (Pushpendra)
Date: Wed, 14 Mar 2018 17:34:57 +0530
Subject: [Project Clearwater] stress testing
Message-ID: <CAPqjCcHWiFOEP5EcYhTiiYqTruaGugAFgnkDkndSmMEqa9SOdA@mail.gmail.com>

 Hi All,
I need to ask few questions regarding stress testing for manual
installation of clearwater. I am following http://clearwater.
readthedocs.io/en/stable/Clearwater_stress_testing.html for stress testing.
 I have set the* local_ip of this node* only in
* /etc/clearwater/local_config. *I also set *reg_max_expires=1800* in all
the other node in *shared_config *after that I created the 50000 no. in
vellum node as given in doc
 https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%
20Numbers.md?utf8=%E2%9C%93
and then I install the debian package *sudo apt-get install
clearwater-sip-stress-careonly *on new node.
when I run
/usr/share/clearwater/bin/run_stress <home_domain> 1800 2
Starting initial registration, will take 22 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:02:00
Start: 2018-03-14 18:06:06.635672
End: 2018-03-14 18:08:06.714723

Total calls: 39
Successful calls: 0 (0.0%)
Failed calls: 39 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 120
Successful re-REGISTERs: 120 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 30.0ms

Log files at /var/log/clearwater-sip-stress/13921_*

1. I have not mention new node's IP in any other node of clearwater, how
the other node know about it?

My question is, *Do I need to mention the IP of new node in any other node
of clearwater?*


when I run -
[]ubuntu at stress:~$  /usr/share/clearwater/bin/run_stress <home_domain> 1800
10 --icscf-target TARGET=sprout.{domain}:5052 --scscf-target
TARGET=sprout.{domain}:5054

Starting initial registration, will take 22 seconds
Initial registration failed - see /var/log/clearwater-sip-
stress/14115_initial_reg_errors.log for details of the errors



*[]ubuntu at stress:/var/log/clearwater-sip-stress$ tail -30
14115_initial_reg_errors.log*
sipp: The following events occured:
2018-03-14      18:11:06.637370 1521031266.637370: Unknown remote host
'TARGET=sprout.<domain>' (Name or service not known, Inappropriate ioctl
for device).
Use 'sipp -h' for details.


*[]ubuntu at stress:/var/log/clearwater-sip-stress$ cat 9923_caller_errors.log*
2018-03-14      00:32:45.274812 1520967765.274812: Aborting call on
unexpected message for Call-Id '54-9937 at 127.0.1.1': while expecting '183'
(index 2), received 'SIP/2.0 503 Service Unavailable
Via: SIP/2.0/TCP 127.0.1.1:34015;received=10.224.61.13;branch=z9hG4bK-9937-
54-0
Record-Route: <sip:scscf.sprout. <home_domain> ;transport=TCP;lr;billing-
role=charge-term>
Record-Route: <sip:scscf.sprout. <home_domain> ;transport=TCP;lr;billing-
role=charge-orig>
Call-ID: 54-9937 at 127.0.1.1
From: <sip:2010000252@<home_domain>>;tag=9937SIPpTag0054
To: <sip:2010000647@ <home_domain> >;tag=z9hG4bKPjJVqopmpeih7pUoxJrYOG7
CO4-Bm69ozI
CSeq: 1 INVITE
P-Charging-Vector: icid-value="9937SIPpTag0054";orig-ioi=<home_domain>;term-
ioi=<home_domain>
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0

'.



Regards,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180314/aeb07964/attachment.html>

From skgola1997 at gmail.com  Wed Mar 14 01:32:43 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Wed, 14 Mar 2018 11:02:43 +0530
Subject: [Project Clearwater] Stress Testing
Message-ID: <CAHwYWpA9XAaVz3P7a8Q4o8jghXXrB_F6wQXiRVE7QcznNfS0mg@mail.gmail.com>

Hi All,
I need to ask few questions regarding stress testing for manual
installation of clearwater. I am following
http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html
for stress testing.
 I have set the* local_ip of this node* only in*
/etc/clearwater/local_config. *I also set *reg_max_expires=1800* in all the
other node in *shared_config *after that I created the 50000 no. in vellum
node as given in doc

https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%20Numbers.md?utf8=%E2%9C%93
and then I install the debian package *sudo apt-get install
clearwater-sip-stress-careonly *on new node.
when I run
/usr/share/clearwater/bin/run_stress <home_domain> 1800 2
Starting initial registration, will take 22 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:02:00
Start: 2018-03-14 18:06:06.635672
End: 2018-03-14 18:08:06.714723

Total calls: 39
Successful calls: 0 (0.0%)
Failed calls: 39 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 120
Successful re-REGISTERs: 120 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 30.0ms

Log files at /var/log/clearwater-sip-stress/13921_*

1. I have not mention new node's IP in any other node of clearwater, how
the other node know about it?

My question is, *Do I need to mention the IP of new node in any other node
of clearwater?*


when I run -
[]ubuntu at stress:~$  /usr/share/clearwater/bin/run_stress <home_domain> 1800
10 --icscf-target TARGET=sprout.{domain}:5052 --scscf-target
TARGET=sprout.{domain}:5054

Starting initial registration, will take 22 seconds
Initial registration failed - see
/var/log/clearwater-sip-stress/14115_initial_reg_errors.log for details of
the errors



*[]ubuntu at stress:/var/log/clearwater-sip-stress$ tail -30
14115_initial_reg_errors.log*
sipp: The following events occured:
2018-03-14      18:11:06.637370 1521031266.637370: Unknown remote host
'TARGET=sprout.<domain>' (Name or service not known, Inappropriate ioctl
for device).
Use 'sipp -h' for details.


*[]ubuntu at stress:/var/log/clearwater-sip-stress$ cat 9923_caller_errors.log*
2018-03-14      00:32:45.274812 1520967765.274812: Aborting call on
unexpected message for Call-Id '54-9937 at 127.0.1.1': while expecting '183'
(index 2), received 'SIP/2.0 503 Service Unavailable
Via: SIP/2.0/TCP 127.0.1.1:34015
;received=10.224.61.13;branch=z9hG4bK-9937-54-0
Record-Route: <sip:scscf.sprout. <home_domain>
;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout. <home_domain>
;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 54-9937 at 127.0.1.1
From: <sip:2010000252@<home_domain>>;tag=9937SIPpTag0054
To: <sip:2010000647@ <home_domain>
>;tag=z9hG4bKPjJVqopmpeih7pUoxJrYOG7CO4-Bm69ozI
CSeq: 1 INVITE
P-Charging-Vector:
icid-value="9937SIPpTag0054";orig-ioi=<home_domain>;term-ioi=<home_domain>
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0

'.



Regards,
Sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180314/7f34d08c/attachment.html>

From Michael.Duppre at metaswitch.com  Fri Mar 16 13:02:59 2018
From: Michael.Duppre at metaswitch.com (=?utf-8?B?TWljaGFlbCBEdXBwcsOp?=)
Date: Fri, 16 Mar 2018 17:02:59 +0000
Subject: [Project Clearwater] Does Clearwater support to provision
 wildcarded PSI?
In-Reply-To: <CA+pBo5E80Oa5qLdYoAfd573cJFngifKScLqDfUJ7hVVzDVPc8w@mail.gmail.com>
References: <CA+pBo5GQ2N6A8pMr7FfeH-9dvGYR6S967fZwSHWq5eOuO_5rzA@mail.gmail.com>
	<CA+pBo5E+qago6L-TOU8Z60LfgBvXApZpCWCLvfehM9YvXAq_Dw@mail.gmail.com>
	<CA+pBo5FdjOaTxVManL+QAQYZ4OS3pHbVSVHUT7kXxEBph6=YUQ@mail.gmail.com>
	<CA+pBo5EgQubf=DPzZxaoBJG4p7JikhzsjeE4a2pJ_mdoS4B+WA@mail.gmail.com>
	<CY4PR02MB2517ADB82CBAD0472608BAFB9CD20@CY4PR02MB2517.namprd02.prod.outlook.com>
	<CA+pBo5E80Oa5qLdYoAfd573cJFngifKScLqDfUJ7hVVzDVPc8w@mail.gmail.com>
Message-ID: <SN1PR02MB1677ABC2144647D53CEB3667F5D70@SN1PR02MB1677.namprd02.prod.outlook.com>

Hi Anthony,

Sorry, I?m not quite sure you mean here? There are five ways you could route to a PSI hosted by an App Server:

  *   Subscribers could send requests to the SIP URI of the App Server directly ? e.g. the INVITE could have a request URI of sip:as.example.com
  *   ENUM could return a translation for the PSI to point directly to the Application Server
     *   e.g. 0.0.0.0.5.5.5.3.2.2.1.e164.arpa could resolve to a translation pointing to sip:as.example.com, which would result in calls for  going to as.example.com See http://clearwater.readthedocs.io/en/stable/ENUM.html
  *   You can directly attach a PSI by configuring the HSS to point to it
     *   In this case, the Location Information Answer (LIA) (in response to the LIR that Clearwater sends) will return the location of the application server, and the I-CSCF (Sprout) will then route to it.
  *   You can indirectly attach a PSI by configuring the HSS to point to an S-CSCF, and then provide IFCs which route to the App Server
  *   You could configure each subscriber who will create a session with the PSI to have IFCs which invoke the App Server during originating processing, matching on the Request URI in the IFCs.

In each of these cases, there will be a SIP URI, which will be resolved as per RFC 3261, with the relevant DNS queries if it represents a domain, or just routed if it?s an IP address.
I hope this helps and points you into the right direction?

Kind regards,
Michael


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Anthony Lee
Sent: 13 March 2018 18:50
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Does Clearwater support to provision wildcarded PSI?

Thanks, Ben.


I wonder what the difference is between using DNS to map the (w)PSI domain to my application server
and using (w)PSI in HSS profile.


Anthony


On Tue, Mar 13, 2018 at 1:17 PM, Bennett Allen <Bennett.Allen at metaswitch.com<mailto:Bennett.Allen at metaswitch.com>> wrote:
Hi Anthony,
From the logs you have sent I see that you are using homestead-prov, we only support wildcarded entries with a HSS, which would explain the issues you are hitting. Here is a link to the docs about a HSS - http://clearwater.readthedocs.io/en/stable/External_HSS_Integration.html .
Let us know how it goes,
Ben


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Anthony Lee
Sent: 10 March 2018 20:02
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Does Clearwater support to provision wildcarded PSI?

Now I added P-Profile-Key header into the request, the header value is my wildcard PSI uri( sip:.+ at list-service.example.com<mailto:sip%3A.%2B at list-service.example.com> ).

Homestead  log:
10-03-2018 19:39:50.073 UTC [7f22987f8700] Debug http_handlers.cpp:619: Determining request type from '{"reqtype": "call", "server_name": "sip:scscf.192.168.1.10:5054;transport=TCP", "wildcard_identity": "sip:.+ at list-service.example.com<mailto:sip%3A.%2B at list-service.example.com>"}'

10-03-2018 19:39:50.074 UTC [7f22c4fb1700] Debug memcached_cache.cpp:27: Pausing stopwatch due to Memcached GET fetch result for impu\\sip:.+ at list-service.example.com<http://example.com>
10-03-2018 19:39:50.075 UTC [7f22c4fb1700] Debug memcached_cache.cpp:33: Resuming stopwatch due to Memcached GET fetch result for impu\\sip:.+ at list-service.example.com<mailto:sip%3A.%2B at list-service.example.com>

10-03-2018 19:39:50.075 UTC [7f22c4fb1700] Debug memcachedstore.cpp:414: Key not found

10-03-2018 19:39:50.075 UTC [7f229affd700] Debug hsprov_store.cpp:78: Issuing get for key sip:7gbcen27b7lqvh3noa3nt8nftc7gbcen27b7lqvh3noa3nt8nftc at list-service.example.com<mailto:sip%3A7gbcen27b7lqvh3noa3nt8nftc7gbcen27b7lqvh3noa3nt8nftc at list-service.example.com>

10-03-2018 19:39:50.078 UTC [7f229affd700] Debug cassandra_store.cpp:536: Cassandra request failed: rc=2, Row sip:7gbcen27b7lqvh3noa3nt8nftc7gbcen27b7lqvh3noa3nt8nftc at list-service.example.com<http://example.com> not present in column_family impu


Looks like Homestead uses P-Profile-Key's value to search in cache but uses R-URI to search in database.

Is it expected behavior?








On Sat, Mar 10, 2018 at 10:48 AM, Anthony Lee <anthonynlee at gmail.com<mailto:anthonynlee at gmail.com>> wrote:
Sorry, forgot to mention that  Sprout's log  shows that 404 not found for that wildcard PSI return from Homestead. .

On Sat, Mar 10, 2018 at 10:44 AM, Anthony Lee <anthonynlee at gmail.com<mailto:anthonynlee at gmail.com>> wrote:
My wildcard PSI is  sip:.+ at list-service.example.com<mailto:sip%3A.%2B at list-service.example.com>


I found some information in Homestead_current.txt:

Debug http_handlers.cpp:695: Did not receive valid JSON with a 'wildcard_identity' element

Does it mean scscf need to add "wildcard_identity" into the JSON request?
Is there any configuration need to be done in Sprout?



On Fri, Mar 9, 2018 at 10:54 PM, Anthony Lee <anthonynlee at gmail.com<mailto:anthonynlee at gmail.com>> wrote:
I need to provision wildcarded PSI, does clearwater support to do that?
If yes, could you provide an example?

Thanks

Anthony




_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180316/1d96a6dc/attachment.html>

From Michael.Duppre at metaswitch.com  Fri Mar 16 13:20:06 2018
From: Michael.Duppre at metaswitch.com (=?utf-8?B?TWljaGFlbCBEdXBwcsOp?=)
Date: Fri, 16 Mar 2018 17:20:06 +0000
Subject: [Project Clearwater] stress testing
In-Reply-To: <CAPqjCcHWiFOEP5EcYhTiiYqTruaGugAFgnkDkndSmMEqa9SOdA@mail.gmail.com>
References: <CAPqjCcHWiFOEP5EcYhTiiYqTruaGugAFgnkDkndSmMEqa9SOdA@mail.gmail.com>
Message-ID: <SN1PR02MB1677B6DCCE8F163251561073F5D70@SN1PR02MB1677.namprd02.prod.outlook.com>

Hello Sunil,

It sounds like you have two different problems:

  1.  Running `/usr/share/clearwater/bin/run_stress <home_domain> 1800 2` doesn?t succeed making any calls
     *   The good news is that the initial REGISTERs that the tool sends out are working fine as you can see in the output `Initial registration succeeded`!
     *   To make the calls work, I would suggest just running one call with the tool initially and once this works go back to running 1800 calls. To debug this, you should turn on debug logs on Sprout (see http://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html#sprout), run the tool again with one call. Then have a look at the logs of the tool and the Sprout logs and find out why Sprout is sending back a 503 response.
  2.  Running `/usr/share/clearwater/bin/run_stress <home_domain> 1800 10 --icscf-target TARGET=sprout.{domain}:5052 --scscf-target TARGET=sprout.{domain}:5054` crashes
     *   You probably don?t need to provide more arguments when running the tool as it should work with the defaults. So I suggest getting the tool work without any additional arguments and only use these if you have a specific need for additional configuration.
     *   The error ` Unknown remote host 'TARGET=sprout.<domain>` says, the tool wasn?t able to lookup this domain. In fact, you need to replace the word `TARGET` with your full sprout domain. If for example `my_ims_domain` is your domain, you would need to run the command as follows:
     *   `/usr/share/clearwater/bin/run_stress <home_domain> 1800 10 --icscf-target sprout.my_ims_domain:5052 --scscf-target sprout.my_ims_domain:5054`

Hope this helps, good luck getting the calls to work!

Kind regards,
Michael


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Pushpendra
Sent: 14 March 2018 12:05
To: Bennett Allen <Bennett.Allen at metaswitch.com>; clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] stress testing

Hi All,
I need to ask few questions regarding stress testing for manual installation of clearwater. I am following http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html for stress testing.
 I have set the local_ip of this node only in /etc/clearwater/local_config. I also set reg_max_expires=1800 in all the other node in shared_config after that I created the 50000 no. in vellum node as given in doc
 https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%20Numbers.md?utf8=%E2%9C%93
and then I install the debian package sudo apt-get install clearwater-sip-stress-careonly on new node.
when I run
/usr/share/clearwater/bin/run_stress <home_domain> 1800 2
Starting initial registration, will take 22 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:02:00
Start: 2018-03-14 18:06:06.635672
End: 2018-03-14 18:08:06.714723

Total calls: 39
Successful calls: 0 (0.0%)
Failed calls: 39 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 120
Successful re-REGISTERs: 120 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 30.0ms

Log files at /var/log/clearwater-sip-stress/13921_*

1. I have not mention new node's IP in any other node of clearwater, how the other node know about it?

My question is, Do I need to mention the IP of new node in any other node of clearwater?


when I run -
[]ubuntu at stress:~$  /usr/share/clearwater/bin/run_stress <home_domain> 1800 10 --icscf-target TARGET=sprout.{domain}:5052 --scscf-target TARGET=sprout.{domain}:5054

Starting initial registration, will take 22 seconds
Initial registration failed - see /var/log/clearwater-sip-stress/14115_initial_reg_errors.log for details of the errors



[]ubuntu at stress:/var/log/clearwater-sip-stress$ tail -30 14115_initial_reg_errors.log
sipp: The following events occured:
2018-03-14      18:11:06.637370 1521031266.637370: Unknown remote host 'TARGET=sprout.<domain>' (Name or service not known, Inappropriate ioctl for device).
Use 'sipp -h' for details.


[]ubuntu at stress:/var/log/clearwater-sip-stress$ cat 9923_caller_errors.log
2018-03-14      00:32:45.274812 1520967765.274812: Aborting call on unexpected message for Call-Id '54-9937 at 127.0.1.1<mailto:54-9937 at 127.0.1.1>': while expecting '183' (index 2), received 'SIP/2.0 503 Service Unavailable
Via: SIP/2.0/TCP 127.0.1.1:34015;received=10.224.61.13;branch=z9hG4bK-9937-54-0
Record-Route: <sip:scscf.sprout. <home_domain<sip:scscf.sprout.?%3chome_domain>> ;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout. <home_domain<sip:scscf.sprout.?%3chome_domain>> ;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 54-9937 at 127.0.1.1<mailto:54-9937 at 127.0.1.1>
From: <sip:2010000252@<home_domain<sip:2010000252@%3chome_domain>>>;tag=9937SIPpTag0054
To: <sip:2010000647@ <home_domain<sip:2010000647@?%3chome_domain>> >;tag=z9hG4bKPjJVqopmpeih7pUoxJrYOG7CO4-Bm69ozI
CSeq: 1 INVITE
P-Charging-Vector: icid-value="9937SIPpTag0054";orig-ioi=<home_domain>;term-ioi=<home_domain>
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0

'.



Regards,



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180316/14a9b893/attachment.html>

From nagen_kr at yahoo.co.in  Sat Mar 17 04:35:24 2018
From: nagen_kr at yahoo.co.in (Nagendra Kumar)
Date: Sat, 17 Mar 2018 08:35:24 +0000 (UTC)
Subject: [Project Clearwater] Manual installation of Ellis failed
References: <1792897644.1986065.1521275724069.ref@mail.yahoo.com>
Message-ID: <1792897644.1986065.1521275724069@mail.yahoo.com>

Hi Clearwater Team,Greetings !!!
????????????? I am trying to install clearwater on Amazon Web Services EC2 Ubuntu Server 16.04 LTS (Amazon Machine Image (AMI)), I am getting the following error, any help highly appreciated:
ubuntu at ip-10-0-0-128:~$ sudo DEBIAN_FRONTEND=noninteractive apt-get install ellis --yes?????????????????????????????????? Reading package lists... Done
Building dependency tree
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
?ellis : Depends: clearwater-infrastructure but it is not going to be installed
???????? Depends: clearwater-nginx but it is not going to be installed
???????? Depends: clearwater-log-cleanup but it is not going to be installed
???????? Depends: clearwater-monit but it is not going to be installed
E: Unable to correct problems, you have held broken packages.

Thanks
-Nagu
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180317/85ebfd46/attachment.html>

From nagen_kr at yahoo.co.in  Sat Mar 17 10:00:45 2018
From: nagen_kr at yahoo.co.in (Nagendra Kumar)
Date: Sat, 17 Mar 2018 14:00:45 +0000 (UTC)
Subject: [Project Clearwater] Manual installation of Ellis failed
In-Reply-To: <1792897644.1986065.1521275724069@mail.yahoo.com>
References: <1792897644.1986065.1521275724069.ref@mail.yahoo.com>
	<1792897644.1986065.1521275724069@mail.yahoo.com>
Message-ID: <816025537.2092460.1521295245801@mail.yahoo.com>

Hi Team,?????????????? It am able to install on Ubuntu 14.04. It doesn't get installed on Ubuntu 16.04 because it try to link with few old packages(e.g. libzmq3) of 14.04 and doesn't find them on 16.04 because 16.04 has libzmq5 installed.
Thanks-Nagu
 

    On Saturday 17 March 2018, 2:05:24 PM IST, Nagendra Kumar <nagen_kr at yahoo.co.in> wrote:  
 
 Hi Clearwater Team,Greetings !!!
????????????? I am trying to install clearwater on Amazon Web Services EC2 Ubuntu Server 16.04 LTS (Amazon Machine Image (AMI)), I am getting the following error, any help highly appreciated:
ubuntu at ip-10-0-0-128:~$ sudo DEBIAN_FRONTEND=noninteractive apt-get install ellis --yes?????????????????????????????????? Reading package lists... Done
Building dependency tree
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
?ellis : Depends: clearwater-infrastructure but it is not going to be installed
???????? Depends: clearwater-nginx but it is not going to be installed
???????? Depends: clearwater-log-cleanup but it is not going to be installed
???????? Depends: clearwater-monit but it is not going to be installed
E: Unable to correct problems, you have held broken packages.

Thanks
-Nagu
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180317/701760fc/attachment.html>

From anthonynlee at gmail.com  Sat Mar 17 22:51:07 2018
From: anthonynlee at gmail.com (Anthony Lee)
Date: Sat, 17 Mar 2018 22:51:07 -0400
Subject: [Project Clearwater] Does Clearwater support to provision
 wildcarded PSI?
In-Reply-To: <SN1PR02MB1677ABC2144647D53CEB3667F5D70@SN1PR02MB1677.namprd02.prod.outlook.com>
References: <CA+pBo5GQ2N6A8pMr7FfeH-9dvGYR6S967fZwSHWq5eOuO_5rzA@mail.gmail.com>
	<CA+pBo5E+qago6L-TOU8Z60LfgBvXApZpCWCLvfehM9YvXAq_Dw@mail.gmail.com>
	<CA+pBo5FdjOaTxVManL+QAQYZ4OS3pHbVSVHUT7kXxEBph6=YUQ@mail.gmail.com>
	<CA+pBo5EgQubf=DPzZxaoBJG4p7JikhzsjeE4a2pJ_mdoS4B+WA@mail.gmail.com>
	<CY4PR02MB2517ADB82CBAD0472608BAFB9CD20@CY4PR02MB2517.namprd02.prod.outlook.com>
	<CA+pBo5E80Oa5qLdYoAfd573cJFngifKScLqDfUJ7hVVzDVPc8w@mail.gmail.com>
	<SN1PR02MB1677ABC2144647D53CEB3667F5D70@SN1PR02MB1677.namprd02.prod.outlook.com>
Message-ID: <CA+pBo5H2ha8GiD+rAiPg4ZMJY7LHOXfe-kVBTkKK08ywuspQ-g@mail.gmail.com>

Hi Michael,

Thanks for your reply.

In my case, I need to implement a Group Chat service which is in the middle
of the Service Chain,
on the left side there is Originating Function and on the right side there
is Terminating Function,
Both Originating Function and Terminating Function are using iFCs and all
the subscribers are share
service triggers.

Currently I try to use Clearwater to build a end to end test environment,
for each Group Chat there is
unique ID and once it is generated the Originating Function will send
Subscribe and other request to this
dynamically generated Sip URI. Since Clearwater don't support wildcard PSI
I wonder if I could setup
ICSCF so that it would use subdomain based routing to reach the Group Chat
Service.

Or I have to use external HSS?


Thanks
Anthony


On Fri, Mar 16, 2018 at 1:02 PM, Michael Duppr? <
Michael.Duppre at metaswitch.com> wrote:

> Hi Anthony,
>
>
>
> Sorry, I?m not quite sure you mean here? There are five ways you could
> route to a PSI hosted by an App Server:
>
>    - Subscribers could send requests to the SIP URI of the App Server
>    directly ? e.g. the INVITE could have a request URI of sip:
>    as.example.com
>    - ENUM could return a translation for the PSI to point directly to the
>    Application Server
>       - e.g. 0.0.0.0.5.5.5.3.2.2.1.e164.arpa could resolve to a
>       translation pointing to sip:as.example.com, which would result in
>       calls for  going to as.example.com See
>       http://clearwater.readthedocs.io/en/stable/ENUM.html
>       <http://clearwater.readthedocs.io/en/stable/ENUM.html>
>    - You can directly attach a PSI by configuring the HSS to point to it
>       - In this case, the Location Information Answer (LIA) (in response
>       to the LIR that Clearwater sends) will return the location of the
>       application server, and the I-CSCF (Sprout) will then route to it.
>    - You can indirectly attach a PSI by configuring the HSS to point to
>    an S-CSCF, and then provide IFCs which route to the App Server
>    - You could configure each subscriber who will create a session with
>    the PSI to have IFCs which invoke the App Server during originating
>    processing, matching on the Request URI in the IFCs.
>
>
> In each of these cases, there will be a SIP URI, which will be resolved as
> per RFC 3261, with the relevant DNS queries if it represents a domain, or
> just routed if it?s an IP address.
>
> I hope this helps and points you into the right direction?
>
>
>
> Kind regards,
>
> Michael
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Anthony Lee
> *Sent:* 13 March 2018 18:50
>
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Does Clearwater support to provision
> wildcarded PSI?
>
>
>
> Thanks, Ben.
>
>
>
>
>
> I wonder what the difference is between using DNS to map the (w)PSI domain
> to my application server
>
> and using (w)PSI in HSS profile.
>
>
>
>
>
> Anthony
>
>
>
>
>
> On Tue, Mar 13, 2018 at 1:17 PM, Bennett Allen <
> Bennett.Allen at metaswitch.com> wrote:
>
> Hi Anthony,
>
> From the logs you have sent I see that you are using homestead-prov, we
> only support wildcarded entries with a HSS, which would explain the issues
> you are hitting. Here is a link to the docs about a HSS -
> http://clearwater.readthedocs.io/en/stable/External_HSS_Integration.html .
>
> Let us know how it goes,
>
> Ben
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Anthony Lee
> *Sent:* 10 March 2018 20:02
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Does Clearwater support to provision
> wildcarded PSI?
>
>
>
> Now I added P-Profile-Key header into the request, the header value is my
> wildcard PSI uri( sip:.+ at list-service.example.com ).
>
>
>
> Homestead  log:
>
> 10-03-2018 19:39:50.073 UTC [7f22987f8700] Debug http_handlers.cpp:619:
> Determining request type from '{"reqtype": "call", "server_name": "
> sip:scscf.192.168.1.10:5054;transport=TCP", "wildcard_identity": "
> sip:.+ at list-service.example.com"}'
>
>
>
> 10-03-2018 19:39:50.074 UTC [7f22c4fb1700] Debug memcached_cache.cpp:27:
> Pausing stopwatch due to Memcached GET fetch result for
> impu\\sip:.+ at list-service.example.com
>
> 10-03-2018 19:39:50.075 UTC [7f22c4fb1700] Debug memcached_cache.cpp:33:
> Resuming stopwatch due to Memcached GET fetch result for impu\\
> sip:.+ at list-service.example.com
>
>
>
> 10-03-2018 19:39:50.075 UTC [7f22c4fb1700] Debug memcachedstore.cpp:414:
> Key not found
>
>
>
> 10-03-2018 19:39:50.075 UTC [7f229affd700] Debug hsprov_store.cpp:78:
> Issuing get for key sip:7gbcen27b7lqvh3noa3nt8nftc7gbc
> en27b7lqvh3noa3nt8nftc at list-service.example.com
>
>
>
> 10-03-2018 19:39:50.078 UTC [7f229affd700] Debug cassandra_store.cpp:536:
> Cassandra request failed: rc=2, Row sip:7gbcen27b7lqvh3noa3nt8nftc7gbc
> en27b7lqvh3noa3nt8nftc at list-service.example.com not present in
> column_family impu
>
>
>
>
>
> Looks like Homestead uses P-Profile-Key's value to search in cache but
> uses R-URI to search in database.
>
>
>
> Is it expected behavior?
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> On Sat, Mar 10, 2018 at 10:48 AM, Anthony Lee <anthonynlee at gmail.com>
> wrote:
>
> Sorry, forgot to mention that  Sprout's log  shows that 404 not found for
> that wildcard PSI return from Homestead. .
>
>
>
> On Sat, Mar 10, 2018 at 10:44 AM, Anthony Lee <anthonynlee at gmail.com>
> wrote:
>
> My wildcard PSI is  sip:.+ at list-service.example.com
>
>
>
>
>
> I found some information in Homestead_current.txt:
>
>
>
> Debug http_handlers.cpp:695: Did not receive valid JSON with a
> 'wildcard_identity' element
>
>
>
> Does it mean scscf need to add "wildcard_identity" into the JSON request?
>
> Is there any configuration need to be done in Sprout?
>
>
>
>
>
>
>
> On Fri, Mar 9, 2018 at 10:54 PM, Anthony Lee <anthonynlee at gmail.com>
> wrote:
>
> I need to provision wildcarded PSI, does clearwater support to do that?
>
> If yes, could you provide an example?
>
>
>
> Thanks
>
>
>
> Anthony
>
>
>
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180317/214828d7/attachment.html>

From david.kyselica at gmail.com  Mon Mar 19 04:55:44 2018
From: david.kyselica at gmail.com (=?UTF-8?Q?D=C3=A1vid_Kyselica?=)
Date: Mon, 19 Mar 2018 09:55:44 +0100
Subject: [Project Clearwater] Clearwater, ellis freezing
Message-ID: <CAJpo2SrM2rViuhGy86MHv9sZ-QVXjS2J89KZzGbeh=UZ0e_1AQ@mail.gmail.com>

Hi,
I`m new to clearwater project. I installed all-in-one AMI into amazon ec2.
After trying to access web interface, I was asked by web browser to reload
the page by this message:Failed to update the server (see detailed
diagnostics in developer console). Please refresh the page.
I`m really confused what it can by caused by. I will be thankful for any
type of tips. It`s a fresh installation with no important configuration
changes.

*log file of homestead: *
19-03-2018 08:42:49.718 UTC Status http_connection_pool.cpp:35: Connection
pool will use calculated $
19-03-2018 08:42:49.837 UTC Status httpconnection.h:58: Configuring HTTP
Connection
19-03-2018 08:42:49.837 UTC Status httpconnection.h:59:   Connection
created for server ec2-52-91-24$
19-03-2018 08:42:49.837 UTC Status main.cpp:973: No HSS configured - using
Homestead-prov
19-03-2018 08:42:49.837 UTC Status a_record_resolver.cpp:29: Created
ARecordResolver
19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:266: Configuring
store connection
19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:267:   Hostname:
127.0.0.1
19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:268:   Port:
9160
19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:296: Configuring
store worker pool
19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:297:   Threads:   10
19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:298:   Max Queue: 0
19-03-2018 08:42:49.839 UTC Error main.cpp:1016: Failed to initialize the
Cassandra store with error$
19-03-2018 08:42:49.839 UTC Status main.cpp:1017: Homestead is shutting down

*log file of ellis:*
19-03-2018 08:31:30.662 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.64ms
19-03-2018 08:31:46.424 UTC INFO web.py:1447: 200 GET / (0.0.0.0) 616.39ms
19-03-2018 08:31:46.753 UTC INFO web.py:1447: 200 GET
/css/bootstrap.min.css (0.0.0.0) 86.74ms
19-03-2018 08:31:46.793 UTC INFO web.py:1447: 200 GET
/css/bootstrap-responsive.css (0.0.0.0) 39.08ms
19-03-2018 08:31:46.834 UTC INFO web.py:1447: 200 GET /css/style.css
(0.0.0.0) 1.16ms
19-03-2018 08:31:46.835 UTC INFO web.py:1447: 200 GET
/css/jquery.miniColors.css (0.0.0.0) 1.03ms
19-03-2018 08:31:46.836 UTC INFO web.py:1447: 200 GET /css/fileuploader.css
(0.0.0.0) 0.94ms
19-03-2018 08:31:47.695 UTC INFO web.py:1447: 200 GET /js/jquery.js
(0.0.0.0) 573.28ms
19-03-2018 08:31:47.740 UTC INFO web.py:1447: 200 GET
/js/jquery.ba-bbq.min.js (0.0.0.0) 42.90ms
19-03-2018 08:31:48.104 UTC INFO web.py:1447: 200 GET /js/fileuploader.js
(0.0.0.0) 364.63ms
19-03-2018 08:31:48.106 UTC INFO web.py:1447: 200 GET
/js/jquery.miniColors.min.js (0.0.0.0) 1.58ms
19-03-2018 08:31:48.107 UTC INFO web.py:1447: 200 GET /js/jquery.cookie.js
(0.0.0.0) 1.02ms
19-03-2018 08:31:48.149 UTC INFO web.py:1447: 200 GET
/js/jquery.total-storage.min.js (0.0.0.0) 41.65ms
19-03-2018 08:31:48.314 UTC INFO web.py:1447: 200 GET /js/bootstrap.min.js
(0.0.0.0) 2.34ms
19-03-2018 08:31:48.354 UTC INFO web.py:1447: 200 GET /js/common.js
(0.0.0.0) 1.91ms
19-03-2018 08:31:48.561 UTC INFO web.py:1447: 200 GET /js/app.js (0.0.0.0)
168.24ms
19-03-2018 08:31:49.012 UTC WARNING web.py:1447: 404 GET
/images/favicon.ico (0.0.0.0) 81.57ms
19-03-2018 08:31:50.286 UTC INFO web.py:1447: 200 GET /accounts/david%
40hmz.sk/numbers/?cb=2b0a26dea4K0 (0.0.0.0) 40.51ms
19-03-2018 08:32:20.995 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.21ms
19-03-2018 08:32:35.655 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
162.92ms
19-03-2018 08:33:21.845 UTC INFO web.py:1447: 200 GET /addressbook.html
(0.0.0.0) 329.69ms
19-03-2018 08:33:22.052 UTC INFO web.py:1447: 200 GET /css/addressbook.css
(0.0.0.0) 1.18ms
19-03-2018 08:33:22.709 UTC INFO web.py:1447: 200 GET /js/backbone.js
(0.0.0.0) 455.22ms
19-03-2018 08:33:22.752 UTC INFO web.py:1447: 200 GET /js/underscore.js
(0.0.0.0) 43.11ms
19-03-2018 08:33:22.874 UTC INFO web.py:1447: 200 GET /js/addressbook.js
(0.0.0.0) 43.49ms
19-03-2018 08:33:23.121 UTC INFO web.py:1447: 200 GET
/js/templates/addressbook-contacts.html (0.0.0.0) 2.48ms
19-03-2018 08:33:23.405 UTC INFO web.py:1447: 200 GET /gab (0.0.0.0) 2.09ms
19-03-2018 08:33:27.915 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.18ms
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180319/0947078a/attachment.html>

From Michael.Duppre at metaswitch.com  Mon Mar 19 05:53:14 2018
From: Michael.Duppre at metaswitch.com (=?utf-8?B?TWljaGFlbCBEdXBwcsOp?=)
Date: Mon, 19 Mar 2018 09:53:14 +0000
Subject: [Project Clearwater] Manual installation of Ellis failed
In-Reply-To: <816025537.2092460.1521295245801@mail.yahoo.com>
References: <1792897644.1986065.1521275724069.ref@mail.yahoo.com>
	<1792897644.1986065.1521275724069@mail.yahoo.com>
	<816025537.2092460.1521295245801@mail.yahoo.com>
Message-ID: <SN1PR02MB1677FCF807D5DF05AF55B84FF5D40@SN1PR02MB1677.namprd02.prod.outlook.com>

Hi Nagu,

Thanks for getting in touch and welcome to the Clearwater community!
You are right, Clearwater should run on Ubuntu 14.04 according to the installation instructions at http://clearwater.readthedocs.io/en/stable/Installation_Instructions.html. Unfortunately Ubuntu 16.04 isn?t supported at the moment. Good luck with your next steps using Clearwater, let us know if you hit any issues!

Kind regards,
Michael

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Nagendra Kumar
Sent: 17 March 2018 14:01
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Manual installation of Ellis failed

Hi Team,
               It am able to install on Ubuntu 14.04.
It doesn't get installed on Ubuntu 16.04 because it try to link with few old packages(e.g. libzmq3) of 14.04 and doesn't find them on 16.04 because 16.04 has libzmq5 installed.

Thanks
-Nagu


On Saturday 17 March 2018, 2:05:24 PM IST, Nagendra Kumar <nagen_kr at yahoo.co.in<mailto:nagen_kr at yahoo.co.in>> wrote:


Hi Clearwater Team,
Greetings !!!
              I am trying to install clearwater on Amazon Web Services EC2 Ubuntu Server 16.04 LTS (Amazon Machine Image (AMI)), I am getting the following error, any help highly appreciated:

ubuntu at ip-10-0-0-128:~$ sudo DEBIAN_FRONTEND=noninteractive apt-get install ellis --yes                                   Reading package lists... Done
Building dependency tree
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 ellis : Depends: clearwater-infrastructure but it is not going to be installed
         Depends: clearwater-nginx but it is not going to be installed
         Depends: clearwater-log-cleanup but it is not going to be installed
         Depends: clearwater-monit but it is not going to be installed
E: Unable to correct problems, you have held broken packages.

Thanks

-Nagu
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180319/60a26f15/attachment.html>

From pushpendra16mnnit at gmail.com  Mon Mar 19 07:46:45 2018
From: pushpendra16mnnit at gmail.com (Pushpendra)
Date: Mon, 19 Mar 2018 17:16:45 +0530
Subject: [Project Clearwater] problem in stress testing
Message-ID: <CAPqjCcF0UGHfrW0sDcJLi75m13NTJL0GEiEehzQ3MnPR-o1DPw@mail.gmail.com>

 Hi,
When I am running single call it giving some errors like:

*[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress <home_domain>1 2*
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
    with open(CALLER_STATS) as f:
IOError: [Errno 2] No such file or directory: '/var/log/clearwater-sip-
stress/11218_caller_stats.log'


*[]ubuntu at stress:~$ cat
/var/log/clearwater-sip-stress/11218_re_reg_stats.log*
StartTime;LastResetTime;CurrentTime;ElapsedTime(P);
ElapsedTime(C);TargetRate;CallRate(P);CallRate(C);
IncomingCall(P);IncomingCall(C);OutgoingCall(P);OutgoingCall(C);
TotalCallCreated;CurrentCall;SuccessfulCall(P);SuccessfulCall(C);FailedCall(
P);FailedCall(C);FailedCannotSendMessage(P);FailedCannotSendMessage(C);
FailedMaxUDPRetrans(P);FailedMaxUDPRetrans(C);FailedTcpConnect(P);
FailedTcpConnect(C);FailedTcpClosed(P);FailedTcpClosed(C);
FailedUnexpectedMessage(P);FailedUnexpectedMessage(C);FailedCallRejected(P);
FailedCallRejected(C);FailedCmdNotSent(P);FailedCmdNotSent(C);
FailedRegexpDoesntMatch(P);FailedRegexpDoesntMatch(C);
FailedRegexpShouldntMatch(P);FailedRegexpShouldntMatch(C);
FailedRegexpHdrNotFound(P);FailedRegexpHdrNotFound(C);
FailedOutboundCongestion(P);FailedOutboundCongestion(C);
FailedTimeoutOnRecv(P);FailedTimeoutOnRecv(C);FailedTimeoutOnSend(P);
FailedTimeoutOnSend(C);OutOfCallMsgs(P);OutOfCallMsgs(C);DeadCallMsgs(
P);DeadCallMsgs(C);Retransmissions(P);Retransmissions(C);
AutoAnswered(P);AutoAnswered(C);Warnings(P);Warnings(C);
FatalErrors(P);FatalErrors(C);WatchdogMajor(P);WatchdogMajor(C);
WatchdogMinor(P);WatchdogMinor(C);ResponseTime1(P);ResponseTime1(C);
ResponseTime1StDev(P);ResponseTime1StDev(C);CallLength(P);CallLength(C);
CallLengthStDev(P);CallLengthStDev(C);ResponseTimeRepartition1;
ResponseTimeRepartition1_<2;ResponseTimeRepartition1_<10;
ResponseTimeRepartition1_<20;ResponseTimeRepartition1_<50;
ResponseTimeRepartition1_<100;ResponseTimeRepartition1_<200;
ResponseTimeRepartition1_<500;ResponseTimeRepartition1_<
1000;ResponseTimeRepartition1_<2000;ResponseTimeRepartition1_>=2000;
2018-03-17      06:43:11.763066 1521249191.763066;2018-03-17
06:43:11.763066 1521249191.763066;2018-03-17    06:43:11.770581
1521249191.770581;00:00:00;00:00:00;0.000555556;0;0;0;0;0;0;
0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;
0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;00:00:00:000000;00:00:
00:000000;00:00:00:000000;00:00:00:000000;00:00:00:000000;
00:00:00:000000;00:00:00:000000;00:00:00:000000;;0;0;0;0;0;0;0;0;0;0;
2018-03-17      06:43:11.763066 1521249191.763066;2018-03-17
06:43:11.770864 1521249191.770864;2018-03-17    06:43:11.772711
1521249191.772711;00:00:00;00:00:00;0.000555556;0;0;0;0;0;0;
0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;
0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;00:00:00:000000;00:00:
00:000000;00:00:00:000000;00:00:00:000000;00:00:00:000000;
00:00:00:000000;00:00:00:000000;00:00:00:000000;;0;0;0;0;0;0;0;0;0;0;
2018-03-17      06:43:11.763066 1521249191.763066;2018-03-17
06:43:11.772796 1521249191.772796;2018-03-17    06:43:11.772860
1521249191.772860;00:00:00;00:00:00;0.000555556;0;0;0;0;0;0;
0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;
0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;00:00:00:000000;00:00:
00:000000;00:00:00:000000;00:00:00:000000;00:00:00:000000;
00:00:00:000000;00:00:00:000000;00:00:00:000000;;0;0;0;0;0;0;0;0;0;0;



*[sprout]ubuntu at sprout:/var/log/sprout$ tail -30 sprout_current.txt*
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:183:
Request latency so far = 102us
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug pjsip: sip_endpoint.c
Distributing rdata to modules: Request msg OPTIONS/cseq=370860
(rdata0x7f7548081478)
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug uri_classifier.cpp:139:
home domain: false, local_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug uri_classifier.cpp:172:
Classified URI as 3
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug pjsip:       endpoint
Response msg 200/OPTIONS/cseq=370860 (tdta0x7f7548382670) created
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Verbose
common_sip_processing.cpp:103: TX 282 bytes Response msg
200/OPTIONS/cseq=370860 (tdta0x7f7548382670) to TCP 10.224.61.22:51202:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=51202;received=10.224.61.22;branch=
z9hG4bK-370860
Call-ID: poll-sip-370860
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=370860
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-370860
CSeq: 370860 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug pjsip: tdta0x7f754838
Destroying txdata Response msg 200/OPTIONS/cseq=370860 (tdta0x7f7548382670)
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:270:
Worker thread completed processing message 0x7f7548081478
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:284:
Request latency = 278us
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
event_statistic_accumulator.cpp:32: Accumulate 278 for 0xf627a8
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
event_statistic_accumulator.cpp:32: Accumulate 278 for 0xf62820
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug load_monitor.cpp:341: Not
recalculating rate as we haven't processed 20 requests yet (only 18).
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug utils.cpp:878: Removed
IOHook 0x7f75a6face30 to stack. There are now 0 hooks
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:158:
Attempting to process queue element
17-03-2018 01:18:53.122 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842
TCP connection closed
17-03-2018 01:18:53.122 UTC [7f754feff700] Debug connection_tracker.cpp:67:
Connection 0x7f75484260d8 has been destroyed
17-03-2018 01:18:53.122 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842
TCP transport destroyed with reason 70016: End of file (PJ_EEOF)



*[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt*
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-369729
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=369729
Call-ID: poll-sip-369729
CSeq: 369729 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug uri_classifier.cpp:139:
home domain: false, local_to_node: true, is_gruu: false,
enforce_user_phone: false, pref$
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug uri_classifier.cpp:172:
Classified URI as 3
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug
common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug thread_dispatcher.cpp:554:
Recieved message 0x7f7548427c20 on worker thread
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug thread_dispatcher.cpp:571:
Admitted request 0x7f7548427c20 on worker thread
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug thread_dispatcher.cpp:606:
Incoming message 0x7f7548427c20 cloned to 0x7f7548467bd8
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug thread_dispatcher.cpp:625:
Queuing cloned received message 0x7f7548467bd8 for worker threads with
priority 15
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug
event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66738
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug
event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66780
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug utils.cpp:872: Added
IOHook 0x7f75a8fb0e30 to stack. There are now 1 hooks
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug thread_dispatcher.cpp:178:
Worker thread dequeue message 0x7f7548467bd8
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug thread_dispatcher.cpp:183:
Request latency so far = 57us
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug pjsip: sip_endpoint.c
Distributing rdata to modules: Request msg OPTIONS/cseq=369729
(rdata0x7f7548467bd8)
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug uri_classifier.cpp:139:
home domain: false, local_to_node: true, is_gruu: false,
enforce_user_phone: false, pref$
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug uri_classifier.cpp:172:
Classified URI as 3
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug pjsip:       endpoint
Response msg 200/OPTIONS/cseq=369729 (tdta0x7f75e82f5370) created
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Verbose
common_sip_processing.cpp:103: TX 282 bytes Response msg
200/OPTIONS/cseq=369729 (tdta0x7f75e82f5370) to TCP 1$
--start msg--





when i ran 10, it showing error like:

[*]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress <home_domain> 10 2*
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
    call_success_rate = 100 * float(row['SuccessfulCall(C)']) /
float(row['TotalCallCreated'])
ZeroDivisionError: float division by zero


PFA the sprout_current log file. Please guide me the solution, I am newbie
to clearwater :-)


Thanks and Regards,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180319/507feb07/attachment.html>
-------------- next part --------------
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373106
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373106
Call-ID: poll-sip-373106
CSeq: 373106 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548073820 on worker thread
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548073820 on worker thread
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548073820 cloned to 0x7f754842f9c8
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f754842f9c8 for worker threads with priority 15
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug utils.cpp:872: Added IOHook 0x7f75ae7bbe30 to stack. There are now 1 hooks
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f754842f9c8
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug thread_dispatcher.cpp:183: Request latency so far = 108us
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373106 (rdata0x7f754842f9c8)
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373106 (tdta0x7f75e02378d0) created
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373106 (tdta0x7f75e02378d0) to TCP 10.224.61.22:36738:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=36738;received=10.224.61.22;branch=z9hG4bK-373106
Call-ID: poll-sip-373106
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373106
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373106
CSeq: 373106 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug pjsip: tdta0x7f75e023 Destroying txdata Response msg 200/OPTIONS/cseq=373106 (tdta0x7f75e02378d0)
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f754842f9c8
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug thread_dispatcher.cpp:284: Request latency = 273us
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug event_statistic_accumulator.cpp:32: Accumulate 273 for 0xf62778
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug event_statistic_accumulator.cpp:32: Accumulate 273 for 0xf62820
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 13).
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug utils.cpp:878: Removed IOHook 0x7f75ae7bbe30 to stack. There are now 0 hooks
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:19.842 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:56:19.842 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f75480734e8 has been destroyed
17-03-2018 01:56:19.842 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:20.008 UTC [7f75ee6a5700] Status alarm.cpp:244: Reraising all alarms with a known state
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1001.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1005.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1011.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1012.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1013.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1004.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1002.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1009.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1010.1 alarm
17-03-2018 01:56:25.891 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58522, sock=690
17-03-2018 01:56:25.891 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:25.891 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373114 (rdata0x7f7548073820)
17-03-2018 01:56:25.913 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373114 (rdata0x7f7548073820) from TCP 10.224.61.22:58522:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373114
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373114
Call-ID: poll-sip-373114
CSeq: 373114 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548073820 on worker thread
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548073820 on worker thread
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548073820 cloned to 0x7f754842f9c8
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f754842f9c8 for worker threads with priority 15
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug utils.cpp:872: Added IOHook 0x7f75aefbce30 to stack. There are now 1 hooks
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f754842f9c8
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug thread_dispatcher.cpp:183: Request latency so far = 62us
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373114 (rdata0x7f754842f9c8)
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373114 (tdta0x7f75d033ba90) created
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373114 (tdta0x7f75d033ba90) to TCP 10.224.61.22:58522:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58522;received=10.224.61.22;branch=z9hG4bK-373114
Call-ID: poll-sip-373114
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373114
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373114
CSeq: 373114 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug pjsip: tdta0x7f75d033 Destroying txdata Response msg 200/OPTIONS/cseq=373114 (tdta0x7f75d033ba90)
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f754842f9c8
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug thread_dispatcher.cpp:284: Request latency = 127us
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug event_statistic_accumulator.cpp:32: Accumulate 127 for 0xf62778
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug event_statistic_accumulator.cpp:32: Accumulate 127 for 0xf62820
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 14).
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug utils.cpp:878: Removed IOHook 0x7f75aefbce30 to stack. There are now 0 hooks
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:25.935 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:56:25.935 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:56:27.915 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:56:27.915 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f75480734e8 has been destroyed
17-03-2018 01:56:27.915 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:27.926 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:36810, sock=690
17-03-2018 01:56:27.926 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:27.926 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373116 (rdata0x7f7548073820)
17-03-2018 01:56:27.926 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373116 (rdata0x7f7548073820) from TCP 10.224.61.22:36810:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373116
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373116
Call-ID: poll-sip-373116
CSeq: 373116 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548073820 on worker thread
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548073820 on worker thread
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548073820 cloned to 0x7f754842f9c8
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f754842f9c8 for worker threads with priority 15
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug utils.cpp:872: Added IOHook 0x7f75be7dbe30 to stack. There are now 1 hooks
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f754842f9c8
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug thread_dispatcher.cpp:183: Request latency so far = 78us
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373116 (rdata0x7f754842f9c8)
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373116 (tdta0x7f753c24d450) created
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373116 (tdta0x7f753c24d450) to TCP 10.224.61.22:36810:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=36810;received=10.224.61.22;branch=z9hG4bK-373116
Call-ID: poll-sip-373116
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373116
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373116
CSeq: 373116 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug pjsip: tdta0x7f753c24 Destroying txdata Response msg 200/OPTIONS/cseq=373116 (tdta0x7f753c24d450)
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f754842f9c8
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug thread_dispatcher.cpp:284: Request latency = 186us
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug event_statistic_accumulator.cpp:32: Accumulate 186 for 0xf62778
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug event_statistic_accumulator.cpp:32: Accumulate 186 for 0xf62820
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 15).
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug utils.cpp:878: Removed IOHook 0x7f75be7dbe30 to stack. There are now 0 hooks
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:29.928 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:56:29.928 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f75480734e8 has been destroyed
17-03-2018 01:56:29.928 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:30.428 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:56:35.964 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58584, sock=690
17-03-2018 01:56:35.964 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:35.964 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373124 (rdata0x7f7548073820)
17-03-2018 01:56:35.990 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373124 (rdata0x7f7548073820) from TCP 10.224.61.22:58584:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373124
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373124
Call-ID: poll-sip-373124
CSeq: 373124 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548073820 on worker thread
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548073820 on worker thread
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548073820 cloned to 0x7f754842f9c8
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f754842f9c8 for worker threads with priority 15
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug utils.cpp:872: Added IOHook 0x7f75adfbae30 to stack. There are now 1 hooks
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f754842f9c8
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug thread_dispatcher.cpp:183: Request latency so far = 55us
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373124 (rdata0x7f754842f9c8)
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373124 (tdta0x7f75e82b6d00) created
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373124 (tdta0x7f75e82b6d00) to TCP 10.224.61.22:58584:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58584;received=10.224.61.22;branch=z9hG4bK-373124
Call-ID: poll-sip-373124
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373124
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373124
CSeq: 373124 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug pjsip: tdta0x7f75e82b Destroying txdata Response msg 200/OPTIONS/cseq=373124 (tdta0x7f75e82b6d00)
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f754842f9c8
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug thread_dispatcher.cpp:284: Request latency = 127us
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug event_statistic_accumulator.cpp:32: Accumulate 127 for 0xf62778
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug event_statistic_accumulator.cpp:32: Accumulate 127 for 0xf62820
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 16).
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug utils.cpp:878: Removed IOHook 0x7f75adfbae30 to stack. There are now 0 hooks
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:36.008 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:56:36.008 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:56:37.990 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:56:37.990 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f75480734e8 has been destroyed
17-03-2018 01:56:37.991 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:38.003 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:36876, sock=690
17-03-2018 01:56:38.003 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:38.003 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373126 (rdata0x7f7548073820)
17-03-2018 01:56:38.003 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373126 (rdata0x7f7548073820) from TCP 10.224.61.22:36876:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373126
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373126
Call-ID: poll-sip-373126
CSeq: 373126 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548073820 on worker thread
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548073820 on worker thread
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548073820 cloned to 0x7f754842f9c8
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f754842f9c8 for worker threads with priority 15
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug utils.cpp:872: Added IOHook 0x7f75b07bfe30 to stack. There are now 1 hooks
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f754842f9c8
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug thread_dispatcher.cpp:183: Request latency so far = 92us
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373126 (rdata0x7f754842f9c8)
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373126 (tdta0x7f7544235830) created
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373126 (tdta0x7f7544235830) to TCP 10.224.61.22:36876:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=36876;received=10.224.61.22;branch=z9hG4bK-373126
Call-ID: poll-sip-373126
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373126
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373126
CSeq: 373126 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug pjsip: tdta0x7f754423 Destroying txdata Response msg 200/OPTIONS/cseq=373126 (tdta0x7f7544235830)
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f754842f9c8
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug thread_dispatcher.cpp:284: Request latency = 203us
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug event_statistic_accumulator.cpp:32: Accumulate 203 for 0xf62778
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug event_statistic_accumulator.cpp:32: Accumulate 203 for 0xf62820
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 17).
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug utils.cpp:878: Removed IOHook 0x7f75b07bfe30 to stack. There are now 0 hooks
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:40.004 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:56:40.004 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f75480734e8 has been destroyed
17-03-2018 01:56:40.004 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:43.622 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842 TCP transport destroyed normally
17-03-2018 01:56:43.795 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:45605, sock=665
17-03-2018 01:56:43.795 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:43.795 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:56:45.440 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:56:45.623 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed normally
17-03-2018 01:56:45.796 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:34166, sock=478
17-03-2018 01:56:45.796 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:45.796 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:56:46.035 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58646, sock=690
17-03-2018 01:56:46.035 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:46.035 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP server transport created
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373135 (rdata0x7f7548046f20)
17-03-2018 01:56:46.075 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373135 (rdata0x7f7548046f20) from TCP 10.224.61.22:58646:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373135
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373135
Call-ID: poll-sip-373135
CSeq: 373135 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548046f20 on worker thread
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548046f20 on worker thread
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548046f20 cloned to 0x7f7548422128
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug utils.cpp:872: Added IOHook 0x7f75acfb8e30 to stack. There are now 1 hooks
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug thread_dispatcher.cpp:183: Request latency so far = 55us
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373135 (rdata0x7f7548422128)
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373135 (tdta0x7f753c24d450) created
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373135 (tdta0x7f753c24d450) to TCP 10.224.61.22:58646:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58646;received=10.224.61.22;branch=z9hG4bK-373135
Call-ID: poll-sip-373135
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373135
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373135
CSeq: 373135 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug pjsip: tdta0x7f753c24 Destroying txdata Response msg 200/OPTIONS/cseq=373135 (tdta0x7f753c24d450)
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug thread_dispatcher.cpp:284: Request latency = 116us
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug event_statistic_accumulator.cpp:32: Accumulate 116 for 0xf62778
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug event_statistic_accumulator.cpp:32: Accumulate 116 for 0xf62820
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 18).
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug utils.cpp:878: Removed IOHook 0x7f75acfb8e30 to stack. There are now 0 hooks
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:46.084 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:56:46.084 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:56:48.077 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP connection closed
17-03-2018 01:56:48.077 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548046be8 has been destroyed
17-03-2018 01:56:48.077 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:48.083 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:36938, sock=690
17-03-2018 01:56:48.083 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:48.083 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP server transport created
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373137 (rdata0x7f7548046f20)
17-03-2018 01:56:48.083 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373137 (rdata0x7f7548046f20) from TCP 10.224.61.22:36938:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373137
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373137
Call-ID: poll-sip-373137
CSeq: 373137 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548046f20 on worker thread
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548046f20 on worker thread
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548046f20 cloned to 0x7f7548422128
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug utils.cpp:872: Added IOHook 0x7f75ac7b7e30 to stack. There are now 1 hooks
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug thread_dispatcher.cpp:183: Request latency so far = 69us
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373137 (rdata0x7f7548422128)
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373137 (tdta0x7f75382ac890) created
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373137 (tdta0x7f75382ac890) to TCP 10.224.61.22:36938:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=36938;received=10.224.61.22;branch=z9hG4bK-373137
Call-ID: poll-sip-373137
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373137
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373137
CSeq: 373137 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug pjsip: tdta0x7f75382a Destroying txdata Response msg 200/OPTIONS/cseq=373137 (tdta0x7f75382ac890)
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug thread_dispatcher.cpp:284: Request latency = 172us
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug event_statistic_accumulator.cpp:32: Accumulate 172 for 0xf62778
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug event_statistic_accumulator.cpp:32: Accumulate 172 for 0xf62820
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 19).
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug utils.cpp:878: Removed IOHook 0x7f75ac7b7e30 to stack. There are now 0 hooks
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:244: Reraising all alarms with a known state
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1001.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1005.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1011.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1012.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1013.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1004.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1002.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1009.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1010.1 alarm
17-03-2018 01:56:50.084 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP connection closed
17-03-2018 01:56:50.084 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548046be8 has been destroyed
17-03-2018 01:56:50.084 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:55.799 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:45930, sock=690
17-03-2018 01:56:55.799 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:55.799 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP server transport created
17-03-2018 01:56:55.799 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:56:55.799 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:56:55.799 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:56.110 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58706, sock=712
17-03-2018 01:56:56.110 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:56.110 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373145 (rdata0x7f754803fa40)
17-03-2018 01:56:56.115 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373145 (rdata0x7f754803fa40) from TCP 10.224.61.22:58706:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373145
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373145
Call-ID: poll-sip-373145
CSeq: 373145 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug utils.cpp:872: Added IOHook 0x7f7592f84e30 to stack. There are now 1 hooks
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug thread_dispatcher.cpp:183: Request latency so far = 56us
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373145 (rdata0x7f7548422128)
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373145 (tdta0x7f75403a7370) created
17-03-2018 01:56:56.115 UTC [7f7592f85700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373145 (tdta0x7f75403a7370) to TCP 10.224.61.22:58706:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58706;received=10.224.61.22;branch=z9hG4bK-373145
Call-ID: poll-sip-373145
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373145
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373145
CSeq: 373145 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug pjsip: tdta0x7f75403a Destroying txdata Response msg 200/OPTIONS/cseq=373145 (tdta0x7f75403a7370)
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug thread_dispatcher.cpp:284: Request latency = 127us
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug event_statistic_accumulator.cpp:32: Accumulate 127 for 0xf62778
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug event_statistic_accumulator.cpp:32: Accumulate 127 for 0xf62820
17-03-2018 01:56:56.115 UTC [7f7592f85700] Info load_monitor.cpp:217: Rate adjustment calculation inputs: err -0.981600, smoothed latency 184, target latency 10000
17-03-2018 01:56:56.115 UTC [7f7592f85700] Info load_monitor.cpp:302: Maximum incoming request rate/second unchanged at 2000.000000 (current request rate is 0.550000 requests/sec, minimum threshold for a change is 1000.000000 requests/sec).
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug utils.cpp:878: Removed IOHook 0x7f7592f84e30 to stack. There are now 0 hooks
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:56.159 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:56:56.159 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:56:58.115 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:56:58.116 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:56:58.116 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:58.127 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:36998, sock=712
17-03-2018 01:56:58.127 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:58.127 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:56:58.127 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373147 (rdata0x7f754803fa40)
17-03-2018 01:56:58.127 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373147 (rdata0x7f754803fa40) from TCP 10.224.61.22:36998:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373147
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373147
Call-ID: poll-sip-373147
CSeq: 373147 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:58.127 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:58.127 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug utils.cpp:872: Added IOHook 0x7f758ef7ce30 to stack. There are now 1 hooks
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug thread_dispatcher.cpp:183: Request latency so far = 90us
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373147 (rdata0x7f7548422128)
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373147 (tdta0x7f75e02378d0) created
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373147 (tdta0x7f75e02378d0) to TCP 10.224.61.22:36998:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=36998;received=10.224.61.22;branch=z9hG4bK-373147
Call-ID: poll-sip-373147
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373147
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373147
CSeq: 373147 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug pjsip: tdta0x7f75e023 Destroying txdata Response msg 200/OPTIONS/cseq=373147 (tdta0x7f75e02378d0)
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug thread_dispatcher.cpp:284: Request latency = 214us
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug event_statistic_accumulator.cpp:32: Accumulate 214 for 0xf62778
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug event_statistic_accumulator.cpp:32: Accumulate 214 for 0xf62820
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 1).
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug utils.cpp:878: Removed IOHook 0x7f758ef7ce30 to stack. There are now 0 hooks
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:00.129 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:00.129 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:00.129 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:00.455 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:57:06.192 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58770, sock=717
17-03-2018 01:57:06.192 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:06.192 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373155 (rdata0x7f754803fa40)
17-03-2018 01:57:06.224 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373155 (rdata0x7f754803fa40) from TCP 10.224.61.22:58770:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373155
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373155
Call-ID: poll-sip-373155
CSeq: 373155 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug utils.cpp:872: Added IOHook 0x7f75aafb4e30 to stack. There are now 1 hooks
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug thread_dispatcher.cpp:183: Request latency so far = 61us
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373155 (rdata0x7f7548422128)
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373155 (tdta0x7f7548423090) created
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373155 (tdta0x7f7548423090) to TCP 10.224.61.22:58770:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58770;received=10.224.61.22;branch=z9hG4bK-373155
Call-ID: poll-sip-373155
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373155
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373155
CSeq: 373155 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug pjsip: tdta0x7f754842 Destroying txdata Response msg 200/OPTIONS/cseq=373155 (tdta0x7f7548423090)
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug thread_dispatcher.cpp:284: Request latency = 133us
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug event_statistic_accumulator.cpp:32: Accumulate 133 for 0xf62778
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug event_statistic_accumulator.cpp:32: Accumulate 133 for 0xf62820
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 2).
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug utils.cpp:878: Removed IOHook 0x7f75aafb4e30 to stack. There are now 0 hooks
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:06.236 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:57:06.236 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:57:08.225 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:08.225 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:08.225 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:08.237 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37060, sock=712
17-03-2018 01:57:08.237 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:08.237 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373157 (rdata0x7f754803fa40)
17-03-2018 01:57:08.237 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373157 (rdata0x7f754803fa40) from TCP 10.224.61.22:37060:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373157
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373157
Call-ID: poll-sip-373157
CSeq: 373157 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug utils.cpp:872: Added IOHook 0x7f758e77be30 to stack. There are now 1 hooks
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug thread_dispatcher.cpp:183: Request latency so far = 80us
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373157 (rdata0x7f7548422128)
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373157 (tdta0x7f7544235830) created
17-03-2018 01:57:08.237 UTC [7f758e77c700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373157 (tdta0x7f7544235830) to TCP 10.224.61.22:37060:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37060;received=10.224.61.22;branch=z9hG4bK-373157
Call-ID: poll-sip-373157
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373157
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373157
CSeq: 373157 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug pjsip: tdta0x7f754423 Destroying txdata Response msg 200/OPTIONS/cseq=373157 (tdta0x7f7544235830)
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug thread_dispatcher.cpp:284: Request latency = 202us
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug event_statistic_accumulator.cpp:32: Accumulate 202 for 0xf62778
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug event_statistic_accumulator.cpp:32: Accumulate 202 for 0xf62820
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 3).
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug utils.cpp:878: Removed IOHook 0x7f758e77be30 to stack. There are now 0 hooks
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:10.238 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:10.239 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:10.239 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:15.470 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:57:16.260 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58830, sock=712
17-03-2018 01:57:16.260 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:16.260 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373165 (rdata0x7f754803fa40)
17-03-2018 01:57:16.263 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373165 (rdata0x7f754803fa40) from TCP 10.224.61.22:58830:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373165
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373165
Call-ID: poll-sip-373165
CSeq: 373165 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug utils.cpp:872: Added IOHook 0x7f759a793e30 to stack. There are now 1 hooks
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug thread_dispatcher.cpp:183: Request latency so far = 53us
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373165 (rdata0x7f7548422128)
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373165 (tdta0x7f7548423090) created
17-03-2018 01:57:16.263 UTC [7f759a794700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373165 (tdta0x7f7548423090) to TCP 10.224.61.22:58830:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58830;received=10.224.61.22;branch=z9hG4bK-373165
Call-ID: poll-sip-373165
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373165
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373165
CSeq: 373165 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug pjsip: tdta0x7f754842 Destroying txdata Response msg 200/OPTIONS/cseq=373165 (tdta0x7f7548423090)
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug thread_dispatcher.cpp:284: Request latency = 129us
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0xf62778
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0xf62820
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 4).
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug utils.cpp:878: Removed IOHook 0x7f759a793e30 to stack. There are now 0 hooks
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:16.304 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:57:16.304 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:57:18.264 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:18.264 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:18.264 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:18.275 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37122, sock=712
17-03-2018 01:57:18.275 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:18.275 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373167 (rdata0x7f754803fa40)
17-03-2018 01:57:18.276 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373167 (rdata0x7f754803fa40) from TCP 10.224.61.22:37122:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373167
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373167
Call-ID: poll-sip-373167
CSeq: 373167 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug utils.cpp:872: Added IOHook 0x7f7596f8ce30 to stack. There are now 1 hooks
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug thread_dispatcher.cpp:183: Request latency so far = 95us
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373167 (rdata0x7f7548422128)
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373167 (tdta0x7f7544235830) created
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373167 (tdta0x7f7544235830) to TCP 10.224.61.22:37122:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37122;received=10.224.61.22;branch=z9hG4bK-373167
Call-ID: poll-sip-373167
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373167
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373167
CSeq: 373167 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug pjsip: tdta0x7f754423 Destroying txdata Response msg 200/OPTIONS/cseq=373167 (tdta0x7f7544235830)
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug thread_dispatcher.cpp:284: Request latency = 236us
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug event_statistic_accumulator.cpp:32: Accumulate 236 for 0xf62778
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug event_statistic_accumulator.cpp:32: Accumulate 236 for 0xf62820
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 5).
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug utils.cpp:878: Removed IOHook 0x7f7596f8ce30 to stack. There are now 0 hooks
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:244: Reraising all alarms with a known state
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1001.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1005.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1011.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1012.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1013.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1004.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1002.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1009.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1010.1 alarm
17-03-2018 01:57:20.277 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:20.277 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:20.277 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:26.334 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58890, sock=712
17-03-2018 01:57:26.334 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:26.334 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:26.367 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373175 (rdata0x7f754803fa40)
17-03-2018 01:57:26.367 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373175 (rdata0x7f754803fa40) from TCP 10.224.61.22:58890:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373175
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373175
Call-ID: poll-sip-373175
CSeq: 373175 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:26.367 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:26.367 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:26.367 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:26.367 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:26.367 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:26.368 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:57:26.368 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:57:26.368 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:26.368 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug utils.cpp:872: Added IOHook 0x7f7594787e30 to stack. There are now 1 hooks
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug thread_dispatcher.cpp:183: Request latency so far = 106us
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373175 (rdata0x7f7548422128)
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373175 (tdta0x7f75382ac890) created
17-03-2018 01:57:26.368 UTC [7f7594788700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373175 (tdta0x7f75382ac890) to TCP 10.224.61.22:58890:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58890;received=10.224.61.22;branch=z9hG4bK-373175
Call-ID: poll-sip-373175
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373175
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373175
CSeq: 373175 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug pjsip: tdta0x7f75382a Destroying txdata Response msg 200/OPTIONS/cseq=373175 (tdta0x7f75382ac890)
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug thread_dispatcher.cpp:284: Request latency = 211us
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug event_statistic_accumulator.cpp:32: Accumulate 211 for 0xf62778
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug event_statistic_accumulator.cpp:32: Accumulate 211 for 0xf62820
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 6).
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug utils.cpp:878: Removed IOHook 0x7f7594787e30 to stack. There are now 0 hooks
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:26.382 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:57:26.382 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:57:26.807 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:33397, sock=717
17-03-2018 01:57:26.807 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:26.807 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842 TCP server transport created
17-03-2018 01:57:26.807 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 TCP connection closed
17-03-2018 01:57:26.807 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:27.807 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:40859, sock=722
17-03-2018 01:57:27.808 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:27.808 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 TCP server transport created
17-03-2018 01:57:27.808 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 TCP connection closed
17-03-2018 01:57:27.808 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:28.368 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:28.368 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:28.368 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:28.380 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37182, sock=705
17-03-2018 01:57:28.380 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:28.380 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373177 (rdata0x7f754803fa40)
17-03-2018 01:57:28.380 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373177 (rdata0x7f754803fa40) from TCP 10.224.61.22:37182:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373177
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373177
Call-ID: poll-sip-373177
CSeq: 373177 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f75484497a8
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f75484497a8 for worker threads with priority 15
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug utils.cpp:872: Added IOHook 0x7f758df7ae30 to stack. There are now 1 hooks
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f75484497a8
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug thread_dispatcher.cpp:183: Request latency so far = 78us
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373177 (rdata0x7f75484497a8)
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373177 (tdta0x7f75d033ba90) created
17-03-2018 01:57:28.380 UTC [7f758df7b700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373177 (tdta0x7f75d033ba90) to TCP 10.224.61.22:37182:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37182;received=10.224.61.22;branch=z9hG4bK-373177
Call-ID: poll-sip-373177
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373177
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373177
CSeq: 373177 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug pjsip: tdta0x7f75d033 Destroying txdata Response msg 200/OPTIONS/cseq=373177 (tdta0x7f75d033ba90)
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f75484497a8
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug thread_dispatcher.cpp:284: Request latency = 197us
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug event_statistic_accumulator.cpp:32: Accumulate 197 for 0xf62778
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug event_statistic_accumulator.cpp:32: Accumulate 197 for 0xf62820
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 7).
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug utils.cpp:878: Removed IOHook 0x7f758df7ae30 to stack. There are now 0 hooks
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:30.381 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:30.381 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:30.381 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:30.483 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:57:36.410 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58954, sock=712
17-03-2018 01:57:36.410 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:36.410 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373185 (rdata0x7f754803fa40)
17-03-2018 01:57:36.431 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373185 (rdata0x7f754803fa40) from TCP 10.224.61.22:58954:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373185
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373185
Call-ID: poll-sip-373185
CSeq: 373185 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f75484497a8
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f75484497a8 for worker threads with priority 15
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug utils.cpp:872: Added IOHook 0x7f75ad7b9e30 to stack. There are now 1 hooks
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f75484497a8
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug thread_dispatcher.cpp:183: Request latency so far = 56us
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373185 (rdata0x7f75484497a8)
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373185 (tdta0x21d5100) created
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373185 (tdta0x21d5100) to TCP 10.224.61.22:58954:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58954;received=10.224.61.22;branch=z9hG4bK-373185
Call-ID: poll-sip-373185
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373185
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373185
CSeq: 373185 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug pjsip:  tdta0x21d5100 Destroying txdata Response msg 200/OPTIONS/cseq=373185 (tdta0x21d5100)
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f75484497a8
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug thread_dispatcher.cpp:284: Request latency = 118us
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug event_statistic_accumulator.cpp:32: Accumulate 118 for 0xf62778
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug event_statistic_accumulator.cpp:32: Accumulate 118 for 0xf62820
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 8).
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug utils.cpp:878: Removed IOHook 0x7f75ad7b9e30 to stack. There are now 0 hooks
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:36.452 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:57:36.452 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:57:38.432 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:38.432 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:38.432 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:38.438 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37244, sock=705
17-03-2018 01:57:38.438 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:38.438 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373187 (rdata0x7f754803fa40)
17-03-2018 01:57:38.438 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373187 (rdata0x7f754803fa40) from TCP 10.224.61.22:37244:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373187
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373187
Call-ID: poll-sip-373187
CSeq: 373187 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f75484497a8
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f75484497a8 for worker threads with priority 15
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:38.438 UTC [7f75ab7b6700] Debug utils.cpp:872: Added IOHook 0x7f75ab7b5e30 to stack. There are now 1 hooks
17-03-2018 01:57:38.438 UTC [7f75ab7b6700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f75484497a8
17-03-2018 01:57:38.438 UTC [7f75ab7b6700] Debug thread_dispatcher.cpp:183: Request latency so far = 60us
17-03-2018 01:57:38.438 UTC [7f75ab7b6700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373187 (rdata0x7f75484497a8)
17-03-2018 01:57:38.438 UTC [7f75ab7b6700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373187 (tdta0x7f75403a7370) created
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373187 (tdta0x7f75403a7370) to TCP 10.224.61.22:37244:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37244;received=10.224.61.22;branch=z9hG4bK-373187
Call-ID: poll-sip-373187
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373187
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373187
CSeq: 373187 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug pjsip: tdta0x7f75403a Destroying txdata Response msg 200/OPTIONS/cseq=373187 (tdta0x7f75403a7370)
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f75484497a8
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug thread_dispatcher.cpp:284: Request latency = 136us
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug event_statistic_accumulator.cpp:32: Accumulate 136 for 0xf62778
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug event_statistic_accumulator.cpp:32: Accumulate 136 for 0xf62820
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 9).
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug utils.cpp:878: Removed IOHook 0x7f75ab7b5e30 to stack. There are now 0 hooks
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:40.439 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:40.439 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:40.440 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:40.637 UTC [7f754feff700] Verbose pjsip: tcps0x7f75480b TCP transport destroyed normally
17-03-2018 01:57:40.810 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:41810, sock=508
17-03-2018 01:57:40.810 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:40.810 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:41.811 UTC [7f754feff700] Verbose pjsip: tcps0x7f754839 TCP connection closed
17-03-2018 01:57:41.811 UTC [7f754feff700] Verbose pjsip: tcps0x7f754839 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:41.811 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:37220, sock=316
17-03-2018 01:57:41.811 UTC [7f754feff700] Verbose pjsip: tcps0x7f754839 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:41.811 UTC [7f754feff700] Verbose pjsip: tcps0x7f754839 TCP server transport created
17-03-2018 01:57:42.812 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:39283, sock=705
17-03-2018 01:57:42.812 UTC [7f754feff700] Verbose pjsip: tcps0x7f75480b tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:42.812 UTC [7f754feff700] Verbose pjsip: tcps0x7f75480b TCP server transport created
17-03-2018 01:57:42.812 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP connection closed
17-03-2018 01:57:42.812 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:45.493 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:57:46.480 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:59014, sock=291
17-03-2018 01:57:46.480 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:46.480 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP server transport created
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373195 (rdata0x7f7548023260)
17-03-2018 01:57:46.496 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373195 (rdata0x7f7548023260) from TCP 10.224.61.22:59014:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373195
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373195
Call-ID: poll-sip-373195
CSeq: 373195 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548023260 on worker thread
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548023260 on worker thread
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548023260 cloned to 0x7f7548463ba8
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548463ba8 for worker threads with priority 15
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug utils.cpp:872: Added IOHook 0x7f7588f70e30 to stack. There are now 1 hooks
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548463ba8
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug thread_dispatcher.cpp:183: Request latency so far = 78us
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373195 (rdata0x7f7548463ba8)
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373195 (tdta0x7f753c24d450) created
17-03-2018 01:57:46.496 UTC [7f7588f71700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373195 (tdta0x7f753c24d450) to TCP 10.224.61.22:59014:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=59014;received=10.224.61.22;branch=z9hG4bK-373195
Call-ID: poll-sip-373195
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373195
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373195
CSeq: 373195 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug pjsip: tdta0x7f753c24 Destroying txdata Response msg 200/OPTIONS/cseq=373195 (tdta0x7f753c24d450)
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548463ba8
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug thread_dispatcher.cpp:284: Request latency = 143us
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug event_statistic_accumulator.cpp:32: Accumulate 143 for 0xf62778
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug event_statistic_accumulator.cpp:32: Accumulate 143 for 0xf62820
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 10).
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug utils.cpp:878: Removed IOHook 0x7f7588f70e30 to stack. There are now 0 hooks
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:46.529 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:57:46.529 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:57:48.496 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP connection closed
17-03-2018 01:57:48.496 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548022f28 has been destroyed
17-03-2018 01:57:48.496 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:48.508 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37306, sock=291
17-03-2018 01:57:48.508 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:48.508 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP server transport created
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373197 (rdata0x7f7548023260)
17-03-2018 01:57:48.509 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373197 (rdata0x7f7548023260) from TCP 10.224.61.22:37306:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373197
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373197
Call-ID: poll-sip-373197
CSeq: 373197 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548023260 on worker thread
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548023260 on worker thread
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548023260 cloned to 0x7f7548463ba8
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548463ba8 for worker threads with priority 15
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug utils.cpp:872: Added IOHook 0x7f7594f88e30 to stack. There are now 1 hooks
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548463ba8
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug thread_dispatcher.cpp:183: Request latency so far = 112us
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373197 (rdata0x7f7548463ba8)
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373197 (tdta0x7f753c24d450) created
17-03-2018 01:57:48.509 UTC [7f7594f89700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373197 (tdta0x7f753c24d450) to TCP 10.224.61.22:37306:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37306;received=10.224.61.22;branch=z9hG4bK-373197
Call-ID: poll-sip-373197
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373197
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373197
CSeq: 373197 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug pjsip: tdta0x7f753c24 Destroying txdata Response msg 200/OPTIONS/cseq=373197 (tdta0x7f753c24d450)
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548463ba8
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug thread_dispatcher.cpp:284: Request latency = 304us
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug event_statistic_accumulator.cpp:32: Accumulate 304 for 0xf62778
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug event_statistic_accumulator.cpp:32: Accumulate 304 for 0xf62820
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 11).
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug utils.cpp:878: Removed IOHook 0x7f7594f88e30 to stack. There are now 0 hooks
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:49.814 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:43474, sock=712
17-03-2018 01:57:49.814 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:49.814 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 TCP server transport created
17-03-2018 01:57:49.814 UTC [7f754feff700] Verbose pjsip: tcps0x7f754836 TCP connection closed
17-03-2018 01:57:49.814 UTC [7f754feff700] Verbose pjsip: tcps0x7f754836 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:244: Reraising all alarms with a known state
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1001.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1005.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1011.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1012.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1013.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1004.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1002.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1009.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1010.1 alarm
17-03-2018 01:57:50.510 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP connection closed
17-03-2018 01:57:50.510 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548022f28 has been destroyed
17-03-2018 01:57:50.510 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:55.815 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:57:55.815 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:41363, sock=291
17-03-2018 01:57:55.815 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:55.815 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP server transport created
17-03-2018 01:57:55.815 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:56.551 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:59074, sock=563
17-03-2018 01:57:56.551 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:56.551 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373205 (rdata0x7f7548076e90)
17-03-2018 01:57:56.569 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373205 (rdata0x7f7548076e90) from TCP 10.224.61.22:59074:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373205
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373205
Call-ID: poll-sip-373205
CSeq: 373205 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548076e90 on worker thread
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548076e90 on worker thread
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548076e90 cloned to 0x7f7548081478
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug utils.cpp:872: Added IOHook 0x7f759df9ae30 to stack. There are now 1 hooks
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug thread_dispatcher.cpp:183: Request latency so far = 50us
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373205 (rdata0x7f7548081478)
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373205 (tdta0x7f75e02378d0) created
17-03-2018 01:57:56.569 UTC [7f759df9b700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373205 (tdta0x7f75e02378d0) to TCP 10.224.61.22:59074:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=59074;received=10.224.61.22;branch=z9hG4bK-373205
Call-ID: poll-sip-373205
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373205
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373205
CSeq: 373205 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug pjsip: tdta0x7f75e023 Destroying txdata Response msg 200/OPTIONS/cseq=373205 (tdta0x7f75e02378d0)
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug thread_dispatcher.cpp:284: Request latency = 113us
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug event_statistic_accumulator.cpp:32: Accumulate 113 for 0xf62778
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug event_statistic_accumulator.cpp:32: Accumulate 113 for 0xf62820
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 12).
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug utils.cpp:878: Removed IOHook 0x7f759df9ae30 to stack. There are now 0 hooks
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:56.600 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:57:56.600 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:57:58.573 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:57:58.573 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548076b58 has been destroyed
17-03-2018 01:57:58.573 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:58.585 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37366, sock=563
17-03-2018 01:57:58.585 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:58.585 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373207 (rdata0x7f7548076e90)
17-03-2018 01:57:58.586 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373207 (rdata0x7f7548076e90) from TCP 10.224.61.22:37366:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373207
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373207
Call-ID: poll-sip-373207
CSeq: 373207 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548076e90 on worker thread
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548076e90 on worker thread
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548076e90 cloned to 0x7f7548081478
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug utils.cpp:872: Added IOHook 0x7f758c777e30 to stack. There are now 1 hooks
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug thread_dispatcher.cpp:183: Request latency so far = 113us
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373207 (rdata0x7f7548081478)
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373207 (tdta0x7f75403a7370) created
17-03-2018 01:57:58.586 UTC [7f758c778700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373207 (tdta0x7f75403a7370) to TCP 10.224.61.22:37366:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37366;received=10.224.61.22;branch=z9hG4bK-373207
Call-ID: poll-sip-373207
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373207
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373207
CSeq: 373207 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug pjsip: tdta0x7f75403a Destroying txdata Response msg 200/OPTIONS/cseq=373207 (tdta0x7f75403a7370)
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug thread_dispatcher.cpp:284: Request latency = 293us
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug event_statistic_accumulator.cpp:32: Accumulate 293 for 0xf62778
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug event_statistic_accumulator.cpp:32: Accumulate 293 for 0xf62820
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 13).
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug utils.cpp:878: Removed IOHook 0x7f758c777e30 to stack. There are now 0 hooks
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:00.504 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:58:00.587 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:58:00.587 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548076b58 has been destroyed
17-03-2018 01:58:00.587 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:00.817 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:34246, sock=563
17-03-2018 01:58:00.817 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:00.817 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:58:00.817 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:58:00.817 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:06.628 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:59138, sock=721
17-03-2018 01:58:06.628 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:06.628 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373215 (rdata0x7f7548075680)
17-03-2018 01:58:06.637 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373215 (rdata0x7f7548075680) from TCP 10.224.61.22:59138:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373215
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373215
Call-ID: poll-sip-373215
CSeq: 373215 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548075680 on worker thread
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548075680 on worker thread
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548075680 cloned to 0x7f7548081478
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug utils.cpp:872: Added IOHook 0x7f7598f90e30 to stack. There are now 1 hooks
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug thread_dispatcher.cpp:183: Request latency so far = 66us
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373215 (rdata0x7f7548081478)
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373215 (tdta0x7f75d033ba90) created
17-03-2018 01:58:06.637 UTC [7f7598f91700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373215 (tdta0x7f75d033ba90) to TCP 10.224.61.22:59138:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=59138;received=10.224.61.22;branch=z9hG4bK-373215
Call-ID: poll-sip-373215
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373215
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373215
CSeq: 373215 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug pjsip: tdta0x7f75d033 Destroying txdata Response msg 200/OPTIONS/cseq=373215 (tdta0x7f75d033ba90)
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug thread_dispatcher.cpp:284: Request latency = 128us
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug event_statistic_accumulator.cpp:32: Accumulate 128 for 0xf62778
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug event_statistic_accumulator.cpp:32: Accumulate 128 for 0xf62820
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 14).
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug utils.cpp:878: Removed IOHook 0x7f7598f90e30 to stack. There are now 0 hooks
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:06.668 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:58:06.668 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:58:08.638 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:58:08.638 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548075348 has been destroyed
17-03-2018 01:58:08.638 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:08.650 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37428, sock=631
17-03-2018 01:58:08.650 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:08.650 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373217 (rdata0x7f7548075680)
17-03-2018 01:58:08.650 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373217 (rdata0x7f7548075680) from TCP 10.224.61.22:37428:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373217
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373217
Call-ID: poll-sip-373217
CSeq: 373217 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548075680 on worker thread
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548075680 on worker thread
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548075680 cloned to 0x7f7548081478
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug utils.cpp:872: Added IOHook 0x7f7589771e30 to stack. There are now 1 hooks
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug thread_dispatcher.cpp:183: Request latency so far = 100us
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373217 (rdata0x7f7548081478)
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373217 (tdta0x7f75e02378d0) created
17-03-2018 01:58:08.650 UTC [7f7589772700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373217 (tdta0x7f75e02378d0) to TCP 10.224.61.22:37428:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37428;received=10.224.61.22;branch=z9hG4bK-373217
Call-ID: poll-sip-373217
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373217
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373217
CSeq: 373217 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug pjsip: tdta0x7f75e023 Destroying txdata Response msg 200/OPTIONS/cseq=373217 (tdta0x7f75e02378d0)
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug thread_dispatcher.cpp:284: Request latency = 245us
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug event_statistic_accumulator.cpp:32: Accumulate 245 for 0xf62778
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug event_statistic_accumulator.cpp:32: Accumulate 245 for 0xf62820
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 15).
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug utils.cpp:878: Removed IOHook 0x7f7589771e30 to stack. There are now 0 hooks
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:10.652 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:58:10.652 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548075348 has been destroyed
17-03-2018 01:58:10.652 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:15.506 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:58:16.698 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:59198, sock=631
17-03-2018 01:58:16.698 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:16.698 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:58:16.704 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373225 (rdata0x7f7548075680)
17-03-2018 01:58:16.704 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373225 (rdata0x7f7548075680) from TCP 10.224.61.22:59198:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373225
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373225
Call-ID: poll-sip-373225
CSeq: 373225 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:16.704 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:16.704 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:16.704 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:16.704 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548075680 on worker thread
17-03-2018 01:58:16.705 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548075680 on worker thread
17-03-2018 01:58:16.705 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548075680 cloned to 0x7f7548081478
17-03-2018 01:58:16.705 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:16.705 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:16.705 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug utils.cpp:872: Added IOHook 0x7f7595789e30 to stack. There are now 1 hooks
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug thread_dispatcher.cpp:183: Request latency so far = 52us
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373225 (rdata0x7f7548081478)
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373225 (tdta0x7f75e02378d0) created
17-03-2018 01:58:16.705 UTC [7f759578a700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373225 (tdta0x7f75e02378d0) to TCP 10.224.61.22:59198:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=59198;received=10.224.61.22;branch=z9hG4bK-373225
Call-ID: poll-sip-373225
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373225
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373225
CSeq: 373225 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug pjsip: tdta0x7f75e023 Destroying txdata Response msg 200/OPTIONS/cseq=373225 (tdta0x7f75e02378d0)
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug thread_dispatcher.cpp:284: Request latency = 123us
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug event_statistic_accumulator.cpp:32: Accumulate 123 for 0xf62778
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug event_statistic_accumulator.cpp:32: Accumulate 123 for 0xf62820
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 16).
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug utils.cpp:878: Removed IOHook 0x7f7595789e30 to stack. There are now 0 hooks
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:16.742 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:58:16.742 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:58:18.705 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:58:18.705 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548075348 has been destroyed
17-03-2018 01:58:18.706 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:18.724 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37490, sock=631
17-03-2018 01:58:18.724 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:18.724 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373227 (rdata0x7f7548075680)
17-03-2018 01:58:18.725 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373227 (rdata0x7f7548075680) from TCP 10.224.61.22:37490:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373227
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373227
Call-ID: poll-sip-373227
CSeq: 373227 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548075680 on worker thread
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548075680 on worker thread
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548075680 cloned to 0x7f7548081478
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug utils.cpp:872: Added IOHook 0x7f75a5faae30 to stack. There are now 1 hooks
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug thread_dispatcher.cpp:183: Request latency so far = 108us
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373227 (rdata0x7f7548081478)
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373227 (tdta0x7f753c24d450) created
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373227 (tdta0x7f753c24d450) to TCP 10.224.61.22:37490:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37490;received=10.224.61.22;branch=z9hG4bK-373227
Call-ID: poll-sip-373227
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373227
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373227
CSeq: 373227 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug pjsip: tdta0x7f753c24 Destroying txdata Response msg 200/OPTIONS/cseq=373227 (tdta0x7f753c24d450)
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug thread_dispatcher.cpp:284: Request latency = 218us
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug event_statistic_accumulator.cpp:32: Accumulate 218 for 0xf62778
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug event_statistic_accumulator.cpp:32: Accumulate 218 for 0xf62820
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 17).
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug utils.cpp:878: Removed IOHook 0x7f75a5faae30 to stack. There are now 0 hooks
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:20.009 UTC [7f75ee6a5700] Status alarm.cpp:244: Reraising all alarms with a known state
17-03-2018 01:58:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1001.1 alarm
17-03-2018 01:58:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1005.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1011.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1012.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1013.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1004.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1002.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1009.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1010.1 alarm
17-03-2018 01:58:20.726 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:58:20.726 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548075348 has been destroyed
17-03-2018 01:58:20.726 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:21.823 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:34093, sock=631
17-03-2018 01:58:21.823 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:21.823 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:58:21.823 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP connection closed
17-03-2018 01:58:21.823 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:26.780 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:59260, sock=721
17-03-2018 01:58:26.780 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:26.780 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP server transport created
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373235 (rdata0x7f7548331690)
17-03-2018 01:58:26.780 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373235 (rdata0x7f7548331690) from TCP 10.224.61.22:59260:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373235
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373235
Call-ID: poll-sip-373235
CSeq: 373235 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548331690 on worker thread
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548331690 on worker thread
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548331690 cloned to 0x7f7548081478
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug utils.cpp:872: Added IOHook 0x7f75a6face30 to stack. There are now 1 hooks
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:183: Request latency so far = 53us
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373235 (rdata0x7f7548081478)
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373235 (tdta0x7f7548365540) created
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373235 (tdta0x7f7548365540) to TCP 10.224.61.22:59260:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=59260;received=10.224.61.22;branch=z9hG4bK-373235
Call-ID: poll-sip-373235
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373235
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373235
CSeq: 373235 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug pjsip: tdta0x7f754836 Destroying txdata Response msg 200/OPTIONS/cseq=373235 (tdta0x7f7548365540)
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:284: Request latency = 124us
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug event_statistic_accumulator.cpp:32: Accumulate 124 for 0xf62778
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug event_statistic_accumulator.cpp:32: Accumulate 124 for 0xf62820
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 18).
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug utils.cpp:878: Removed IOHook 0x7f75a6face30 to stack. There are now 0 hooks
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:26.823 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:58:26.823 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:58:28.781 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP connection closed
17-03-2018 01:58:28.781 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548331358 has been destroyed
17-03-2018 01:58:28.781 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:28.794 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37550, sock=718
17-03-2018 01:58:28.794 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:28.794 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP server transport created
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373237 (rdata0x7f7548331690)
17-03-2018 01:58:28.794 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373237 (rdata0x7f7548331690) from TCP 10.224.61.22:37550:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373237
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373237
Call-ID: poll-sip-373237
CSeq: 373237 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548331690 on worker thread
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548331690 on worker thread
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548331690 cloned to 0x7f7548081478
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug utils.cpp:872: Added IOHook 0x7f759f79de30 to stack. There are now 1 hooks
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug thread_dispatcher.cpp:183: Request latency so far = 80us
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373237 (rdata0x7f7548081478)
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373237 (tdta0x7f7544235830) created
17-03-2018 01:58:28.794 UTC [7f759f79e700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373237 (tdta0x7f7544235830) to TCP 10.224.61.22:37550:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37550;received=10.224.61.22;branch=z9hG4bK-373237
Call-ID: poll-sip-373237
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373237
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373237
CSeq: 373237 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug pjsip: tdta0x7f754423 Destroying txdata Response msg 200/OPTIONS/cseq=373237 (tdta0x7f7544235830)
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug thread_dispatcher.cpp:284: Request latency = 191us
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug event_statistic_accumulator.cpp:32: Accumulate 191 for 0xf62778
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug event_statistic_accumulator.cpp:32: Accumulate 191 for 0xf62820
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 19).
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug utils.cpp:878: Removed IOHook 0x7f759f79de30 to stack. There are now 0 hooks
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:30.512 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:58:30.795 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP connection closed
17-03-2018 01:58:30.795 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548331358 has been destroyed
17-03-2018 01:58:30.795 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:36.856 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:59320, sock=718
17-03-2018 01:58:36.856 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:36.856 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP server transport created
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373245 (rdata0x7f7548331690)
17-03-2018 01:58:36.856 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373245 (rdata0x7f7548331690) from TCP 10.224.61.22:59320:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373245
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373245
Call-ID: poll-sip-373245
CSeq: 373245 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548331690 on worker thread
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548331690 on worker thread
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548331690 cloned to 0x7f7548081478
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug utils.cpp:872: Added IOHook 0x7f758a773e30 to stack. There are now 1 hooks
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug thread_dispatcher.cpp:183: Request latency so far = 51us
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373245 (rdata0x7f7548081478)
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373245 (tdta0x7f75e82b6d00) created
17-03-2018 01:58:36.856 UTC [7f758a774700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373245 (tdta0x7f75e82b6d00) to TCP 10.224.61.22:59320:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=59320;received=10.224.61.22;branch=z9hG4bK-373245
Call-ID: poll-sip-373245
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373245
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373245
CSeq: 373245 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug pjsip: tdta0x7f75e82b Destroying txdata Response msg 200/OPTIONS/cseq=373245 (tdta0x7f75e82b6d00)
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug thread_dispatcher.cpp:284: Request latency = 113us
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug event_statistic_accumulator.cpp:32: Accumulate 113 for 0xf62778
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug event_statistic_accumulator.cpp:32: Accumulate 113 for 0xf62820
17-03-2018 01:58:36.856 UTC [7f758a774700] Info load_monitor.cpp:217: Rate adjustment calculation inputs: err -0.982600, smoothed latency 174, target latency 10000
17-03-2018 01:58:36.856 UTC [7f758a774700] Info load_monitor.cpp:302: Maximum incoming request rate/second unchanged at 2000.000000 (current request rate is 0.550000 requests/sec, minimum threshold for a change is 1000.000000 requests/sec).
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug utils.cpp:878: Removed IOHook 0x7f758a773e30 to stack. There are now 0 hooks
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:36.900 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:58:36.900 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:58:38.857 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP connection closed
17-03-2018 01:58:38.857 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548331358 has been destroyed
17-03-2018 01:58:38.857 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:38.868 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37612, sock=718
17-03-2018 01:58:38.868 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:38.868 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP server transport created
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373247 (rdata0x7f7548331690)
17-03-2018 01:58:38.868 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373247 (rdata0x7f7548331690) from TCP 10.224.61.22:37612:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373247
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373247
Call-ID: poll-sip-373247
CSeq: 373247 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548331690 on worker thread
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548331690 on worker thread
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548331690 cloned to 0x7f7548081478
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:38.868 UTC [7f759bf97700] Debug utils.cpp:872: Added IOHook 0x7f759bf96e30 to stack. There are now 1 hooks
17-03-2018 01:58:38.868 UTC [7f759bf97700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:38.868 UTC [7f759bf97700] Debug thread_dispatcher.cpp:183: Request latency so far = 102us
17-03-2018 01:58:38.868 UTC [7f759bf97700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373247 (rdata0x7f7548081478)
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373247 (tdta0x7f75d033ba90) created
17-03-2018 01:58:38.869 UTC [7f759bf97700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373247 (tdta0x7f75d033ba90) to TCP 10.224.61.22:37612:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37612;received=10.224.61.22;branch=z9hG4bK-373247
Call-ID: poll-sip-373247
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373247
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373247
CSeq: 373247 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug pjsip: tdta0x7f75d033 Destroying txdata Response msg 200/OPTIONS/cseq=373247 (tdta0x7f75d033ba90)
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug thread_dispatcher.cpp:284: Request latency = 236us
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug event_statistic_accumulator.cpp:32: Accumulate 236 for 0xf62778
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug event_statistic_accumulator.cpp:32: Accumulate 236 for 0xf62820
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 1).
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug utils.cpp:878: Removed IOHook 0x7f759bf96e30 to stack. There are now 0 hooks
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:40.870 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP connection closed
17-03-2018 01:58:40.870 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548331358 has been destroyed
17-03-2018 01:58:40.870 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)

From david.kyselica at gmail.com  Mon Mar 19 12:25:27 2018
From: david.kyselica at gmail.com (=?UTF-8?Q?D=C3=A1vid_Kyselica?=)
Date: Mon, 19 Mar 2018 17:25:27 +0100
Subject: [Project Clearwater] Fwd: Clearwater, ellis freezing
In-Reply-To: <CAJpo2SrM2rViuhGy86MHv9sZ-QVXjS2J89KZzGbeh=UZ0e_1AQ@mail.gmail.com>
References: <CAJpo2SrM2rViuhGy86MHv9sZ-QVXjS2J89KZzGbeh=UZ0e_1AQ@mail.gmail.com>
Message-ID: <CAJpo2Sqt-V3EZ8iy13Rf+01DYgS+OAZpWjtnVv-JLuUJk8TCcg@mail.gmail.com>

Hi,
I`m new to clearwater. I installed all-in-one AMI into amazon ec2. After
trying to access web interface, I was asked by web browser to reload the
page by this message:Failed to update the server (see detailed diagnostics
in developer console). Please refresh the page.
I`m really confused what it can by caused by. I will be thankful for any
type of tips. It`s a fresh installation with no important configuration
changes.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180319/3e273f21/attachment.html>

From skgola1997 at gmail.com  Tue Mar 20 01:00:35 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Tue, 20 Mar 2018 10:30:35 +0530
Subject: [Project Clearwater] CW team please help - stress testing
Message-ID: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>

Hi CW team,
Anyone out there please help me. I am facing problem in stress testing. I
have installed CW manually. whenever I was running 1 or less than 20 it
give some errors like:

*[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
<http://ims.com> 1 2*
[sudo] password for ubuntu:
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
    with open(CALLER_STATS) as f:
IOError: [Errno 2] No such file or directory:
'/var/log/clearwater-sip-stress/18065_caller_stats.log'


*[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
<http://ims.com> 10 5*
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
    call_success_rate = 100 * float(row['SuccessfulCall(C)']) /
float(row['TotalCallCreated'])
ZeroDivisionError: float division by zero


*[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress
iind.intel.com <http://iind.intel.com> 50 5*
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:03:41
Start: 2018-03-20 17:46:43.268136
End: 2018-03-20 17:51:31.363406

Total calls: 2
Successful calls: 0 (0.0%)
Failed calls: 2 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 8
Successful re-REGISTERs: 8 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 86.0ms

Log files at /var/log/clearwater-sip-stress/18566_*



*[]ubuntu at stress:~$ cat
/var/log/clearwater-sip-stress/18566_caller_errors.log*
sipp: The following events occured:
2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on
unexpected message for Call-Id '1-18576 at 127.0.1.1': while expecting '183'
(index 2), received '*SIP/2.0 503 Service Unavailable*
Via: SIP/2.0/TCP 127.0.1.1:42276
;received=10.224.61.13;branch=z9hG4bK-18576-1-0
Record-Route: <sip:scscf.sprout.ims.com
;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout. ims.com
;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 1-18576 at 127.0.1.1
From: <sip:2010000042@ ims.com >;tag=18576SIPpTag001
To: <sip:2010000015@ ims.co>;tag=z9hG4bKPj1Lm9whhQMslKrcZxnN6qCH0tb9Lj5Neu
CSeq: 1 INVITE
P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com
;term-ioi= ims.com
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0


*[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt*
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP
10.224.61.22;rport=49294;received=10.224.61.22;branch=z9hG4bK-670172
Call-ID: poll-sip-670172
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670172
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670172
CSeq: 670172 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b
Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:270:
Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:284:
Request latency = 254us
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217: Rate
adjustment calculation inputs: err -0.981500, smoothed latency 185, target
latency 10000
20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302:
Maximum incoming request rate/second unchanged at 2000.000000 (current
request rate is 0.200000 requests/sec, minimum threshold for a change is
1000.000000 requests/sec).
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
2000ui into continuous accumulator statistic
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
2000ui into continuous accumulator statistic
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed
IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:158:
Attempting to process queue element
20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
TCP connection closed
20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug connection_tracker.cpp:67:
Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning:
Failed to connect to the agentx master agent ([NIL]):
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054
TCP listener 10.224.61.22:5054: got incoming TCP connection from
10.224.61.22:42848, sock=573
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
tcp->base.local_name: 10.224.61.22
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
TCP server transport created
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
Processing incoming message: Request msg OPTIONS/cseq=670180
(rdata0x7f34ec027690)
20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose
common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180
(rdata0x7f34ec027690) from TCP 10.224.61.22:42848:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670180
Call-ID: poll-sip-670180
CSeq: 670180 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
home domain: false, local_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
Classified URI as 3
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:554:
Recieved message 0x7f34ec027690 on worker thread
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:571:
Admitted request 0x7f34ec027690 on worker thread
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:606:
Incoming message 0x7f34ec027690 cloned to 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:625:
Queuing cloned received message 0x7f34ec34a3e8 for worker threads with
priority 15
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added
IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:178:
Worker thread dequeue message 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:183:
Request latency so far = 57us
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c
Distributing rdata to modules: Request msg OPTIONS/cseq=670180
(rdata0x7f34ec34a3e8)
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139:
home domain: false, local_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172:
Classified URI as 3
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint
Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose
common_sip_processing.cpp:103: TX 282 bytes Response msg
200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP
10.224.61.22;rport=42848;received=10.224.61.22;branch=z9hG4bK-670180
Call-ID: poll-sip-670180
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670180
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670180
CSeq: 670180 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809
Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:270:
Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:284:
Request latency = 129us
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341: Not
recalculating rate as we haven't processed 20 requests yet (only 1).
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed
IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:158:
Attempting to process queue element
20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327:
Process request for URL /ping, args (null)
20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68:
Sending response 200 to request for URL /ping, args (null)
20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
TCP connection closed
20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug connection_tracker.cpp:67:
Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053
TCP listener 10.224.61.22:5053: got incoming TCP connection from
10.224.61.22:49356, sock=573
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
tcp->base.local_name: 10.224.61.22
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
TCP server transport created
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
Processing incoming message: Request msg OPTIONS/cseq=670182
(rdata0x7f34ec027690)
20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose
common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182
(rdata0x7f34ec027690) from TCP 10.224.61.22:49356:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670182
Call-ID: poll-sip-670182
CSeq: 670182 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
home domain: false, local_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
Classified URI as 3
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:554:
Recieved message 0x7f34ec027690 on worker thread
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:571:
Admitted request 0x7f34ec027690 on worker thread
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:606:
Incoming message 0x7f34ec027690 cloned to 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:625:
Queuing cloned received message 0x7f34ec34a3e8 for worker threads with
priority 15
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added
IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:178:
Worker thread dequeue message 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:183:
Request latency so far = 102us
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c
Distributing rdata to modules: Request msg OPTIONS/cseq=670182
(rdata0x7f34ec34a3e8)
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139:
home domain: false, local_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172:
Classified URI as 3
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint
Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose
common_sip_processing.cpp:103: TX 282 bytes Response msg
200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP
10.224.61.22;rport=49356;received=10.224.61.22;branch=z9hG4bK-670182
Call-ID: poll-sip-670182
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670182
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670182
CSeq: 670182 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00
Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:270:
Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:284:
Request latency = 232us
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341: Not
recalculating rate as we haven't processed 20 requests yet (only 2).
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed
IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:158:
Attempting to process queue element
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244: Reraising
all alarms with a known state
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
issued 1001.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
issued 1005.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
issued 1011.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
issued 1012.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
issued 1013.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
issued 1004.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
issued 1002.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
issued 1009.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
issued 1010.1 alarm
20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
TCP connection closed
20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug connection_tracker.cpp:67:
Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
TCP transport destroyed with reason 70016: End of file (PJ_EEOF)



all calls are failing I don't know what is going on, I am newbie to CW
please guide some solution it will be great help.


Thanks,
Sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180320/db15815e/attachment.html>

From skgola1997 at gmail.com  Tue Mar 20 07:51:18 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Tue, 20 Mar 2018 17:21:18 +0530
Subject: [Project Clearwater] CW team please help - stress testing
In-Reply-To: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
References: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
Message-ID: <CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>

Hi all,
I have taken tcpdump also but there is no SIP message. Please through some
light on this problem. I am trying from last weak, not able to catch the
problem. Thanks in advance.

cheers,
Sunil

On Tue, Mar 20, 2018 at 10:30 AM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi CW team,
> Anyone out there please help me. I am facing problem in stress testing. I
> have installed CW manually. whenever I was running 1 or less than 20 it
> give some errors like:
>
> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
> <http://ims.com> 1 2*
> [sudo] password for ubuntu:
> Starting initial registration, will take 0 seconds
> Initial registration succeeded
> Starting test
> Test complete
> Traceback (most recent call last):
>   File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
>     with open(CALLER_STATS) as f:
> IOError: [Errno 2] No such file or directory: '/var/log/clearwater-sip-
> stress/18065_caller_stats.log'
>
>
> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
> <http://ims.com> 10 5*
> Starting initial registration, will take 0 seconds
> Initial registration succeeded
> Starting test
> Test complete
> Traceback (most recent call last):
>   File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
>     call_success_rate = 100 * float(row['SuccessfulCall(C)']) /
> float(row['TotalCallCreated'])
> ZeroDivisionError: float division by zero
>
>
> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress
> iind.intel.com <http://iind.intel.com> 50 5*
> Starting initial registration, will take 0 seconds
> Initial registration succeeded
> Starting test
> Test complete
>
> Elapsed time: 00:03:41
> Start: 2018-03-20 17:46:43.268136
> End: 2018-03-20 17:51:31.363406
>
> Total calls: 2
> Successful calls: 0 (0.0%)
> Failed calls: 2 (100.0%)
> Unfinished calls: 0
>
> Retransmissions: 0
>
> Average time from INVITE to 180 Ringing: 0.0ms
> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
> Failed: call success rate 0.0% is lower than target 100.0%!
>
> Total re-REGISTERs: 8
> Successful re-REGISTERs: 8 (100.0%)
> Failed re-REGISTERS: 0 (0.0%)
>
> REGISTER retransmissions: 0
>
> Average time from REGISTER to 200 OK: 86.0ms
>
> Log files at /var/log/clearwater-sip-stress/18566_*
>
>
>
> *[]ubuntu at stress:~$ cat
> /var/log/clearwater-sip-stress/18566_caller_errors.log*
> sipp: The following events occured:
> 2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on
> unexpected message for Call-Id '1-18576 at 127.0.1.1': while expecting '183'
> (index 2), received '*SIP/2.0 503 Service Unavailable*
> Via: SIP/2.0/TCP 127.0.1.1:42276;received=10.224.61.13;branch=z9hG4bK-
> 18576-1-0
> Record-Route: <sip:scscf.sprout.ims.com;transport=TCP;lr;billing-role=
> charge-term>
> Record-Route: <sip:scscf.sprout. ims.com ;transport=TCP;lr;billing-
> role=charge-orig>
> Call-ID: 1-18576 at 127.0.1.1
> From: <sip:2010000042@ ims.com >;tag=18576SIPpTag001
> To: <sip:2010000015@ ims.co>;tag=z9hG4bKPj1Lm9whhQMslKrcZxnN6qCH0tb9Lj5Neu
> CSeq: 1 INVITE
> P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com
> ;term-ioi= ims.com
> P-Charging-Function-Addresses: ccf=0.0.0.0
> Content-Length:  0
>
>
> *[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt*
> --start msg--
>
> SIP/2.0 200 OK
> Via: SIP/2.0/TCP 10.224.61.22;rport=49294;received=10.224.61.22;branch=
> z9hG4bK-670172
> Call-ID: poll-sip-670172
> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670172
> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670172
> CSeq: 670172 OPTIONS
> Content-Length:  0
>
>
> --end msg--
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b
> Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> thread_dispatcher.cpp:270: Worker thread completed processing message
> 0x7f34ec34a3e8
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> thread_dispatcher.cpp:284: Request latency = 254us
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217: Rate
> adjustment calculation inputs: err -0.981500, smoothed latency 185, target
> latency 10000
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302:
> Maximum incoming request rate/second unchanged at 2000.000000 (current
> request rate is 0.200000 requests/sec, minimum threshold for a change is
> 1000.000000 requests/sec).
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
> 2000ui into continuous accumulator statistic
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
> 2000ui into continuous accumulator statistic
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed
> IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> thread_dispatcher.cpp:158: Attempting to process queue element
> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP connection closed
> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug
> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
> 20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning:
> Failed to connect to the agentx master agent ([NIL]):
> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054
> TCP listener 10.224.61.22:5054: got incoming TCP connection from
> 10.224.61.22:42848, sock=573
> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> tcp->base.local_name: 10.224.61.22
> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP server transport created
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
> Processing incoming message: Request msg OPTIONS/cseq=670180
> (rdata0x7f34ec027690)
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose
> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180
> (rdata0x7f34ec027690) from TCP 10.224.61.22:42848:
> --start msg--
>
> OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
> Max-Forwards: 2
> To: <sip:poll-sip at 10.224.61.22:5054>
> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670180
> Call-ID: poll-sip-670180
> CSeq: 670180 OPTIONS
> Contact: <sip:10.224.61.22>
> Accept: application/sdp
> Content-Length: 0
> User-Agent: poll-sip
>
>
> --end msg--
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
> Classified URI as 3
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
> 0x7f34ec34a3e8
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
> for worker threads with priority 15
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added
> IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:183: Request latency so far = 57us
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c
> Distributing rdata to modules: Request msg OPTIONS/cseq=670180
> (rdata0x7f34ec34a3e8)
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172:
> Classified URI as 3
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint
> Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose
> common_sip_processing.cpp:103: TX 282 bytes Response msg
> 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848:
> --start msg--
>
> SIP/2.0 200 OK
> Via: SIP/2.0/TCP 10.224.61.22;rport=42848;received=10.224.61.22;branch=
> z9hG4bK-670180
> Call-ID: poll-sip-670180
> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670180
> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670180
> CSeq: 670180 OPTIONS
> Content-Length:  0
>
>
> --end msg--
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809
> Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:270: Worker thread completed processing message
> 0x7f34ec34a3e8
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:284: Request latency = 129us
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341: Not
> recalculating rate as we haven't processed 20 requests yet (only 1).
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed
> IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:158: Attempting to process queue element
> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327:
> Process request for URL /ping, args (null)
> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68:
> Sending response 200 to request for URL /ping, args (null)
> 20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP connection closed
> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug
> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053
> TCP listener 10.224.61.22:5053: got incoming TCP connection from
> 10.224.61.22:49356, sock=573
> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> tcp->base.local_name: 10.224.61.22
> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP server transport created
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
> Processing incoming message: Request msg OPTIONS/cseq=670182
> (rdata0x7f34ec027690)
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose
> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182
> (rdata0x7f34ec027690) from TCP 10.224.61.22:49356:
> --start msg--
>
> OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
> Max-Forwards: 2
> To: <sip:poll-sip at 10.224.61.22:5053>
> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670182
> Call-ID: poll-sip-670182
> CSeq: 670182 OPTIONS
> Contact: <sip:10.224.61.22>
> Accept: application/sdp
> Content-Length: 0
> User-Agent: poll-sip
>
>
> --end msg--
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
> Classified URI as 3
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
> 0x7f34ec34a3e8
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
> for worker threads with priority 15
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added
> IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:183: Request latency so far = 102us
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c
> Distributing rdata to modules: Request msg OPTIONS/cseq=670182
> (rdata0x7f34ec34a3e8)
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172:
> Classified URI as 3
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint
> Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose
> common_sip_processing.cpp:103: TX 282 bytes Response msg
> 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356:
> --start msg--
>
> SIP/2.0 200 OK
> Via: SIP/2.0/TCP 10.224.61.22;rport=49356;received=10.224.61.22;branch=
> z9hG4bK-670182
> Call-ID: poll-sip-670182
> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670182
> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670182
> CSeq: 670182 OPTIONS
> Content-Length:  0
>
>
> --end msg--
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00
> Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:270: Worker thread completed processing message
> 0x7f34ec34a3e8
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:284: Request latency = 232us
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341: Not
> recalculating rate as we haven't processed 20 requests yet (only 2).
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed
> IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:158: Attempting to process queue element
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244: Reraising
> all alarms with a known state
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1001.1 alarm
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1005.1 alarm
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1011.1 alarm
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1012.1 alarm
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1013.1 alarm
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1004.1 alarm
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1002.1 alarm
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1009.1 alarm
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1010.1 alarm
> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP connection closed
> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug
> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>
>
>
> all calls are failing I don't know what is going on, I am newbie to CW
> please guide some solution it will be great help.
>
>
> Thanks,
> Sunil
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180320/3d2bd655/attachment.html>

From skgola1997 at gmail.com  Tue Mar 20 09:23:18 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Tue, 20 Mar 2018 18:53:18 +0530
Subject: [Project Clearwater] CW team please help - stress testing
In-Reply-To: <CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>
References: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
	<CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>
Message-ID: <CAHwYWpB4WqEVnG69B_BazQC6yJVHW7A3pMPOWmRoVX_gYJhPgw@mail.gmail.com>

Hi,
It is using some other port on stress node not 5082. Is this a problem, if
yes how can I fix this i have tried to open 5082 port on stress node
using *sudo
ufw allow 5082/tcp, *but no effect.
Please check the wireshark log:

Frame 2406: 703 bytes on wire (5624 bits), 703 bytes captured (5624 bits)
Ethernet II, Src: PcsCompu_ff:d2:88 (08:00:27:ff:d2:88), Dst:
PcsCompu_ab:71:0f (08:00:27:ab:71:0f)
Internet Protocol Version 4, Src: 10.224.61.22, Dst: 10.224.61.13
Transmission Control Protocol, Src Port: rlm-admin (5054), Dst Port: 34312
(34312), Seq: 349, Ack: 2199, Len: 637
Session Initiation Protocol (503)


cheers,
sunil

On Tue, Mar 20, 2018 at 5:21 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi all,
> I have taken tcpdump also but there is no SIP message. Please through some
> light on this problem. I am trying from last weak, not able to catch the
> problem. Thanks in advance.
>
> cheers,
> Sunil
>
> On Tue, Mar 20, 2018 at 10:30 AM, Sunil Kumar <skgola1997 at gmail.com>
> wrote:
>
>> Hi CW team,
>> Anyone out there please help me. I am facing problem in stress testing. I
>> have installed CW manually. whenever I was running 1 or less than 20 it
>> give some errors like:
>>
>> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
>> <http://ims.com> 1 2*
>> [sudo] password for ubuntu:
>> Starting initial registration, will take 0 seconds
>> Initial registration succeeded
>> Starting test
>> Test complete
>> Traceback (most recent call last):
>>   File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
>>     with open(CALLER_STATS) as f:
>> IOError: [Errno 2] No such file or directory:
>> '/var/log/clearwater-sip-stress/18065_caller_stats.log'
>>
>>
>> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
>> <http://ims.com> 10 5*
>> Starting initial registration, will take 0 seconds
>> Initial registration succeeded
>> Starting test
>> Test complete
>> Traceback (most recent call last):
>>   File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
>>     call_success_rate = 100 * float(row['SuccessfulCall(C)']) /
>> float(row['TotalCallCreated'])
>> ZeroDivisionError: float division by zero
>>
>>
>> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress
>> iind.intel.com <http://iind.intel.com> 50 5*
>> Starting initial registration, will take 0 seconds
>> Initial registration succeeded
>> Starting test
>> Test complete
>>
>> Elapsed time: 00:03:41
>> Start: 2018-03-20 17:46:43.268136
>> End: 2018-03-20 17:51:31.363406
>>
>> Total calls: 2
>> Successful calls: 0 (0.0%)
>> Failed calls: 2 (100.0%)
>> Unfinished calls: 0
>>
>> Retransmissions: 0
>>
>> Average time from INVITE to 180 Ringing: 0.0ms
>> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
>> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
>> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
>> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
>> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
>> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
>> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
>> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
>> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
>> # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
>> Failed: call success rate 0.0% is lower than target 100.0%!
>>
>> Total re-REGISTERs: 8
>> Successful re-REGISTERs: 8 (100.0%)
>> Failed re-REGISTERS: 0 (0.0%)
>>
>> REGISTER retransmissions: 0
>>
>> Average time from REGISTER to 200 OK: 86.0ms
>>
>> Log files at /var/log/clearwater-sip-stress/18566_*
>>
>>
>>
>> *[]ubuntu at stress:~$ cat
>> /var/log/clearwater-sip-stress/18566_caller_errors.log*
>> sipp: The following events occured:
>> 2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on
>> unexpected message for Call-Id '1-18576 at 127.0.1.1': while expecting
>> '183' (index 2), received '*SIP/2.0 503 Service Unavailable*
>> Via: SIP/2.0/TCP 127.0.1.1:42276;received=10.22
>> 4.61.13;branch=z9hG4bK-18576-1-0
>> Record-Route: <sip:scscf.sprout.ims.com;tran
>> sport=TCP;lr;billing-role=charge-term>
>> Record-Route: <sip:scscf.sprout. ims.com ;transport=TCP;lr;billing-role
>> =charge-orig>
>> Call-ID: 1-18576 at 127.0.1.1
>> From: <sip:2010000042@ ims.com >;tag=18576SIPpTag001
>> To: <sip:2010000015@ ims.co>;tag=z9hG4bKPj1Lm9whhQM
>> slKrcZxnN6qCH0tb9Lj5Neu
>> CSeq: 1 INVITE
>> P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com
>> ;term-ioi= ims.com
>> P-Charging-Function-Addresses: ccf=0.0.0.0
>> Content-Length:  0
>>
>>
>> *[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt*
>> --start msg--
>>
>> SIP/2.0 200 OK
>> Via: SIP/2.0/TCP 10.224.61.22;rport=49294;recei
>> ved=10.224.61.22;branch=z9hG4bK-670172
>> Call-ID: poll-sip-670172
>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670172
>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670172
>> CSeq: 670172 OPTIONS
>> Content-Length:  0
>>
>>
>> --end msg--
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b
>> Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> thread_dispatcher.cpp:270: Worker thread completed processing message
>> 0x7f34ec34a3e8
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> thread_dispatcher.cpp:284: Request latency = 254us
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217:
>> Rate adjustment calculation inputs: err -0.981500, smoothed latency 185,
>> target latency 10000
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302:
>> Maximum incoming request rate/second unchanged at 2000.000000 (current
>> request rate is 0.200000 requests/sec, minimum threshold for a change is
>> 1000.000000 requests/sec).
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
>> 2000ui into continuous accumulator statistic
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
>> 2000ui into continuous accumulator statistic
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed
>> IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> thread_dispatcher.cpp:158: Attempting to process queue element
>> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP connection closed
>> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug
>> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>> 20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning:
>> Failed to connect to the agentx master agent ([NIL]):
>> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054
>> TCP listener 10.224.61.22:5054: got incoming TCP connection from
>> 10.224.61.22:42848, sock=573
>> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> tcp->base.local_name: 10.224.61.22
>> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP server transport created
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
>> Processing incoming message: Request msg OPTIONS/cseq=670180
>> (rdata0x7f34ec027690)
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose
>> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180
>> (rdata0x7f34ec027690) from TCP 10.224.61.22:42848:
>> --start msg--
>>
>> OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
>> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
>> Max-Forwards: 2
>> To: <sip:poll-sip at 10.224.61.22:5054>
>> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670180
>> Call-ID: poll-sip-670180
>> CSeq: 670180 OPTIONS
>> Contact: <sip:10.224.61.22>
>> Accept: application/sdp
>> Content-Length: 0
>> User-Agent: poll-sip
>>
>>
>> --end msg--
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
>> home domain: false, local_to_node: true, is_gruu: false,
>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
>> Classified URI as 3
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
>> 0x7f34ec34a3e8
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
>> for worker threads with priority 15
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added
>> IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> thread_dispatcher.cpp:183: Request latency so far = 57us
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c
>> Distributing rdata to modules: Request msg OPTIONS/cseq=670180
>> (rdata0x7f34ec34a3e8)
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139:
>> home domain: false, local_to_node: true, is_gruu: false,
>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172:
>> Classified URI as 3
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint
>> Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose
>> common_sip_processing.cpp:103: TX 282 bytes Response msg
>> 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848:
>> --start msg--
>>
>> SIP/2.0 200 OK
>> Via: SIP/2.0/TCP 10.224.61.22;rport=42848;recei
>> ved=10.224.61.22;branch=z9hG4bK-670180
>> Call-ID: poll-sip-670180
>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670180
>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670180
>> CSeq: 670180 OPTIONS
>> Content-Length:  0
>>
>>
>> --end msg--
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809
>> Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> thread_dispatcher.cpp:270: Worker thread completed processing message
>> 0x7f34ec34a3e8
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> thread_dispatcher.cpp:284: Request latency = 129us
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341:
>> Not recalculating rate as we haven't processed 20 requests yet (only 1).
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed
>> IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> thread_dispatcher.cpp:158: Attempting to process queue element
>> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327:
>> Process request for URL /ping, args (null)
>> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68:
>> Sending response 200 to request for URL /ping, args (null)
>> 20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP connection closed
>> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug
>> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053
>> TCP listener 10.224.61.22:5053: got incoming TCP connection from
>> 10.224.61.22:49356, sock=573
>> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> tcp->base.local_name: 10.224.61.22
>> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP server transport created
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
>> Processing incoming message: Request msg OPTIONS/cseq=670182
>> (rdata0x7f34ec027690)
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose
>> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182
>> (rdata0x7f34ec027690) from TCP 10.224.61.22:49356:
>> --start msg--
>>
>> OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
>> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
>> Max-Forwards: 2
>> To: <sip:poll-sip at 10.224.61.22:5053>
>> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670182
>> Call-ID: poll-sip-670182
>> CSeq: 670182 OPTIONS
>> Contact: <sip:10.224.61.22>
>> Accept: application/sdp
>> Content-Length: 0
>> User-Agent: poll-sip
>>
>>
>> --end msg--
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
>> home domain: false, local_to_node: true, is_gruu: false,
>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
>> Classified URI as 3
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
>> 0x7f34ec34a3e8
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
>> for worker threads with priority 15
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added
>> IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> thread_dispatcher.cpp:183: Request latency so far = 102us
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c
>> Distributing rdata to modules: Request msg OPTIONS/cseq=670182
>> (rdata0x7f34ec34a3e8)
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139:
>> home domain: false, local_to_node: true, is_gruu: false,
>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172:
>> Classified URI as 3
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint
>> Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose
>> common_sip_processing.cpp:103: TX 282 bytes Response msg
>> 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356:
>> --start msg--
>>
>> SIP/2.0 200 OK
>> Via: SIP/2.0/TCP 10.224.61.22;rport=49356;recei
>> ved=10.224.61.22;branch=z9hG4bK-670182
>> Call-ID: poll-sip-670182
>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670182
>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670182
>> CSeq: 670182 OPTIONS
>> Content-Length:  0
>>
>>
>> --end msg--
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00
>> Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> thread_dispatcher.cpp:270: Worker thread completed processing message
>> 0x7f34ec34a3e8
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> thread_dispatcher.cpp:284: Request latency = 232us
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341:
>> Not recalculating rate as we haven't processed 20 requests yet (only 2).
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed
>> IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> thread_dispatcher.cpp:158: Attempting to process queue element
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244:
>> Reraising all alarms with a known state
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1001.1 alarm
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1005.1 alarm
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1011.1 alarm
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1012.1 alarm
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1013.1 alarm
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1004.1 alarm
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1002.1 alarm
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1009.1 alarm
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1010.1 alarm
>> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP connection closed
>> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug
>> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>>
>>
>>
>> all calls are failing I don't know what is going on, I am newbie to CW
>> please guide some solution it will be great help.
>>
>>
>> Thanks,
>> Sunil
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180320/c8e26d2d/attachment.html>

From skgola1997 at gmail.com  Tue Mar 20 10:56:36 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Tue, 20 Mar 2018 20:26:36 +0530
Subject: [Project Clearwater] CW team please help - stress testing
In-Reply-To: <CAHwYWpB4WqEVnG69B_BazQC6yJVHW7A3pMPOWmRoVX_gYJhPgw@mail.gmail.com>
References: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
	<CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>
	<CAHwYWpB4WqEVnG69B_BazQC6yJVHW7A3pMPOWmRoVX_gYJhPgw@mail.gmail.com>
Message-ID: <CAHwYWpBH8qdZbAEuFoE61Y13s1s9QGku912aBb5w8XpX+ZmAkQ@mail.gmail.com>

Hi,
I am facing problem in stress testing, Please look into the log. I am not
able to debug the problem.

I have taken this from wireshark, actually i use tcpdump.

REGISTER sip:ims.com SIP/2.0
Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-0
From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
Content-Length: 0
Require: Path
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
Supported: path
To: <sip:2010000039 at ims.com>
Route: <sip:icscf at sprout.ims.com;lr>
Max-Forwards: 70
Contact: <sip:2010000039 at 127.0.1.1:34768
>;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
Call-ID: 1-784 at 127.0.1.1
CSeq: 1 REGISTER
Expires: 3600
Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY,
MESSAGE, PRACK
Supported: path, gruu
Authorization: Digest username="2010000039 at ims.com",realm="ims.com
",uri="sip:ims.com",nonce="",response="",algorithm=Digest-MD5
User-Agent: 00-00000-0000000000000 Phone IMS 10.0
P-Access-Network-Info:
IEEE-802.11;i-wlan-node-id=000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
P-Visited-Network-ID: ims.com

SIP/2.0 401 Unauthorized
Via: SIP/2.0/TCP 127.0.1.1:34768
;received=10.224.61.13;branch=z9hG4bK-784-1-0
Call-ID: 1-784 at 127.0.1.1
From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
To: <sip:2010000039 at ims.com>;tag=z9hG4bKPjDBaGZjqTrLQiDSlHihO36oMPm7fxz2sQ
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
WWW-Authenticate: Digest  realm="ims.com
",nonce="0e07c1b77b566f37",opaque="5171f001504c2c3a",algorithm=MD5,qop="auth"
Content-Length:  0

REGISTER sip:ims.com SIP/2.0
Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-2
From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
Content-Length: 0
Require: Path
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
Supported: path
To: <sip:2010000039 at ims.com>
Route: <sip:icscf at sprout.ims.com;lr>
Max-Forwards: 70
Contact: <sip:2010000039 at 127.0.1.1:34768
>;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
Call-ID: 1-784 at 127.0.1.1
CSeq: 1 REGISTER
Expires: 3600
Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY,
MESSAGE, PRACK
Supported: path, gruu
Authorization: Digest username="2010000039 at ims.com",realm="ims.com
",cnonce="66334873",nc=00000001,qop=auth,uri="sip:sprout.ims.com:5052
",nonce="0e07c1b77b566f37",response="788d4520717e4e7b29f7fab43fdc448f",algorithm=MD5,opaque="5171f001504c2c3a"
User-Agent: 00-00000-0000000000000 Phone IMS 10.0
P-Access-Network-Info:
IEEE-802.11;i-wlan-node-id=000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
P-Visited-Network-ID: ims.com

SIP/2.0 200 OK
Service-Route: <sip:scscf.sprout.ims.com
;transport=TCP;lr;orig;username=2010000039%40ims.com;nonce=0e07c1b77b566f37>
Via: SIP/2.0/TCP 127.0.1.1:34768
;received=10.224.61.13;branch=z9hG4bK-784-1-2
Call-ID: 1-784 at 127.0.1.1
From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
To: <sip:2010000039 at ims.com>;tag=z9hG4bKPjIvjh2DjwvU.vVNEv.nOiYAfsZRgMjHDF
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
Supported: outbound
Contact: <sip:2010000039 at 127.0.1.1:34768
>;expires=1800;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>";reg-id=1;pub-gruu="
sip:2010000039 at ims.com;gr=urn:uuid:00000000-0000-0000-0000-000000000001"
Require: outbound
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Associated-URI: <sip:2010000039 at ims.com>
Content-Length:  0


thanks in advance, Please resply.

cheers,
sunil


On Tue, Mar 20, 2018 at 6:53 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi,
> It is using some other port on stress node not 5082. Is this a problem, if
> yes how can I fix this i have tried to open 5082 port on stress node using *sudo
> ufw allow 5082/tcp, *but no effect.
> Please check the wireshark log:
>
> Frame 2406: 703 bytes on wire (5624 bits), 703 bytes captured (5624 bits)
> Ethernet II, Src: PcsCompu_ff:d2:88 (08:00:27:ff:d2:88), Dst:
> PcsCompu_ab:71:0f (08:00:27:ab:71:0f)
> Internet Protocol Version 4, Src: 10.224.61.22, Dst: 10.224.61.13
> Transmission Control Protocol, Src Port: rlm-admin (5054), Dst Port: 34312
> (34312), Seq: 349, Ack: 2199, Len: 637
> Session Initiation Protocol (503)
>
>
> cheers,
> sunil
>
> On Tue, Mar 20, 2018 at 5:21 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:
>
>> Hi all,
>> I have taken tcpdump also but there is no SIP message. Please through
>> some light on this problem. I am trying from last weak, not able to catch
>> the problem. Thanks in advance.
>>
>> cheers,
>> Sunil
>>
>> On Tue, Mar 20, 2018 at 10:30 AM, Sunil Kumar <skgola1997 at gmail.com>
>> wrote:
>>
>>> Hi CW team,
>>> Anyone out there please help me. I am facing problem in stress testing.
>>> I have installed CW manually. whenever I was running 1 or less than 20 it
>>> give some errors like:
>>>
>>> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
>>> <http://ims.com> 1 2*
>>> [sudo] password for ubuntu:
>>> Starting initial registration, will take 0 seconds
>>> Initial registration succeeded
>>> Starting test
>>> Test complete
>>> Traceback (most recent call last):
>>>   File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
>>>     with open(CALLER_STATS) as f:
>>> IOError: [Errno 2] No such file or directory:
>>> '/var/log/clearwater-sip-stress/18065_caller_stats.log'
>>>
>>>
>>> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
>>> <http://ims.com> 10 5*
>>> Starting initial registration, will take 0 seconds
>>> Initial registration succeeded
>>> Starting test
>>> Test complete
>>> Traceback (most recent call last):
>>>   File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
>>>     call_success_rate = 100 * float(row['SuccessfulCall(C)']) /
>>> float(row['TotalCallCreated'])
>>> ZeroDivisionError: float division by zero
>>>
>>>
>>> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress
>>> iind.intel.com <http://iind.intel.com> 50 5*
>>> Starting initial registration, will take 0 seconds
>>> Initial registration succeeded
>>> Starting test
>>> Test complete
>>>
>>> Elapsed time: 00:03:41
>>> Start: 2018-03-20 17:46:43.268136
>>> End: 2018-03-20 17:51:31.363406
>>>
>>> Total calls: 2
>>> Successful calls: 0 (0.0%)
>>> Failed calls: 2 (100.0%)
>>> Unfinished calls: 0
>>>
>>> Retransmissions: 0
>>>
>>> Average time from INVITE to 180 Ringing: 0.0ms
>>> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
>>> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
>>> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
>>> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
>>> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
>>> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
>>> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
>>> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
>>> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
>>> # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
>>> Failed: call success rate 0.0% is lower than target 100.0%!
>>>
>>> Total re-REGISTERs: 8
>>> Successful re-REGISTERs: 8 (100.0%)
>>> Failed re-REGISTERS: 0 (0.0%)
>>>
>>> REGISTER retransmissions: 0
>>>
>>> Average time from REGISTER to 200 OK: 86.0ms
>>>
>>> Log files at /var/log/clearwater-sip-stress/18566_*
>>>
>>>
>>>
>>> *[]ubuntu at stress:~$ cat
>>> /var/log/clearwater-sip-stress/18566_caller_errors.log*
>>> sipp: The following events occured:
>>> 2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on
>>> unexpected message for Call-Id '1-18576 at 127.0.1.1': while expecting
>>> '183' (index 2), received '*SIP/2.0 503 Service Unavailable*
>>> Via: SIP/2.0/TCP 127.0.1.1:42276;received=10.22
>>> 4.61.13;branch=z9hG4bK-18576-1-0
>>> Record-Route: <sip:scscf.sprout.ims.com;tran
>>> sport=TCP;lr;billing-role=charge-term>
>>> Record-Route: <sip:scscf.sprout. ims.com ;transport=TCP;lr;billing-role
>>> =charge-orig>
>>> Call-ID: 1-18576 at 127.0.1.1
>>> From: <sip:2010000042@ ims.com >;tag=18576SIPpTag001
>>> To: <sip:2010000015@ ims.co>;tag=z9hG4bKPj1Lm9whhQM
>>> slKrcZxnN6qCH0tb9Lj5Neu
>>> CSeq: 1 INVITE
>>> P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com
>>> ;term-ioi= ims.com
>>> P-Charging-Function-Addresses: ccf=0.0.0.0
>>> Content-Length:  0
>>>
>>>
>>> *[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt*
>>> --start msg--
>>>
>>> SIP/2.0 200 OK
>>> Via: SIP/2.0/TCP 10.224.61.22;rport=49294;recei
>>> ved=10.224.61.22;branch=z9hG4bK-670172
>>> Call-ID: poll-sip-670172
>>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670172
>>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670172
>>> CSeq: 670172 OPTIONS
>>> Content-Length:  0
>>>
>>>
>>> --end msg--
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b
>>> Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> thread_dispatcher.cpp:270: Worker thread completed processing message
>>> 0x7f34ec34a3e8
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> thread_dispatcher.cpp:284: Request latency = 254us
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217:
>>> Rate adjustment calculation inputs: err -0.981500, smoothed latency 185,
>>> target latency 10000
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302:
>>> Maximum incoming request rate/second unchanged at 2000.000000 (current
>>> request rate is 0.200000 requests/sec, minimum threshold for a change is
>>> 1000.000000 requests/sec).
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
>>> 2000ui into continuous accumulator statistic
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
>>> 2000ui into continuous accumulator statistic
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed
>>> IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> thread_dispatcher.cpp:158: Attempting to process queue element
>>> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP connection closed
>>> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug
>>> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>>> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>>> 20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning:
>>> Failed to connect to the agentx master agent ([NIL]):
>>> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054
>>> TCP listener 10.224.61.22:5054: got incoming TCP connection from
>>> 10.224.61.22:42848, sock=573
>>> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> tcp->base.local_name: 10.224.61.22
>>> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP server transport created
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
>>> Processing incoming message: Request msg OPTIONS/cseq=670180
>>> (rdata0x7f34ec027690)
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose
>>> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180
>>> (rdata0x7f34ec027690) from TCP 10.224.61.22:42848:
>>> --start msg--
>>>
>>> OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
>>> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
>>> Max-Forwards: 2
>>> To: <sip:poll-sip at 10.224.61.22:5054>
>>> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670180
>>> Call-ID: poll-sip-670180
>>> CSeq: 670180 OPTIONS
>>> Contact: <sip:10.224.61.22>
>>> Accept: application/sdp
>>> Content-Length: 0
>>> User-Agent: poll-sip
>>>
>>>
>>> --end msg--
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
>>> home domain: false, local_to_node: true, is_gruu: false,
>>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
>>> Classified URI as 3
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
>>> 0x7f34ec34a3e8
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
>>> for worker threads with priority 15
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added
>>> IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> thread_dispatcher.cpp:183: Request latency so far = 57us
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c
>>> Distributing rdata to modules: Request msg OPTIONS/cseq=670180
>>> (rdata0x7f34ec34a3e8)
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139:
>>> home domain: false, local_to_node: true, is_gruu: false,
>>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172:
>>> Classified URI as 3
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint
>>> Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose
>>> common_sip_processing.cpp:103: TX 282 bytes Response msg
>>> 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848:
>>> --start msg--
>>>
>>> SIP/2.0 200 OK
>>> Via: SIP/2.0/TCP 10.224.61.22;rport=42848;recei
>>> ved=10.224.61.22;branch=z9hG4bK-670180
>>> Call-ID: poll-sip-670180
>>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670180
>>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670180
>>> CSeq: 670180 OPTIONS
>>> Content-Length:  0
>>>
>>>
>>> --end msg--
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809
>>> Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> thread_dispatcher.cpp:270: Worker thread completed processing message
>>> 0x7f34ec34a3e8
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> thread_dispatcher.cpp:284: Request latency = 129us
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341:
>>> Not recalculating rate as we haven't processed 20 requests yet (only 1).
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed
>>> IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> thread_dispatcher.cpp:158: Attempting to process queue element
>>> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327:
>>> Process request for URL /ping, args (null)
>>> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68:
>>> Sending response 200 to request for URL /ping, args (null)
>>> 20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP connection closed
>>> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug
>>> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>>> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>>> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053
>>> TCP listener 10.224.61.22:5053: got incoming TCP connection from
>>> 10.224.61.22:49356, sock=573
>>> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> tcp->base.local_name: 10.224.61.22
>>> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP server transport created
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
>>> Processing incoming message: Request msg OPTIONS/cseq=670182
>>> (rdata0x7f34ec027690)
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose
>>> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182
>>> (rdata0x7f34ec027690) from TCP 10.224.61.22:49356:
>>> --start msg--
>>>
>>> OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
>>> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
>>> Max-Forwards: 2
>>> To: <sip:poll-sip at 10.224.61.22:5053>
>>> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670182
>>> Call-ID: poll-sip-670182
>>> CSeq: 670182 OPTIONS
>>> Contact: <sip:10.224.61.22>
>>> Accept: application/sdp
>>> Content-Length: 0
>>> User-Agent: poll-sip
>>>
>>>
>>> --end msg--
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
>>> home domain: false, local_to_node: true, is_gruu: false,
>>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
>>> Classified URI as 3
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
>>> 0x7f34ec34a3e8
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
>>> for worker threads with priority 15
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added
>>> IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> thread_dispatcher.cpp:183: Request latency so far = 102us
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c
>>> Distributing rdata to modules: Request msg OPTIONS/cseq=670182
>>> (rdata0x7f34ec34a3e8)
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139:
>>> home domain: false, local_to_node: true, is_gruu: false,
>>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172:
>>> Classified URI as 3
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint
>>> Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose
>>> common_sip_processing.cpp:103: TX 282 bytes Response msg
>>> 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356:
>>> --start msg--
>>>
>>> SIP/2.0 200 OK
>>> Via: SIP/2.0/TCP 10.224.61.22;rport=49356;recei
>>> ved=10.224.61.22;branch=z9hG4bK-670182
>>> Call-ID: poll-sip-670182
>>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670182
>>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670182
>>> CSeq: 670182 OPTIONS
>>> Content-Length:  0
>>>
>>>
>>> --end msg--
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00
>>> Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> thread_dispatcher.cpp:270: Worker thread completed processing message
>>> 0x7f34ec34a3e8
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> thread_dispatcher.cpp:284: Request latency = 232us
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341:
>>> Not recalculating rate as we haven't processed 20 requests yet (only 2).
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed
>>> IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> thread_dispatcher.cpp:158: Attempting to process queue element
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244:
>>> Reraising all alarms with a known state
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1001.1 alarm
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1005.1 alarm
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1011.1 alarm
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1012.1 alarm
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1013.1 alarm
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1004.1 alarm
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1002.1 alarm
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1009.1 alarm
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1010.1 alarm
>>> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP connection closed
>>> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug
>>> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>>> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>>>
>>>
>>>
>>> all calls are failing I don't know what is going on, I am newbie to CW
>>> please guide some solution it will be great help.
>>>
>>>
>>> Thanks,
>>> Sunil
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180320/f9825676/attachment.html>

From skgola1997 at gmail.com  Fri Mar 16 13:49:24 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Fri, 16 Mar 2018 23:19:24 +0530
Subject: [Project Clearwater] Problems Stress testing
Message-ID: <CAHwYWpAU25SQ1cAmFKUnPPGYyVSo8ByG_1X_i3RtL0X4NKDZsg@mail.gmail.com>

 Hi Michael,
Thanks for replying. When I am running single call it giving some errors
like:

*[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress <home_domaiin>1 2*
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
    with open(CALLER_STATS) as f:
IOError: [Errno 2] No such file or directory: '/var/log/clearwater-sip-
stress/11218_caller_stats.log'


*[]ubuntu at stress:~$ cat
/var/log/clearwater-sip-stress/11218_re_reg_stats.log*
StartTime;LastResetTime;CurrentTime;ElapsedTime(P);
ElapsedTime(C);TargetRate;CallRate(P);CallRate(C);
IncomingCall(P);IncomingCall(C);OutgoingCall(P);OutgoingCall(C);
TotalCallCreated;CurrentCall;SuccessfulCall(P);SuccessfulCall(C);FailedCall(
P);FailedCall(C);FailedCannotSendMessage(P);FailedCannotSendMessage(C);
FailedMaxUDPRetrans(P);FailedMaxUDPRetrans(C);FailedTcpConnect(P);
FailedTcpConnect(C);FailedTcpClosed(P);FailedTcpClosed(C);
FailedUnexpectedMessage(P);FailedUnexpectedMessage(C);FailedCallRejected(P);
FailedCallRejected(C);FailedCmdNotSent(P);FailedCmdNotSent(C);
FailedRegexpDoesntMatch(P);FailedRegexpDoesntMatch(C);
FailedRegexpShouldntMatch(P);FailedRegexpShouldntMatch(C);
FailedRegexpHdrNotFound(P);FailedRegexpHdrNotFound(C);
FailedOutboundCongestion(P);FailedOutboundCongestion(C);
FailedTimeoutOnRecv(P);FailedTimeoutOnRecv(C);FailedTimeoutOnSend(P);
FailedTimeoutOnSend(C);OutOfCallMsgs(P);OutOfCallMsgs(C);DeadCallMsgs(
P);DeadCallMsgs(C);Retransmissions(P);Retransmissions(C);
AutoAnswered(P);AutoAnswered(C);Warnings(P);Warnings(C);
FatalErrors(P);FatalErrors(C);WatchdogMajor(P);WatchdogMajor(C);
WatchdogMinor(P);WatchdogMinor(C);ResponseTime1(P);ResponseTime1(C);
ResponseTime1StDev(P);ResponseTime1StDev(C);CallLength(P);CallLength(C);
CallLengthStDev(P);CallLengthStDev(C);ResponseTimeRepartition1;
ResponseTimeRepartition1_<2;ResponseTimeRepartition1_<10;
ResponseTimeRepartition1_<20;ResponseTimeRepartition1_<50;
ResponseTimeRepartition1_<100;ResponseTimeRepartition1_<200;
ResponseTimeRepartition1_<500;ResponseTimeRepartition1_<
1000;ResponseTimeRepartition1_<2000;ResponseTimeRepartition1_>=2000;
2018-03-17      06:43:11.763066 1521249191.763066;2018-03-17
06:43:11.763066 1521249191.763066;2018-03-17    06:43:11.770581
1521249191.770581;00:00:00;00:00:00;0.000555556;0;0;0;0;0;0;
0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;
0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;00:00:00:000000;00:00:
00:000000;00:00:00:000000;00:00:00:000000;00:00:00:000000;
00:00:00:000000;00:00:00:000000;00:00:00:000000;;0;0;0;0;0;0;0;0;0;0;
2018-03-17      06:43:11.763066 1521249191.763066;2018-03-17
06:43:11.770864 1521249191.770864;2018-03-17    06:43:11.772711
1521249191.772711;00:00:00;00:00:00;0.000555556;0;0;0;0;0;0;
0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;
0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;00:00:00:000000;00:00:
00:000000;00:00:00:000000;00:00:00:000000;00:00:00:000000;
00:00:00:000000;00:00:00:000000;00:00:00:000000;;0;0;0;0;0;0;0;0;0;0;
2018-03-17      06:43:11.763066 1521249191.763066;2018-03-17
06:43:11.772796 1521249191.772796;2018-03-17    06:43:11.772860
1521249191.772860;00:00:00;00:00:00;0.000555556;0;0;0;0;0;0;
0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;
0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;00:00:00:000000;00:00:
00:000000;00:00:00:000000;00:00:00:000000;00:00:00:000000;
00:00:00:000000;00:00:00:000000;00:00:00:000000;;0;0;0;0;0;0;0;0;0;0;



*[sprout]ubuntu at sprout:/var/log/sprout$ tail -30 sprout_current.txt*
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:183:
Request latency so far = 102us
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug pjsip: sip_endpoint.c
Distributing rdata to modules: Request msg OPTIONS/cseq=370860
(rdata0x7f7548081478)
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug uri_classifier.cpp:139:
home domain: false, local_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug uri_classifier.cpp:172:
Classified URI as 3
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug pjsip:       endpoint
Response msg 200/OPTIONS/cseq=370860 (tdta0x7f7548382670) created
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Verbose
common_sip_processing.cpp:103: TX 282 bytes Response msg
200/OPTIONS/cseq=370860 (tdta0x7f7548382670) to TCP 10.224.61.22:51202:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=51202;received=10.224.61.22;branch=
z9hG4bK-370860
Call-ID: poll-sip-370860
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=370860
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-370860
CSeq: 370860 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug pjsip: tdta0x7f754838
Destroying txdata Response msg 200/OPTIONS/cseq=370860 (tdta0x7f7548382670)
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:270:
Worker thread completed processing message 0x7f7548081478
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:284:
Request latency = 278us
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
event_statistic_accumulator.cpp:32: Accumulate 278 for 0xf627a8
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
event_statistic_accumulator.cpp:32: Accumulate 278 for 0xf62820
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug load_monitor.cpp:341: Not
recalculating rate as we haven't processed 20 requests yet (only 18).
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug utils.cpp:878: Removed
IOHook 0x7f75a6face30 to stack. There are now 0 hooks
17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:158:
Attempting to process queue element
17-03-2018 01:18:53.122 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842
TCP connection closed
17-03-2018 01:18:53.122 UTC [7f754feff700] Debug connection_tracker.cpp:67:
Connection 0x7f75484260d8 has been destroyed
17-03-2018 01:18:53.122 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842
TCP transport destroyed with reason 70016: End of file (PJ_EEOF)




*[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt*
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-369729
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=369729
Call-ID: poll-sip-369729
CSeq: 369729 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug uri_classifier.cpp:139:
home domain: false, local_to_node: true, is_gruu: false,
enforce_user_phone: false, pref$
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug uri_classifier.cpp:172:
Classified URI as 3
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug
common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug thread_dispatcher.cpp:554:
Recieved message 0x7f7548427c20 on worker thread
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug thread_dispatcher.cpp:571:
Admitted request 0x7f7548427c20 on worker thread
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug thread_dispatcher.cpp:606:
Incoming message 0x7f7548427c20 cloned to 0x7f7548467bd8
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug thread_dispatcher.cpp:625:
Queuing cloned received message 0x7f7548467bd8 for worker threads with
priority 15
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug
event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66738
17-03-2018 01:00:00.711 UTC [7f754feff700] Debug
event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66780
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug utils.cpp:872: Added
IOHook 0x7f75a8fb0e30 to stack. There are now 1 hooks
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug thread_dispatcher.cpp:178:
Worker thread dequeue message 0x7f7548467bd8
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug thread_dispatcher.cpp:183:
Request latency so far = 57us
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug pjsip: sip_endpoint.c
Distributing rdata to modules: Request msg OPTIONS/cseq=369729
(rdata0x7f7548467bd8)
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug uri_classifier.cpp:139:
home domain: false, local_to_node: true, is_gruu: false,
enforce_user_phone: false, pref$
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug uri_classifier.cpp:172:
Classified URI as 3
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Debug pjsip:       endpoint
Response msg 200/OPTIONS/cseq=369729 (tdta0x7f75e82f5370) created
17-03-2018 01:00:00.712 UTC [7f75a8fb1700] Verbose
common_sip_processing.cpp:103: TX 282 bytes Response msg
200/OPTIONS/cseq=369729 (tdta0x7f75e82f5370) to TCP 1$
--start msg--



Thanks and Regards,
Sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180316/72e042fe/attachment.html>

From skgola1997 at gmail.com  Fri Mar 16 14:09:50 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Fri, 16 Mar 2018 23:39:50 +0530
Subject: [Project Clearwater] stress testing
In-Reply-To: <SN1PR02MB1677B6DCCE8F163251561073F5D70@SN1PR02MB1677.namprd02.prod.outlook.com>
References: <CAPqjCcHWiFOEP5EcYhTiiYqTruaGugAFgnkDkndSmMEqa9SOdA@mail.gmail.com>
	<SN1PR02MB1677B6DCCE8F163251561073F5D70@SN1PR02MB1677.namprd02.prod.outlook.com>
Message-ID: <CAHwYWpBDVS+LwYKnOdbVA_D0DrnOH-Fj54Q=rd6WL30xLKZoPw@mail.gmail.com>

Hi,
when i ran 10, it showing error like:

[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress <home_domain> 10 2
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
    call_success_rate = 100 * float(row['SuccessfulCall(C)']) /
float(row['TotalCallCreated'])
ZeroDivisionError: float division by zero

Thanks, Please give some solution

Sunil

On Fri, Mar 16, 2018 at 10:50 PM, Michael Duppr? <
Michael.Duppre at metaswitch.com> wrote:

> Hello Sunil,
>
>
>
> It sounds like you have two different problems:
>
>    1. Running `/usr/share/clearwater/bin/run_stress <home_domain> 1800 2`
>    doesn?t succeed making any calls
>       - The good news is that the initial REGISTERs that the tool sends
>       out are working fine as you can see in the output `Initial registration
>       succeeded`!
>       - To make the calls work, I would suggest just running one call
>       with the tool initially and once this works go back to running 1800 calls.
>       To debug this, you should turn on debug logs on Sprout (see
>       http://clearwater.readthedocs.io/en/stable/Troubleshooting_
>       and_Recovery.html#sprout
>       <http://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html#sprout>),
>       run the tool again with one call. Then have a look at the logs of the tool
>       and the Sprout logs and find out why Sprout is sending back a 503 response.
>    2. Running `/usr/share/clearwater/bin/run_stress <home_domain> 1800 10
>    --icscf-target TARGET=sprout.{domain}:5052 --scscf-target
>    TARGET=sprout.{domain}:5054` crashes
>       - You probably don?t need to provide more arguments when running
>       the tool as it should work with the defaults. So I suggest getting the tool
>       work without any additional arguments and only use these if you have a
>       specific need for additional configuration.
>       - The error ` Unknown remote host 'TARGET=sprout.<domain>` says,
>       the tool wasn?t able to lookup this domain. In fact, you need to replace
>       the word `TARGET` with your full sprout domain. If for example
>       `my_ims_domain` is your domain, you would need to run the command as
>       follows:
>       - `/usr/share/clearwater/bin/run_stress <home_domain> 1800 10
>       --icscf-target sprout.my_ims_domain:5052 --scscf-target
>       sprout.my_ims_domain:5054`
>
>
>
> Hope this helps, good luck getting the calls to work!
>
>
>
> Kind regards,
>
> Michael
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Pushpendra
> *Sent:* 14 March 2018 12:05
> *To:* Bennett Allen <Bennett.Allen at metaswitch.com>; clearwater at lists.
> projectclearwater.org
> *Subject:* [Project Clearwater] stress testing
>
>
>
> Hi All,
>
> I need to ask few questions regarding stress testing for manual
> installation of clearwater. I am following http://clearwater.
> readthedocs.io/en/stable/Clearwater_stress_testing.html for stress
> testing.
>
>  I have set the* local_ip of this node* only in
> * /etc/clearwater/local_config. *I also set *reg_max_expires=1800* in all
> the other node in *shared_config *after that I created the 50000 no. in
> vellum node as given in doc
>
>  https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%
> 20Numbers.md?utf8=%E2%9C%93
>
> and then I install the debian package *sudo apt-get install
> clearwater-sip-stress-careonly *on new node.
>
> when I run
>
> /usr/share/clearwater/bin/run_stress <home_domain> 1800 2
>
> Starting initial registration, will take 22 seconds
>
> Initial registration succeeded
>
> Starting test
>
> Test complete
>
>
>
> Elapsed time: 00:02:00
>
> Start: 2018-03-14 18:06:06.635672
>
> End: 2018-03-14 18:08:06.714723
>
>
>
> Total calls: 39
>
> Successful calls: 0 (0.0%)
>
> Failed calls: 39 (100.0%)
>
> Unfinished calls: 0
>
>
>
> Retransmissions: 0
>
>
>
> Average time from INVITE to 180 Ringing: 0.0ms
>
> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
>
> Failed: call success rate 0.0% is lower than target 100.0%!
>
>
>
> Total re-REGISTERs: 120
>
> Successful re-REGISTERs: 120 (100.0%)
>
> Failed re-REGISTERS: 0 (0.0%)
>
>
>
> REGISTER retransmissions: 0
>
>
>
> Average time from REGISTER to 200 OK: 30.0ms
>
>
>
> Log files at /var/log/clearwater-sip-stress/13921_*
>
>
>
> 1. I have not mention new node's IP in any other node of clearwater, how
> the other node know about it?
>
>
>
> My question is, *Do I need to mention the IP of new node in any other
> node of clearwater?*
>
>
>
>
>
> when I run -
>
> []ubuntu at stress:~$  /usr/share/clearwater/bin/run_stress <home_domain>
> 1800 10 --icscf-target TARGET=sprout.{domain}:5052 --scscf-target
> TARGET=sprout.{domain}:5054
>
>
>
> Starting initial registration, will take 22 seconds
>
> Initial registration failed - see /var/log/clearwater-sip-
> stress/14115_initial_reg_errors.log for details of the errors
>
>
>
>
>
>
>
> *[]ubuntu at stress:/var/log/clearwater-sip-stress$ tail -30
> 14115_initial_reg_errors.log*
>
> sipp: The following events occured:
>
> 2018-03-14      18:11:06.637370 1521031266.637370: Unknown remote host
> 'TARGET=sprout.<domain>' (Name or service not known, Inappropriate ioctl
> for device).
>
> Use 'sipp -h' for details.
>
>
>
>
>
> *[]ubuntu at stress:/var/log/clearwater-sip-stress$ cat
> 9923_caller_errors.log*
>
> 2018-03-14      00:32:45.274812 1520967765.274812: Aborting call on
> unexpected message for Call-Id '54-9937 at 127.0.1.1': while expecting '183'
> (index 2), received 'SIP/2.0 503 Service Unavailable
>
> Via: SIP/2.0/TCP 127.0.1.1:34015;received=10.
> 224.61.13;branch=z9hG4bK-9937-54-0
>
> Record-Route: <sip:scscf.sprout. <home_domain> ;transport=TCP;lr;
> billing-role=charge-term>
>
> Record-Route: <sip:scscf.sprout. <home_domain> ;transport=TCP;lr;
> billing-role=charge-orig>
>
> Call-ID: 54-9937 at 127.0.1.1
>
> From: <sip:2010000252@<home_domain>>;tag=9937SIPpTag0054
>
> To: <sip:2010000647@ <home_domain> >;tag=z9hG4bKPjJVqopmpeih7pUoxJrYOG7
> CO4-Bm69ozI
>
> CSeq: 1 INVITE
>
> P-Charging-Vector: icid-value="9937SIPpTag0054";
> orig-ioi=<home_domain>;term-ioi=<home_domain>
>
> P-Charging-Function-Addresses: ccf=0.0.0.0
>
> Content-Length:  0
>
>
>
> '.
>
>
>
>
>
>
>
> Regards,
>
>
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180316/f64429f2/attachment.html>

From skgola1997 at gmail.com  Fri Mar 16 14:21:49 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Fri, 16 Mar 2018 23:51:49 +0530
Subject: [Project Clearwater] stress testing
In-Reply-To: <CAHwYWpDPR-=hD3Cw9znfsrpXBkN=QfzvhLsZrTggjsxd0nyWPA@mail.gmail.com>
References: <CAPqjCcHWiFOEP5EcYhTiiYqTruaGugAFgnkDkndSmMEqa9SOdA@mail.gmail.com>
	<SN1PR02MB1677B6DCCE8F163251561073F5D70@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpDPR-=hD3Cw9znfsrpXBkN=QfzvhLsZrTggjsxd0nyWPA@mail.gmail.com>
Message-ID: <CAHwYWpAvzUUeUpoyA7oi40SDkcW4i2TW7Y+vyNeEppt6piJZAg@mail.gmail.com>

Hi,
PFA the sprout_current log file. Please guide me the solution, I am newbie
to clearwater :-)

Thanks,
Sunil

On Fri, Mar 16, 2018 at 11:07 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi Michael,
> Thanks for replying. When I am running single call it giving some errors
> like:
>
> *[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress iind.intel.com
> <http://iind.intel.com> 1 2*
> Starting initial registration, will take 0 seconds
> Initial registration succeeded
> Starting test
> Test complete
> Traceback (most recent call last):
>   File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
>     with open(CALLER_STATS) as f:
> IOError: [Errno 2] No such file or directory: '/var/log/clearwater-sip-
> stress/11218_caller_stats.log'
>
>
> *[]ubuntu at stress:~$ cat
> /var/log/clearwater-sip-stress/11218_re_reg_stats.log*
> StartTime;LastResetTime;CurrentTime;ElapsedTime(P);
> ElapsedTime(C);TargetRate;CallRate(P);CallRate(C);
> IncomingCall(P);IncomingCall(C);OutgoingCall(P);OutgoingCall(C);
> TotalCallCreated;CurrentCall;SuccessfulCall(P);
> SuccessfulCall(C);FailedCall(P);FailedCall(C);FailedCannotSendMessage(P);
> FailedCannotSendMessage(C);FailedMaxUDPRetrans(P);FailedMaxUDPRetrans(C);
> FailedTcpConnect(P);FailedTcpConnect(C);FailedTcpClosed(P);
> FailedTcpClosed(C);FailedUnexpectedMessage(P);FailedUnexpectedMessage(C);
> FailedCallRejected(P);FailedCallRejected(C);FailedCmdNotSent(P);
> FailedCmdNotSent(C);FailedRegexpDoesntMatch(P);FailedRegexpDoesntMatch(C);
> FailedRegexpShouldntMatch(P);FailedRegexpShouldntMatch(C);
> FailedRegexpHdrNotFound(P);FailedRegexpHdrNotFound(C);
> FailedOutboundCongestion(P);FailedOutboundCongestion(C);
> FailedTimeoutOnRecv(P);FailedTimeoutOnRecv(C);FailedTimeoutOnSend(P);
> FailedTimeoutOnSend(C);OutOfCallMsgs(P);OutOfCallMsgs(C);DeadCallMsgs(
> P);DeadCallMsgs(C);Retransmissions(P);Retransmissions(C);
> AutoAnswered(P);AutoAnswered(C);Warnings(P);Warnings(C);
> FatalErrors(P);FatalErrors(C);WatchdogMajor(P);WatchdogMajor(C);
> WatchdogMinor(P);WatchdogMinor(C);ResponseTime1(P);ResponseTime1(C);
> ResponseTime1StDev(P);ResponseTime1StDev(C);CallLength(P);CallLength(C);
> CallLengthStDev(P);CallLengthStDev(C);ResponseTimeRepartition1;
> ResponseTimeRepartition1_<2;ResponseTimeRepartition1_<10;
> ResponseTimeRepartition1_<20;ResponseTimeRepartition1_<50;
> ResponseTimeRepartition1_<100;ResponseTimeRepartition1_<200;
> ResponseTimeRepartition1_<500;ResponseTimeRepartition1_<
> 1000;ResponseTimeRepartition1_<2000;ResponseTimeRepartition1_>=2000;
> 2018-03-17      06:43:11.763066 1521249191.763066;2018-03-17
> 06:43:11.763066 1521249191.763066;2018-03-17    06:43:11.770581
> 1521249191.770581;00:00:00;00:00:00;0.000555556;0;0;0;0;0;0;
> 0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;
> 0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;00:00:00:000000;00:00:
> 00:000000;00:00:00:000000;00:00:00:000000;00:00:00:000000;
> 00:00:00:000000;00:00:00:000000;00:00:00:000000;;0;0;0;0;0;0;0;0;0;0;
> 2018-03-17      06:43:11.763066 1521249191.763066;2018-03-17
> 06:43:11.770864 1521249191.770864;2018-03-17    06:43:11.772711
> 1521249191.772711;00:00:00;00:00:00;0.000555556;0;0;0;0;0;0;
> 0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;
> 0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;00:00:00:000000;00:00:
> 00:000000;00:00:00:000000;00:00:00:000000;00:00:00:000000;
> 00:00:00:000000;00:00:00:000000;00:00:00:000000;;0;0;0;0;0;0;0;0;0;0;
> 2018-03-17      06:43:11.763066 1521249191.763066;2018-03-17
> 06:43:11.772796 1521249191.772796;2018-03-17    06:43:11.772860
> 1521249191.772860;00:00:00;00:00:00;0.000555556;0;0;0;0;0;0;
> 0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;
> 0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;00:00:00:000000;00:00:
> 00:000000;00:00:00:000000;00:00:00:000000;00:00:00:000000;
> 00:00:00:000000;00:00:00:000000;00:00:00:000000;;0;0;0;0;0;0;0;0;0;0;
>
>
>
> *[sprout]ubuntu at sprout:/var/log/sprout$ tail -30 sprout_current.txt*
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
> thread_dispatcher.cpp:183: Request latency so far = 102us
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug pjsip: sip_endpoint.c
> Distributing rdata to modules: Request msg OPTIONS/cseq=370860
> (rdata0x7f7548081478)
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug uri_classifier.cpp:172:
> Classified URI as 3
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug pjsip:       endpoint
> Response msg 200/OPTIONS/cseq=370860 (tdta0x7f7548382670) created
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Verbose
> common_sip_processing.cpp:103: TX 282 bytes Response msg
> 200/OPTIONS/cseq=370860 (tdta0x7f7548382670) to TCP 10.224.61.22:51202:
> --start msg--
>
> SIP/2.0 200 OK
> Via: SIP/2.0/TCP 10.224.61.22;rport=51202;received=10.224.61.22;branch=
> z9hG4bK-370860
> Call-ID: poll-sip-370860
> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=370860
> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-370860
> CSeq: 370860 OPTIONS
> Content-Length:  0
>
>
> --end msg--
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug pjsip: tdta0x7f754838
> Destroying txdata Response msg 200/OPTIONS/cseq=370860 (tdta0x7f7548382670)
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
> thread_dispatcher.cpp:270: Worker thread completed processing message
> 0x7f7548081478
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
> thread_dispatcher.cpp:284: Request latency = 278us
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 278 for 0xf627a8
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 278 for 0xf62820
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug load_monitor.cpp:341: Not
> recalculating rate as we haven't processed 20 requests yet (only 18).
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug utils.cpp:878: Removed
> IOHook 0x7f75a6face30 to stack. There are now 0 hooks
> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
> thread_dispatcher.cpp:158: Attempting to process queue element
> 17-03-2018 01:18:53.122 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842
> TCP connection closed
> 17-03-2018 01:18:53.122 UTC [7f754feff700] Debug
> connection_tracker.cpp:67: Connection 0x7f75484260d8 has been destroyed
> 17-03-2018 01:18:53.122 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842
> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>
>
> Thanks and Regards,
> Sunil
>
>
>
> On Fri, Mar 16, 2018 at 10:50 PM, Michael Duppr? <
> Michael.Duppre at metaswitch.com> wrote:
>
>> Hello Sunil,
>>
>>
>>
>> It sounds like you have two different problems:
>>
>>    1. Running `/usr/share/clearwater/bin/run_stress <home_domain> 1800
>>    2` doesn?t succeed making any calls
>>       - The good news is that the initial REGISTERs that the tool sends
>>       out are working fine as you can see in the output `Initial registration
>>       succeeded`!
>>       - To make the calls work, I would suggest just running one call
>>       with the tool initially and once this works go back to running 1800 calls.
>>       To debug this, you should turn on debug logs on Sprout (see
>>       http://clearwater.readthedocs.io/en/stable/Troubleshooting_a
>>       nd_Recovery.html#sprout
>>       <http://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html#sprout>),
>>       run the tool again with one call. Then have a look at the logs of the tool
>>       and the Sprout logs and find out why Sprout is sending back a 503 response.
>>    2. Running `/usr/share/clearwater/bin/run_stress <home_domain> 1800
>>    10 --icscf-target TARGET=sprout.{domain}:5052 --scscf-target
>>    TARGET=sprout.{domain}:5054` crashes
>>       - You probably don?t need to provide more arguments when running
>>       the tool as it should work with the defaults. So I suggest getting the tool
>>       work without any additional arguments and only use these if you have a
>>       specific need for additional configuration.
>>       - The error ` Unknown remote host 'TARGET=sprout.<domain>` says,
>>       the tool wasn?t able to lookup this domain. In fact, you need to replace
>>       the word `TARGET` with your full sprout domain. If for example
>>       `my_ims_domain` is your domain, you would need to run the command as
>>       follows:
>>       - `/usr/share/clearwater/bin/run_stress <home_domain> 1800 10
>>       --icscf-target sprout.my_ims_domain:5052 --scscf-target
>>       sprout.my_ims_domain:5054`
>>
>>
>>
>> Hope this helps, good luck getting the calls to work!
>>
>>
>>
>> Kind regards,
>>
>> Michael
>>
>>
>>
>>
>>
>> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
>> *On Behalf Of *Pushpendra
>> *Sent:* 14 March 2018 12:05
>> *To:* Bennett Allen <Bennett.Allen at metaswitch.com>;
>> clearwater at lists.projectclearwater.org
>> *Subject:* [Project Clearwater] stress testing
>>
>>
>>
>> Hi All,
>>
>> I need to ask few questions regarding stress testing for manual
>> installation of clearwater. I am following http://clearwater.re
>> adthedocs.io/en/stable/Clearwater_stress_testing.html for stress testing.
>>
>>  I have set the* local_ip of this node* only in
>> * /etc/clearwater/local_config. *I also set *reg_max_expires=1800* in
>> all the other node in *shared_config *after that I created the 50000 no.
>> in vellum node as given in doc
>>
>>  https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-
>> Provisioning%20Numbers.md?utf8=%E2%9C%93
>>
>> and then I install the debian package *sudo apt-get install
>> clearwater-sip-stress-careonly *on new node.
>>
>> when I run
>>
>> /usr/share/clearwater/bin/run_stress <home_domain> 1800 2
>>
>> Starting initial registration, will take 22 seconds
>>
>> Initial registration succeeded
>>
>> Starting test
>>
>> Test complete
>>
>>
>>
>> Elapsed time: 00:02:00
>>
>> Start: 2018-03-14 18:06:06.635672
>>
>> End: 2018-03-14 18:08:06.714723
>>
>>
>>
>> Total calls: 39
>>
>> Successful calls: 0 (0.0%)
>>
>> Failed calls: 39 (100.0%)
>>
>> Unfinished calls: 0
>>
>>
>>
>> Retransmissions: 0
>>
>>
>>
>> Average time from INVITE to 180 Ringing: 0.0ms
>>
>> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> Failed: call success rate 0.0% is lower than target 100.0%!
>>
>>
>>
>> Total re-REGISTERs: 120
>>
>> Successful re-REGISTERs: 120 (100.0%)
>>
>> Failed re-REGISTERS: 0 (0.0%)
>>
>>
>>
>> REGISTER retransmissions: 0
>>
>>
>>
>> Average time from REGISTER to 200 OK: 30.0ms
>>
>>
>>
>> Log files at /var/log/clearwater-sip-stress/13921_*
>>
>>
>>
>> 1. I have not mention new node's IP in any other node of clearwater, how
>> the other node know about it?
>>
>>
>>
>> My question is, *Do I need to mention the IP of new node in any other
>> node of clearwater?*
>>
>>
>>
>>
>>
>> when I run -
>>
>> []ubuntu at stress:~$  /usr/share/clearwater/bin/run_stress <home_domain>
>> 1800 10 --icscf-target TARGET=sprout.{domain}:5052 --scscf-target
>> TARGET=sprout.{domain}:5054
>>
>>
>>
>> Starting initial registration, will take 22 seconds
>>
>> Initial registration failed - see /var/log/clearwater-sip-stress/14115_initial_reg_errors.log
>> for details of the errors
>>
>>
>>
>>
>>
>>
>>
>> *[]ubuntu at stress:/var/log/clearwater-sip-stress$ tail -30
>> 14115_initial_reg_errors.log*
>>
>> sipp: The following events occured:
>>
>> 2018-03-14      18:11:06.637370 1521031266.637370: Unknown remote host
>> 'TARGET=sprout.<domain>' (Name or service not known, Inappropriate ioctl
>> for device).
>>
>> Use 'sipp -h' for details.
>>
>>
>>
>>
>>
>> *[]ubuntu at stress:/var/log/clearwater-sip-stress$ cat
>> 9923_caller_errors.log*
>>
>> 2018-03-14      00:32:45.274812 1520967765.274812: Aborting call on
>> unexpected message for Call-Id '54-9937 at 127.0.1.1': while expecting
>> '183' (index 2), received 'SIP/2.0 503 Service Unavailable
>>
>> Via: SIP/2.0/TCP 127.0.1.1:34015;received=10.22
>> 4.61.13;branch=z9hG4bK-9937-54-0
>>
>> Record-Route: <sip:scscf.sprout. <home_domain> ;transport=TCP;lr;billing-
>> role=charge-term>
>>
>> Record-Route: <sip:scscf.sprout. <home_domain> ;transport=TCP;lr;billing-
>> role=charge-orig>
>>
>> Call-ID: 54-9937 at 127.0.1.1
>>
>> From: <sip:2010000252@<home_domain>>;tag=9937SIPpTag0054
>>
>> To: <sip:2010000647@ <home_domain> >;tag=z9hG4bKPjJVqopmpeih7pUo
>> xJrYOG7CO4-Bm69ozI
>>
>> CSeq: 1 INVITE
>>
>> P-Charging-Vector: icid-value="9937SIPpTag0054";o
>> rig-ioi=<home_domain>;term-ioi=<home_domain>
>>
>> P-Charging-Function-Addresses: ccf=0.0.0.0
>>
>> Content-Length:  0
>>
>>
>>
>> '.
>>
>>
>>
>>
>>
>>
>>
>> Regards,
>>
>>
>>
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180316/f4036f9c/attachment.html>
-------------- next part --------------
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373106
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373106
Call-ID: poll-sip-373106
CSeq: 373106 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548073820 on worker thread
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548073820 on worker thread
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548073820 cloned to 0x7f754842f9c8
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f754842f9c8 for worker threads with priority 15
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:17.841 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug utils.cpp:872: Added IOHook 0x7f75ae7bbe30 to stack. There are now 1 hooks
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f754842f9c8
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug thread_dispatcher.cpp:183: Request latency so far = 108us
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373106 (rdata0x7f754842f9c8)
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373106 (tdta0x7f75e02378d0) created
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373106 (tdta0x7f75e02378d0) to TCP 10.224.61.22:36738:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=36738;received=10.224.61.22;branch=z9hG4bK-373106
Call-ID: poll-sip-373106
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373106
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373106
CSeq: 373106 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug pjsip: tdta0x7f75e023 Destroying txdata Response msg 200/OPTIONS/cseq=373106 (tdta0x7f75e02378d0)
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f754842f9c8
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug thread_dispatcher.cpp:284: Request latency = 273us
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug event_statistic_accumulator.cpp:32: Accumulate 273 for 0xf62778
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug event_statistic_accumulator.cpp:32: Accumulate 273 for 0xf62820
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 13).
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug utils.cpp:878: Removed IOHook 0x7f75ae7bbe30 to stack. There are now 0 hooks
17-03-2018 01:56:17.841 UTC [7f75ae7bc700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:19.842 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:56:19.842 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f75480734e8 has been destroyed
17-03-2018 01:56:19.842 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:20.008 UTC [7f75ee6a5700] Status alarm.cpp:244: Reraising all alarms with a known state
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1001.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1005.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1011.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1012.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1013.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1004.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1002.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1009.1 alarm
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1010.1 alarm
17-03-2018 01:56:25.891 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58522, sock=690
17-03-2018 01:56:25.891 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:25.891 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373114 (rdata0x7f7548073820)
17-03-2018 01:56:25.913 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373114 (rdata0x7f7548073820) from TCP 10.224.61.22:58522:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373114
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373114
Call-ID: poll-sip-373114
CSeq: 373114 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548073820 on worker thread
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548073820 on worker thread
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548073820 cloned to 0x7f754842f9c8
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f754842f9c8 for worker threads with priority 15
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:25.913 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug utils.cpp:872: Added IOHook 0x7f75aefbce30 to stack. There are now 1 hooks
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f754842f9c8
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug thread_dispatcher.cpp:183: Request latency so far = 62us
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373114 (rdata0x7f754842f9c8)
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373114 (tdta0x7f75d033ba90) created
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373114 (tdta0x7f75d033ba90) to TCP 10.224.61.22:58522:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58522;received=10.224.61.22;branch=z9hG4bK-373114
Call-ID: poll-sip-373114
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373114
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373114
CSeq: 373114 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug pjsip: tdta0x7f75d033 Destroying txdata Response msg 200/OPTIONS/cseq=373114 (tdta0x7f75d033ba90)
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f754842f9c8
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug thread_dispatcher.cpp:284: Request latency = 127us
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug event_statistic_accumulator.cpp:32: Accumulate 127 for 0xf62778
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug event_statistic_accumulator.cpp:32: Accumulate 127 for 0xf62820
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 14).
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug utils.cpp:878: Removed IOHook 0x7f75aefbce30 to stack. There are now 0 hooks
17-03-2018 01:56:25.913 UTC [7f75aefbd700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:25.935 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:56:25.935 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:56:27.915 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:56:27.915 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f75480734e8 has been destroyed
17-03-2018 01:56:27.915 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:27.926 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:36810, sock=690
17-03-2018 01:56:27.926 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:27.926 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373116 (rdata0x7f7548073820)
17-03-2018 01:56:27.926 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373116 (rdata0x7f7548073820) from TCP 10.224.61.22:36810:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373116
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373116
Call-ID: poll-sip-373116
CSeq: 373116 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548073820 on worker thread
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548073820 on worker thread
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548073820 cloned to 0x7f754842f9c8
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f754842f9c8 for worker threads with priority 15
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:27.926 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug utils.cpp:872: Added IOHook 0x7f75be7dbe30 to stack. There are now 1 hooks
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f754842f9c8
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug thread_dispatcher.cpp:183: Request latency so far = 78us
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373116 (rdata0x7f754842f9c8)
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373116 (tdta0x7f753c24d450) created
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373116 (tdta0x7f753c24d450) to TCP 10.224.61.22:36810:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=36810;received=10.224.61.22;branch=z9hG4bK-373116
Call-ID: poll-sip-373116
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373116
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373116
CSeq: 373116 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug pjsip: tdta0x7f753c24 Destroying txdata Response msg 200/OPTIONS/cseq=373116 (tdta0x7f753c24d450)
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f754842f9c8
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug thread_dispatcher.cpp:284: Request latency = 186us
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug event_statistic_accumulator.cpp:32: Accumulate 186 for 0xf62778
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug event_statistic_accumulator.cpp:32: Accumulate 186 for 0xf62820
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 15).
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug utils.cpp:878: Removed IOHook 0x7f75be7dbe30 to stack. There are now 0 hooks
17-03-2018 01:56:27.927 UTC [7f75be7dc700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:29.928 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:56:29.928 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f75480734e8 has been destroyed
17-03-2018 01:56:29.928 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:30.428 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:56:35.964 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58584, sock=690
17-03-2018 01:56:35.964 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:35.964 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373124 (rdata0x7f7548073820)
17-03-2018 01:56:35.990 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373124 (rdata0x7f7548073820) from TCP 10.224.61.22:58584:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373124
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373124
Call-ID: poll-sip-373124
CSeq: 373124 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548073820 on worker thread
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548073820 on worker thread
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548073820 cloned to 0x7f754842f9c8
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f754842f9c8 for worker threads with priority 15
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:35.990 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug utils.cpp:872: Added IOHook 0x7f75adfbae30 to stack. There are now 1 hooks
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f754842f9c8
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug thread_dispatcher.cpp:183: Request latency so far = 55us
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373124 (rdata0x7f754842f9c8)
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373124 (tdta0x7f75e82b6d00) created
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373124 (tdta0x7f75e82b6d00) to TCP 10.224.61.22:58584:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58584;received=10.224.61.22;branch=z9hG4bK-373124
Call-ID: poll-sip-373124
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373124
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373124
CSeq: 373124 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug pjsip: tdta0x7f75e82b Destroying txdata Response msg 200/OPTIONS/cseq=373124 (tdta0x7f75e82b6d00)
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f754842f9c8
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug thread_dispatcher.cpp:284: Request latency = 127us
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug event_statistic_accumulator.cpp:32: Accumulate 127 for 0xf62778
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug event_statistic_accumulator.cpp:32: Accumulate 127 for 0xf62820
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 16).
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug utils.cpp:878: Removed IOHook 0x7f75adfbae30 to stack. There are now 0 hooks
17-03-2018 01:56:35.990 UTC [7f75adfbb700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:36.008 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:56:36.008 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:56:37.990 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:56:37.990 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f75480734e8 has been destroyed
17-03-2018 01:56:37.991 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:38.003 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:36876, sock=690
17-03-2018 01:56:38.003 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:38.003 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373126 (rdata0x7f7548073820)
17-03-2018 01:56:38.003 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373126 (rdata0x7f7548073820) from TCP 10.224.61.22:36876:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373126
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373126
Call-ID: poll-sip-373126
CSeq: 373126 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548073820 on worker thread
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548073820 on worker thread
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548073820 cloned to 0x7f754842f9c8
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f754842f9c8 for worker threads with priority 15
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:38.003 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug utils.cpp:872: Added IOHook 0x7f75b07bfe30 to stack. There are now 1 hooks
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f754842f9c8
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug thread_dispatcher.cpp:183: Request latency so far = 92us
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373126 (rdata0x7f754842f9c8)
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373126 (tdta0x7f7544235830) created
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373126 (tdta0x7f7544235830) to TCP 10.224.61.22:36876:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=36876;received=10.224.61.22;branch=z9hG4bK-373126
Call-ID: poll-sip-373126
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373126
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373126
CSeq: 373126 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug pjsip: tdta0x7f754423 Destroying txdata Response msg 200/OPTIONS/cseq=373126 (tdta0x7f7544235830)
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f754842f9c8
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug thread_dispatcher.cpp:284: Request latency = 203us
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug event_statistic_accumulator.cpp:32: Accumulate 203 for 0xf62778
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug event_statistic_accumulator.cpp:32: Accumulate 203 for 0xf62820
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 17).
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug utils.cpp:878: Removed IOHook 0x7f75b07bfe30 to stack. There are now 0 hooks
17-03-2018 01:56:38.003 UTC [7f75b07c0700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:40.004 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:56:40.004 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f75480734e8 has been destroyed
17-03-2018 01:56:40.004 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:43.622 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842 TCP transport destroyed normally
17-03-2018 01:56:43.795 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:45605, sock=665
17-03-2018 01:56:43.795 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:43.795 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:56:45.440 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:56:45.623 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed normally
17-03-2018 01:56:45.796 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:34166, sock=478
17-03-2018 01:56:45.796 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:45.796 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:56:46.035 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58646, sock=690
17-03-2018 01:56:46.035 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:46.035 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP server transport created
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373135 (rdata0x7f7548046f20)
17-03-2018 01:56:46.075 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373135 (rdata0x7f7548046f20) from TCP 10.224.61.22:58646:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373135
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373135
Call-ID: poll-sip-373135
CSeq: 373135 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548046f20 on worker thread
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548046f20 on worker thread
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548046f20 cloned to 0x7f7548422128
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:46.075 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug utils.cpp:872: Added IOHook 0x7f75acfb8e30 to stack. There are now 1 hooks
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug thread_dispatcher.cpp:183: Request latency so far = 55us
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373135 (rdata0x7f7548422128)
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373135 (tdta0x7f753c24d450) created
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373135 (tdta0x7f753c24d450) to TCP 10.224.61.22:58646:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58646;received=10.224.61.22;branch=z9hG4bK-373135
Call-ID: poll-sip-373135
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373135
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373135
CSeq: 373135 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug pjsip: tdta0x7f753c24 Destroying txdata Response msg 200/OPTIONS/cseq=373135 (tdta0x7f753c24d450)
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug thread_dispatcher.cpp:284: Request latency = 116us
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug event_statistic_accumulator.cpp:32: Accumulate 116 for 0xf62778
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug event_statistic_accumulator.cpp:32: Accumulate 116 for 0xf62820
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 18).
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug utils.cpp:878: Removed IOHook 0x7f75acfb8e30 to stack. There are now 0 hooks
17-03-2018 01:56:46.075 UTC [7f75acfb9700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:46.084 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:56:46.084 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:56:48.077 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP connection closed
17-03-2018 01:56:48.077 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548046be8 has been destroyed
17-03-2018 01:56:48.077 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:48.083 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:36938, sock=690
17-03-2018 01:56:48.083 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:48.083 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP server transport created
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373137 (rdata0x7f7548046f20)
17-03-2018 01:56:48.083 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373137 (rdata0x7f7548046f20) from TCP 10.224.61.22:36938:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373137
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373137
Call-ID: poll-sip-373137
CSeq: 373137 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548046f20 on worker thread
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548046f20 on worker thread
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548046f20 cloned to 0x7f7548422128
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:48.083 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug utils.cpp:872: Added IOHook 0x7f75ac7b7e30 to stack. There are now 1 hooks
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug thread_dispatcher.cpp:183: Request latency so far = 69us
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373137 (rdata0x7f7548422128)
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373137 (tdta0x7f75382ac890) created
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373137 (tdta0x7f75382ac890) to TCP 10.224.61.22:36938:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=36938;received=10.224.61.22;branch=z9hG4bK-373137
Call-ID: poll-sip-373137
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373137
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373137
CSeq: 373137 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug pjsip: tdta0x7f75382a Destroying txdata Response msg 200/OPTIONS/cseq=373137 (tdta0x7f75382ac890)
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug thread_dispatcher.cpp:284: Request latency = 172us
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug event_statistic_accumulator.cpp:32: Accumulate 172 for 0xf62778
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug event_statistic_accumulator.cpp:32: Accumulate 172 for 0xf62820
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 19).
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug utils.cpp:878: Removed IOHook 0x7f75ac7b7e30 to stack. There are now 0 hooks
17-03-2018 01:56:48.083 UTC [7f75ac7b8700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:244: Reraising all alarms with a known state
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1001.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1005.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1011.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1012.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1013.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1004.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1002.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1009.1 alarm
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:56:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1010.1 alarm
17-03-2018 01:56:50.084 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP connection closed
17-03-2018 01:56:50.084 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548046be8 has been destroyed
17-03-2018 01:56:50.084 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:55.799 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:45930, sock=690
17-03-2018 01:56:55.799 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:55.799 UTC [7f754feff700] Verbose pjsip: tcps0x7f754804 TCP server transport created
17-03-2018 01:56:55.799 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:56:55.799 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:56:55.799 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:56.110 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58706, sock=712
17-03-2018 01:56:56.110 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:56.110 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373145 (rdata0x7f754803fa40)
17-03-2018 01:56:56.115 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373145 (rdata0x7f754803fa40) from TCP 10.224.61.22:58706:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373145
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373145
Call-ID: poll-sip-373145
CSeq: 373145 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:56.115 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug utils.cpp:872: Added IOHook 0x7f7592f84e30 to stack. There are now 1 hooks
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug thread_dispatcher.cpp:183: Request latency so far = 56us
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373145 (rdata0x7f7548422128)
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373145 (tdta0x7f75403a7370) created
17-03-2018 01:56:56.115 UTC [7f7592f85700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373145 (tdta0x7f75403a7370) to TCP 10.224.61.22:58706:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58706;received=10.224.61.22;branch=z9hG4bK-373145
Call-ID: poll-sip-373145
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373145
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373145
CSeq: 373145 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug pjsip: tdta0x7f75403a Destroying txdata Response msg 200/OPTIONS/cseq=373145 (tdta0x7f75403a7370)
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug thread_dispatcher.cpp:284: Request latency = 127us
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug event_statistic_accumulator.cpp:32: Accumulate 127 for 0xf62778
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug event_statistic_accumulator.cpp:32: Accumulate 127 for 0xf62820
17-03-2018 01:56:56.115 UTC [7f7592f85700] Info load_monitor.cpp:217: Rate adjustment calculation inputs: err -0.981600, smoothed latency 184, target latency 10000
17-03-2018 01:56:56.115 UTC [7f7592f85700] Info load_monitor.cpp:302: Maximum incoming request rate/second unchanged at 2000.000000 (current request rate is 0.550000 requests/sec, minimum threshold for a change is 1000.000000 requests/sec).
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug utils.cpp:878: Removed IOHook 0x7f7592f84e30 to stack. There are now 0 hooks
17-03-2018 01:56:56.115 UTC [7f7592f85700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:56:56.159 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:56:56.159 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:56:58.115 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:56:58.116 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:56:58.116 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:56:58.127 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:36998, sock=712
17-03-2018 01:56:58.127 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:56:58.127 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:56:58.127 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373147 (rdata0x7f754803fa40)
17-03-2018 01:56:58.127 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373147 (rdata0x7f754803fa40) from TCP 10.224.61.22:36998:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373147
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373147
Call-ID: poll-sip-373147
CSeq: 373147 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:56:58.127 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:58.127 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:56:58.128 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug utils.cpp:872: Added IOHook 0x7f758ef7ce30 to stack. There are now 1 hooks
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug thread_dispatcher.cpp:183: Request latency so far = 90us
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373147 (rdata0x7f7548422128)
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373147 (tdta0x7f75e02378d0) created
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373147 (tdta0x7f75e02378d0) to TCP 10.224.61.22:36998:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=36998;received=10.224.61.22;branch=z9hG4bK-373147
Call-ID: poll-sip-373147
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373147
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373147
CSeq: 373147 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug pjsip: tdta0x7f75e023 Destroying txdata Response msg 200/OPTIONS/cseq=373147 (tdta0x7f75e02378d0)
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug thread_dispatcher.cpp:284: Request latency = 214us
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug event_statistic_accumulator.cpp:32: Accumulate 214 for 0xf62778
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug event_statistic_accumulator.cpp:32: Accumulate 214 for 0xf62820
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 1).
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug utils.cpp:878: Removed IOHook 0x7f758ef7ce30 to stack. There are now 0 hooks
17-03-2018 01:56:58.128 UTC [7f758ef7d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:00.129 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:00.129 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:00.129 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:00.455 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:57:06.192 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58770, sock=717
17-03-2018 01:57:06.192 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:06.192 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373155 (rdata0x7f754803fa40)
17-03-2018 01:57:06.224 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373155 (rdata0x7f754803fa40) from TCP 10.224.61.22:58770:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373155
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373155
Call-ID: poll-sip-373155
CSeq: 373155 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:06.224 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug utils.cpp:872: Added IOHook 0x7f75aafb4e30 to stack. There are now 1 hooks
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug thread_dispatcher.cpp:183: Request latency so far = 61us
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373155 (rdata0x7f7548422128)
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373155 (tdta0x7f7548423090) created
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373155 (tdta0x7f7548423090) to TCP 10.224.61.22:58770:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58770;received=10.224.61.22;branch=z9hG4bK-373155
Call-ID: poll-sip-373155
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373155
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373155
CSeq: 373155 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug pjsip: tdta0x7f754842 Destroying txdata Response msg 200/OPTIONS/cseq=373155 (tdta0x7f7548423090)
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug thread_dispatcher.cpp:284: Request latency = 133us
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug event_statistic_accumulator.cpp:32: Accumulate 133 for 0xf62778
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug event_statistic_accumulator.cpp:32: Accumulate 133 for 0xf62820
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 2).
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug utils.cpp:878: Removed IOHook 0x7f75aafb4e30 to stack. There are now 0 hooks
17-03-2018 01:57:06.224 UTC [7f75aafb5700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:06.236 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:57:06.236 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:57:08.225 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:08.225 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:08.225 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:08.237 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37060, sock=712
17-03-2018 01:57:08.237 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:08.237 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373157 (rdata0x7f754803fa40)
17-03-2018 01:57:08.237 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373157 (rdata0x7f754803fa40) from TCP 10.224.61.22:37060:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373157
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373157
Call-ID: poll-sip-373157
CSeq: 373157 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:08.237 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug utils.cpp:872: Added IOHook 0x7f758e77be30 to stack. There are now 1 hooks
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug thread_dispatcher.cpp:183: Request latency so far = 80us
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373157 (rdata0x7f7548422128)
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373157 (tdta0x7f7544235830) created
17-03-2018 01:57:08.237 UTC [7f758e77c700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373157 (tdta0x7f7544235830) to TCP 10.224.61.22:37060:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37060;received=10.224.61.22;branch=z9hG4bK-373157
Call-ID: poll-sip-373157
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373157
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373157
CSeq: 373157 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:08.237 UTC [7f758e77c700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug pjsip: tdta0x7f754423 Destroying txdata Response msg 200/OPTIONS/cseq=373157 (tdta0x7f7544235830)
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug thread_dispatcher.cpp:284: Request latency = 202us
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug event_statistic_accumulator.cpp:32: Accumulate 202 for 0xf62778
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug event_statistic_accumulator.cpp:32: Accumulate 202 for 0xf62820
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 3).
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug utils.cpp:878: Removed IOHook 0x7f758e77be30 to stack. There are now 0 hooks
17-03-2018 01:57:08.238 UTC [7f758e77c700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:10.238 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:10.239 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:10.239 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:15.470 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:57:16.260 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58830, sock=712
17-03-2018 01:57:16.260 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:16.260 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373165 (rdata0x7f754803fa40)
17-03-2018 01:57:16.263 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373165 (rdata0x7f754803fa40) from TCP 10.224.61.22:58830:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373165
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373165
Call-ID: poll-sip-373165
CSeq: 373165 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:16.263 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug utils.cpp:872: Added IOHook 0x7f759a793e30 to stack. There are now 1 hooks
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug thread_dispatcher.cpp:183: Request latency so far = 53us
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373165 (rdata0x7f7548422128)
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373165 (tdta0x7f7548423090) created
17-03-2018 01:57:16.263 UTC [7f759a794700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373165 (tdta0x7f7548423090) to TCP 10.224.61.22:58830:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58830;received=10.224.61.22;branch=z9hG4bK-373165
Call-ID: poll-sip-373165
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373165
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373165
CSeq: 373165 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug pjsip: tdta0x7f754842 Destroying txdata Response msg 200/OPTIONS/cseq=373165 (tdta0x7f7548423090)
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug thread_dispatcher.cpp:284: Request latency = 129us
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0xf62778
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0xf62820
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 4).
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug utils.cpp:878: Removed IOHook 0x7f759a793e30 to stack. There are now 0 hooks
17-03-2018 01:57:16.263 UTC [7f759a794700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:16.304 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:57:16.304 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:57:18.264 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:18.264 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:18.264 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:18.275 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37122, sock=712
17-03-2018 01:57:18.275 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:18.275 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373167 (rdata0x7f754803fa40)
17-03-2018 01:57:18.276 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373167 (rdata0x7f754803fa40) from TCP 10.224.61.22:37122:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373167
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373167
Call-ID: poll-sip-373167
CSeq: 373167 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:18.276 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug utils.cpp:872: Added IOHook 0x7f7596f8ce30 to stack. There are now 1 hooks
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug thread_dispatcher.cpp:183: Request latency so far = 95us
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373167 (rdata0x7f7548422128)
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373167 (tdta0x7f7544235830) created
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373167 (tdta0x7f7544235830) to TCP 10.224.61.22:37122:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37122;received=10.224.61.22;branch=z9hG4bK-373167
Call-ID: poll-sip-373167
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373167
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373167
CSeq: 373167 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug pjsip: tdta0x7f754423 Destroying txdata Response msg 200/OPTIONS/cseq=373167 (tdta0x7f7544235830)
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug thread_dispatcher.cpp:284: Request latency = 236us
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug event_statistic_accumulator.cpp:32: Accumulate 236 for 0xf62778
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug event_statistic_accumulator.cpp:32: Accumulate 236 for 0xf62820
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 5).
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug utils.cpp:878: Removed IOHook 0x7f7596f8ce30 to stack. There are now 0 hooks
17-03-2018 01:57:18.276 UTC [7f7596f8d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:244: Reraising all alarms with a known state
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1001.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1005.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1011.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1012.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1013.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1004.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1002.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1009.1 alarm
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1010.1 alarm
17-03-2018 01:57:20.277 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:20.277 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:20.277 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:26.334 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58890, sock=712
17-03-2018 01:57:26.334 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:26.334 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:26.367 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373175 (rdata0x7f754803fa40)
17-03-2018 01:57:26.367 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373175 (rdata0x7f754803fa40) from TCP 10.224.61.22:58890:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373175
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373175
Call-ID: poll-sip-373175
CSeq: 373175 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:26.367 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:26.367 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:26.367 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:26.367 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:26.367 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:26.368 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f7548422128
17-03-2018 01:57:26.368 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548422128 for worker threads with priority 15
17-03-2018 01:57:26.368 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:26.368 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug utils.cpp:872: Added IOHook 0x7f7594787e30 to stack. There are now 1 hooks
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548422128
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug thread_dispatcher.cpp:183: Request latency so far = 106us
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373175 (rdata0x7f7548422128)
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373175 (tdta0x7f75382ac890) created
17-03-2018 01:57:26.368 UTC [7f7594788700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373175 (tdta0x7f75382ac890) to TCP 10.224.61.22:58890:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58890;received=10.224.61.22;branch=z9hG4bK-373175
Call-ID: poll-sip-373175
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373175
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373175
CSeq: 373175 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug pjsip: tdta0x7f75382a Destroying txdata Response msg 200/OPTIONS/cseq=373175 (tdta0x7f75382ac890)
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548422128
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug thread_dispatcher.cpp:284: Request latency = 211us
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug event_statistic_accumulator.cpp:32: Accumulate 211 for 0xf62778
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug event_statistic_accumulator.cpp:32: Accumulate 211 for 0xf62820
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 6).
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug utils.cpp:878: Removed IOHook 0x7f7594787e30 to stack. There are now 0 hooks
17-03-2018 01:57:26.368 UTC [7f7594788700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:26.382 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:57:26.382 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:57:26.807 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:33397, sock=717
17-03-2018 01:57:26.807 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:26.807 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842 TCP server transport created
17-03-2018 01:57:26.807 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 TCP connection closed
17-03-2018 01:57:26.807 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:27.807 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:40859, sock=722
17-03-2018 01:57:27.808 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:27.808 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 TCP server transport created
17-03-2018 01:57:27.808 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 TCP connection closed
17-03-2018 01:57:27.808 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:28.368 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:28.368 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:28.368 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:28.380 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37182, sock=705
17-03-2018 01:57:28.380 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:28.380 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373177 (rdata0x7f754803fa40)
17-03-2018 01:57:28.380 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373177 (rdata0x7f754803fa40) from TCP 10.224.61.22:37182:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373177
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373177
Call-ID: poll-sip-373177
CSeq: 373177 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f75484497a8
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f75484497a8 for worker threads with priority 15
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:28.380 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug utils.cpp:872: Added IOHook 0x7f758df7ae30 to stack. There are now 1 hooks
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f75484497a8
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug thread_dispatcher.cpp:183: Request latency so far = 78us
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373177 (rdata0x7f75484497a8)
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373177 (tdta0x7f75d033ba90) created
17-03-2018 01:57:28.380 UTC [7f758df7b700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373177 (tdta0x7f75d033ba90) to TCP 10.224.61.22:37182:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37182;received=10.224.61.22;branch=z9hG4bK-373177
Call-ID: poll-sip-373177
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373177
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373177
CSeq: 373177 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug pjsip: tdta0x7f75d033 Destroying txdata Response msg 200/OPTIONS/cseq=373177 (tdta0x7f75d033ba90)
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f75484497a8
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug thread_dispatcher.cpp:284: Request latency = 197us
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug event_statistic_accumulator.cpp:32: Accumulate 197 for 0xf62778
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug event_statistic_accumulator.cpp:32: Accumulate 197 for 0xf62820
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 7).
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug utils.cpp:878: Removed IOHook 0x7f758df7ae30 to stack. There are now 0 hooks
17-03-2018 01:57:28.380 UTC [7f758df7b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:30.381 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:30.381 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:30.381 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:30.483 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:57:36.410 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:58954, sock=712
17-03-2018 01:57:36.410 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:36.410 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373185 (rdata0x7f754803fa40)
17-03-2018 01:57:36.431 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373185 (rdata0x7f754803fa40) from TCP 10.224.61.22:58954:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373185
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373185
Call-ID: poll-sip-373185
CSeq: 373185 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f75484497a8
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f75484497a8 for worker threads with priority 15
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:36.431 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug utils.cpp:872: Added IOHook 0x7f75ad7b9e30 to stack. There are now 1 hooks
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f75484497a8
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug thread_dispatcher.cpp:183: Request latency so far = 56us
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373185 (rdata0x7f75484497a8)
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373185 (tdta0x21d5100) created
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373185 (tdta0x21d5100) to TCP 10.224.61.22:58954:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=58954;received=10.224.61.22;branch=z9hG4bK-373185
Call-ID: poll-sip-373185
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373185
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373185
CSeq: 373185 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug pjsip:  tdta0x21d5100 Destroying txdata Response msg 200/OPTIONS/cseq=373185 (tdta0x21d5100)
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f75484497a8
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug thread_dispatcher.cpp:284: Request latency = 118us
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug event_statistic_accumulator.cpp:32: Accumulate 118 for 0xf62778
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug event_statistic_accumulator.cpp:32: Accumulate 118 for 0xf62820
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 8).
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug utils.cpp:878: Removed IOHook 0x7f75ad7b9e30 to stack. There are now 0 hooks
17-03-2018 01:57:36.431 UTC [7f75ad7ba700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:36.452 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:57:36.452 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:57:38.432 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:38.432 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:38.432 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:38.438 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37244, sock=705
17-03-2018 01:57:38.438 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:38.438 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373187 (rdata0x7f754803fa40)
17-03-2018 01:57:38.438 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373187 (rdata0x7f754803fa40) from TCP 10.224.61.22:37244:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373187
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373187
Call-ID: poll-sip-373187
CSeq: 373187 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f754803fa40 on worker thread
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f754803fa40 on worker thread
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f754803fa40 cloned to 0x7f75484497a8
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f75484497a8 for worker threads with priority 15
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:38.438 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:38.438 UTC [7f75ab7b6700] Debug utils.cpp:872: Added IOHook 0x7f75ab7b5e30 to stack. There are now 1 hooks
17-03-2018 01:57:38.438 UTC [7f75ab7b6700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f75484497a8
17-03-2018 01:57:38.438 UTC [7f75ab7b6700] Debug thread_dispatcher.cpp:183: Request latency so far = 60us
17-03-2018 01:57:38.438 UTC [7f75ab7b6700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373187 (rdata0x7f75484497a8)
17-03-2018 01:57:38.438 UTC [7f75ab7b6700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373187 (tdta0x7f75403a7370) created
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373187 (tdta0x7f75403a7370) to TCP 10.224.61.22:37244:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37244;received=10.224.61.22;branch=z9hG4bK-373187
Call-ID: poll-sip-373187
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373187
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373187
CSeq: 373187 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug pjsip: tdta0x7f75403a Destroying txdata Response msg 200/OPTIONS/cseq=373187 (tdta0x7f75403a7370)
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f75484497a8
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug thread_dispatcher.cpp:284: Request latency = 136us
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug event_statistic_accumulator.cpp:32: Accumulate 136 for 0xf62778
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug event_statistic_accumulator.cpp:32: Accumulate 136 for 0xf62820
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 9).
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug utils.cpp:878: Removed IOHook 0x7f75ab7b5e30 to stack. There are now 0 hooks
17-03-2018 01:57:38.439 UTC [7f75ab7b6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:40.439 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP connection closed
17-03-2018 01:57:40.439 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f754803f708 has been destroyed
17-03-2018 01:57:40.440 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:40.637 UTC [7f754feff700] Verbose pjsip: tcps0x7f75480b TCP transport destroyed normally
17-03-2018 01:57:40.810 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:41810, sock=508
17-03-2018 01:57:40.810 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:40.810 UTC [7f754feff700] Verbose pjsip: tcps0x7f754803 TCP server transport created
17-03-2018 01:57:41.811 UTC [7f754feff700] Verbose pjsip: tcps0x7f754839 TCP connection closed
17-03-2018 01:57:41.811 UTC [7f754feff700] Verbose pjsip: tcps0x7f754839 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:41.811 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:37220, sock=316
17-03-2018 01:57:41.811 UTC [7f754feff700] Verbose pjsip: tcps0x7f754839 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:41.811 UTC [7f754feff700] Verbose pjsip: tcps0x7f754839 TCP server transport created
17-03-2018 01:57:42.812 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:39283, sock=705
17-03-2018 01:57:42.812 UTC [7f754feff700] Verbose pjsip: tcps0x7f75480b tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:42.812 UTC [7f754feff700] Verbose pjsip: tcps0x7f75480b TCP server transport created
17-03-2018 01:57:42.812 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP connection closed
17-03-2018 01:57:42.812 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:45.493 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:57:46.480 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:59014, sock=291
17-03-2018 01:57:46.480 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:46.480 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP server transport created
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373195 (rdata0x7f7548023260)
17-03-2018 01:57:46.496 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373195 (rdata0x7f7548023260) from TCP 10.224.61.22:59014:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373195
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373195
Call-ID: poll-sip-373195
CSeq: 373195 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548023260 on worker thread
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548023260 on worker thread
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548023260 cloned to 0x7f7548463ba8
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548463ba8 for worker threads with priority 15
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:46.496 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug utils.cpp:872: Added IOHook 0x7f7588f70e30 to stack. There are now 1 hooks
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548463ba8
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug thread_dispatcher.cpp:183: Request latency so far = 78us
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373195 (rdata0x7f7548463ba8)
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373195 (tdta0x7f753c24d450) created
17-03-2018 01:57:46.496 UTC [7f7588f71700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373195 (tdta0x7f753c24d450) to TCP 10.224.61.22:59014:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=59014;received=10.224.61.22;branch=z9hG4bK-373195
Call-ID: poll-sip-373195
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373195
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373195
CSeq: 373195 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug pjsip: tdta0x7f753c24 Destroying txdata Response msg 200/OPTIONS/cseq=373195 (tdta0x7f753c24d450)
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548463ba8
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug thread_dispatcher.cpp:284: Request latency = 143us
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug event_statistic_accumulator.cpp:32: Accumulate 143 for 0xf62778
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug event_statistic_accumulator.cpp:32: Accumulate 143 for 0xf62820
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 10).
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug utils.cpp:878: Removed IOHook 0x7f7588f70e30 to stack. There are now 0 hooks
17-03-2018 01:57:46.496 UTC [7f7588f71700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:46.529 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:57:46.529 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:57:48.496 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP connection closed
17-03-2018 01:57:48.496 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548022f28 has been destroyed
17-03-2018 01:57:48.496 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:48.508 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37306, sock=291
17-03-2018 01:57:48.508 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:48.508 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP server transport created
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373197 (rdata0x7f7548023260)
17-03-2018 01:57:48.509 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373197 (rdata0x7f7548023260) from TCP 10.224.61.22:37306:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373197
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373197
Call-ID: poll-sip-373197
CSeq: 373197 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548023260 on worker thread
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548023260 on worker thread
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548023260 cloned to 0x7f7548463ba8
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548463ba8 for worker threads with priority 15
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:48.509 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug utils.cpp:872: Added IOHook 0x7f7594f88e30 to stack. There are now 1 hooks
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548463ba8
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug thread_dispatcher.cpp:183: Request latency so far = 112us
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373197 (rdata0x7f7548463ba8)
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373197 (tdta0x7f753c24d450) created
17-03-2018 01:57:48.509 UTC [7f7594f89700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373197 (tdta0x7f753c24d450) to TCP 10.224.61.22:37306:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37306;received=10.224.61.22;branch=z9hG4bK-373197
Call-ID: poll-sip-373197
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373197
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373197
CSeq: 373197 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug pjsip: tdta0x7f753c24 Destroying txdata Response msg 200/OPTIONS/cseq=373197 (tdta0x7f753c24d450)
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548463ba8
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug thread_dispatcher.cpp:284: Request latency = 304us
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug event_statistic_accumulator.cpp:32: Accumulate 304 for 0xf62778
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug event_statistic_accumulator.cpp:32: Accumulate 304 for 0xf62820
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 11).
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug utils.cpp:878: Removed IOHook 0x7f7594f88e30 to stack. There are now 0 hooks
17-03-2018 01:57:48.509 UTC [7f7594f89700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:49.814 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:43474, sock=712
17-03-2018 01:57:49.814 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:49.814 UTC [7f754feff700] Verbose pjsip: tcps0x7f754846 TCP server transport created
17-03-2018 01:57:49.814 UTC [7f754feff700] Verbose pjsip: tcps0x7f754836 TCP connection closed
17-03-2018 01:57:49.814 UTC [7f754feff700] Verbose pjsip: tcps0x7f754836 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:244: Reraising all alarms with a known state
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1001.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1005.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1011.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1012.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1013.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1004.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1002.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1009.1 alarm
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:57:50.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1010.1 alarm
17-03-2018 01:57:50.510 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP connection closed
17-03-2018 01:57:50.510 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548022f28 has been destroyed
17-03-2018 01:57:50.510 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:55.815 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:57:55.815 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:41363, sock=291
17-03-2018 01:57:55.815 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:55.815 UTC [7f754feff700] Verbose pjsip: tcps0x7f754802 TCP server transport created
17-03-2018 01:57:55.815 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:56.551 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:59074, sock=563
17-03-2018 01:57:56.551 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:56.551 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373205 (rdata0x7f7548076e90)
17-03-2018 01:57:56.569 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373205 (rdata0x7f7548076e90) from TCP 10.224.61.22:59074:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373205
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373205
Call-ID: poll-sip-373205
CSeq: 373205 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548076e90 on worker thread
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548076e90 on worker thread
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548076e90 cloned to 0x7f7548081478
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:56.569 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug utils.cpp:872: Added IOHook 0x7f759df9ae30 to stack. There are now 1 hooks
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug thread_dispatcher.cpp:183: Request latency so far = 50us
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373205 (rdata0x7f7548081478)
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373205 (tdta0x7f75e02378d0) created
17-03-2018 01:57:56.569 UTC [7f759df9b700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373205 (tdta0x7f75e02378d0) to TCP 10.224.61.22:59074:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=59074;received=10.224.61.22;branch=z9hG4bK-373205
Call-ID: poll-sip-373205
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373205
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373205
CSeq: 373205 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug pjsip: tdta0x7f75e023 Destroying txdata Response msg 200/OPTIONS/cseq=373205 (tdta0x7f75e02378d0)
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug thread_dispatcher.cpp:284: Request latency = 113us
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug event_statistic_accumulator.cpp:32: Accumulate 113 for 0xf62778
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug event_statistic_accumulator.cpp:32: Accumulate 113 for 0xf62820
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 12).
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug utils.cpp:878: Removed IOHook 0x7f759df9ae30 to stack. There are now 0 hooks
17-03-2018 01:57:56.569 UTC [7f759df9b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:57:56.600 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:57:56.600 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:57:58.573 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:57:58.573 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548076b58 has been destroyed
17-03-2018 01:57:58.573 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:57:58.585 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37366, sock=563
17-03-2018 01:57:58.585 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:57:58.585 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373207 (rdata0x7f7548076e90)
17-03-2018 01:57:58.586 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373207 (rdata0x7f7548076e90) from TCP 10.224.61.22:37366:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373207
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373207
Call-ID: poll-sip-373207
CSeq: 373207 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548076e90 on worker thread
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548076e90 on worker thread
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548076e90 cloned to 0x7f7548081478
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:57:58.586 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug utils.cpp:872: Added IOHook 0x7f758c777e30 to stack. There are now 1 hooks
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug thread_dispatcher.cpp:183: Request latency so far = 113us
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373207 (rdata0x7f7548081478)
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373207 (tdta0x7f75403a7370) created
17-03-2018 01:57:58.586 UTC [7f758c778700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373207 (tdta0x7f75403a7370) to TCP 10.224.61.22:37366:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37366;received=10.224.61.22;branch=z9hG4bK-373207
Call-ID: poll-sip-373207
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373207
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373207
CSeq: 373207 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug pjsip: tdta0x7f75403a Destroying txdata Response msg 200/OPTIONS/cseq=373207 (tdta0x7f75403a7370)
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug thread_dispatcher.cpp:284: Request latency = 293us
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug event_statistic_accumulator.cpp:32: Accumulate 293 for 0xf62778
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug event_statistic_accumulator.cpp:32: Accumulate 293 for 0xf62820
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 13).
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug utils.cpp:878: Removed IOHook 0x7f758c777e30 to stack. There are now 0 hooks
17-03-2018 01:57:58.586 UTC [7f758c778700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:00.504 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:58:00.587 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:58:00.587 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548076b58 has been destroyed
17-03-2018 01:58:00.587 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:00.817 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:34246, sock=563
17-03-2018 01:58:00.817 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:00.817 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:58:00.817 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:58:00.817 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:06.628 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:59138, sock=721
17-03-2018 01:58:06.628 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:06.628 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373215 (rdata0x7f7548075680)
17-03-2018 01:58:06.637 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373215 (rdata0x7f7548075680) from TCP 10.224.61.22:59138:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373215
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373215
Call-ID: poll-sip-373215
CSeq: 373215 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548075680 on worker thread
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548075680 on worker thread
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548075680 cloned to 0x7f7548081478
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:06.637 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug utils.cpp:872: Added IOHook 0x7f7598f90e30 to stack. There are now 1 hooks
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug thread_dispatcher.cpp:183: Request latency so far = 66us
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373215 (rdata0x7f7548081478)
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373215 (tdta0x7f75d033ba90) created
17-03-2018 01:58:06.637 UTC [7f7598f91700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373215 (tdta0x7f75d033ba90) to TCP 10.224.61.22:59138:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=59138;received=10.224.61.22;branch=z9hG4bK-373215
Call-ID: poll-sip-373215
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373215
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373215
CSeq: 373215 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug pjsip: tdta0x7f75d033 Destroying txdata Response msg 200/OPTIONS/cseq=373215 (tdta0x7f75d033ba90)
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug thread_dispatcher.cpp:284: Request latency = 128us
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug event_statistic_accumulator.cpp:32: Accumulate 128 for 0xf62778
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug event_statistic_accumulator.cpp:32: Accumulate 128 for 0xf62820
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 14).
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug utils.cpp:878: Removed IOHook 0x7f7598f90e30 to stack. There are now 0 hooks
17-03-2018 01:58:06.637 UTC [7f7598f91700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:06.668 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:58:06.668 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:58:08.638 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:58:08.638 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548075348 has been destroyed
17-03-2018 01:58:08.638 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:08.650 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37428, sock=631
17-03-2018 01:58:08.650 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:08.650 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373217 (rdata0x7f7548075680)
17-03-2018 01:58:08.650 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373217 (rdata0x7f7548075680) from TCP 10.224.61.22:37428:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373217
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373217
Call-ID: poll-sip-373217
CSeq: 373217 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548075680 on worker thread
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548075680 on worker thread
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548075680 cloned to 0x7f7548081478
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:08.650 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug utils.cpp:872: Added IOHook 0x7f7589771e30 to stack. There are now 1 hooks
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug thread_dispatcher.cpp:183: Request latency so far = 100us
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373217 (rdata0x7f7548081478)
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373217 (tdta0x7f75e02378d0) created
17-03-2018 01:58:08.650 UTC [7f7589772700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373217 (tdta0x7f75e02378d0) to TCP 10.224.61.22:37428:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37428;received=10.224.61.22;branch=z9hG4bK-373217
Call-ID: poll-sip-373217
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373217
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373217
CSeq: 373217 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug pjsip: tdta0x7f75e023 Destroying txdata Response msg 200/OPTIONS/cseq=373217 (tdta0x7f75e02378d0)
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug thread_dispatcher.cpp:284: Request latency = 245us
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug event_statistic_accumulator.cpp:32: Accumulate 245 for 0xf62778
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug event_statistic_accumulator.cpp:32: Accumulate 245 for 0xf62820
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 15).
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug utils.cpp:878: Removed IOHook 0x7f7589771e30 to stack. There are now 0 hooks
17-03-2018 01:58:08.650 UTC [7f7589772700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:10.652 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:58:10.652 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548075348 has been destroyed
17-03-2018 01:58:10.652 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:15.506 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:58:16.698 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:59198, sock=631
17-03-2018 01:58:16.698 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:16.698 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:58:16.704 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373225 (rdata0x7f7548075680)
17-03-2018 01:58:16.704 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373225 (rdata0x7f7548075680) from TCP 10.224.61.22:59198:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373225
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373225
Call-ID: poll-sip-373225
CSeq: 373225 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:16.704 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:16.704 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:16.704 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:16.704 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548075680 on worker thread
17-03-2018 01:58:16.705 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548075680 on worker thread
17-03-2018 01:58:16.705 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548075680 cloned to 0x7f7548081478
17-03-2018 01:58:16.705 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:16.705 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:16.705 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug utils.cpp:872: Added IOHook 0x7f7595789e30 to stack. There are now 1 hooks
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug thread_dispatcher.cpp:183: Request latency so far = 52us
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373225 (rdata0x7f7548081478)
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373225 (tdta0x7f75e02378d0) created
17-03-2018 01:58:16.705 UTC [7f759578a700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373225 (tdta0x7f75e02378d0) to TCP 10.224.61.22:59198:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=59198;received=10.224.61.22;branch=z9hG4bK-373225
Call-ID: poll-sip-373225
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373225
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373225
CSeq: 373225 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug pjsip: tdta0x7f75e023 Destroying txdata Response msg 200/OPTIONS/cseq=373225 (tdta0x7f75e02378d0)
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug thread_dispatcher.cpp:284: Request latency = 123us
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug event_statistic_accumulator.cpp:32: Accumulate 123 for 0xf62778
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug event_statistic_accumulator.cpp:32: Accumulate 123 for 0xf62820
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 16).
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug utils.cpp:878: Removed IOHook 0x7f7595789e30 to stack. There are now 0 hooks
17-03-2018 01:58:16.705 UTC [7f759578a700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:16.742 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:58:16.742 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:58:18.705 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:58:18.705 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548075348 has been destroyed
17-03-2018 01:58:18.706 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:18.724 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37490, sock=631
17-03-2018 01:58:18.724 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:18.724 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373227 (rdata0x7f7548075680)
17-03-2018 01:58:18.725 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373227 (rdata0x7f7548075680) from TCP 10.224.61.22:37490:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373227
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373227
Call-ID: poll-sip-373227
CSeq: 373227 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548075680 on worker thread
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548075680 on worker thread
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548075680 cloned to 0x7f7548081478
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:18.725 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug utils.cpp:872: Added IOHook 0x7f75a5faae30 to stack. There are now 1 hooks
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug thread_dispatcher.cpp:183: Request latency so far = 108us
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373227 (rdata0x7f7548081478)
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373227 (tdta0x7f753c24d450) created
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373227 (tdta0x7f753c24d450) to TCP 10.224.61.22:37490:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37490;received=10.224.61.22;branch=z9hG4bK-373227
Call-ID: poll-sip-373227
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373227
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373227
CSeq: 373227 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug pjsip: tdta0x7f753c24 Destroying txdata Response msg 200/OPTIONS/cseq=373227 (tdta0x7f753c24d450)
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug thread_dispatcher.cpp:284: Request latency = 218us
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug event_statistic_accumulator.cpp:32: Accumulate 218 for 0xf62778
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug event_statistic_accumulator.cpp:32: Accumulate 218 for 0xf62820
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 17).
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug utils.cpp:878: Removed IOHook 0x7f75a5faae30 to stack. There are now 0 hooks
17-03-2018 01:58:18.725 UTC [7f75a5fab700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:20.009 UTC [7f75ee6a5700] Status alarm.cpp:244: Reraising all alarms with a known state
17-03-2018 01:58:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.009 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1001.1 alarm
17-03-2018 01:58:20.009 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1005.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1011.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1012.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1013.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1004.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1002.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1009.1 alarm
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
17-03-2018 01:58:20.010 UTC [7f75ee6a5700] Status alarm.cpp:37: sprout issued 1010.1 alarm
17-03-2018 01:58:20.726 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP connection closed
17-03-2018 01:58:20.726 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548075348 has been destroyed
17-03-2018 01:58:20.726 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:21.823 UTC [7f754feff700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:34093, sock=631
17-03-2018 01:58:21.823 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:21.823 UTC [7f754feff700] Verbose pjsip: tcps0x7f754807 TCP server transport created
17-03-2018 01:58:21.823 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP connection closed
17-03-2018 01:58:21.823 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:26.780 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:59260, sock=721
17-03-2018 01:58:26.780 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:26.780 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP server transport created
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373235 (rdata0x7f7548331690)
17-03-2018 01:58:26.780 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373235 (rdata0x7f7548331690) from TCP 10.224.61.22:59260:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373235
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373235
Call-ID: poll-sip-373235
CSeq: 373235 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548331690 on worker thread
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548331690 on worker thread
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548331690 cloned to 0x7f7548081478
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:26.780 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug utils.cpp:872: Added IOHook 0x7f75a6face30 to stack. There are now 1 hooks
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:183: Request latency so far = 53us
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373235 (rdata0x7f7548081478)
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373235 (tdta0x7f7548365540) created
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373235 (tdta0x7f7548365540) to TCP 10.224.61.22:59260:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=59260;received=10.224.61.22;branch=z9hG4bK-373235
Call-ID: poll-sip-373235
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373235
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373235
CSeq: 373235 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug pjsip: tdta0x7f754836 Destroying txdata Response msg 200/OPTIONS/cseq=373235 (tdta0x7f7548365540)
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:284: Request latency = 124us
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug event_statistic_accumulator.cpp:32: Accumulate 124 for 0xf62778
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug event_statistic_accumulator.cpp:32: Accumulate 124 for 0xf62820
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 18).
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug utils.cpp:878: Removed IOHook 0x7f75a6face30 to stack. There are now 0 hooks
17-03-2018 01:58:26.780 UTC [7f75a6fad700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:26.823 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:58:26.823 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:58:28.781 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP connection closed
17-03-2018 01:58:28.781 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548331358 has been destroyed
17-03-2018 01:58:28.781 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:28.794 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37550, sock=718
17-03-2018 01:58:28.794 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:28.794 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP server transport created
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373237 (rdata0x7f7548331690)
17-03-2018 01:58:28.794 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373237 (rdata0x7f7548331690) from TCP 10.224.61.22:37550:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373237
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373237
Call-ID: poll-sip-373237
CSeq: 373237 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548331690 on worker thread
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548331690 on worker thread
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548331690 cloned to 0x7f7548081478
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:28.794 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug utils.cpp:872: Added IOHook 0x7f759f79de30 to stack. There are now 1 hooks
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug thread_dispatcher.cpp:183: Request latency so far = 80us
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373237 (rdata0x7f7548081478)
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373237 (tdta0x7f7544235830) created
17-03-2018 01:58:28.794 UTC [7f759f79e700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373237 (tdta0x7f7544235830) to TCP 10.224.61.22:37550:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37550;received=10.224.61.22;branch=z9hG4bK-373237
Call-ID: poll-sip-373237
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373237
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373237
CSeq: 373237 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug pjsip: tdta0x7f754423 Destroying txdata Response msg 200/OPTIONS/cseq=373237 (tdta0x7f7544235830)
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug thread_dispatcher.cpp:284: Request latency = 191us
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug event_statistic_accumulator.cpp:32: Accumulate 191 for 0xf62778
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug event_statistic_accumulator.cpp:32: Accumulate 191 for 0xf62820
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 19).
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug utils.cpp:878: Removed IOHook 0x7f759f79de30 to stack. There are now 0 hooks
17-03-2018 01:58:28.794 UTC [7f759f79e700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:30.512 UTC [7f75cd7fa700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
17-03-2018 01:58:30.795 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP connection closed
17-03-2018 01:58:30.795 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548331358 has been destroyed
17-03-2018 01:58:30.795 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:36.856 UTC [7f754feff700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:59320, sock=718
17-03-2018 01:58:36.856 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:36.856 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP server transport created
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373245 (rdata0x7f7548331690)
17-03-2018 01:58:36.856 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373245 (rdata0x7f7548331690) from TCP 10.224.61.22:59320:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373245
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373245
Call-ID: poll-sip-373245
CSeq: 373245 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548331690 on worker thread
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548331690 on worker thread
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548331690 cloned to 0x7f7548081478
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:36.856 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug utils.cpp:872: Added IOHook 0x7f758a773e30 to stack. There are now 1 hooks
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug thread_dispatcher.cpp:183: Request latency so far = 51us
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373245 (rdata0x7f7548081478)
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373245 (tdta0x7f75e82b6d00) created
17-03-2018 01:58:36.856 UTC [7f758a774700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373245 (tdta0x7f75e82b6d00) to TCP 10.224.61.22:59320:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=59320;received=10.224.61.22;branch=z9hG4bK-373245
Call-ID: poll-sip-373245
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373245
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373245
CSeq: 373245 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug pjsip: tdta0x7f75e82b Destroying txdata Response msg 200/OPTIONS/cseq=373245 (tdta0x7f75e82b6d00)
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug thread_dispatcher.cpp:284: Request latency = 113us
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug event_statistic_accumulator.cpp:32: Accumulate 113 for 0xf62778
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug event_statistic_accumulator.cpp:32: Accumulate 113 for 0xf62820
17-03-2018 01:58:36.856 UTC [7f758a774700] Info load_monitor.cpp:217: Rate adjustment calculation inputs: err -0.982600, smoothed latency 174, target latency 10000
17-03-2018 01:58:36.856 UTC [7f758a774700] Info load_monitor.cpp:302: Maximum incoming request rate/second unchanged at 2000.000000 (current request rate is 0.550000 requests/sec, minimum threshold for a change is 1000.000000 requests/sec).
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug utils.cpp:878: Removed IOHook 0x7f758a773e30 to stack. There are now 0 hooks
17-03-2018 01:58:36.856 UTC [7f758a774700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:36.900 UTC [7f754f6fe700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
17-03-2018 01:58:36.900 UTC [7f754f6fe700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
17-03-2018 01:58:38.857 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP connection closed
17-03-2018 01:58:38.857 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548331358 has been destroyed
17-03-2018 01:58:38.857 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
17-03-2018 01:58:38.868 UTC [7f754feff700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:37612, sock=718
17-03-2018 01:58:38.868 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 tcp->base.local_name: 10.224.61.22
17-03-2018 01:58:38.868 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP server transport created
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=373247 (rdata0x7f7548331690)
17-03-2018 01:58:38.868 UTC [7f754feff700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=373247 (rdata0x7f7548331690) from TCP 10.224.61.22:37612:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-373247
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=373247
Call-ID: poll-sip-373247
CSeq: 373247 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f7548331690 on worker thread
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f7548331690 on worker thread
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f7548331690 cloned to 0x7f7548081478
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f7548081478 for worker threads with priority 15
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf66708
17-03-2018 01:58:38.868 UTC [7f754feff700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0xf667b0
17-03-2018 01:58:38.868 UTC [7f759bf97700] Debug utils.cpp:872: Added IOHook 0x7f759bf96e30 to stack. There are now 1 hooks
17-03-2018 01:58:38.868 UTC [7f759bf97700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f7548081478
17-03-2018 01:58:38.868 UTC [7f759bf97700] Debug thread_dispatcher.cpp:183: Request latency so far = 102us
17-03-2018 01:58:38.868 UTC [7f759bf97700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=373247 (rdata0x7f7548081478)
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug uri_classifier.cpp:172: Classified URI as 3
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=373247 (tdta0x7f75d033ba90) created
17-03-2018 01:58:38.869 UTC [7f759bf97700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=373247 (tdta0x7f75d033ba90) to TCP 10.224.61.22:37612:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=37612;received=10.224.61.22;branch=z9hG4bK-373247
Call-ID: poll-sip-373247
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=373247
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-373247
CSeq: 373247 OPTIONS
Content-Length:  0


--end msg--
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug pjsip: tdta0x7f75d033 Destroying txdata Response msg 200/OPTIONS/cseq=373247 (tdta0x7f75d033ba90)
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f7548081478
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug thread_dispatcher.cpp:284: Request latency = 236us
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug event_statistic_accumulator.cpp:32: Accumulate 236 for 0xf62778
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug event_statistic_accumulator.cpp:32: Accumulate 236 for 0xf62820
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 1).
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug utils.cpp:878: Removed IOHook 0x7f759bf96e30 to stack. There are now 0 hooks
17-03-2018 01:58:38.869 UTC [7f759bf97700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
17-03-2018 01:58:40.870 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP connection closed
17-03-2018 01:58:40.870 UTC [7f754feff700] Debug connection_tracker.cpp:67: Connection 0x7f7548331358 has been destroyed
17-03-2018 01:58:40.870 UTC [7f754feff700] Verbose pjsip: tcps0x7f754833 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)

From skgola1997 at gmail.com  Mon Mar 19 04:42:55 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Mon, 19 Mar 2018 14:12:55 +0530
Subject: [Project Clearwater] stress testing
In-Reply-To: <CAHwYWpAvzUUeUpoyA7oi40SDkcW4i2TW7Y+vyNeEppt6piJZAg@mail.gmail.com>
References: <CAPqjCcHWiFOEP5EcYhTiiYqTruaGugAFgnkDkndSmMEqa9SOdA@mail.gmail.com>
	<SN1PR02MB1677B6DCCE8F163251561073F5D70@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpDPR-=hD3Cw9znfsrpXBkN=QfzvhLsZrTggjsxd0nyWPA@mail.gmail.com>
	<CAHwYWpAvzUUeUpoyA7oi40SDkcW4i2TW7Y+vyNeEppt6piJZAg@mail.gmail.com>
Message-ID: <CAHwYWpALb2ijs_S9ZDqAaRh_xVmDDQziqMFycYaNwhdc6cEa9Q@mail.gmail.com>

Hi,
Please reply, I am not able to debug what the issue is.

Thanks

On Fri, Mar 16, 2018 at 11:51 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi,
> PFA the sprout_current log file. Please guide me the solution, I am newbie
> to clearwater :-)
>
> Thanks,
> Sunil
>
> On Fri, Mar 16, 2018 at 11:07 PM, Sunil Kumar <skgola1997 at gmail.com>
> wrote:
>
>> Hi Michael,
>> Thanks for replying. When I am running single call it giving some errors
>> like:
>>
>> *[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress iind.intel.com
>> <http://iind.intel.com> 1 2*
>> Starting initial registration, will take 0 seconds
>> Initial registration succeeded
>> Starting test
>> Test complete
>> Traceback (most recent call last):
>>   File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
>>     with open(CALLER_STATS) as f:
>> IOError: [Errno 2] No such file or directory:
>> '/var/log/clearwater-sip-stress/11218_caller_stats.log'
>>
>>
>> *[]ubuntu at stress:~$ cat
>> /var/log/clearwater-sip-stress/11218_re_reg_stats.log*
>> StartTime;LastResetTime;CurrentTime;ElapsedTime(P);ElapsedTi
>> me(C);TargetRate;CallRate(P);CallRate(C);IncomingCall(P);
>> IncomingCall(C);OutgoingCall(P);OutgoingCall(C);TotalCallCr
>> eated;CurrentCall;SuccessfulCall(P);SuccessfulCall(C);
>> FailedCall(P);FailedCall(C);FailedCannotSendMessage(P);Faile
>> dCannotSendMessage(C);FailedMaxUDPRetrans(P);FailedMaxUDPRet
>> rans(C);FailedTcpConnect(P);FailedTcpConnect(C);FailedTcpClo
>> sed(P);FailedTcpClosed(C);FailedUnexpectedMessage(P);FailedU
>> nexpectedMessage(C);FailedCallRejected(P);FailedCallRejected
>> (C);FailedCmdNotSent(P);FailedCmdNotSent(C);FailedRegexpDoes
>> ntMatch(P);FailedRegexpDoesntMatch(C);FailedRegexpShouldntMa
>> tch(P);FailedRegexpShouldntMatch(C);FailedRegexpHdrNotFound(
>> P);FailedRegexpHdrNotFound(C);FailedOutboundCongestion(P);Fa
>> iledOutboundCongestion(C);FailedTimeoutOnRecv(P);FailedTimeo
>> utOnRecv(C);FailedTimeoutOnSend(P);FailedTimeoutOnSend(C);Ou
>> tOfCallMsgs(P);OutOfCallMsgs(C);DeadCallMsgs(P);
>> DeadCallMsgs(C);Retransmissions(P);Retransmissions(C);AutoAn
>> swered(P);AutoAnswered(C);Warnings(P);Warnings(C);FatalE
>> rrors(P);FatalErrors(C);WatchdogMajor(P);WatchdogMajor(C);Wa
>> tchdogMinor(P);WatchdogMinor(C);ResponseTime1(P);ResponseTi
>> me1(C);ResponseTime1StDev(P);ResponseTime1StDev(C);CallLengt
>> h(P);CallLength(C);CallLengthStDev(P);CallLengthStDev(C);Res
>> ponseTimeRepartition1;ResponseTimeRepartition1_<2;ResponseTi
>> meRepartition1_<10;ResponseTimeRepartition1_<20;ResponseTime
>> Repartition1_<50;ResponseTimeRepartition1_<100;ResponseTimeR
>> epartition1_<200;ResponseTimeRepartition1_<500;ResponseTimeR
>> epartition1_<1000;ResponseTimeRepartition1_<
>> 2000;ResponseTimeRepartition1_>=2000;
>> 2018-03-17      06:43:11.763066 1521249191.763066;2018-03-17
>> 06:43:11.763066 1521249191.763066;2018-03-17    06:43:11.770581
>> 1521249191.770581;00:00:00;00:00:00;0.000555556;0;0;0;0;0;0;
>> 0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;
>> 0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;00:00:00:000000;00:00:00
>> :000000;00:00:00:000000;00:00:00:000000;00:00:00:000000;00:
>> 00:00:000000;00:00:00:000000;00:00:00:000000;;0;0;0;0;0;0;0;0;0;0;
>> 2018-03-17      06:43:11.763066 1521249191.763066;2018-03-17
>> 06:43:11.770864 1521249191.770864;2018-03-17    06:43:11.772711
>> 1521249191.772711;00:00:00;00:00:00;0.000555556;0;0;0;0;0;0;
>> 0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;
>> 0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;00:00:00:000000;00:00:00
>> :000000;00:00:00:000000;00:00:00:000000;00:00:00:000000;00:
>> 00:00:000000;00:00:00:000000;00:00:00:000000;;0;0;0;0;0;0;0;0;0;0;
>> 2018-03-17      06:43:11.763066 1521249191.763066;2018-03-17
>> 06:43:11.772796 1521249191.772796;2018-03-17    06:43:11.772860
>> 1521249191.772860;00:00:00;00:00:00;0.000555556;0;0;0;0;0;0;
>> 0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;
>> 0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;00:00:00:000000;00:00:00
>> :000000;00:00:00:000000;00:00:00:000000;00:00:00:000000;00:
>> 00:00:000000;00:00:00:000000;00:00:00:000000;;0;0;0;0;0;0;0;0;0;0;
>>
>>
>>
>> *[sprout]ubuntu at sprout:/var/log/sprout$ tail -30 sprout_current.txt*
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
>> thread_dispatcher.cpp:183: Request latency so far = 102us
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug pjsip: sip_endpoint.c
>> Distributing rdata to modules: Request msg OPTIONS/cseq=370860
>> (rdata0x7f7548081478)
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug uri_classifier.cpp:139:
>> home domain: false, local_to_node: true, is_gruu: false,
>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug uri_classifier.cpp:172:
>> Classified URI as 3
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug pjsip:       endpoint
>> Response msg 200/OPTIONS/cseq=370860 (tdta0x7f7548382670) created
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Verbose
>> common_sip_processing.cpp:103: TX 282 bytes Response msg
>> 200/OPTIONS/cseq=370860 (tdta0x7f7548382670) to TCP 10.224.61.22:51202:
>> --start msg--
>>
>> SIP/2.0 200 OK
>> Via: SIP/2.0/TCP 10.224.61.22;rport=51202;recei
>> ved=10.224.61.22;branch=z9hG4bK-370860
>> Call-ID: poll-sip-370860
>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=370860
>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-370860
>> CSeq: 370860 OPTIONS
>> Content-Length:  0
>>
>>
>> --end msg--
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug pjsip: tdta0x7f754838
>> Destroying txdata Response msg 200/OPTIONS/cseq=370860 (tdta0x7f7548382670)
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
>> thread_dispatcher.cpp:270: Worker thread completed processing message
>> 0x7f7548081478
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
>> thread_dispatcher.cpp:284: Request latency = 278us
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 278 for 0xf627a8
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 278 for 0xf62820
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug load_monitor.cpp:341:
>> Not recalculating rate as we haven't processed 20 requests yet (only 18).
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug utils.cpp:878: Removed
>> IOHook 0x7f75a6face30 to stack. There are now 0 hooks
>> 17-03-2018 01:18:51.121 UTC [7f75a6fad700] Debug
>> thread_dispatcher.cpp:158: Attempting to process queue element
>> 17-03-2018 01:18:53.122 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842
>> TCP connection closed
>> 17-03-2018 01:18:53.122 UTC [7f754feff700] Debug
>> connection_tracker.cpp:67: Connection 0x7f75484260d8 has been destroyed
>> 17-03-2018 01:18:53.122 UTC [7f754feff700] Verbose pjsip: tcps0x7f754842
>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>>
>>
>> Thanks and Regards,
>> Sunil
>>
>>
>>
>> On Fri, Mar 16, 2018 at 10:50 PM, Michael Duppr? <
>> Michael.Duppre at metaswitch.com> wrote:
>>
>>> Hello Sunil,
>>>
>>>
>>>
>>> It sounds like you have two different problems:
>>>
>>>    1. Running `/usr/share/clearwater/bin/run_stress <home_domain> 1800
>>>    2` doesn?t succeed making any calls
>>>       - The good news is that the initial REGISTERs that the tool sends
>>>       out are working fine as you can see in the output `Initial registration
>>>       succeeded`!
>>>       - To make the calls work, I would suggest just running one call
>>>       with the tool initially and once this works go back to running 1800 calls.
>>>       To debug this, you should turn on debug logs on Sprout (see
>>>       http://clearwater.readthedocs.io/en/stable/Troubleshooting_a
>>>       nd_Recovery.html#sprout
>>>       <http://clearwater.readthedocs.io/en/stable/Troubleshooting_and_Recovery.html#sprout>),
>>>       run the tool again with one call. Then have a look at the logs of the tool
>>>       and the Sprout logs and find out why Sprout is sending back a 503 response.
>>>    2. Running `/usr/share/clearwater/bin/run_stress <home_domain> 1800
>>>    10 --icscf-target TARGET=sprout.{domain}:5052 --scscf-target
>>>    TARGET=sprout.{domain}:5054` crashes
>>>       - You probably don?t need to provide more arguments when running
>>>       the tool as it should work with the defaults. So I suggest getting the tool
>>>       work without any additional arguments and only use these if you have a
>>>       specific need for additional configuration.
>>>       - The error ` Unknown remote host 'TARGET=sprout.<domain>` says,
>>>       the tool wasn?t able to lookup this domain. In fact, you need to replace
>>>       the word `TARGET` with your full sprout domain. If for example
>>>       `my_ims_domain` is your domain, you would need to run the command as
>>>       follows:
>>>       - `/usr/share/clearwater/bin/run_stress <home_domain> 1800 10
>>>       --icscf-target sprout.my_ims_domain:5052 --scscf-target
>>>       sprout.my_ims_domain:5054`
>>>
>>>
>>>
>>> Hope this helps, good luck getting the calls to work!
>>>
>>>
>>>
>>> Kind regards,
>>>
>>> Michael
>>>
>>>
>>>
>>>
>>>
>>> *From:* Clearwater [mailto:clearwater-bounces at lis
>>> ts.projectclearwater.org] *On Behalf Of *Pushpendra
>>> *Sent:* 14 March 2018 12:05
>>> *To:* Bennett Allen <Bennett.Allen at metaswitch.com>;
>>> clearwater at lists.projectclearwater.org
>>> *Subject:* [Project Clearwater] stress testing
>>>
>>>
>>>
>>> Hi All,
>>>
>>> I need to ask few questions regarding stress testing for manual
>>> installation of clearwater. I am following http://clearwater.re
>>> adthedocs.io/en/stable/Clearwater_stress_testing.html for stress
>>> testing.
>>>
>>>  I have set the* local_ip of this node* only in
>>> * /etc/clearwater/local_config. *I also set *reg_max_expires=1800* in
>>> all the other node in *shared_config *after that I created the 50000
>>> no. in vellum node as given in doc
>>>
>>>  https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Prov
>>> isioning%20Numbers.md?utf8=%E2%9C%93
>>>
>>> and then I install the debian package *sudo apt-get install
>>> clearwater-sip-stress-careonly *on new node.
>>>
>>> when I run
>>>
>>> /usr/share/clearwater/bin/run_stress <home_domain> 1800 2
>>>
>>> Starting initial registration, will take 22 seconds
>>>
>>> Initial registration succeeded
>>>
>>> Starting test
>>>
>>> Test complete
>>>
>>>
>>>
>>> Elapsed time: 00:02:00
>>>
>>> Start: 2018-03-14 18:06:06.635672
>>>
>>> End: 2018-03-14 18:08:06.714723
>>>
>>>
>>>
>>> Total calls: 39
>>>
>>> Successful calls: 0 (0.0%)
>>>
>>> Failed calls: 39 (100.0%)
>>>
>>> Unfinished calls: 0
>>>
>>>
>>>
>>> Retransmissions: 0
>>>
>>>
>>>
>>> Average time from INVITE to 180 Ringing: 0.0ms
>>>
>>> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> Failed: call success rate 0.0% is lower than target 100.0%!
>>>
>>>
>>>
>>> Total re-REGISTERs: 120
>>>
>>> Successful re-REGISTERs: 120 (100.0%)
>>>
>>> Failed re-REGISTERS: 0 (0.0%)
>>>
>>>
>>>
>>> REGISTER retransmissions: 0
>>>
>>>
>>>
>>> Average time from REGISTER to 200 OK: 30.0ms
>>>
>>>
>>>
>>> Log files at /var/log/clearwater-sip-stress/13921_*
>>>
>>>
>>>
>>> 1. I have not mention new node's IP in any other node of clearwater, how
>>> the other node know about it?
>>>
>>>
>>>
>>> My question is, *Do I need to mention the IP of new node in any other
>>> node of clearwater?*
>>>
>>>
>>>
>>>
>>>
>>> when I run -
>>>
>>> []ubuntu at stress:~$  /usr/share/clearwater/bin/run_stress <home_domain>
>>> 1800 10 --icscf-target TARGET=sprout.{domain}:5052 --scscf-target
>>> TARGET=sprout.{domain}:5054
>>>
>>>
>>>
>>> Starting initial registration, will take 22 seconds
>>>
>>> Initial registration failed - see /var/log/clearwater-sip-stress/14115_initial_reg_errors.log
>>> for details of the errors
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> *[]ubuntu at stress:/var/log/clearwater-sip-stress$ tail -30
>>> 14115_initial_reg_errors.log*
>>>
>>> sipp: The following events occured:
>>>
>>> 2018-03-14      18:11:06.637370 1521031266.637370: Unknown remote host
>>> 'TARGET=sprout.<domain>' (Name or service not known, Inappropriate ioctl
>>> for device).
>>>
>>> Use 'sipp -h' for details.
>>>
>>>
>>>
>>>
>>>
>>> *[]ubuntu at stress:/var/log/clearwater-sip-stress$ cat
>>> 9923_caller_errors.log*
>>>
>>> 2018-03-14      00:32:45.274812 1520967765.274812: Aborting call on
>>> unexpected message for Call-Id '54-9937 at 127.0.1.1': while expecting
>>> '183' (index 2), received 'SIP/2.0 503 Service Unavailable
>>>
>>> Via: SIP/2.0/TCP 127.0.1.1:34015;received=10.22
>>> 4.61.13;branch=z9hG4bK-9937-54-0
>>>
>>> Record-Route: <sip:scscf.sprout. <home_domain>
>>>  ;transport=TCP;lr;billing-role=charge-term>
>>>
>>> Record-Route: <sip:scscf.sprout. <home_domain>
>>>  ;transport=TCP;lr;billing-role=charge-orig>
>>>
>>> Call-ID: 54-9937 at 127.0.1.1
>>>
>>> From: <sip:2010000252@<home_domain>>;tag=9937SIPpTag0054
>>>
>>> To: <sip:2010000647@ <home_domain> >;tag=z9hG4bKPjJVqopmpeih7pUo
>>> xJrYOG7CO4-Bm69ozI
>>>
>>> CSeq: 1 INVITE
>>>
>>> P-Charging-Vector: icid-value="9937SIPpTag0054";o
>>> rig-ioi=<home_domain>;term-ioi=<home_domain>
>>>
>>> P-Charging-Function-Addresses: ccf=0.0.0.0
>>>
>>> Content-Length:  0
>>>
>>>
>>>
>>> '.
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> Regards,
>>>
>>>
>>>
>>>
>>>
>>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180319/e6f0863b/attachment.html>

From Michael.Duppre at metaswitch.com  Thu Mar 22 10:39:35 2018
From: Michael.Duppre at metaswitch.com (=?utf-8?B?TWljaGFlbCBEdXBwcsOp?=)
Date: Thu, 22 Mar 2018 14:39:35 +0000
Subject: [Project Clearwater] CW team please help - stress testing
In-Reply-To: <CAHwYWpBH8qdZbAEuFoE61Y13s1s9QGku912aBb5w8XpX+ZmAkQ@mail.gmail.com>
References: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
	<CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>
	<CAHwYWpB4WqEVnG69B_BazQC6yJVHW7A3pMPOWmRoVX_gYJhPgw@mail.gmail.com>
	<CAHwYWpBH8qdZbAEuFoE61Y13s1s9QGku912aBb5w8XpX+ZmAkQ@mail.gmail.com>
Message-ID: <SN1PR02MB16779E13A341C6567440B42CF5A90@SN1PR02MB1677.namprd02.prod.outlook.com>

Hello Sunil,

Sorry about the stress tool not working properly with a lower number of subscribers, that looks like a bug in the tool. I have raised issue https://github.com/Metaswitch/project-clearwater-issues/issues/30 to track and fix this problem, feel free to provide any other information on that ticket if you hit similar problems. Thanks for your help finding this bug!

Looks like you?ve done the right things and went back to a slightly higher number of subscribers (50) and looked at the stress log file and the sprout log file. Unfortunately it looks like you?ve copied out the wrong time period from the sprout logs: In your email below, the stress tool logs are from 17:48, however the sprout logs that you?ve sent are from 5 hours before at 12:27.
Similar for your tcpdump ? a good idea to have a look at this, but unfortunately what you?ve copied into your email is only the register flow, which is successful! :-)

You?re probably pretty close finding the reason why the calls are failing in the sprout logs, could you please make sure you have a look at the timestamp of the time you ran the stress tool?

Good luck and kind regards,
Michael


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sunil Kumar
Sent: 20 March 2018 14:57
To: clearwater at lists.projectclearwater.org; Bennett Allen <Bennett.Allen at metaswitch.com>
Subject: Re: [Project Clearwater] CW team please help - stress testing

Hi,
I am facing problem in stress testing, Please look into the log. I am not able to debug the problem.

I have taken this from wireshark, actually i use tcpdump.

REGISTER sip:ims.com<http://ims.com> SIP/2.0
Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-0
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
Content-Length: 0
Require: Path
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
Supported: path
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Route: <sip:icscf at sprout.ims.com<mailto:sip%3Aicscf at sprout.ims.com>;lr>
Max-Forwards: 70
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768>>;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
CSeq: 1 REGISTER
Expires: 3600
Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY, MESSAGE, PRACK
Supported: path, gruu
Authorization: Digest username="2010000039 at ims.com<mailto:2010000039 at ims.com>",realm="ims.com<http://ims.com>",uri="sip:ims.com<http://ims.com>",nonce="",response="",algorithm=Digest-MD5
User-Agent: 00-00000-0000000000000 Phone IMS 10.0
P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
P-Visited-Network-ID: ims.com<http://ims.com>

SIP/2.0 401 Unauthorized
Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-1-0
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=z9hG4bKPjDBaGZjqTrLQiDSlHihO36oMPm7fxz2sQ
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
WWW-Authenticate: Digest  realm="ims.com<http://ims.com>",nonce="0e07c1b77b566f37",opaque="5171f001504c2c3a",algorithm=MD5,qop="auth"
Content-Length:  0

REGISTER sip:ims.com<http://ims.com> SIP/2.0
Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-2
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
Content-Length: 0
Require: Path
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
Supported: path
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Route: <sip:icscf at sprout.ims.com<mailto:sip%3Aicscf at sprout.ims.com>;lr>
Max-Forwards: 70
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768>>;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
CSeq: 1 REGISTER
Expires: 3600
Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY, MESSAGE, PRACK
Supported: path, gruu
Authorization: Digest username="2010000039 at ims.com<mailto:2010000039 at ims.com>",realm="ims.com<http://ims.com>",cnonce="66334873",nc=00000001,qop=auth,uri="sip:sprout.ims.com:5052<http://sprout.ims.com:5052>",nonce="0e07c1b77b566f37",response="788d4520717e4e7b29f7fab43fdc448f",algorithm=MD5,opaque="5171f001504c2c3a"
User-Agent: 00-00000-0000000000000 Phone IMS 10.0
P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
P-Visited-Network-ID: ims.com<http://ims.com>

SIP/2.0 200 OK
Service-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com>;transport=TCP;lr;orig;username=2010000039%40ims.com<http://40ims.com>;nonce=0e07c1b77b566f37>
Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-1-2
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=z9hG4bKPjIvjh2DjwvU.vVNEv.nOiYAfsZRgMjHDF
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
Supported: outbound
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768>>;expires=1800;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>";reg-id=1;pub-gruu="sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>;gr=urn:uuid:00000000-0000-0000-0000-000000000001"
Require: outbound
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Associated-URI: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Content-Length:  0


thanks in advance, Please resply.

cheers,
sunil


On Tue, Mar 20, 2018 at 6:53 PM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi,
It is using some other port on stress node not 5082. Is this a problem, if yes how can I fix this i have tried to open 5082 port on stress node using sudo ufw allow 5082/tcp, but no effect.
Please check the wireshark log:

Frame 2406: 703 bytes on wire (5624 bits), 703 bytes captured (5624 bits)
Ethernet II, Src: PcsCompu_ff:d2:88 (08:00:27:ff:d2:88), Dst: PcsCompu_ab:71:0f (08:00:27:ab:71:0f)
Internet Protocol Version 4, Src: 10.224.61.22, Dst: 10.224.61.13
Transmission Control Protocol, Src Port: rlm-admin (5054), Dst Port: 34312 (34312), Seq: 349, Ack: 2199, Len: 637
Session Initiation Protocol (503)


cheers,
sunil

On Tue, Mar 20, 2018 at 5:21 PM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi all,
I have taken tcpdump also but there is no SIP message. Please through some light on this problem. I am trying from last weak, not able to catch the problem. Thanks in advance.

cheers,
Sunil

On Tue, Mar 20, 2018 at 10:30 AM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi CW team,
Anyone out there please help me. I am facing problem in stress testing. I have installed CW manually. whenever I was running 1 or less than 20 it give some errors like:

[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com<http://ims.com> 1 2
[sudo] password for ubuntu:
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
    with open(CALLER_STATS) as f:
IOError: [Errno 2] No such file or directory: '/var/log/clearwater-sip-stress/18065_caller_stats.log'


[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com<http://ims.com> 10 5
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
    call_success_rate = 100 * float(row['SuccessfulCall(C)']) / float(row['TotalCallCreated'])
ZeroDivisionError: float division by zero


[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress iind.intel.com<http://iind.intel.com> 50 5
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:03:41
Start: 2018-03-20 17:46:43.268136
End: 2018-03-20 17:51:31.363406

Total calls: 2
Successful calls: 0 (0.0%)
Failed calls: 2 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 8
Successful re-REGISTERs: 8 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 86.0ms

Log files at /var/log/clearwater-sip-stress/18566_*



[]ubuntu at stress:~$ cat /var/log/clearwater-sip-stress/18566_caller_errors.log
sipp: The following events occured:
2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on unexpected message for Call-Id '1-18576 at 127.0.1.1<mailto:1-18576 at 127.0.1.1>': while expecting '183' (index 2), received 'SIP/2.0 503 Service Unavailable
Via: SIP/2.0/TCP 127.0.1.1:42276;received=10.224.61.13;branch=z9hG4bK-18576-1-0
Record-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com>;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout. ims.com<http://ims.com> ;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 1-18576 at 127.0.1.1<mailto:1-18576 at 127.0.1.1>
From: <sip:2010000042@ ims.com<http://ims.com> >;tag=18576SIPpTag001
To: <sip:2010000015@ ims.co<http://ims.co>>;tag=z9hG4bKPj1Lm9whhQMslKrcZxnN6qCH0tb9Lj5Neu
CSeq: 1 INVITE
P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com<http://ims.com> ;term-ioi= ims.com<http://ims.com>
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0


[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=49294;received=10.224.61.22;branch=z9hG4bK-670172
Call-ID: poll-sip-670172
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670172
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670172
CSeq: 670172 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:284: Request latency = 254us
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217: Rate adjustment calculation inputs: err -0.981500, smoothed latency 185, target latency 10000
20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302: Maximum incoming request rate/second unchanged at 2000.000000 (current request rate is 0.200000 requests/sec, minimum threshold for a change is 1000.000000 requests/sec).
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054<http://10.224.61.22:5054>: got incoming TCP connection from 10.224.61.22:42848<http://10.224.61.22:42848>, sock=573
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 tcp->base.local_name: 10.224.61.22
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP server transport created
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=670180 (rdata0x7f34ec027690)
20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180 (rdata0x7f34ec027690) from TCP 10.224.61.22:42848<http://10.224.61.22:42848>:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054<http://sip:poll-sip at 10.224.61.22:5054> SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054<http://sip:poll-sip at 10.224.61.22:5054>>
From: poll-sip <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670180
Call-ID: poll-sip-670180
CSeq: 670180 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8 for worker threads with priority 15
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:183: Request latency so far = 57us
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=670180 (rdata0x7f34ec34a3e8)
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848<http://10.224.61.22:42848>:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=42848;received=10.224.61.22;branch=z9hG4bK-670180
Call-ID: poll-sip-670180
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670180
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670180
CSeq: 670180 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809 Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:284: Request latency = 129us
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 1).
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053<http://10.224.61.22:5053>: got incoming TCP connection from 10.224.61.22:49356<http://10.224.61.22:49356>, sock=573
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 tcp->base.local_name: 10.224.61.22
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP server transport created
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=670182 (rdata0x7f34ec027690)
20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182 (rdata0x7f34ec027690) from TCP 10.224.61.22:49356<http://10.224.61.22:49356>:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053<http://sip:poll-sip at 10.224.61.22:5053> SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053<http://sip:poll-sip at 10.224.61.22:5053>>
From: poll-sip <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670182
Call-ID: poll-sip-670182
CSeq: 670182 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8 for worker threads with priority 15
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:183: Request latency so far = 102us
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=670182 (rdata0x7f34ec34a3e8)
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356<http://10.224.61.22:49356>:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=49356;received=10.224.61.22;branch=z9hG4bK-670182
Call-ID: poll-sip-670182
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670182
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670182
CSeq: 670182 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00 Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:284: Request latency = 232us
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 2).
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1001.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1005.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1011.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1012.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1013.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1004.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1002.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1009.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1010.1 alarm
20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)



all calls are failing I don't know what is going on, I am newbie to CW please guide some solution it will be great help.


Thanks,
Sunil




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180322/a02f6eb9/attachment.html>

From anthonynlee at gmail.com  Thu Mar 22 17:20:21 2018
From: anthonynlee at gmail.com (Anthony Lee)
Date: Thu, 22 Mar 2018 17:20:21 -0400
Subject: [Project Clearwater] Why the port 3868 is listened by homestead?
Message-ID: <CA+pBo5G2Ob_Jk6v9ot_=+AweiB3xL1QsVgZLVPcJR_dTvEe32g@mail.gmail.com>

Hi,

I tried to integrate clearwater all-in-one with OpenIMS FHoSS hss,
I installed FHoSS in the same VM.
I have below settings in /etc/clearwater/shared_config:
hss_hostname=cw-aio
hss_port=3868

and then I run "sudo reboot"

I run "sudo netstat -tulpn | grep 3868" and the output shows that
the process listens on 3868 is homestead.

When I run a test I see there are some Diameter request in homestead log
file.
But all get error response because it never talk to real Diameter peer.

I don't know what I did wrong.
The port supposed to be used by FHoSS but it is occupied by homestead. ???

Any idea?


Thanks
Anthony
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180322/962a8014/attachment.html>

From skgola1997 at gmail.com  Thu Mar 22 22:29:52 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Fri, 23 Mar 2018 07:59:52 +0530
Subject: [Project Clearwater] CW team please help - stress testing
In-Reply-To: <SN1PR02MB16779E13A341C6567440B42CF5A90@SN1PR02MB1677.namprd02.prod.outlook.com>
References: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
	<CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>
	<CAHwYWpB4WqEVnG69B_BazQC6yJVHW7A3pMPOWmRoVX_gYJhPgw@mail.gmail.com>
	<CAHwYWpBH8qdZbAEuFoE61Y13s1s9QGku912aBb5w8XpX+ZmAkQ@mail.gmail.com>
	<SN1PR02MB16779E13A341C6567440B42CF5A90@SN1PR02MB1677.namprd02.prod.outlook.com>
Message-ID: <CAHwYWpCj9xAoiE=eLe7+GMCrX7RjPDgMHXVazVhX4evT8AbTUw@mail.gmail.com>

Hi,
Thanks for replying, but you guys are replying very late, its not good, I
have been waiting for your reply from last 3 days :-( .
Anyway, I thought the script hast other problem also, May be you will check
it and fix it so that others wouldn't got that problem. Somehow I fix the
problem, though it takes lot of of time to read the script and make some
changes.

I want ask few questions and *expecting reply within a day* :-)
1. when I use 1000 subscriber and running for 10 min duration, only few
call are successful (around 300) and no calls are failed. How can I
increase no. of calls.
2. Can you explain the exact use of* --multiplier *parameter in detail. I
request you to add all the parameter in doc itself so other would not get
problem while finding.

Thanks,
Sunil


On Thu, Mar 22, 2018 at 8:09 PM, Michael Duppr? <
Michael.Duppre at metaswitch.com> wrote:

> Hello Sunil,
>
>
>
> Sorry about the stress tool not working properly with a lower number of
> subscribers, that looks like a bug in the tool. I have raised issue
> https://github.com/Metaswitch/project-clearwater-issues/issues/30 to
> track and fix this problem, feel free to provide any other information on
> that ticket if you hit similar problems. Thanks for your help finding this
> bug!
>
>
>
> Looks like you?ve done the right things and went back to a slightly higher
> number of subscribers (50) and looked at the stress log file and the sprout
> log file. Unfortunately it looks like you?ve copied out the wrong time
> period from the sprout logs: In your email below, the stress tool logs are
> from 17:48, however the sprout logs that you?ve sent are from 5 hours
> before at 12:27.
>
> Similar for your tcpdump ? a good idea to have a look at this, but
> unfortunately what you?ve copied into your email is only the register flow,
> which is successful! :-)
>
>
>
> You?re probably pretty close finding the reason why the calls are failing
> in the sprout logs, could you please make sure you have a look at the
> timestamp of the time you ran the stress tool?
>
>
>
> Good luck and kind regards,
>
> Michael
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sunil Kumar
> *Sent:* 20 March 2018 14:57
> *To:* clearwater at lists.projectclearwater.org; Bennett Allen <
> Bennett.Allen at metaswitch.com>
> *Subject:* Re: [Project Clearwater] CW team please help - stress testing
>
>
>
> Hi,
>
> I am facing problem in stress testing, Please look into the log. I am not
> able to debug the problem.
>
>
>
> I have taken this from wireshark, actually i use tcpdump.
>
>
>
> REGISTER sip:ims.com SIP/2.0
>
> Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-0
>
> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>
> Content-Length: 0
>
> Require: Path
>
> Path: <sip:127.0.1.1:5082;transport=tcp;lr>
>
> P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
>
> Supported: path
>
> To: <sip:2010000039 at ims.com>
>
> Route: <sip:icscf at sprout.ims.com;lr>
>
> Max-Forwards: 70
>
> Contact: <sip:2010000039 at 127.0.1.1:34768>;reg-id=1;+sip.instance=
> "<urn:uuid:00000000-0000-0000-0000-000000000001>"
>
> Call-ID: 1-784 at 127.0.1.1
>
> CSeq: 1 REGISTER
>
> Expires: 3600
>
> Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY,
> MESSAGE, PRACK
>
> Supported: path, gruu
>
> Authorization: Digest username="2010000039 at ims.com",realm="ims.com
> ",uri="sip:ims.com",nonce="",response="",algorithm=Digest-MD5
>
> User-Agent: 00-00000-0000000000000 Phone IMS 10.0
>
> P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=
> 000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
>
> P-Visited-Network-ID: ims.com
>
>
>
> SIP/2.0 401 Unauthorized
>
> Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-
> 1-0
>
> Call-ID: 1-784 at 127.0.1.1
>
> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>
> To: <sip:2010000039 at ims.com>;tag=z9hG4bKPjDBaGZjqTrLQiDSlHihO36oMPm7fxz2sQ
>
> CSeq: 1 REGISTER
>
> P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
>
> WWW-Authenticate: Digest  realm="ims.com",nonce="
> 0e07c1b77b566f37",opaque="5171f001504c2c3a",algorithm=MD5,qop="auth"
>
> Content-Length:  0
>
>
>
> REGISTER sip:ims.com SIP/2.0
>
> Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-2
>
> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>
> Content-Length: 0
>
> Require: Path
>
> Path: <sip:127.0.1.1:5082;transport=tcp;lr>
>
> P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
>
> Supported: path
>
> To: <sip:2010000039 at ims.com>
>
> Route: <sip:icscf at sprout.ims.com;lr>
>
> Max-Forwards: 70
>
> Contact: <sip:2010000039 at 127.0.1.1:34768>;reg-id=1;+sip.instance=
> "<urn:uuid:00000000-0000-0000-0000-000000000001>"
>
> Call-ID: 1-784 at 127.0.1.1
>
> CSeq: 1 REGISTER
>
> Expires: 3600
>
> Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY,
> MESSAGE, PRACK
>
> Supported: path, gruu
>
> Authorization: Digest username="2010000039 at ims.com",realm="ims.com
> ",cnonce="66334873",nc=00000001,qop=auth,uri="sip:sprout.ims.com:5052
> ",nonce="0e07c1b77b566f37",response="788d4520717e4e7b29f7fab43fdc44
> 8f",algorithm=MD5,opaque="5171f001504c2c3a"
>
> User-Agent: 00-00000-0000000000000 Phone IMS 10.0
>
> P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=
> 000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
>
> P-Visited-Network-ID: ims.com
>
>
>
> SIP/2.0 200 OK
>
> Service-Route: <sip:scscf.sprout.ims.com;transport=TCP;lr;orig;
> username=2010000039%40ims.com;nonce=0e07c1b77b566f37>
>
> Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-
> 1-2
>
> Call-ID: 1-784 at 127.0.1.1
>
> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>
> To: <sip:2010000039 at ims.com>;tag=z9hG4bKPjIvjh2DjwvU.vVNEv.nOiYAfsZRgMjHDF
>
> CSeq: 1 REGISTER
>
> P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
>
> Supported: outbound
>
> Contact: <sip:2010000039 at 127.0.1.1:34768>;expires=1800;+sip.
> instance="<urn:uuid:00000000-0000-0000-0000-000000000001>";
> reg-id=1;pub-gruu="sip:2010000039 at ims.com;gr=urn:
> uuid:00000000-0000-0000-0000-000000000001"
>
> Require: outbound
>
> Path: <sip:127.0.1.1:5082;transport=tcp;lr>
>
> P-Associated-URI: <sip:2010000039 at ims.com>
>
> Content-Length:  0
>
>
>
>
>
> thanks in advance, Please resply.
>
>
>
> cheers,
>
> sunil
>
>
>
>
>
> On Tue, Mar 20, 2018 at 6:53 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:
>
> Hi,
>
> It is using some other port on stress node not 5082. Is this a problem, if
> yes how can I fix this i have tried to open 5082 port on stress node using *sudo
> ufw allow 5082/tcp, *but no effect.
>
> Please check the wireshark log:
>
>
>
> Frame 2406: 703 bytes on wire (5624 bits), 703 bytes captured (5624 bits)
>
> Ethernet II, Src: PcsCompu_ff:d2:88 (08:00:27:ff:d2:88), Dst:
> PcsCompu_ab:71:0f (08:00:27:ab:71:0f)
>
> Internet Protocol Version 4, Src: 10.224.61.22, Dst: 10.224.61.13
>
> Transmission Control Protocol, Src Port: rlm-admin (5054), Dst Port: 34312
> (34312), Seq: 349, Ack: 2199, Len: 637
>
> Session Initiation Protocol (503)
>
>
>
>
>
> cheers,
>
> sunil
>
>
>
> On Tue, Mar 20, 2018 at 5:21 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:
>
> Hi all,
>
> I have taken tcpdump also but there is no SIP message. Please through some
> light on this problem. I am trying from last weak, not able to catch the
> problem. Thanks in advance.
>
>
>
> cheers,
>
> Sunil
>
>
>
> On Tue, Mar 20, 2018 at 10:30 AM, Sunil Kumar <skgola1997 at gmail.com>
> wrote:
>
> Hi CW team,
>
> Anyone out there please help me. I am facing problem in stress testing. I
> have installed CW manually. whenever I was running 1 or less than 20 it
> give some errors like:
>
>
>
> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
> <http://ims.com> 1 2*
>
> [sudo] password for ubuntu:
>
> Starting initial registration, will take 0 seconds
>
> Initial registration succeeded
>
> Starting test
>
> Test complete
>
> Traceback (most recent call last):
>
>   File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
>
>     with open(CALLER_STATS) as f:
>
> IOError: [Errno 2] No such file or directory: '/var/log/clearwater-sip-
> stress/18065_caller_stats.log'
>
>
>
>
>
> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
> <http://ims.com> 10 5*
>
> Starting initial registration, will take 0 seconds
>
> Initial registration succeeded
>
> Starting test
>
> Test complete
>
> Traceback (most recent call last):
>
>   File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
>
>     call_success_rate = 100 * float(row['SuccessfulCall(C)']) /
> float(row['TotalCallCreated'])
>
> ZeroDivisionError: float division by zero
>
>
>
>
>
> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress
> iind.intel.com <http://iind.intel.com> 50 5*
>
> Starting initial registration, will take 0 seconds
>
> Initial registration succeeded
>
> Starting test
>
> Test complete
>
>
>
> Elapsed time: 00:03:41
>
> Start: 2018-03-20 17:46:43.268136
>
> End: 2018-03-20 17:51:31.363406
>
>
>
> Total calls: 2
>
> Successful calls: 0 (0.0%)
>
> Failed calls: 2 (100.0%)
>
> Unfinished calls: 0
>
>
>
> Retransmissions: 0
>
>
>
> Average time from INVITE to 180 Ringing: 0.0ms
>
> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
>
> Failed: call success rate 0.0% is lower than target 100.0%!
>
>
>
> Total re-REGISTERs: 8
>
> Successful re-REGISTERs: 8 (100.0%)
>
> Failed re-REGISTERS: 0 (0.0%)
>
>
>
> REGISTER retransmissions: 0
>
>
>
> Average time from REGISTER to 200 OK: 86.0ms
>
>
>
> Log files at /var/log/clearwater-sip-stress/18566_*
>
>
>
>
>
>
>
> *[]ubuntu at stress:~$ cat
> /var/log/clearwater-sip-stress/18566_caller_errors.log*
>
> sipp: The following events occured:
>
> 2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on
> unexpected message for Call-Id '1-18576 at 127.0.1.1': while expecting '183'
> (index 2), received '*SIP/2.0 503 Service Unavailable*
>
> Via: SIP/2.0/TCP 127.0.1.1:42276;received=10.224.61.13;branch=z9hG4bK-
> 18576-1-0
>
> Record-Route: <sip:scscf.sprout.ims.com;transport=TCP;lr;billing-role=
> charge-term>
>
> Record-Route: <sip:scscf.sprout. ims.com ;transport=TCP;lr;billing-
> role=charge-orig>
>
> Call-ID: 1-18576 at 127.0.1.1
>
> From: <sip:2010000042@ ims.com >;tag=18576SIPpTag001
>
> To: <sip:2010000015@ ims.co>;tag=z9hG4bKPj1Lm9whhQMslKrcZxnN6qCH0tb9Lj5Neu
>
> CSeq: 1 INVITE
>
> P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com
> ;term-ioi= ims.com
>
> P-Charging-Function-Addresses: ccf=0.0.0.0
>
> Content-Length:  0
>
>
>
>
>
> *[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt*
>
> --start msg--
>
>
>
> SIP/2.0 200 OK
>
> Via: SIP/2.0/TCP 10.224.61.22;rport=49294;received=10.224.61.22;branch=
> z9hG4bK-670172
>
> Call-ID: poll-sip-670172
>
> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670172
>
> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670172
>
> CSeq: 670172 OPTIONS
>
> Content-Length:  0
>
>
>
>
>
> --end msg--
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b
> Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> thread_dispatcher.cpp:270: Worker thread completed processing message
> 0x7f34ec34a3e8
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> thread_dispatcher.cpp:284: Request latency = 254us
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217: Rate
> adjustment calculation inputs: err -0.981500, smoothed latency 185, target
> latency 10000
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302:
> Maximum incoming request rate/second unchanged at 2000.000000 (current
> request rate is 0.200000 requests/sec, minimum threshold for a change is
> 1000.000000 requests/sec).
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
> 2000ui into continuous accumulator statistic
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
> 2000ui into continuous accumulator statistic
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed
> IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> thread_dispatcher.cpp:158: Attempting to process queue element
>
> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP connection closed
>
> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug
> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>
> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>
> 20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning:
> Failed to connect to the agentx master agent ([NIL]):
>
> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054
> TCP listener 10.224.61.22:5054: got incoming TCP connection from
> 10.224.61.22:42848, sock=573
>
> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> tcp->base.local_name: 10.224.61.22
>
> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP server transport created
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
> Processing incoming message: Request msg OPTIONS/cseq=670180
> (rdata0x7f34ec027690)
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose
> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180
> (rdata0x7f34ec027690) from TCP 10.224.61.22:42848:
>
> --start msg--
>
>
>
> OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
>
> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
>
> Max-Forwards: 2
>
> To: <sip:poll-sip at 10.224.61.22:5054>
>
> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670180
>
> Call-ID: poll-sip-670180
>
> CSeq: 670180 OPTIONS
>
> Contact: <sip:10.224.61.22>
>
> Accept: application/sdp
>
> Content-Length: 0
>
> User-Agent: poll-sip
>
>
>
>
>
> --end msg--
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
> Classified URI as 3
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
> 0x7f34ec34a3e8
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
> for worker threads with priority 15
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added
> IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:183: Request latency so far = 57us
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c
> Distributing rdata to modules: Request msg OPTIONS/cseq=670180
> (rdata0x7f34ec34a3e8)
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172:
> Classified URI as 3
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint
> Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose
> common_sip_processing.cpp:103: TX 282 bytes Response msg
> 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848:
>
> --start msg--
>
>
>
> SIP/2.0 200 OK
>
> Via: SIP/2.0/TCP 10.224.61.22;rport=42848;received=10.224.61.22;branch=
> z9hG4bK-670180
>
> Call-ID: poll-sip-670180
>
> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670180
>
> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670180
>
> CSeq: 670180 OPTIONS
>
> Content-Length:  0
>
>
>
>
>
> --end msg--
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809
> Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:270: Worker thread completed processing message
> 0x7f34ec34a3e8
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:284: Request latency = 129us
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341: Not
> recalculating rate as we haven't processed 20 requests yet (only 1).
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed
> IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:158: Attempting to process queue element
>
> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327:
> Process request for URL /ping, args (null)
>
> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68:
> Sending response 200 to request for URL /ping, args (null)
>
> 20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP connection closed
>
> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug
> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>
> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>
> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053
> TCP listener 10.224.61.22:5053: got incoming TCP connection from
> 10.224.61.22:49356, sock=573
>
> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> tcp->base.local_name: 10.224.61.22
>
> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP server transport created
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
> Processing incoming message: Request msg OPTIONS/cseq=670182
> (rdata0x7f34ec027690)
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose
> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182
> (rdata0x7f34ec027690) from TCP 10.224.61.22:49356:
>
> --start msg--
>
>
>
> OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
>
> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
>
> Max-Forwards: 2
>
> To: <sip:poll-sip at 10.224.61.22:5053>
>
> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670182
>
> Call-ID: poll-sip-670182
>
> CSeq: 670182 OPTIONS
>
> Contact: <sip:10.224.61.22>
>
> Accept: application/sdp
>
> Content-Length: 0
>
> User-Agent: poll-sip
>
>
>
>
>
> --end msg--
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
> Classified URI as 3
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
> 0x7f34ec34a3e8
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
> for worker threads with priority 15
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added
> IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:183: Request latency so far = 102us
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c
> Distributing rdata to modules: Request msg OPTIONS/cseq=670182
> (rdata0x7f34ec34a3e8)
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172:
> Classified URI as 3
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint
> Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose
> common_sip_processing.cpp:103: TX 282 bytes Response msg
> 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356:
>
> --start msg--
>
>
>
> SIP/2.0 200 OK
>
> Via: SIP/2.0/TCP 10.224.61.22;rport=49356;received=10.224.61.22;branch=
> z9hG4bK-670182
>
> Call-ID: poll-sip-670182
>
> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670182
>
> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670182
>
> CSeq: 670182 OPTIONS
>
> Content-Length:  0
>
>
>
>
>
> --end msg--
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00
> Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:270: Worker thread completed processing message
> 0x7f34ec34a3e8
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:284: Request latency = 232us
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341: Not
> recalculating rate as we haven't processed 20 requests yet (only 2).
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed
> IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:158: Attempting to process queue element
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1001.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1005.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1011.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1012.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1013.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1004.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1002.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1009.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1010.1 alarm
>
> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP connection closed
>
> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug
> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>
> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>
>
>
>
>
>
>
> all calls are failing I don't know what is going on, I am newbie to CW
> please guide some solution it will be great help.
>
>
>
>
>
> Thanks,
>
> Sunil
>
>
>
>
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180323/434c4e8f/attachment.html>

From jim.page at redmatter.com  Fri Mar 23 05:20:03 2018
From: jim.page at redmatter.com (Jim Page)
Date: Fri, 23 Mar 2018 09:20:03 +0000
Subject: [Project Clearwater] CW team please help - stress testing
In-Reply-To: <CAHwYWpCj9xAoiE=eLe7+GMCrX7RjPDgMHXVazVhX4evT8AbTUw@mail.gmail.com>
References: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
	<CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>
	<CAHwYWpB4WqEVnG69B_BazQC6yJVHW7A3pMPOWmRoVX_gYJhPgw@mail.gmail.com>
	<CAHwYWpBH8qdZbAEuFoE61Y13s1s9QGku912aBb5w8XpX+ZmAkQ@mail.gmail.com>
	<SN1PR02MB16779E13A341C6567440B42CF5A90@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpCj9xAoiE=eLe7+GMCrX7RjPDgMHXVazVhX4evT8AbTUw@mail.gmail.com>
Message-ID: <C29325B9-84E3-49B8-96B4-044A1AB88CB8@redmatter.com>

Dude, this is a free service. I am sure if you buy some of their commercial licenses they will prioritise your request, but if you are on here you need to understand that service given here is ?best effort?, and in my view they perform a fantastic job. So calm down.

RedMatter Ltd
Jim Page
VP Mobile Services
+44 (0)333 150 1666
+44 (0)7870 361412
jim.page at redmatter.com<mailto:jim.page at redmatter.com>

On 23 Mar 2018, at 02:29, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:

Hi,
Thanks for replying, but you guys are replying very late, its not good, I have been waiting for your reply from last 3 days :-( .
Anyway, I thought the script hast other problem also, May be you will check it and fix it so that others wouldn't got that problem. Somehow I fix the problem, though it takes lot of of time to read the script and make some changes.

I want ask few questions and expecting reply within a day :-)
1. when I use 1000 subscriber and running for 10 min duration, only few call are successful (around 300) and no calls are failed. How can I increase no. of calls.
2. Can you explain the exact use of --multiplier parameter in detail. I request you to add all the parameter in doc itself so other would not get problem while finding.

Thanks,
Sunil


On Thu, Mar 22, 2018 at 8:09 PM, Michael Duppr? <Michael.Duppre at metaswitch.com<mailto:Michael.Duppre at metaswitch.com>> wrote:
Hello Sunil,

Sorry about the stress tool not working properly with a lower number of subscribers, that looks like a bug in the tool. I have raised issue https://github.com/Metaswitch/project-clearwater-issues/issues/30 to track and fix this problem, feel free to provide any other information on that ticket if you hit similar problems. Thanks for your help finding this bug!

Looks like you?ve done the right things and went back to a slightly higher number of subscribers (50) and looked at the stress log file and the sprout log file. Unfortunately it looks like you?ve copied out the wrong time period from the sprout logs: In your email below, the stress tool logs are from 17:48, however the sprout logs that you?ve sent are from 5 hours before at 12:27.
Similar for your tcpdump ? a good idea to have a look at this, but unfortunately what you?ve copied into your email is only the register flow, which is successful! :-)

You?re probably pretty close finding the reason why the calls are failing in the sprout logs, could you please make sure you have a look at the timestamp of the time you ran the stress tool?

Good luck and kind regards,
Michael


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Sunil Kumar
Sent: 20 March 2018 14:57
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Bennett Allen <Bennett.Allen at metaswitch.com<mailto:Bennett.Allen at metaswitch.com>>
Subject: Re: [Project Clearwater] CW team please help - stress testing

Hi,
I am facing problem in stress testing, Please look into the log. I am not able to debug the problem.

I have taken this from wireshark, actually i use tcpdump.

REGISTER sip:ims.com<http://ims.com/> SIP/2.0
Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-0
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
Content-Length: 0
Require: Path
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
Supported: path
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Route: <sip:icscf at sprout.ims.com<mailto:sip%3Aicscf at sprout.ims.com>;lr>
Max-Forwards: 70
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768/>>;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
CSeq: 1 REGISTER
Expires: 3600
Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY, MESSAGE, PRACK
Supported: path, gruu
Authorization: Digest username="2010000039 at ims.com<mailto:2010000039 at ims.com>",realm="ims.com<http://ims.com/>",uri="sip:ims.com<http://ims.com/>",nonce="",response="",algorithm=Digest-MD5
User-Agent: 00-00000-0000000000000 Phone IMS 10.0
P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
P-Visited-Network-ID: ims.com<http://ims.com/>

SIP/2.0 401 Unauthorized
Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-1-0
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=z9hG4bKPjDBaGZjqTrLQiDSlHihO36oMPm7fxz2sQ
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
WWW-Authenticate: Digest  realm="ims.com<http://ims.com/>",nonce="0e07c1b77b566f37",opaque="5171f001504c2c3a",algorithm=MD5,qop="auth"
Content-Length:  0

REGISTER sip:ims.com<http://ims.com/> SIP/2.0
Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-2
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
Content-Length: 0
Require: Path
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
Supported: path
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Route: <sip:icscf at sprout.ims.com<mailto:sip%3Aicscf at sprout.ims.com>;lr>
Max-Forwards: 70
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768/>>;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
CSeq: 1 REGISTER
Expires: 3600
Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY, MESSAGE, PRACK
Supported: path, gruu
Authorization: Digest username="2010000039 at ims.com<mailto:2010000039 at ims.com>",realm="ims.com<http://ims.com/>",cnonce="66334873",nc=00000001,qop=auth,uri="sip:sprout.ims.com:5052<http://sprout.ims.com:5052/>",nonce="0e07c1b77b566f37",response="788d4520717e4e7b29f7fab43fdc448f",algorithm=MD5,opaque="5171f001504c2c3a"
User-Agent: 00-00000-0000000000000 Phone IMS 10.0
P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
P-Visited-Network-ID: ims.com<http://ims.com/>

SIP/2.0 200 OK
Service-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com/>;transport=TCP;lr;orig;username=2010000039%40ims.com<http://40ims.com/>;nonce=0e07c1b77b566f37>
Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-1-2
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=z9hG4bKPjIvjh2DjwvU.vVNEv.nOiYAfsZRgMjHDF
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
Supported: outbound
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768/>>;expires=1800;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>";reg-id=1;pub-gruu="sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>;gr=urn:uuid:00000000-0000-0000-0000-000000000001"
Require: outbound
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Associated-URI: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Content-Length:  0


thanks in advance, Please resply.

cheers,
sunil


On Tue, Mar 20, 2018 at 6:53 PM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi,
It is using some other port on stress node not 5082. Is this a problem, if yes how can I fix this i have tried to open 5082 port on stress node using sudo ufw allow 5082/tcp, but no effect.
Please check the wireshark log:

Frame 2406: 703 bytes on wire (5624 bits), 703 bytes captured (5624 bits)
Ethernet II, Src: PcsCompu_ff:d2:88 (08:00:27:ff:d2:88), Dst: PcsCompu_ab:71:0f (08:00:27:ab:71:0f)
Internet Protocol Version 4, Src: 10.224.61.22, Dst: 10.224.61.13
Transmission Control Protocol, Src Port: rlm-admin (5054), Dst Port: 34312 (34312), Seq: 349, Ack: 2199, Len: 637
Session Initiation Protocol (503)


cheers,
sunil

On Tue, Mar 20, 2018 at 5:21 PM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi all,
I have taken tcpdump also but there is no SIP message. Please through some light on this problem. I am trying from last weak, not able to catch the problem. Thanks in advance.

cheers,
Sunil

On Tue, Mar 20, 2018 at 10:30 AM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi CW team,
Anyone out there please help me. I am facing problem in stress testing. I have installed CW manually. whenever I was running 1 or less than 20 it give some errors like:

[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com<http://ims.com/> 1 2
[sudo] password for ubuntu:
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
    with open(CALLER_STATS) as f:
IOError: [Errno 2] No such file or directory: '/var/log/clearwater-sip-stress/18065_caller_stats.log'


[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com<http://ims.com/> 10 5
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
    call_success_rate = 100 * float(row['SuccessfulCall(C)']) / float(row['TotalCallCreated'])
ZeroDivisionError: float division by zero


[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress iind.intel.com<http://iind.intel.com/> 50 5
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:03:41
Start: 2018-03-20 17:46:43.268136
End: 2018-03-20 17:51:31.363406

Total calls: 2
Successful calls: 0 (0.0%)
Failed calls: 2 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 8
Successful re-REGISTERs: 8 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 86.0ms

Log files at /var/log/clearwater-sip-stress/18566_*



[]ubuntu at stress:~$ cat /var/log/clearwater-sip-stress/18566_caller_errors.log
sipp: The following events occured:
2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on unexpected message for Call-Id '1-18576 at 127.0.1.1<mailto:1-18576 at 127.0.1.1>': while expecting '183' (index 2), received 'SIP/2.0 503 Service Unavailable
Via: SIP/2.0/TCP 127.0.1.1:42276;received=10.224.61.13;branch=z9hG4bK-18576-1-0
Record-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com/>;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout. ims.com<http://ims.com/> ;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 1-18576 at 127.0.1.1<mailto:1-18576 at 127.0.1.1>
From: <sip:2010000042@ ims.com<http://ims.com/> >;tag=18576SIPpTag001
To: <sip:2010000015@ ims.co<http://ims.co/>>;tag=z9hG4bKPj1Lm9whhQMslKrcZxnN6qCH0tb9Lj5Neu
CSeq: 1 INVITE
P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com<http://ims.com/> ;term-ioi= ims.com<http://ims.com/>
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0


[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=49294;received=10.224.61.22;branch=z9hG4bK-670172
Call-ID: poll-sip-670172
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670172
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670172
CSeq: 670172 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:284: Request latency = 254us
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217: Rate adjustment calculation inputs: err -0.981500, smoothed latency 185, target latency 10000
20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302: Maximum incoming request rate/second unchanged at 2000.000000 (current request rate is 0.200000 requests/sec, minimum threshold for a change is 1000.000000 requests/sec).
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054<http://10.224.61.22:5054/>: got incoming TCP connection from 10.224.61.22:42848<http://10.224.61.22:42848/>, sock=573
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 tcp->base.local_name: 10.224.61.22
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP server transport created
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=670180 (rdata0x7f34ec027690)
20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180 (rdata0x7f34ec027690) from TCP 10.224.61.22:42848<http://10.224.61.22:42848/>:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054<http://sip:poll-sip at 10.224.61.22:5054/> SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054<http://sip:poll-sip at 10.224.61.22:5054/>>
From: poll-sip <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670180
Call-ID: poll-sip-670180
CSeq: 670180 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8 for worker threads with priority 15
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:183: Request latency so far = 57us
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=670180 (rdata0x7f34ec34a3e8)
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848<http://10.224.61.22:42848/>:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=42848;received=10.224.61.22;branch=z9hG4bK-670180
Call-ID: poll-sip-670180
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670180
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670180
CSeq: 670180 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809 Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:284: Request latency = 129us
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 1).
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053<http://10.224.61.22:5053/>: got incoming TCP connection from 10.224.61.22:49356<http://10.224.61.22:49356/>, sock=573
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 tcp->base.local_name: 10.224.61.22
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP server transport created
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=670182 (rdata0x7f34ec027690)
20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182 (rdata0x7f34ec027690) from TCP 10.224.61.22:49356<http://10.224.61.22:49356/>:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053<http://sip:poll-sip at 10.224.61.22:5053/> SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053<http://sip:poll-sip at 10.224.61.22:5053/>>
From: poll-sip <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670182
Call-ID: poll-sip-670182
CSeq: 670182 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8 for worker threads with priority 15
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:183: Request latency so far = 102us
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=670182 (rdata0x7f34ec34a3e8)
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356<http://10.224.61.22:49356/>:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=49356;received=10.224.61.22;branch=z9hG4bK-670182
Call-ID: poll-sip-670182
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670182
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670182
CSeq: 670182 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00 Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:284: Request latency = 232us
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 2).
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1001.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1005.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1011.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1012.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1013.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1004.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1002.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1009.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1010.1 alarm
20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)



all calls are failing I don't know what is going on, I am newbie to CW please guide some solution it will be great help.


Thanks,
Sunil





_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180323/6cadbebf/attachment.html>

From Michael.Duppre at metaswitch.com  Fri Mar 23 06:44:01 2018
From: Michael.Duppre at metaswitch.com (=?utf-8?B?TWljaGFlbCBEdXBwcsOp?=)
Date: Fri, 23 Mar 2018 10:44:01 +0000
Subject: [Project Clearwater] Why the port 3868 is listened by homestead?
In-Reply-To: <CA+pBo5G2Ob_Jk6v9ot_=+AweiB3xL1QsVgZLVPcJR_dTvEe32g@mail.gmail.com>
References: <CA+pBo5G2Ob_Jk6v9ot_=+AweiB3xL1QsVgZLVPcJR_dTvEe32g@mail.gmail.com>
Message-ID: <SN1PR02MB1677D807C9E3EF00C60A51CAF5A80@SN1PR02MB1677.namprd02.prod.outlook.com>

Hi Anthony,

Thanks for getting in touch! Installing an HSS on the same VM is something we haven?t actively tested and don?t support at the moment. What?s happening is that both the homestead process and FHoSS are trying to listen on the same, default Diameter port 3868 and therefore are clashing. In general, we would advise to not install additional software on the same VM to avoid these kind of problems, so the cleanest (and easiest to troubleshoot) solution is to install FHoSS on a separate VM. You?ve probably already seen the documentation about setting up an external HSS, just in case you haven?t you can find instructions at http://clearwater.readthedocs.io/en/stable/External_HSS_Integration.html and http://clearwater.readthedocs.io/en/stable/OpenIMSCore_HSS_Integration.html.
Hope this helps and good luck getting the HSS up and running!

Kind regards,
Michael


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Anthony Lee
Sent: 22 March 2018 21:20
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Why the port 3868 is listened by homestead?

Hi,

I tried to integrate clearwater all-in-one with OpenIMS FHoSS hss,
I installed FHoSS in the same VM.
I have below settings in /etc/clearwater/shared_config:
hss_hostname=cw-aio
hss_port=3868

and then I run "sudo reboot"

I run "sudo netstat -tulpn | grep 3868" and the output shows that
the process listens on 3868 is homestead.

When I run a test I see there are some Diameter request in homestead log file.
But all get error response because it never talk to real Diameter peer.

I don't know what I did wrong.
The port supposed to be used by FHoSS but it is occupied by homestead. ???

Any idea?


Thanks
Anthony





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180323/3b77ee0c/attachment.html>

From Michael.Duppre at metaswitch.com  Fri Mar 23 13:56:12 2018
From: Michael.Duppre at metaswitch.com (=?utf-8?B?TWljaGFlbCBEdXBwcsOp?=)
Date: Fri, 23 Mar 2018 17:56:12 +0000
Subject: [Project Clearwater] CW team please help - stress testing
In-Reply-To: <C29325B9-84E3-49B8-96B4-044A1AB88CB8@redmatter.com>
References: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
	<CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>
	<CAHwYWpB4WqEVnG69B_BazQC6yJVHW7A3pMPOWmRoVX_gYJhPgw@mail.gmail.com>
	<CAHwYWpBH8qdZbAEuFoE61Y13s1s9QGku912aBb5w8XpX+ZmAkQ@mail.gmail.com>
	<SN1PR02MB16779E13A341C6567440B42CF5A90@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpCj9xAoiE=eLe7+GMCrX7RjPDgMHXVazVhX4evT8AbTUw@mail.gmail.com>
	<C29325B9-84E3-49B8-96B4-044A1AB88CB8@redmatter.com>
Message-ID: <SN1PR02MB1677A0FE6650898235E6F58FF5A80@SN1PR02MB1677.namprd02.prod.outlook.com>

Hi Sunil,

Sorry to hear that you?ve found our reply too slow to be helpful this time, but I?m glad to hear that you seem to have made some progress on your own. As Jim mentioned, our mailing list is `best effort` while we focus on development work to keep the quality of Clearwater as high as possible ? thanks, Jim, I?m happy to hear that you like our free service!

Regarding your questions:

  *   You?ve mentioned that you have hit other problems with stress tool, could you please head over to https://github.com/Metaswitch/project-clearwater-issues/issues and open up an issue with some specific diagnostics? That way we can track and prioritize the problem to get it fixed as soon as possible.
  *   It sounds like you now got some calls working, that?s good news! Could you please share your stress tool and sprout logs to pinpoint if this is a problem with the generation of load or with Clearwater handling the calls? Also, you?ve mentioned that you?ve made some changes to the tool, what exactly did you change? I?d just like to be sure that none of your changes are causing this behaviour.
  *   About the `multiplier` argument: Thanks for pointing out that our documentation is missing something useful here, we will update it. For now, if you take a look at the help text of the stress tool by running `/usr/share/clearwater/bin/run_stress --help` you can find the explanation:
     *   --multiplier MULTIPLIER: Multiplier for the VoLTE load profile (e.g. passing 2 here will mean 2.6 calls and 4 re-registers per subs per hour)

For more information about the VoLTE load profile you can have a look at the documentation for the stress tool at http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html, specifically:

     *   [The stress tool will] send traffic, using a fixed load profile of 1.3 calls/hour for each subscriber (split equally between incoming and outgoing calls) and 2 re-registrations per hour

Good luck with your next steps using Clearwater! Hope you won?t hit any other problems, but if you do, please let us know!

Kind regards,
Michael


From: Jim Page [mailto:jim.page at redmatter.com]
Sent: 23 March 2018 09:20
To: clearwater at lists.projectclearwater.org
Cc: Michael Duppr? <Michael.Duppre at metaswitch.com>
Subject: Re: [Project Clearwater] CW team please help - stress testing

Dude, this is a free service. I am sure if you buy some of their commercial licenses they will prioritise your request, but if you are on here you need to understand that service given here is ?best effort?, and in my view they perform a fantastic job. So calm down.

RedMatter Ltd
Jim Page
VP Mobile Services
+44 (0)333 150 1666
+44 (0)7870 361412
jim.page at redmatter.com<mailto:jim.page at redmatter.com>


On 23 Mar 2018, at 02:29, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:

Hi,
Thanks for replying, but you guys are replying very late, its not good, I have been waiting for your reply from last 3 days :-( .
Anyway, I thought the script hast other problem also, May be you will check it and fix it so that others wouldn't got that problem. Somehow I fix the problem, though it takes lot of of time to read the script and make some changes.

I want ask few questions and expecting reply within a day :-)
1. when I use 1000 subscriber and running for 10 min duration, only few call are successful (around 300) and no calls are failed. How can I increase no. of calls.
2. Can you explain the exact use of --multiplier parameter in detail. I request you to add all the parameter in doc itself so other would not get problem while finding.

Thanks,
Sunil


On Thu, Mar 22, 2018 at 8:09 PM, Michael Duppr? <Michael.Duppre at metaswitch.com<mailto:Michael.Duppre at metaswitch.com>> wrote:
Hello Sunil,

Sorry about the stress tool not working properly with a lower number of subscribers, that looks like a bug in the tool. I have raised issue https://github.com/Metaswitch/project-clearwater-issues/issues/30 to track and fix this problem, feel free to provide any other information on that ticket if you hit similar problems. Thanks for your help finding this bug!

Looks like you?ve done the right things and went back to a slightly higher number of subscribers (50) and looked at the stress log file and the sprout log file. Unfortunately it looks like you?ve copied out the wrong time period from the sprout logs: In your email below, the stress tool logs are from 17:48, however the sprout logs that you?ve sent are from 5 hours before at 12:27.
Similar for your tcpdump ? a good idea to have a look at this, but unfortunately what you?ve copied into your email is only the register flow, which is successful! :-)

You?re probably pretty close finding the reason why the calls are failing in the sprout logs, could you please make sure you have a look at the timestamp of the time you ran the stress tool?

Good luck and kind regards,
Michael


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Sunil Kumar
Sent: 20 March 2018 14:57
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Bennett Allen <Bennett.Allen at metaswitch.com<mailto:Bennett.Allen at metaswitch.com>>
Subject: Re: [Project Clearwater] CW team please help - stress testing

Hi,
I am facing problem in stress testing, Please look into the log. I am not able to debug the problem.

I have taken this from wireshark, actually i use tcpdump.

REGISTER sip:ims.com<http://ims.com/> SIP/2.0
Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-0
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
Content-Length: 0
Require: Path
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
Supported: path
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Route: <sip:icscf at sprout.ims.com<mailto:sip%3Aicscf at sprout.ims.com>;lr>
Max-Forwards: 70
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768/>>;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
CSeq: 1 REGISTER
Expires: 3600
Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY, MESSAGE, PRACK
Supported: path, gruu
Authorization: Digest username="2010000039 at ims.com<mailto:2010000039 at ims.com>",realm="ims.com<http://ims.com/>",uri="sip:ims.com<http://ims.com/>",nonce="",response="",algorithm=Digest-MD5
User-Agent: 00-00000-0000000000000 Phone IMS 10.0
P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
P-Visited-Network-ID: ims.com<http://ims.com/>

SIP/2.0 401 Unauthorized
Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-1-0
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=z9hG4bKPjDBaGZjqTrLQiDSlHihO36oMPm7fxz2sQ
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
WWW-Authenticate: Digest  realm="ims.com<http://ims.com/>",nonce="0e07c1b77b566f37",opaque="5171f001504c2c3a",algorithm=MD5,qop="auth"
Content-Length:  0

REGISTER sip:ims.com<http://ims.com/> SIP/2.0
Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-2
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
Content-Length: 0
Require: Path
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
Supported: path
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Route: <sip:icscf at sprout.ims.com<mailto:sip%3Aicscf at sprout.ims.com>;lr>
Max-Forwards: 70
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768/>>;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
CSeq: 1 REGISTER
Expires: 3600
Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY, MESSAGE, PRACK
Supported: path, gruu
Authorization: Digest username="2010000039 at ims.com<mailto:2010000039 at ims.com>",realm="ims.com<http://ims.com/>",cnonce="66334873",nc=00000001,qop=auth,uri="sip:sprout.ims.com:5052<http://sprout.ims.com:5052/>",nonce="0e07c1b77b566f37",response="788d4520717e4e7b29f7fab43fdc448f",algorithm=MD5,opaque="5171f001504c2c3a"
User-Agent: 00-00000-0000000000000 Phone IMS 10.0
P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
P-Visited-Network-ID: ims.com<http://ims.com/>

SIP/2.0 200 OK
Service-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com/>;transport=TCP;lr;orig;username=2010000039%40ims.com<http://40ims.com/>;nonce=0e07c1b77b566f37>
Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-1-2
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=z9hG4bKPjIvjh2DjwvU.vVNEv.nOiYAfsZRgMjHDF
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
Supported: outbound
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768/>>;expires=1800;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>";reg-id=1;pub-gruu="sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>;gr=urn:uuid:00000000-0000-0000-0000-000000000001"
Require: outbound
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Associated-URI: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Content-Length:  0


thanks in advance, Please resply.

cheers,
sunil


On Tue, Mar 20, 2018 at 6:53 PM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi,
It is using some other port on stress node not 5082. Is this a problem, if yes how can I fix this i have tried to open 5082 port on stress node using sudo ufw allow 5082/tcp, but no effect.
Please check the wireshark log:

Frame 2406: 703 bytes on wire (5624 bits), 703 bytes captured (5624 bits)
Ethernet II, Src: PcsCompu_ff:d2:88 (08:00:27:ff:d2:88), Dst: PcsCompu_ab:71:0f (08:00:27:ab:71:0f)
Internet Protocol Version 4, Src: 10.224.61.22, Dst: 10.224.61.13
Transmission Control Protocol, Src Port: rlm-admin (5054), Dst Port: 34312 (34312), Seq: 349, Ack: 2199, Len: 637
Session Initiation Protocol (503)


cheers,
sunil

On Tue, Mar 20, 2018 at 5:21 PM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi all,
I have taken tcpdump also but there is no SIP message. Please through some light on this problem. I am trying from last weak, not able to catch the problem. Thanks in advance.

cheers,
Sunil

On Tue, Mar 20, 2018 at 10:30 AM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi CW team,
Anyone out there please help me. I am facing problem in stress testing. I have installed CW manually. whenever I was running 1 or less than 20 it give some errors like:

[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com<http://ims.com/> 1 2
[sudo] password for ubuntu:
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
    with open(CALLER_STATS) as f:
IOError: [Errno 2] No such file or directory: '/var/log/clearwater-sip-stress/18065_caller_stats.log'


[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com<http://ims.com/> 10 5
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
    call_success_rate = 100 * float(row['SuccessfulCall(C)']) / float(row['TotalCallCreated'])
ZeroDivisionError: float division by zero


[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress iind.intel.com<http://iind.intel.com/> 50 5
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:03:41
Start: 2018-03-20 17:46:43.268136
End: 2018-03-20 17:51:31.363406

Total calls: 2
Successful calls: 0 (0.0%)
Failed calls: 2 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 8
Successful re-REGISTERs: 8 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 86.0ms

Log files at /var/log/clearwater-sip-stress/18566_*



[]ubuntu at stress:~$ cat /var/log/clearwater-sip-stress/18566_caller_errors.log
sipp: The following events occured:
2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on unexpected message for Call-Id '1-18576 at 127.0.1.1<mailto:1-18576 at 127.0.1.1>': while expecting '183' (index 2), received 'SIP/2.0 503 Service Unavailable
Via: SIP/2.0/TCP 127.0.1.1:42276;received=10.224.61.13;branch=z9hG4bK-18576-1-0
Record-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com/>;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout. ims.com<http://ims.com/> ;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 1-18576 at 127.0.1.1<mailto:1-18576 at 127.0.1.1>
From: <sip:2010000042@ ims.com<http://ims.com/> >;tag=18576SIPpTag001
To: <sip:2010000015@ ims.co<http://ims.co/>>;tag=z9hG4bKPj1Lm9whhQMslKrcZxnN6qCH0tb9Lj5Neu
CSeq: 1 INVITE
P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com<http://ims.com/> ;term-ioi= ims.com<http://ims.com/>
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0


[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=49294;received=10.224.61.22;branch=z9hG4bK-670172
Call-ID: poll-sip-670172
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670172
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670172
CSeq: 670172 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:284: Request latency = 254us
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217: Rate adjustment calculation inputs: err -0.981500, smoothed latency 185, target latency 10000
20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302: Maximum incoming request rate/second unchanged at 2000.000000 (current request rate is 0.200000 requests/sec, minimum threshold for a change is 1000.000000 requests/sec).
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054<http://10.224.61.22:5054/>: got incoming TCP connection from 10.224.61.22:42848<http://10.224.61.22:42848/>, sock=573
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 tcp->base.local_name: 10.224.61.22
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP server transport created
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=670180 (rdata0x7f34ec027690)
20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180 (rdata0x7f34ec027690) from TCP 10.224.61.22:42848<http://10.224.61.22:42848/>:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054<http://sip:poll-sip at 10.224.61.22:5054/> SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054<http://sip:poll-sip at 10.224.61.22:5054/>>
From: poll-sip <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670180
Call-ID: poll-sip-670180
CSeq: 670180 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8 for worker threads with priority 15
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:183: Request latency so far = 57us
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=670180 (rdata0x7f34ec34a3e8)
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848<http://10.224.61.22:42848/>:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=42848;received=10.224.61.22;branch=z9hG4bK-670180
Call-ID: poll-sip-670180
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670180
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670180
CSeq: 670180 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809 Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:284: Request latency = 129us
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 1).
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053<http://10.224.61.22:5053/>: got incoming TCP connection from 10.224.61.22:49356<http://10.224.61.22:49356/>, sock=573
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 tcp->base.local_name: 10.224.61.22
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP server transport created
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=670182 (rdata0x7f34ec027690)
20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182 (rdata0x7f34ec027690) from TCP 10.224.61.22:49356<http://10.224.61.22:49356/>:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053<http://sip:poll-sip at 10.224.61.22:5053/> SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053<http://sip:poll-sip at 10.224.61.22:5053/>>
From: poll-sip <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670182
Call-ID: poll-sip-670182
CSeq: 670182 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8 for worker threads with priority 15
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:183: Request latency so far = 102us
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=670182 (rdata0x7f34ec34a3e8)
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356<http://10.224.61.22:49356/>:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=49356;received=10.224.61.22;branch=z9hG4bK-670182
Call-ID: poll-sip-670182
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670182
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670182
CSeq: 670182 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00 Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:284: Request latency = 232us
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 2).
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1001.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1005.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1011.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1012.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1013.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1004.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1002.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1009.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1010.1 alarm
20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)



all calls are failing I don't know what is going on, I am newbie to CW please guide some solution it will be great help.


Thanks,
Sunil





_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180323/431a7d96/attachment.html>

From anthonynlee at gmail.com  Fri Mar 23 16:07:48 2018
From: anthonynlee at gmail.com (Anthony Lee)
Date: Fri, 23 Mar 2018 16:07:48 -0400
Subject: [Project Clearwater] Why the port 3868 is listened by homestead?
In-Reply-To: <SN1PR02MB1677D807C9E3EF00C60A51CAF5A80@SN1PR02MB1677.namprd02.prod.outlook.com>
References: <CA+pBo5G2Ob_Jk6v9ot_=+AweiB3xL1QsVgZLVPcJR_dTvEe32g@mail.gmail.com>
	<SN1PR02MB1677D807C9E3EF00C60A51CAF5A80@SN1PR02MB1677.namprd02.prod.outlook.com>
Message-ID: <CA+pBo5EkST1eSVJYdL2faKwbq=jhWgRD0aK1KODwq2kETg40pw@mail.gmail.com>

Hi Michael,

Thanks for the reply.

Now I'm running FHoSS in different machine, now I see below error messages
in FHoSS console:

Communicator: Expecting diameter version 1, received version -1
Communicator: Expecting diameter version 1, received version -13
Communicator: Expecting diameter version 1, received version -1
Communicator: Expecting diameter version 1, received version -3
Communicator: Expecting diameter version 1, received version 6

Any idea?



...




On Fri, Mar 23, 2018 at 6:44 AM, Michael Duppr? <
Michael.Duppre at metaswitch.com> wrote:

> Hi Anthony,
>
>
>
> Thanks for getting in touch! Installing an HSS on the same VM is something
> we haven?t actively tested and don?t support at the moment. What?s
> happening is that both the homestead process and FHoSS are trying to listen
> on the same, default Diameter port 3868 and therefore are clashing. In
> general, we would advise to not install additional software on the same VM
> to avoid these kind of problems, so the cleanest (and easiest to
> troubleshoot) solution is to install FHoSS on a separate VM. You?ve
> probably already seen the documentation about setting up an external HSS,
> just in case you haven?t you can find instructions at
> http://clearwater.readthedocs.io/en/stable/External_HSS_Integration.html
> and http://clearwater.readthedocs.io/en/stable/OpenIMSCore_HSS_
> Integration.html.
>
> Hope this helps and good luck getting the HSS up and running!
>
>
>
> Kind regards,
>
> Michael
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Anthony Lee
> *Sent:* 22 March 2018 21:20
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Why the port 3868 is listened by
> homestead?
>
>
>
> Hi,
>
>
>
> I tried to integrate clearwater all-in-one with OpenIMS FHoSS hss,
>
> I installed FHoSS in the same VM.
>
> I have below settings in /etc/clearwater/shared_config:
>
> hss_hostname=cw-aio
>
> hss_port=3868
>
>
>
> and then I run "sudo reboot"
>
>
>
> I run "sudo netstat -tulpn | grep 3868" and the output shows that
>
> the process listens on 3868 is homestead.
>
>
>
> When I run a test I see there are some Diameter request in homestead log
> file.
>
> But all get error response because it never talk to real Diameter peer.
>
>
>
> I don't know what I did wrong.
>
> The port supposed to be used by FHoSS but it is occupied by homestead. ???
>
>
>
> Any idea?
>
>
>
>
>
> Thanks
>
> Anthony
>
>
>
>
>
>
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180323/e5ad54f6/attachment.html>

From Robert.Day at metaswitch.com  Tue Mar 27 06:16:21 2018
From: Robert.Day at metaswitch.com (Robert Day)
Date: Tue, 27 Mar 2018 10:16:21 +0000
Subject: [Project Clearwater] Why the port 3868 is listened by homestead?
In-Reply-To: <CA+pBo5EkST1eSVJYdL2faKwbq=jhWgRD0aK1KODwq2kETg40pw@mail.gmail.com>
References: <CA+pBo5G2Ob_Jk6v9ot_=+AweiB3xL1QsVgZLVPcJR_dTvEe32g@mail.gmail.com>
	<SN1PR02MB1677D807C9E3EF00C60A51CAF5A80@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CA+pBo5EkST1eSVJYdL2faKwbq=jhWgRD0aK1KODwq2kETg40pw@mail.gmail.com>
Message-ID: <BY2PR02MB214968B4B0907EAAC838A17DF4AC0@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Anthony,

I?m not sure what?s happening here ? have you tried taking packet capture with tcpdump and looking at it in Wireshark, to confirm that FHoSS is receiving valid Diameter messages? There might be something else in your network sending bad traffic on port 3868.

Best,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Anthony Lee
Sent: 23 March 2018 20:08
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Why the port 3868 is listened by homestead?

Hi Michael,

Thanks for the reply.

Now I'm running FHoSS in different machine, now I see below error messages in FHoSS console:

Communicator: Expecting diameter version 1, received version -1
Communicator: Expecting diameter version 1, received version -13
Communicator: Expecting diameter version 1, received version -1
Communicator: Expecting diameter version 1, received version -3
Communicator: Expecting diameter version 1, received version 6

Any idea?



...




On Fri, Mar 23, 2018 at 6:44 AM, Michael Duppr? <Michael.Duppre at metaswitch.com<mailto:Michael.Duppre at metaswitch.com>> wrote:
Hi Anthony,

Thanks for getting in touch! Installing an HSS on the same VM is something we haven?t actively tested and don?t support at the moment. What?s happening is that both the homestead process and FHoSS are trying to listen on the same, default Diameter port 3868 and therefore are clashing. In general, we would advise to not install additional software on the same VM to avoid these kind of problems, so the cleanest (and easiest to troubleshoot) solution is to install FHoSS on a separate VM. You?ve probably already seen the documentation about setting up an external HSS, just in case you haven?t you can find instructions at http://clearwater.readthedocs.io/en/stable/External_HSS_Integration.html and http://clearwater.readthedocs.io/en/stable/OpenIMSCore_HSS_Integration.html.
Hope this helps and good luck getting the HSS up and running!

Kind regards,
Michael


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Anthony Lee
Sent: 22 March 2018 21:20
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Why the port 3868 is listened by homestead?

Hi,

I tried to integrate clearwater all-in-one with OpenIMS FHoSS hss,
I installed FHoSS in the same VM.
I have below settings in /etc/clearwater/shared_config:
hss_hostname=cw-aio
hss_port=3868

and then I run "sudo reboot"

I run "sudo netstat -tulpn | grep 3868" and the output shows that
the process listens on 3868 is homestead.

When I run a test I see there are some Diameter request in homestead log file.
But all get error response because it never talk to real Diameter peer.

I don't know what I did wrong.
The port supposed to be used by FHoSS but it is occupied by homestead. ???

Any idea?


Thanks
Anthony






_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180327/7c57d71b/attachment.html>

From Robert.Day at metaswitch.com  Tue Mar 27 07:45:42 2018
From: Robert.Day at metaswitch.com (Robert Day)
Date: Tue, 27 Mar 2018 11:45:42 +0000
Subject: [Project Clearwater] CW team please help - stress testing
In-Reply-To: <CAHwYWpDCMqp5yXafz-3zuzHQJbKNFBvfzJhYbofBGc2dUqKiyA@mail.gmail.com>
References: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
	<CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>
	<CAHwYWpB4WqEVnG69B_BazQC6yJVHW7A3pMPOWmRoVX_gYJhPgw@mail.gmail.com>
	<CAHwYWpBH8qdZbAEuFoE61Y13s1s9QGku912aBb5w8XpX+ZmAkQ@mail.gmail.com>
	<SN1PR02MB16779E13A341C6567440B42CF5A90@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpCj9xAoiE=eLe7+GMCrX7RjPDgMHXVazVhX4evT8AbTUw@mail.gmail.com>
	<C29325B9-84E3-49B8-96B4-044A1AB88CB8@redmatter.com>
	<SN1PR02MB1677A0FE6650898235E6F58FF5A80@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpDCMqp5yXafz-3zuzHQJbKNFBvfzJhYbofBGc2dUqKiyA@mail.gmail.com>
Message-ID: <BY2PR02MB21491BAF112161741FC5D912F4AC0@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Sunil,

Are there any ?Error?-level logs in /var/log/sprout or /var/log/homestead on the VMs? That might suggest what?s causing the 503 Service Unavailable error.

Best,
Rob

From: Sunil Kumar [mailto:skgola1997 at gmail.com]
Sent: 26 March 2018 12:28
To: clearwater at lists.projectclearwater.org; Michael Duppr? <Michael.Duppre at metaswitch.com>
Subject: Re: [Project Clearwater] CW team please help - stress testing

Hi Michael,
I am not able to make call got 503 service unavailable. can you please check and guide me some quick solution

[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress ims.com 100 2
Starting initial registration, will take 1 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:01:50
Start: 2018-03-26 23:18:41.605000
End: 2018-03-26 23:20:32.453238

Total calls: 2
Successful calls: 0 (0.0%)
Failed calls: 2 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 6
Successful re-REGISTERs: 6 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 59.0ms

Log files at /var/log/clearwater-sip-stress/12890_*






[]ubuntu at stress:~$ cat /var/log/clearwater-sip-stress/12890_caller_errors.log
sipp: The following events occured:
2018-03-26      23:19:37.070794 1522086577.070794: Aborting call on unexpected message for Call-Id '1-12900 at 127.0.1.1<mailto:1-12900 at 127.0.1.1>': while expecting '183' (index 2), received 'SIP/2.0 503 Service Unavailable
Via: SIP/2.0/TCP 127.0.1.1:34112;received=10.224.61.34;branch=z9hG4bK-12900-1-0
Record-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com>;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com>;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 1-12900 at 127.0.1.1<mailto:1-12900 at 127.0.1.1>
From: <sip:2010000002 at ims.com<mailto:sip%3A2010000002 at ims.com>>;tag=12900SIPpTag001
To: <sip:2010000041@ ims .com<sip:2010000041@%20ims%20.com>>;tag=z9hG4bKPjRhUbQK8NJ5Blp4qIYX7f1YWhM2FQZe2M
CSeq: 1 INVITE
P-Charging-Vector: icid-value="12900SIPpTag001";orig-ioi= ims.com;term-ioi= ims.com
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0


PFA the sprout log

thanks,
 sunil



On Fri, Mar 23, 2018 at 11:26 PM, Michael Duppr? <Michael.Duppre at metaswitch.com<mailto:Michael.Duppre at metaswitch.com>> wrote:
Hi Sunil,

Sorry to hear that you?ve found our reply too slow to be helpful this time, but I?m glad to hear that you seem to have made some progress on your own. As Jim mentioned, our mailing list is `best effort` while we focus on development work to keep the quality of Clearwater as high as possible ? thanks, Jim, I?m happy to hear that you like our free service!

Regarding your questions:
?  You?ve mentioned that you have hit other problems with stress tool, could you please head over to https://github.com/Metaswitch/project-clearwater-issues/issues and open up an issue with some specific diagnostics? That way we can track and prioritize the problem to get it fixed as soon as possible.
?  It sounds like you now got some calls working, that?s good news! Could you please share your stress tool and sprout logs to pinpoint if this is a problem with the generation of load or with Clearwater handling the calls? Also, you?ve mentioned that you?ve made some changes to the tool, what exactly did you change? I?d just like to be sure that none of your changes are causing this behaviour.
?  About the `multiplier` argument: Thanks for pointing out that our documentation is missing something useful here, we will update it. For now, if you take a look at the help text of the stress tool by running `/usr/share/clearwater/bin/run_stress --help` you can find the explanation:
o --multiplier MULTIPLIER: Multiplier for the VoLTE load profile (e.g. passing 2 here will mean 2.6 calls and 4 re-registers per subs per hour)

For more information about the VoLTE load profile you can have a look at the documentation for the stress tool at http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html, specifically:
o [The stress tool will] send traffic, using a fixed load profile of 1.3 calls/hour for each subscriber (split equally between incoming and outgoing calls) and 2 re-registrations per hour

Good luck with your next steps using Clearwater! Hope you won?t hit any other problems, but if you do, please let us know!

Kind regards,
Michael


From: Jim Page [mailto:jim.page at redmatter.com<mailto:jim.page at redmatter.com>]
Sent: 23 March 2018 09:20
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Michael Duppr? <Michael.Duppre at metaswitch.com<mailto:Michael.Duppre at metaswitch.com>>

Subject: Re: [Project Clearwater] CW team please help - stress testing

Dude, this is a free service. I am sure if you buy some of their commercial licenses they will prioritise your request, but if you are on here you need to understand that service given here is ?best effort?, and in my view they perform a fantastic job. So calm down.

RedMatter Ltd
Jim Page
VP Mobile Services
+44 (0)333 150 1666
+44 (0)7870 361412
jim.page at redmatter.com<mailto:jim.page at redmatter.com>

On 23 Mar 2018, at 02:29, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:

Hi,
Thanks for replying, but you guys are replying very late, its not good, I have been waiting for your reply from last 3 days :-( .
Anyway, I thought the script hast other problem also, May be you will check it and fix it so that others wouldn't got that problem. Somehow I fix the problem, though it takes lot of of time to read the script and make some changes.

I want ask few questions and expecting reply within a day :-)
1. when I use 1000 subscriber and running for 10 min duration, only few call are successful (around 300) and no calls are failed. How can I increase no. of calls.
2. Can you explain the exact use of --multiplier parameter in detail. I request you to add all the parameter in doc itself so other would not get problem while finding.

Thanks,
Sunil


On Thu, Mar 22, 2018 at 8:09 PM, Michael Duppr? <Michael.Duppre at metaswitch.com<mailto:Michael.Duppre at metaswitch.com>> wrote:
Hello Sunil,

Sorry about the stress tool not working properly with a lower number of subscribers, that looks like a bug in the tool. I have raised issue https://github.com/Metaswitch/project-clearwater-issues/issues/30 to track and fix this problem, feel free to provide any other information on that ticket if you hit similar problems. Thanks for your help finding this bug!

Looks like you?ve done the right things and went back to a slightly higher number of subscribers (50) and looked at the stress log file and the sprout log file. Unfortunately it looks like you?ve copied out the wrong time period from the sprout logs: In your email below, the stress tool logs are from 17:48, however the sprout logs that you?ve sent are from 5 hours before at 12:27.
Similar for your tcpdump ? a good idea to have a look at this, but unfortunately what you?ve copied into your email is only the register flow, which is successful! :-)

You?re probably pretty close finding the reason why the calls are failing in the sprout logs, could you please make sure you have a look at the timestamp of the time you ran the stress tool?

Good luck and kind regards,
Michael


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Sunil Kumar
Sent: 20 March 2018 14:57
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Bennett Allen <Bennett.Allen at metaswitch.com<mailto:Bennett.Allen at metaswitch.com>>
Subject: Re: [Project Clearwater] CW team please help - stress testing

Hi,
I am facing problem in stress testing, Please look into the log. I am not able to debug the problem.

I have taken this from wireshark, actually i use tcpdump.

REGISTER sip:ims.com<http://ims.com/> SIP/2.0
Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-0
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
Content-Length: 0
Require: Path
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
Supported: path
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Route: <sip:icscf at sprout.ims.com<mailto:sip%3Aicscf at sprout.ims.com>;lr>
Max-Forwards: 70
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768/>>;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
CSeq: 1 REGISTER
Expires: 3600
Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY, MESSAGE, PRACK
Supported: path, gruu
Authorization: Digest username="2010000039 at ims.com<mailto:2010000039 at ims.com>",realm="ims.com<http://ims.com/>",uri="sip:ims.com<http://ims.com/>",nonce="",response="",algorithm=Digest-MD5
User-Agent: 00-00000-0000000000000 Phone IMS 10.0
P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
P-Visited-Network-ID: ims.com<http://ims.com/>

SIP/2.0 401 Unauthorized
Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-1-0
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=z9hG4bKPjDBaGZjqTrLQiDSlHihO36oMPm7fxz2sQ
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
WWW-Authenticate: Digest  realm="ims.com<http://ims.com/>",nonce="0e07c1b77b566f37",opaque="5171f001504c2c3a",algorithm=MD5,qop="auth"
Content-Length:  0

REGISTER sip:ims.com<http://ims.com/> SIP/2.0
Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-2
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
Content-Length: 0
Require: Path
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
Supported: path
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Route: <sip:icscf at sprout.ims.com<mailto:sip%3Aicscf at sprout.ims.com>;lr>
Max-Forwards: 70
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768/>>;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
CSeq: 1 REGISTER
Expires: 3600
Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY, MESSAGE, PRACK
Supported: path, gruu
Authorization: Digest username="2010000039 at ims.com<mailto:2010000039 at ims.com>",realm="ims.com<http://ims.com/>",cnonce="66334873",nc=00000001,qop=auth,uri="sip:sprout.ims.com:5052<http://sprout.ims.com:5052/>",nonce="0e07c1b77b566f37",response="788d4520717e4e7b29f7fab43fdc448f",algorithm=MD5,opaque="5171f001504c2c3a"
User-Agent: 00-00000-0000000000000 Phone IMS 10.0
P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
P-Visited-Network-ID: ims.com<http://ims.com/>

SIP/2.0 200 OK
Service-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com/>;transport=TCP;lr;orig;username=2010000039%40ims.com<http://40ims.com/>;nonce=0e07c1b77b566f37>
Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-1-2
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=z9hG4bKPjIvjh2DjwvU.vVNEv.nOiYAfsZRgMjHDF
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
Supported: outbound
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768/>>;expires=1800;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>";reg-id=1;pub-gruu="sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>;gr=urn:uuid:00000000-0000-0000-0000-000000000001"
Require: outbound
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Associated-URI: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Content-Length:  0


thanks in advance, Please resply.

cheers,
sunil


On Tue, Mar 20, 2018 at 6:53 PM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi,
It is using some other port on stress node not 5082. Is this a problem, if yes how can I fix this i have tried to open 5082 port on stress node using sudo ufw allow 5082/tcp, but no effect.
Please check the wireshark log:

Frame 2406: 703 bytes on wire (5624 bits), 703 bytes captured (5624 bits)
Ethernet II, Src: PcsCompu_ff:d2:88 (08:00:27:ff:d2:88), Dst: PcsCompu_ab:71:0f (08:00:27:ab:71:0f)
Internet Protocol Version 4, Src: 10.224.61.22, Dst: 10.224.61.13
Transmission Control Protocol, Src Port: rlm-admin (5054), Dst Port: 34312 (34312), Seq: 349, Ack: 2199, Len: 637
Session Initiation Protocol (503)


cheers,
sunil

On Tue, Mar 20, 2018 at 5:21 PM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi all,
I have taken tcpdump also but there is no SIP message. Please through some light on this problem. I am trying from last weak, not able to catch the problem. Thanks in advance.

cheers,
Sunil

On Tue, Mar 20, 2018 at 10:30 AM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi CW team,
Anyone out there please help me. I am facing problem in stress testing. I have installed CW manually. whenever I was running 1 or less than 20 it give some errors like:

[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com<http://ims.com/> 1 2
[sudo] password for ubuntu:
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
    with open(CALLER_STATS) as f:
IOError: [Errno 2] No such file or directory: '/var/log/clearwater-sip-stress/18065_caller_stats.log'


[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com<http://ims.com/> 10 5
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
    call_success_rate = 100 * float(row['SuccessfulCall(C)']) / float(row['TotalCallCreated'])
ZeroDivisionError: float division by zero


[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress iind.intel.com<http://iind.intel.com/> 50 5
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:03:41
Start: 2018-03-20 17:46:43.268136
End: 2018-03-20 17:51:31.363406

Total calls: 2
Successful calls: 0 (0.0%)
Failed calls: 2 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 8
Successful re-REGISTERs: 8 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 86.0ms

Log files at /var/log/clearwater-sip-stress/18566_*



[]ubuntu at stress:~$ cat /var/log/clearwater-sip-stress/18566_caller_errors.log
sipp: The following events occured:
2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on unexpected message for Call-Id '1-18576 at 127.0.1.1<mailto:1-18576 at 127.0.1.1>': while expecting '183' (index 2), received 'SIP/2.0 503 Service Unavailable
Via: SIP/2.0/TCP 127.0.1.1:42276;received=10.224.61.13;branch=z9hG4bK-18576-1-0
Record-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com/>;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout. ims.com<http://ims.com/> ;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 1-18576 at 127.0.1.1<mailto:1-18576 at 127.0.1.1>
From: <sip:2010000042@ ims.com<http://ims.com/> >;tag=18576SIPpTag001
To: <sip:2010000015@ ims.co<http://ims.co/>>;tag=z9hG4bKPj1Lm9whhQMslKrcZxnN6qCH0tb9Lj5Neu
CSeq: 1 INVITE
P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com<http://ims.com/> ;term-ioi= ims.com<http://ims.com/>
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0


[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=49294;received=10.224.61.22;branch=z9hG4bK-670172
Call-ID: poll-sip-670172
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670172
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670172
CSeq: 670172 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:284: Request latency = 254us
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217: Rate adjustment calculation inputs: err -0.981500, smoothed latency 185, target latency 10000
20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302: Maximum incoming request rate/second unchanged at 2000.000000 (current request rate is 0.200000 requests/sec, minimum threshold for a change is 1000.000000 requests/sec).
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054<http://10.224.61.22:5054/>: got incoming TCP connection from 10.224.61.22:42848<http://10.224.61.22:42848/>, sock=573
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 tcp->base.local_name: 10.224.61.22
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP server transport created
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=670180 (rdata0x7f34ec027690)
20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180 (rdata0x7f34ec027690) from TCP 10.224.61.22:42848<http://10.224.61.22:42848/>:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054<http://sip:poll-sip at 10.224.61.22:5054/> SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054<http://sip:poll-sip at 10.224.61.22:5054/>>
From: poll-sip <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670180
Call-ID: poll-sip-670180
CSeq: 670180 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8 for worker threads with priority 15
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:183: Request latency so far = 57us
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=670180 (rdata0x7f34ec34a3e8)
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848<http://10.224.61.22:42848/>:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=42848;received=10.224.61.22;branch=z9hG4bK-670180
Call-ID: poll-sip-670180
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670180
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670180
CSeq: 670180 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809 Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:284: Request latency = 129us
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 1).
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053<http://10.224.61.22:5053/>: got incoming TCP connection from 10.224.61.22:49356<http://10.224.61.22:49356/>, sock=573
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 tcp->base.local_name: 10.224.61.22
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP server transport created
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=670182 (rdata0x7f34ec027690)
20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182 (rdata0x7f34ec027690) from TCP 10.224.61.22:49356<http://10.224.61.22:49356/>:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053<http://sip:poll-sip at 10.224.61.22:5053/> SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053<http://sip:poll-sip at 10.224.61.22:5053/>>
From: poll-sip <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670182
Call-ID: poll-sip-670182
CSeq: 670182 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8 for worker threads with priority 15
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:183: Request latency so far = 102us
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=670182 (rdata0x7f34ec34a3e8)
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356<http://10.224.61.22:49356/>:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=49356;received=10.224.61.22;branch=z9hG4bK-670182
Call-ID: poll-sip-670182
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670182
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670182
CSeq: 670182 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00 Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:284: Request latency = 232us
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 2).
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1001.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1005.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1011.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1012.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1013.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1004.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1002.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1009.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1010.1 alarm
20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)



all calls are failing I don't know what is going on, I am newbie to CW please guide some solution it will be great help.


Thanks,
Sunil





_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180327/9eb3e156/attachment.html>

From Robert.Day at metaswitch.com  Tue Mar 27 10:46:13 2018
From: Robert.Day at metaswitch.com (Robert Day)
Date: Tue, 27 Mar 2018 14:46:13 +0000
Subject: [Project Clearwater] Clearwater, ellis freezing
In-Reply-To: <CAJpo2SrM2rViuhGy86MHv9sZ-QVXjS2J89KZzGbeh=UZ0e_1AQ@mail.gmail.com>
References: <CAJpo2SrM2rViuhGy86MHv9sZ-QVXjS2J89KZzGbeh=UZ0e_1AQ@mail.gmail.com>
Message-ID: <BY2PR02MB2149E0B6776DB17C397F44CDF4AC0@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi D?vid,

?19-03-2018 08:42:49.839 UTC Error main.cpp:1016: Failed to initialize the Cassandra store with error$? suggests the error is in Cassandra. Is there anything in /var/log/cassandra/system.log?

Best,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of D?vid Kyselica
Sent: 19 March 2018 08:56
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Clearwater, ellis freezing

Hi,
I`m new to clearwater project. I installed all-in-one AMI into amazon ec2. After trying to access web interface, I was asked by web browser to reload the page by this message:Failed to update the server (see detailed diagnostics in developer console). Please refresh the page.
I`m really confused what it can by caused by. I will be thankful for any type of tips. It`s a fresh installation with no important configuration changes.

log file of homestead:
19-03-2018 08:42:49.718 UTC Status http_connection_pool.cpp:35: Connection pool will use calculated $
19-03-2018 08:42:49.837 UTC Status httpconnection.h:58: Configuring HTTP Connection
19-03-2018 08:42:49.837 UTC Status httpconnection.h:59:   Connection created for server ec2-52-91-24$
19-03-2018 08:42:49.837 UTC Status main.cpp:973: No HSS configured - using Homestead-prov
19-03-2018 08:42:49.837 UTC Status a_record_resolver.cpp:29: Created ARecordResolver
19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:266: Configuring store connection
19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:267:   Hostname:  127.0.0.1
19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:268:   Port:      9160
19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:296: Configuring store worker pool
19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:297:   Threads:   10
19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:298:   Max Queue: 0
19-03-2018 08:42:49.839 UTC Error main.cpp:1016: Failed to initialize the Cassandra store with error$
19-03-2018 08:42:49.839 UTC Status main.cpp:1017: Homestead is shutting down

log file of ellis:
19-03-2018 08:31:30.662 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.64ms
19-03-2018 08:31:46.424 UTC INFO web.py:1447: 200 GET / (0.0.0.0) 616.39ms
19-03-2018 08:31:46.753 UTC INFO web.py:1447: 200 GET /css/bootstrap.min.css (0.0.0.0) 86.74ms
19-03-2018 08:31:46.793 UTC INFO web.py:1447: 200 GET /css/bootstrap-responsive.css (0.0.0.0) 39.08ms
19-03-2018 08:31:46.834 UTC INFO web.py:1447: 200 GET /css/style.css (0.0.0.0) 1.16ms
19-03-2018 08:31:46.835 UTC INFO web.py:1447: 200 GET /css/jquery.miniColors.css (0.0.0.0) 1.03ms
19-03-2018 08:31:46.836 UTC INFO web.py:1447: 200 GET /css/fileuploader.css (0.0.0.0) 0.94ms
19-03-2018 08:31:47.695 UTC INFO web.py:1447: 200 GET /js/jquery.js (0.0.0.0) 573.28ms
19-03-2018 08:31:47.740 UTC INFO web.py:1447: 200 GET /js/jquery.ba-bbq.min.js (0.0.0.0) 42.90ms
19-03-2018 08:31:48.104 UTC INFO web.py:1447: 200 GET /js/fileuploader.js (0.0.0.0) 364.63ms
19-03-2018 08:31:48.106 UTC INFO web.py:1447: 200 GET /js/jquery.miniColors.min.js (0.0.0.0) 1.58ms
19-03-2018 08:31:48.107 UTC INFO web.py:1447: 200 GET /js/jquery.cookie.js (0.0.0.0) 1.02ms
19-03-2018 08:31:48.149 UTC INFO web.py:1447: 200 GET /js/jquery.total-storage.min.js (0.0.0.0) 41.65ms
19-03-2018 08:31:48.314 UTC INFO web.py:1447: 200 GET /js/bootstrap.min.js (0.0.0.0) 2.34ms
19-03-2018 08:31:48.354 UTC INFO web.py:1447: 200 GET /js/common.js (0.0.0.0) 1.91ms
19-03-2018 08:31:48.561 UTC INFO web.py:1447: 200 GET /js/app.js (0.0.0.0) 168.24ms
19-03-2018 08:31:49.012 UTC WARNING web.py:1447: 404 GET /images/favicon.ico (0.0.0.0) 81.57ms
19-03-2018 08:31:50.286 UTC INFO web.py:1447: 200 GET /accounts/david%40hmz.sk/numbers/?cb=2b0a26dea4K0<http://40hmz.sk/numbers/?cb=2b0a26dea4K0> (0.0.0.0) 40.51ms
19-03-2018 08:32:20.995 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.21ms
19-03-2018 08:32:35.655 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 162.92ms
19-03-2018 08:33:21.845 UTC INFO web.py:1447: 200 GET /addressbook.html (0.0.0.0) 329.69ms
19-03-2018 08:33:22.052 UTC INFO web.py:1447: 200 GET /css/addressbook.css (0.0.0.0) 1.18ms
19-03-2018 08:33:22.709 UTC INFO web.py:1447: 200 GET /js/backbone.js (0.0.0.0) 455.22ms
19-03-2018 08:33:22.752 UTC INFO web.py:1447: 200 GET /js/underscore.js (0.0.0.0) 43.11ms
19-03-2018 08:33:22.874 UTC INFO web.py:1447: 200 GET /js/addressbook.js (0.0.0.0) 43.49ms
19-03-2018 08:33:23.121 UTC INFO web.py:1447: 200 GET /js/templates/addressbook-contacts.html (0.0.0.0) 2.48ms
19-03-2018 08:33:23.405 UTC INFO web.py:1447: 200 GET /gab (0.0.0.0) 2.09ms
19-03-2018 08:33:27.915 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.18ms


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180327/fef91f2b/attachment.html>

From skgola1997 at gmail.com  Tue Mar 27 14:19:05 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Tue, 27 Mar 2018 23:49:05 +0530
Subject: [Project Clearwater] Increasing the no. of nodes
Message-ID: <CAHwYWpAOhwWn+WSS6pTpu=4S9=Y5zvz+rD8gZ82cLPW_2ph+Pg@mail.gmail.com>

Hi all,
I have manually installed clearwater with single node. Now i want to
increase the no. of nodes for sprout and for others also in the same setup,
can you please guide me where i have to make changes. Is it possible or not?


Thanks
sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180327/b2ae3f70/attachment.html>

From skgola1997 at gmail.com  Tue Mar 27 14:42:17 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Wed, 28 Mar 2018 00:12:17 +0530
Subject: [Project Clearwater] CW team please help - stress testing
In-Reply-To: <CAHwYWpBPa94n1we8-B=8JEpFKd6uXZ-i=RNTsG_CsW9HC5pAYQ@mail.gmail.com>
References: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
	<CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>
	<CAHwYWpB4WqEVnG69B_BazQC6yJVHW7A3pMPOWmRoVX_gYJhPgw@mail.gmail.com>
	<CAHwYWpBH8qdZbAEuFoE61Y13s1s9QGku912aBb5w8XpX+ZmAkQ@mail.gmail.com>
	<SN1PR02MB16779E13A341C6567440B42CF5A90@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpCj9xAoiE=eLe7+GMCrX7RjPDgMHXVazVhX4evT8AbTUw@mail.gmail.com>
	<C29325B9-84E3-49B8-96B4-044A1AB88CB8@redmatter.com>
	<SN1PR02MB1677A0FE6650898235E6F58FF5A80@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpDCMqp5yXafz-3zuzHQJbKNFBvfzJhYbofBGc2dUqKiyA@mail.gmail.com>
	<BY2PR02MB21491BAF112161741FC5D912F4AC0@BY2PR02MB2149.namprd02.prod.outlook.com>
	<CAHwYWpBPa94n1we8-B=8JEpFKd6uXZ-i=RNTsG_CsW9HC5pAYQ@mail.gmail.com>
Message-ID: <CAHwYWpDfkVkJvRkHThf=KmMMKb7xY4XsNBunOdqkEV+FAGTUfw@mail.gmail.com>

Hi,
I got errors while stress testing. I used 40000 subscribers for 30 min
duration but it is running completing, what is the reasons behind this, is
there some bug in script itself or other problem:

*[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress ims.com
<http://ims.com> 40000 30 --multiplier=16*
Starting initial registration, will take 500 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:02:06
Start: 2018-03-28 06:18:00.636804
End: 2018-03-28 06:20:07.829051

Total calls: 6989
Successful calls: 1391 (19.9027042495%)
Failed calls: 3953 (56.5603090571%)
Unfinished calls: 1645

Retransmissions: 0

Average time from INVITE to 180 Ringing: 15092.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 3008 (43.0390613822%)
*Failed: call success rate 19.9027042495% is lower than target 100.0%!*
*Traceback (most recent call last):*
*  File "/usr/share/clearwater/bin/run_stress", line 443, in <module>*
*    reg_success_rate = 100 * float(row['SuccessfulCall(C)']) /
float(row['TotalCallCreated'])*
*ZeroDivisionError: float division by zero*



please provide some solution. I have created 50K subscribers in vellum.
Using single node for sprout and for other also.

How can i increase the no. of nodes for sprout and for other also?


Thanks


On Tue, Mar 27, 2018 at 5:26 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Thanks Robert for replying,
>
> Yes, the file is there but it is empty..
>
> [sprout]ubuntu at sprout:/var/log/sprout$ ls
> access_20180326T180000Z.txt  access_20180327T000000Z.txt
> access_20180327T060000Z.txt  access_20180327T120000Z.txt
> access_20180327T180000Z.txt  sprout_20180327T150000Z.txt  *sprout_err.log*
> access_20180326T190000Z.txt  access_20180327T010000Z.txt
> access_20180327T070000Z.txt  access_20180327T130000Z.txt
> access_20180327T190000Z.txt  sprout_20180327T160000Z.txt  sprout_out.log
> access_20180326T200000Z.txt  access_20180327T020000Z.txt
> access_20180327T080000Z.txt  access_20180327T140000Z.txt
> access_current.txt           sprout_20180327T170000Z.txt
> access_20180326T210000Z.txt  access_20180327T030000Z.txt
> access_20180327T090000Z.txt  access_20180327T150000Z.txt  analytics.log
>             sprout_20180327T180000Z.txt
> access_20180326T220000Z.txt  access_20180327T040000Z.txt
> access_20180327T100000Z.txt  access_20180327T160000Z.txt
> sprout_20180327T130000Z.txt  sprout_20180327T190000Z.txt
> access_20180326T230000Z.txt  access_20180327T050000Z.txt
> access_20180327T110000Z.txt  access_20180327T170000Z.txt
> sprout_20180327T140000Z.txt  sprout_current.txt
>
> *[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_err.log*
> [sprout]ubuntu at sprout:/var/log/sprout$
>
> Regards,
> Sunil
>
>
> On Tue, Mar 27, 2018 at 5:15 PM, Robert Day <Robert.Day at metaswitch.com>
> wrote:
>
>> Hi Sunil,
>>
>>
>>
>> Are there any ?Error?-level logs in /var/log/sprout or /var/log/homestead
>> on the VMs? That might suggest what?s causing the 503 Service Unavailable
>> error.
>>
>>
>>
>> Best,
>>
>> Rob
>>
>>
>>
>> *From:* Sunil Kumar [mailto:skgola1997 at gmail.com]
>> *Sent:* 26 March 2018 12:28
>> *To:* clearwater at lists.projectclearwater.org; Michael Duppr? <
>> Michael.Duppre at metaswitch.com>
>> *Subject:* Re: [Project Clearwater] CW team please help - stress testing
>>
>>
>>
>> Hi Michael,
>>
>> I am not able to make call got 503 service unavailable. can you please
>> check and guide me some quick solution
>>
>>
>>
>> []ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress ims.com 100 2
>>
>> Starting initial registration, will take 1 seconds
>>
>> Initial registration succeeded
>>
>> Starting test
>>
>> Test complete
>>
>>
>>
>> Elapsed time: 00:01:50
>>
>> Start: 2018-03-26 23:18:41.605000
>>
>> End: 2018-03-26 23:20:32.453238
>>
>>
>>
>> Total calls: 2
>>
>> Successful calls: 0 (0.0%)
>>
>> Failed calls: 2 (100.0%)
>>
>> Unfinished calls: 0
>>
>>
>>
>> Retransmissions: 0
>>
>>
>>
>> Average time from INVITE to 180 Ringing: 0.0ms
>>
>> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> Failed: call success rate 0.0% is lower than target 100.0%!
>>
>>
>>
>> Total re-REGISTERs: 6
>>
>> Successful re-REGISTERs: 6 (100.0%)
>>
>> Failed re-REGISTERS: 0 (0.0%)
>>
>>
>>
>> REGISTER retransmissions: 0
>>
>>
>>
>> Average time from REGISTER to 200 OK: 59.0ms
>>
>>
>>
>> Log files at /var/log/clearwater-sip-stress/12890_*
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> []ubuntu at stress:~$ cat /var/log/clearwater-sip-stress
>> /12890_caller_errors.log
>>
>> sipp: The following events occured:
>>
>> 2018-03-26      23:19:37.070794 1522086577.070794: Aborting call on
>> unexpected message for Call-Id '1-12900 at 127.0.1.1': while expecting
>> '183' (index 2), received 'SIP/2.0 503 Service Unavailable
>>
>> Via: SIP/2.0/TCP 127.0.1.1:34112;received=10.22
>> 4.61.34;branch=z9hG4bK-12900-1-0
>>
>> Record-Route: <sip:scscf.sprout.ims.com;tran
>> sport=TCP;lr;billing-role=charge-term>
>>
>> Record-Route: <sip:scscf.sprout.ims.com;tran
>> sport=TCP;lr;billing-role=charge-orig>
>>
>> Call-ID: 1-12900 at 127.0.1.1
>>
>> From: <sip:2010000002 at ims.com>;tag=12900SIPpTag001
>>
>> To: <sip:2010000041@ ims .com>;tag=z9hG4bKPjRhUbQK8NJ5B
>> lp4qIYX7f1YWhM2FQZe2M
>>
>> CSeq: 1 INVITE
>>
>> P-Charging-Vector: icid-value="12900SIPpTag001";orig-ioi= ims.com;term-ioi=
>> ims.com
>>
>> P-Charging-Function-Addresses: ccf=0.0.0.0
>>
>> Content-Length:  0
>>
>>
>>
>>
>>
>> PFA the sprout log
>>
>>
>>
>> thanks,
>>
>>  sunil
>>
>>
>>
>>
>>
>>
>>
>> On Fri, Mar 23, 2018 at 11:26 PM, Michael Duppr? <
>> Michael.Duppre at metaswitch.com> wrote:
>>
>> Hi Sunil,
>>
>>
>>
>> Sorry to hear that you?ve found our reply too slow to be helpful this
>> time, but I?m glad to hear that you seem to have made some progress on your
>> own. As Jim mentioned, our mailing list is `best effort` while we focus on
>> development work to keep the quality of Clearwater as high as possible ?
>> thanks, Jim, I?m happy to hear that you like our free service!
>>
>>
>>
>> Regarding your questions:
>>
>> ?  You?ve mentioned that you have hit other problems with stress tool,
>> could you please head over to https://github.com/Metaswitch/
>> project-clearwater-issues/issues and open up an issue with some specific
>> diagnostics? That way we can track and prioritize the problem to get it
>> fixed as soon as possible.
>>
>> ?  It sounds like you now got some calls working, that?s good news!
>> Could you please share your stress tool and sprout logs to pinpoint if this
>> is a problem with the generation of load or with Clearwater handling the
>> calls? Also, you?ve mentioned that you?ve made some changes to the tool,
>> what exactly did you change? I?d just like to be sure that none of your
>> changes are causing this behaviour.
>>
>> ?  About the `multiplier` argument: Thanks for pointing out that our
>> documentation is missing something useful here, we will update it. For now,
>> if you take a look at the help text of the stress tool by running
>> `/usr/share/clearwater/bin/run_stress --help` you can find the
>> explanation:
>>
>> o *--multiplier MULTIPLIER: Multiplier for the VoLTE load profile (e.g.
>> passing 2 here will mean 2.6 calls and 4 re-registers per subs per hour)*
>>
>> For more information about the VoLTE load profile you can have a look at
>> the documentation for the stress tool at http://clearwater.readthedocs.
>> io/en/stable/Clearwater_stress_testing.html, specifically:
>>
>> o *[The stress tool will] send traffic, using a fixed load profile of
>> 1.3 calls/hour for each subscriber (split equally between incoming and
>> outgoing calls) and 2 re-registrations per hour*
>>
>>
>>
>> Good luck with your next steps using Clearwater! Hope you won?t hit any
>> other problems, but if you do, please let us know!
>>
>>
>>
>> Kind regards,
>>
>> Michael
>>
>>
>>
>>
>>
>> *From:* Jim Page [mailto:jim.page at redmatter.com]
>> *Sent:* 23 March 2018 09:20
>> *To:* clearwater at lists.projectclearwater.org
>> *Cc:* Michael Duppr? <Michael.Duppre at metaswitch.com>
>>
>>
>> *Subject:* Re: [Project Clearwater] CW team please help - stress testing
>>
>>
>>
>> Dude, this is a free service. I am sure if you buy some of their
>> commercial licenses they will prioritise your request, but if you are on
>> here you need to understand that service given here is ?best effort?, and
>> in my view they perform a fantastic job. So calm down.
>>
>>
>>
>> *RedMatter Ltd*
>>
>> *Jim Page*
>> *VP Mobile Services*
>>
>> +44 (0)333 150 1666 <+44%20333%20150%201666>
>> +44 (0)7870 361412 <+44%207870%20361412>
>>
>> jim.page at redmatter.com
>>
>>
>>
>> On 23 Mar 2018, at 02:29, Sunil Kumar <skgola1997 at gmail.com> wrote:
>>
>>
>>
>> Hi,
>>
>> Thanks for replying, but you guys are replying very late, its not good, I
>> have been waiting for your reply from last 3 days :-( .
>>
>> Anyway, I thought the script hast other problem also, May be you will
>> check it and fix it so that others wouldn't got that problem. Somehow I fix
>> the problem, though it takes lot of of time to read the script and make
>> some changes.
>>
>>
>>
>> I want ask few questions and *expecting reply within a day* :-)
>>
>> 1. when I use 1000 subscriber and running for 10 min duration, only few
>> call are successful (around 300) and no calls are failed. How can I
>> increase no. of calls.
>>
>> 2. Can you explain the exact use of* --multiplier *parameter in detail.
>> I request you to add all the parameter in doc itself so other would not get
>> problem while finding.
>>
>>
>>
>> Thanks,
>>
>> Sunil
>>
>>
>>
>>
>>
>> On Thu, Mar 22, 2018 at 8:09 PM, Michael Duppr? <
>> Michael.Duppre at metaswitch.com> wrote:
>>
>> Hello Sunil,
>>
>>
>>
>> Sorry about the stress tool not working properly with a lower number of
>> subscribers, that looks like a bug in the tool. I have raised issue
>> https://github.com/Metaswitch/project-clearwater-issues/issues/30 to
>> track and fix this problem, feel free to provide any other information on
>> that ticket if you hit similar problems. Thanks for your help finding this
>> bug!
>>
>>
>>
>> Looks like you?ve done the right things and went back to a slightly
>> higher number of subscribers (50) and looked at the stress log file and the
>> sprout log file. Unfortunately it looks like you?ve copied out the wrong
>> time period from the sprout logs: In your email below, the stress tool logs
>> are from 17:48, however the sprout logs that you?ve sent are from 5 hours
>> before at 12:27.
>>
>> Similar for your tcpdump ? a good idea to have a look at this, but
>> unfortunately what you?ve copied into your email is only the register flow,
>> which is successful! :-)
>>
>>
>>
>> You?re probably pretty close finding the reason why the calls are failing
>> in the sprout logs, could you please make sure you have a look at the
>> timestamp of the time you ran the stress tool?
>>
>>
>>
>> Good luck and kind regards,
>>
>> Michael
>>
>>
>>
>>
>>
>> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
>> *On Behalf Of *Sunil Kumar
>> *Sent:* 20 March 2018 14:57
>> *To:* clearwater at lists.projectclearwater.org; Bennett Allen <
>> Bennett.Allen at metaswitch.com>
>> *Subject:* Re: [Project Clearwater] CW team please help - stress testing
>>
>>
>>
>> Hi,
>>
>> I am facing problem in stress testing, Please look into the log. I am not
>> able to debug the problem.
>>
>>
>>
>> I have taken this from wireshark, actually i use tcpdump.
>>
>>
>>
>> REGISTER sip:ims.com SIP/2.0
>>
>> Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-0
>>
>> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>>
>> Content-Length: 0
>>
>> Require: Path
>>
>> Path: <sip:127.0.1.1:5082;transport=tcp;lr>
>>
>> P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
>>
>> Supported: path
>>
>> To: <sip:2010000039 at ims.com>
>>
>> Route: <sip:icscf at sprout.ims.com;lr>
>>
>> Max-Forwards: 70
>>
>> Contact: <sip:2010000039 at 127.0.1.1:34768>;reg-id=1;+sip.instance="<
>> urn:uuid:00000000-0000-0000-0000-000000000001>"
>>
>> Call-ID: 1-784 at 127.0.1.1
>>
>> CSeq: 1 REGISTER
>>
>> Expires: 3600
>>
>> Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY,
>> MESSAGE, PRACK
>>
>> Supported: path, gruu
>>
>> Authorization: Digest username="2010000039 at ims.com",realm="ims.com
>> ",uri="sip:ims.com",nonce="",response="",algorithm=Digest-MD5
>>
>> User-Agent: 00-00000-0000000000000 Phone IMS 10.0
>>
>> P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000
>> 000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
>>
>> P-Visited-Network-ID: ims.com
>>
>>
>>
>> SIP/2.0 401 Unauthorized
>>
>> Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.22
>> 4.61.13;branch=z9hG4bK-784-1-0
>>
>> Call-ID: 1-784 at 127.0.1.1
>>
>> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>>
>> To: <sip:2010000039 at ims.com>;tag=z9hG4bKPjDBaGZjqTrLQiDSlHihO36o
>> MPm7fxz2sQ
>>
>> CSeq: 1 REGISTER
>>
>> P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
>>
>> WWW-Authenticate: Digest  realm="ims.com",nonce="0e07c1b
>> 77b566f37",opaque="5171f001504c2c3a",algorithm=MD5,qop="auth"
>>
>> Content-Length:  0
>>
>>
>>
>> REGISTER sip:ims.com SIP/2.0
>>
>> Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-2
>>
>> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>>
>> Content-Length: 0
>>
>> Require: Path
>>
>> Path: <sip:127.0.1.1:5082;transport=tcp;lr>
>>
>> P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
>>
>> Supported: path
>>
>> To: <sip:2010000039 at ims.com>
>>
>> Route: <sip:icscf at sprout.ims.com;lr>
>>
>> Max-Forwards: 70
>>
>> Contact: <sip:2010000039 at 127.0.1.1:34768>;reg-id=1;+sip.instance="<
>> urn:uuid:00000000-0000-0000-0000-000000000001>"
>>
>> Call-ID: 1-784 at 127.0.1.1
>>
>> CSeq: 1 REGISTER
>>
>> Expires: 3600
>>
>> Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY,
>> MESSAGE, PRACK
>>
>> Supported: path, gruu
>>
>> Authorization: Digest username="2010000039 at ims.com",realm="ims.com
>> ",cnonce="66334873",nc=00000001,qop=auth,uri="sip:sprout.ims.com:5052",
>> nonce="0e07c1b77b566f37",response="788d4520717e4e7b29f7
>> fab43fdc448f",algorithm=MD5,opaque="5171f001504c2c3a"
>>
>> User-Agent: 00-00000-0000000000000 Phone IMS 10.0
>>
>> P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000
>> 000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
>>
>> P-Visited-Network-ID: ims.com
>>
>>
>>
>> SIP/2.0 200 OK
>>
>> Service-Route: <sip:scscf.sprout.ims.com;transport=TCP;lr;orig;username=
>> 2010000039%40ims.com;nonce=0e07c1b77b566f37>
>>
>> Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.22
>> 4.61.13;branch=z9hG4bK-784-1-2
>>
>> Call-ID: 1-784 at 127.0.1.1
>>
>> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>>
>> To: <sip:2010000039 at ims.com>;tag=z9hG4bKPjIvjh2DjwvU.vVNEv.nOiYA
>> fsZRgMjHDF
>>
>> CSeq: 1 REGISTER
>>
>> P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
>>
>> Supported: outbound
>>
>> Contact: <sip:2010000039 at 127.0.1.1:34768>;expires=1800;+sip.instance=
>> "<urn:uuid:00000000-0000-0000-0000-000000000001>";reg-id=1;pub-gruu="
>> sip:2010000039 at ims.com;gr=urn:uuid:00000000-0000-0000-0000-000000000001"
>>
>> Require: outbound
>>
>> Path: <sip:127.0.1.1:5082;transport=tcp;lr>
>>
>> P-Associated-URI: <sip:2010000039 at ims.com>
>>
>> Content-Length:  0
>>
>>
>>
>>
>>
>> thanks in advance, Please resply.
>>
>>
>>
>> cheers,
>>
>> sunil
>>
>>
>>
>>
>>
>> On Tue, Mar 20, 2018 at 6:53 PM, Sunil Kumar <skgola1997 at gmail.com>
>> wrote:
>>
>> Hi,
>>
>> It is using some other port on stress node not 5082. Is this a problem,
>> if yes how can I fix this i have tried to open 5082 port on stress node
>> using *sudo ufw allow 5082/tcp, *but no effect.
>>
>> Please check the wireshark log:
>>
>>
>>
>> Frame 2406: 703 bytes on wire (5624 bits), 703 bytes captured (5624 bits)
>>
>> Ethernet II, Src: PcsCompu_ff:d2:88 (08:00:27:ff:d2:88), Dst:
>> PcsCompu_ab:71:0f (08:00:27:ab:71:0f)
>>
>> Internet Protocol Version 4, Src: 10.224.61.22, Dst: 10.224.61.13
>>
>> Transmission Control Protocol, Src Port: rlm-admin (5054), Dst Port:
>> 34312 (34312), Seq: 349, Ack: 2199, Len: 637
>>
>> Session Initiation Protocol (503)
>>
>>
>>
>>
>>
>> cheers,
>>
>> sunil
>>
>>
>>
>> On Tue, Mar 20, 2018 at 5:21 PM, Sunil Kumar <skgola1997 at gmail.com>
>> wrote:
>>
>> Hi all,
>>
>> I have taken tcpdump also but there is no SIP message. Please through
>> some light on this problem. I am trying from last weak, not able to catch
>> the problem. Thanks in advance.
>>
>>
>>
>> cheers,
>>
>> Sunil
>>
>>
>>
>> On Tue, Mar 20, 2018 at 10:30 AM, Sunil Kumar <skgola1997 at gmail.com>
>> wrote:
>>
>> Hi CW team,
>>
>> Anyone out there please help me. I am facing problem in stress testing. I
>> have installed CW manually. whenever I was running 1 or less than 20 it
>> give some errors like:
>>
>>
>>
>> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
>> <http://ims.com/> 1 2*
>>
>> [sudo] password for ubuntu:
>>
>> Starting initial registration, will take 0 seconds
>>
>> Initial registration succeeded
>>
>> Starting test
>>
>> Test complete
>>
>> Traceback (most recent call last):
>>
>>   File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
>>
>>     with open(CALLER_STATS) as f:
>>
>> IOError: [Errno 2] No such file or directory:
>> '/var/log/clearwater-sip-stress/18065_caller_stats.log'
>>
>>
>>
>>
>>
>> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
>> <http://ims.com/> 10 5*
>>
>> Starting initial registration, will take 0 seconds
>>
>> Initial registration succeeded
>>
>> Starting test
>>
>> Test complete
>>
>> Traceback (most recent call last):
>>
>>   File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
>>
>>     call_success_rate = 100 * float(row['SuccessfulCall(C)']) /
>> float(row['TotalCallCreated'])
>>
>> ZeroDivisionError: float division by zero
>>
>>
>>
>>
>>
>> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress
>> iind.intel.com <http://iind.intel.com/> 50 5*
>>
>> Starting initial registration, will take 0 seconds
>>
>> Initial registration succeeded
>>
>> Starting test
>>
>> Test complete
>>
>>
>>
>> Elapsed time: 00:03:41
>>
>> Start: 2018-03-20 17:46:43.268136
>>
>> End: 2018-03-20 17:51:31.363406
>>
>>
>>
>> Total calls: 2
>>
>> Successful calls: 0 (0.0%)
>>
>> Failed calls: 2 (100.0%)
>>
>> Unfinished calls: 0
>>
>>
>>
>> Retransmissions: 0
>>
>>
>>
>> Average time from INVITE to 180 Ringing: 0.0ms
>>
>> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
>>
>> Failed: call success rate 0.0% is lower than target 100.0%!
>>
>>
>>
>> Total re-REGISTERs: 8
>>
>> Successful re-REGISTERs: 8 (100.0%)
>>
>> Failed re-REGISTERS: 0 (0.0%)
>>
>>
>>
>> REGISTER retransmissions: 0
>>
>>
>>
>> Average time from REGISTER to 200 OK: 86.0ms
>>
>>
>>
>> Log files at /var/log/clearwater-sip-stress/18566_*
>>
>>
>>
>>
>>
>>
>>
>> *[]ubuntu at stress:~$ cat
>> /var/log/clearwater-sip-stress/18566_caller_errors.log*
>>
>> sipp: The following events occured:
>>
>> 2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on
>> unexpected message for Call-Id '1-18576 at 127.0.1.1': while expecting
>> '183' (index 2), received '*SIP/2.0 503 Service Unavailable*
>>
>> Via: SIP/2.0/TCP 127.0.1.1:42276;received=10.22
>> 4.61.13;branch=z9hG4bK-18576-1-0
>>
>> Record-Route: <sip:scscf.sprout.ims.com;tran
>> sport=TCP;lr;billing-role=charge-term>
>>
>> Record-Route: <sip:scscf.sprout. ims.com ;transport=TCP;lr;billing-role
>> =charge-orig>
>>
>> Call-ID: 1-18576 at 127.0.1.1
>>
>> From: <sip:2010000042@ ims.com >;tag=18576SIPpTag001
>>
>> To: <sip:2010000015@ ims.co>;tag=z9hG4bKPj1Lm9whhQM
>> slKrcZxnN6qCH0tb9Lj5Neu
>>
>> CSeq: 1 INVITE
>>
>> P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com
>> ;term-ioi= ims.com
>>
>> P-Charging-Function-Addresses: ccf=0.0.0.0
>>
>> Content-Length:  0
>>
>>
>>
>>
>>
>> *[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt*
>>
>> --start msg--
>>
>>
>>
>> SIP/2.0 200 OK
>>
>> Via: SIP/2.0/TCP 10.224.61.22;rport=49294;recei
>> ved=10.224.61.22;branch=z9hG4bK-670172
>>
>> Call-ID: poll-sip-670172
>>
>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670172
>>
>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670172
>>
>> CSeq: 670172 OPTIONS
>>
>> Content-Length:  0
>>
>>
>>
>>
>>
>> --end msg--
>>
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>>
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b
>> Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
>>
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> thread_dispatcher.cpp:270: Worker thread completed processing message
>> 0x7f34ec34a3e8
>>
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> thread_dispatcher.cpp:284: Request latency = 254us
>>
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
>>
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
>>
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217:
>> Rate adjustment calculation inputs: err -0.981500, smoothed latency 185,
>> target latency 10000
>>
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302:
>> Maximum incoming request rate/second unchanged at 2000.000000 (current
>> request rate is 0.200000 requests/sec, minimum threshold for a change is
>> 1000.000000 requests/sec).
>>
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
>> 2000ui into continuous accumulator statistic
>>
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
>> 2000ui into continuous accumulator statistic
>>
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed
>> IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
>>
>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>> thread_dispatcher.cpp:158: Attempting to process queue element
>>
>> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP connection closed
>>
>> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug
>> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>>
>> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>>
>> 20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning:
>> Failed to connect to the agentx master agent ([NIL]):
>>
>> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054
>> TCP listener 10.224.61.22:5054: got incoming TCP connection from
>> 10.224.61.22:42848, sock=573
>>
>> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> tcp->base.local_name: 10.224.61.22
>>
>> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP server transport created
>>
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
>> Processing incoming message: Request msg OPTIONS/cseq=670180
>> (rdata0x7f34ec027690)
>>
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose
>> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180
>> (rdata0x7f34ec027690) from TCP 10.224.61.22:42848:
>>
>> --start msg--
>>
>>
>>
>> OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
>>
>> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
>>
>> Max-Forwards: 2
>>
>> To: <sip:poll-sip at 10.224.61.22:5054>
>>
>> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670180
>>
>> Call-ID: poll-sip-670180
>>
>> CSeq: 670180 OPTIONS
>>
>> Contact: <sip:10.224.61.22>
>>
>> Accept: application/sdp
>>
>> Content-Length: 0
>>
>> User-Agent: poll-sip
>>
>>
>>
>>
>>
>> --end msg--
>>
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
>> home domain: false, local_to_node: true, is_gruu: false,
>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>>
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
>> Classified URI as 3
>>
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
>>
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
>>
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
>>
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
>> 0x7f34ec34a3e8
>>
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
>> for worker threads with priority 15
>>
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
>>
>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added
>> IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> thread_dispatcher.cpp:183: Request latency so far = 57us
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c
>> Distributing rdata to modules: Request msg OPTIONS/cseq=670180
>> (rdata0x7f34ec34a3e8)
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139:
>> home domain: false, local_to_node: true, is_gruu: false,
>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172:
>> Classified URI as 3
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint
>> Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose
>> common_sip_processing.cpp:103: TX 282 bytes Response msg
>> 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848:
>>
>> --start msg--
>>
>>
>>
>> SIP/2.0 200 OK
>>
>> Via: SIP/2.0/TCP 10.224.61.22;rport=42848;recei
>> ved=10.224.61.22;branch=z9hG4bK-670180
>>
>> Call-ID: poll-sip-670180
>>
>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670180
>>
>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670180
>>
>> CSeq: 670180 OPTIONS
>>
>> Content-Length:  0
>>
>>
>>
>>
>>
>> --end msg--
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809
>> Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> thread_dispatcher.cpp:270: Worker thread completed processing message
>> 0x7f34ec34a3e8
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> thread_dispatcher.cpp:284: Request latency = 129us
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341:
>> Not recalculating rate as we haven't processed 20 requests yet (only 1).
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed
>> IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
>>
>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>> thread_dispatcher.cpp:158: Attempting to process queue element
>>
>> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327:
>> Process request for URL /ping, args (null)
>>
>> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68:
>> Sending response 200 to request for URL /ping, args (null)
>>
>> 20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP connection closed
>>
>> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug
>> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>>
>> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>>
>> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053
>> TCP listener 10.224.61.22:5053: got incoming TCP connection from
>> 10.224.61.22:49356, sock=573
>>
>> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> tcp->base.local_name: 10.224.61.22
>>
>> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP server transport created
>>
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
>> Processing incoming message: Request msg OPTIONS/cseq=670182
>> (rdata0x7f34ec027690)
>>
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose
>> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182
>> (rdata0x7f34ec027690) from TCP 10.224.61.22:49356:
>>
>> --start msg--
>>
>>
>>
>> OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
>>
>> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
>>
>> Max-Forwards: 2
>>
>> To: <sip:poll-sip at 10.224.61.22:5053>
>>
>> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670182
>>
>> Call-ID: poll-sip-670182
>>
>> CSeq: 670182 OPTIONS
>>
>> Contact: <sip:10.224.61.22>
>>
>> Accept: application/sdp
>>
>> Content-Length: 0
>>
>> User-Agent: poll-sip
>>
>>
>>
>>
>>
>> --end msg--
>>
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
>> home domain: false, local_to_node: true, is_gruu: false,
>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>>
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
>> Classified URI as 3
>>
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
>>
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
>>
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
>>
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
>> 0x7f34ec34a3e8
>>
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
>> for worker threads with priority 15
>>
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
>>
>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added
>> IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> thread_dispatcher.cpp:183: Request latency so far = 102us
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c
>> Distributing rdata to modules: Request msg OPTIONS/cseq=670182
>> (rdata0x7f34ec34a3e8)
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139:
>> home domain: false, local_to_node: true, is_gruu: false,
>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172:
>> Classified URI as 3
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint
>> Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose
>> common_sip_processing.cpp:103: TX 282 bytes Response msg
>> 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356:
>>
>> --start msg--
>>
>>
>>
>> SIP/2.0 200 OK
>>
>> Via: SIP/2.0/TCP 10.224.61.22;rport=49356;recei
>> ved=10.224.61.22;branch=z9hG4bK-670182
>>
>> Call-ID: poll-sip-670182
>>
>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670182
>>
>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670182
>>
>> CSeq: 670182 OPTIONS
>>
>> Content-Length:  0
>>
>>
>>
>>
>>
>> --end msg--
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00
>> Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> thread_dispatcher.cpp:270: Worker thread completed processing message
>> 0x7f34ec34a3e8
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> thread_dispatcher.cpp:284: Request latency = 232us
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341:
>> Not recalculating rate as we haven't processed 20 requests yet (only 2).
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed
>> IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
>>
>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>> thread_dispatcher.cpp:158: Attempting to process queue element
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244:
>> Reraising all alarms with a known state
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1001.1 alarm
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1005.1 alarm
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1011.1 alarm
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1012.1 alarm
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1013.1 alarm
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1004.1 alarm
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1002.1 alarm
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1009.1 alarm
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>> AlarmReqAgent: queue overflowed
>>
>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>> issued 1010.1 alarm
>>
>> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP connection closed
>>
>> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug
>> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>>
>> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>>
>>
>>
>>
>>
>>
>>
>> all calls are failing I don't know what is going on, I am newbie to CW
>> please guide some solution it will be great help.
>>
>>
>>
>>
>>
>> Thanks,
>>
>> Sunil
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> _______________________________________________
>> Clearwater mailing list
>> Clearwater at lists.projectclearwater.org
>> http://lists.projectclearwater.org/mailman/listinfo/
>> clearwater_lists.projectclearwater.org
>>
>>
>>
>> _______________________________________________
>> Clearwater mailing list
>> Clearwater at lists.projectclearwater.org
>> http://lists.projectclearwater.org/mailman/listinfo/
>> clearwater_lists.projectclearwater.org
>>
>>
>>
>>
>> _______________________________________________
>> Clearwater mailing list
>> Clearwater at lists.projectclearwater.org
>> http://lists.projectclearwater.org/mailman/listinfo/
>> clearwater_lists.projectclearwater.org
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180328/cf32b2b4/attachment.html>

From skgola1997 at gmail.com  Tue Mar 27 14:55:01 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Wed, 28 Mar 2018 00:25:01 +0530
Subject: [Project Clearwater] CW team please help - stress testing
In-Reply-To: <CAHwYWpDfkVkJvRkHThf=KmMMKb7xY4XsNBunOdqkEV+FAGTUfw@mail.gmail.com>
References: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
	<CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>
	<CAHwYWpB4WqEVnG69B_BazQC6yJVHW7A3pMPOWmRoVX_gYJhPgw@mail.gmail.com>
	<CAHwYWpBH8qdZbAEuFoE61Y13s1s9QGku912aBb5w8XpX+ZmAkQ@mail.gmail.com>
	<SN1PR02MB16779E13A341C6567440B42CF5A90@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpCj9xAoiE=eLe7+GMCrX7RjPDgMHXVazVhX4evT8AbTUw@mail.gmail.com>
	<C29325B9-84E3-49B8-96B4-044A1AB88CB8@redmatter.com>
	<SN1PR02MB1677A0FE6650898235E6F58FF5A80@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpDCMqp5yXafz-3zuzHQJbKNFBvfzJhYbofBGc2dUqKiyA@mail.gmail.com>
	<BY2PR02MB21491BAF112161741FC5D912F4AC0@BY2PR02MB2149.namprd02.prod.outlook.com>
	<CAHwYWpBPa94n1we8-B=8JEpFKd6uXZ-i=RNTsG_CsW9HC5pAYQ@mail.gmail.com>
	<CAHwYWpDfkVkJvRkHThf=KmMMKb7xY4XsNBunOdqkEV+FAGTUfw@mail.gmail.com>
Message-ID: <CAHwYWpArxx7YHuV=MKgPn1c9zbNQp109i44yceXzKBfkj7L51A@mail.gmail.com>

Hi,
error log on stress node is like below:


*caller_error :*
 call.
2018-03-28      06:39:07.231025 1522199347.231025: Failed to shutdown
socket 70, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.231042 1522199347.231042: Call-Id:
8849-13941 at 10.224.61.34, receive timeout on message Clearwater caller:13
without label to jump to (ontimeout attribute): aborting call.
2018-03-28      06:39:07.231053 1522199347.231053: Failed to shutdown
socket 762, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232678 1522199347.232678: Failed to shutdown
socket 11, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232702 1522199347.232702: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.
2018-03-28      06:39:07.232712 1522199347.232712: Failed to shutdown
socket 14, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232721 1522199347.232721: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.
2018-03-28      06:39:07.232729 1522199347.232729: Failed to shutdown
socket 16, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232739 1522199347.232739: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.
2018-03-28      06:39:07.232746 1522199347.232746: Failed to shutdown
socket 18, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232755 1522199347.232755: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.
2018-03-28      06:39:07.232763 1522199347.232763: Failed to shutdown
socket 21, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232771 1522199347.232771: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.
2018-03-28      06:39:07.232781 1522199347.232781: Failed to shutdown
socket 23, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232796 1522199347.232796: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.
2018-03-28      06:39:07.232804 1522199347.232804: Failed to shutdown
socket 26, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232812 1522199347.232812: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.
2018-03-28      06:39:07.232819 1522199347.232819: Failed to shutdown
socket 30, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232828 1522199347.232828: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.
2018-03-28      06:39:07.232835 1522199347.232835: Failed to shutdown
socket 33, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232844 1522199347.232844: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.
2018-03-28      06:39:07.232851 1522199347.232851: Failed to shutdown
socket 34, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232859 1522199347.232859: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.
2018-03-28      06:39:07.232867 1522199347.232867: Failed to shutdown
socket 35, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232875 1522199347.232875: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.
2018-03-28      06:39:07.232882 1522199347.232882: Failed to shutdown
socket 38, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.232891 1522199347.232891: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.



*callee_error:*

2018-03-28      06:38:44.990847 1522199324.990847: Call-Id:
8972-13941 at 10.224.61.34, receive timeout on message Clearwater callee:10
without label to jump to (ontimeout attribute): aborting call.
2018-03-28      06:38:45.479600 1522199325.479600: Call-Id:
8959-13941 at 10.224.61.34, receive timeout on message Clearwater callee:10
without label to jump to (ontimeout attribute): aborting call.
2018-03-28      06:38:45.479692 1522199325.479692: Call-Id:
8946-13941 at 10.224.61.34, receive timeout on message Clearwater callee:10
without label to jump to (ontimeout attribute): aborting call.
2018-03-28      06:39:07.224471 1522199347.224471: Failed to shutdown
socket 8, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.224524 1522199347.224524: Error on TCP connection,
remote peer probably closed the socket: Connection reset by peer.
2018-03-28      06:39:07.330796 1522199347.330796: Socket required a
reconnection..
2018-03-28      06:39:07.330821 1522199347.330821: Overload warning: the
minor watchdog timer 500ms has been tripped (520), 299 trips remaining..
2018-03-28      06:39:07.331416 1522199347.331416: Failed to shutdown
socket 8, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.331431 1522199347.331431: Error on TCP connection,
remote peer probably closed the socket: Connection refused.
2018-03-28      06:39:07.440779 1522199347.440779: Socket required a
reconnection..
2018-03-28      06:39:07.441025 1522199347.441025: Failed to shutdown
socket 8, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.441039 1522199347.441039: Error on TCP connection,
remote peer probably closed the socket: Connection refused.
2018-03-28      06:39:07.541208 1522199347.541208: Socket required a
reconnection..
2018-03-28      06:39:07.541322 1522199347.541322: Failed to shutdown
socket 8, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.541336 1522199347.541336: Error on TCP connection,
remote peer probably closed the socket: Connection refused.
2018-03-28      06:39:07.648743 1522199347.648743: Socket required a
reconnection..
2018-03-28      06:39:07.649013 1522199347.649013: Failed to shutdown
socket 8, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.649029 1522199347.649029: Error on TCP connection,
remote peer probably closed the socket: Connection refused.
2018-03-28      06:39:07.750798 1522199347.750798: Socket required a
reconnection..
2018-03-28      06:39:07.750981 1522199347.750981: Failed to shutdown
socket 8, errno = 107 (Transport endpoint is not connected).
2018-03-28      06:39:07.750996 1522199347.750996: Error on TCP connection,
remote peer probably closed the socket: Connection refused.


On Wed, Mar 28, 2018 at 12:12 AM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi,
> I got errors while stress testing. I used 40000 subscribers for 30 min
> duration but it is running completing, what is the reasons behind this, is
> there some bug in script itself or other problem:
>
> *[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress ims.com
> <http://ims.com> 40000 30 --multiplier=16*
> Starting initial registration, will take 500 seconds
> Initial registration succeeded
> Starting test
> Test complete
>
> Elapsed time: 00:02:06
> Start: 2018-03-28 06:18:00.636804
> End: 2018-03-28 06:20:07.829051
>
> Total calls: 6989
> Successful calls: 1391 (19.9027042495%)
> Failed calls: 3953 (56.5603090571%)
> Unfinished calls: 1645
>
> Retransmissions: 0
>
> Average time from INVITE to 180 Ringing: 15092.0ms
> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
> # of calls with 2000+ms from INVITE to 180 Ringing: 3008 (43.0390613822%)
> *Failed: call success rate 19.9027042495% is lower than target 100.0%!*
> *Traceback (most recent call last):*
> *  File "/usr/share/clearwater/bin/run_stress", line 443, in <module>*
> *    reg_success_rate = 100 * float(row['SuccessfulCall(C)']) /
> float(row['TotalCallCreated'])*
> *ZeroDivisionError: float division by zero*
>
>
>
> please provide some solution. I have created 50K subscribers in vellum.
> Using single node for sprout and for other also.
>
> How can i increase the no. of nodes for sprout and for other also?
>
>
> Thanks
>
>
> On Tue, Mar 27, 2018 at 5:26 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:
>
>> Thanks Robert for replying,
>>
>> Yes, the file is there but it is empty..
>>
>> [sprout]ubuntu at sprout:/var/log/sprout$ ls
>> access_20180326T180000Z.txt  access_20180327T000000Z.txt
>> access_20180327T060000Z.txt  access_20180327T120000Z.txt
>> access_20180327T180000Z.txt  sprout_20180327T150000Z.txt
>> *sprout_err.log*
>> access_20180326T190000Z.txt  access_20180327T010000Z.txt
>> access_20180327T070000Z.txt  access_20180327T130000Z.txt
>> access_20180327T190000Z.txt  sprout_20180327T160000Z.txt  sprout_out.log
>> access_20180326T200000Z.txt  access_20180327T020000Z.txt
>> access_20180327T080000Z.txt  access_20180327T140000Z.txt
>> access_current.txt           sprout_20180327T170000Z.txt
>> access_20180326T210000Z.txt  access_20180327T030000Z.txt
>> access_20180327T090000Z.txt  access_20180327T150000Z.txt  analytics.log
>>             sprout_20180327T180000Z.txt
>> access_20180326T220000Z.txt  access_20180327T040000Z.txt
>> access_20180327T100000Z.txt  access_20180327T160000Z.txt
>> sprout_20180327T130000Z.txt  sprout_20180327T190000Z.txt
>> access_20180326T230000Z.txt  access_20180327T050000Z.txt
>> access_20180327T110000Z.txt  access_20180327T170000Z.txt
>> sprout_20180327T140000Z.txt  sprout_current.txt
>>
>> *[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_err.log*
>> [sprout]ubuntu at sprout:/var/log/sprout$
>>
>> Regards,
>> Sunil
>>
>>
>> On Tue, Mar 27, 2018 at 5:15 PM, Robert Day <Robert.Day at metaswitch.com>
>> wrote:
>>
>>> Hi Sunil,
>>>
>>>
>>>
>>> Are there any ?Error?-level logs in /var/log/sprout or
>>> /var/log/homestead on the VMs? That might suggest what?s causing the 503
>>> Service Unavailable error.
>>>
>>>
>>>
>>> Best,
>>>
>>> Rob
>>>
>>>
>>>
>>> *From:* Sunil Kumar [mailto:skgola1997 at gmail.com]
>>> *Sent:* 26 March 2018 12:28
>>> *To:* clearwater at lists.projectclearwater.org; Michael Duppr? <
>>> Michael.Duppre at metaswitch.com>
>>> *Subject:* Re: [Project Clearwater] CW team please help - stress testing
>>>
>>>
>>>
>>> Hi Michael,
>>>
>>> I am not able to make call got 503 service unavailable. can you please
>>> check and guide me some quick solution
>>>
>>>
>>>
>>> []ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress ims.com 100 2
>>>
>>> Starting initial registration, will take 1 seconds
>>>
>>> Initial registration succeeded
>>>
>>> Starting test
>>>
>>> Test complete
>>>
>>>
>>>
>>> Elapsed time: 00:01:50
>>>
>>> Start: 2018-03-26 23:18:41.605000
>>>
>>> End: 2018-03-26 23:20:32.453238
>>>
>>>
>>>
>>> Total calls: 2
>>>
>>> Successful calls: 0 (0.0%)
>>>
>>> Failed calls: 2 (100.0%)
>>>
>>> Unfinished calls: 0
>>>
>>>
>>>
>>> Retransmissions: 0
>>>
>>>
>>>
>>> Average time from INVITE to 180 Ringing: 0.0ms
>>>
>>> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> Failed: call success rate 0.0% is lower than target 100.0%!
>>>
>>>
>>>
>>> Total re-REGISTERs: 6
>>>
>>> Successful re-REGISTERs: 6 (100.0%)
>>>
>>> Failed re-REGISTERS: 0 (0.0%)
>>>
>>>
>>>
>>> REGISTER retransmissions: 0
>>>
>>>
>>>
>>> Average time from REGISTER to 200 OK: 59.0ms
>>>
>>>
>>>
>>> Log files at /var/log/clearwater-sip-stress/12890_*
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> []ubuntu at stress:~$ cat /var/log/clearwater-sip-stress
>>> /12890_caller_errors.log
>>>
>>> sipp: The following events occured:
>>>
>>> 2018-03-26      23:19:37.070794 1522086577.070794: Aborting call on
>>> unexpected message for Call-Id '1-12900 at 127.0.1.1': while expecting
>>> '183' (index 2), received 'SIP/2.0 503 Service Unavailable
>>>
>>> Via: SIP/2.0/TCP 127.0.1.1:34112;received=10.22
>>> 4.61.34;branch=z9hG4bK-12900-1-0
>>>
>>> Record-Route: <sip:scscf.sprout.ims.com;tran
>>> sport=TCP;lr;billing-role=charge-term>
>>>
>>> Record-Route: <sip:scscf.sprout.ims.com;tran
>>> sport=TCP;lr;billing-role=charge-orig>
>>>
>>> Call-ID: 1-12900 at 127.0.1.1
>>>
>>> From: <sip:2010000002 at ims.com>;tag=12900SIPpTag001
>>>
>>> To: <sip:2010000041@ ims .com>;tag=z9hG4bKPjRhUbQK8NJ5B
>>> lp4qIYX7f1YWhM2FQZe2M
>>>
>>> CSeq: 1 INVITE
>>>
>>> P-Charging-Vector: icid-value="12900SIPpTag001";orig-ioi= ims.com;term-ioi=
>>> ims.com
>>>
>>> P-Charging-Function-Addresses: ccf=0.0.0.0
>>>
>>> Content-Length:  0
>>>
>>>
>>>
>>>
>>>
>>> PFA the sprout log
>>>
>>>
>>>
>>> thanks,
>>>
>>>  sunil
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> On Fri, Mar 23, 2018 at 11:26 PM, Michael Duppr? <
>>> Michael.Duppre at metaswitch.com> wrote:
>>>
>>> Hi Sunil,
>>>
>>>
>>>
>>> Sorry to hear that you?ve found our reply too slow to be helpful this
>>> time, but I?m glad to hear that you seem to have made some progress on your
>>> own. As Jim mentioned, our mailing list is `best effort` while we focus on
>>> development work to keep the quality of Clearwater as high as possible ?
>>> thanks, Jim, I?m happy to hear that you like our free service!
>>>
>>>
>>>
>>> Regarding your questions:
>>>
>>> ?  You?ve mentioned that you have hit other problems with stress tool,
>>> could you please head over to https://github.com/Metaswitch/
>>> project-clearwater-issues/issues and open up an issue with some
>>> specific diagnostics? That way we can track and prioritize the problem to
>>> get it fixed as soon as possible.
>>>
>>> ?  It sounds like you now got some calls working, that?s good news!
>>> Could you please share your stress tool and sprout logs to pinpoint if this
>>> is a problem with the generation of load or with Clearwater handling the
>>> calls? Also, you?ve mentioned that you?ve made some changes to the tool,
>>> what exactly did you change? I?d just like to be sure that none of your
>>> changes are causing this behaviour.
>>>
>>> ?  About the `multiplier` argument: Thanks for pointing out that our
>>> documentation is missing something useful here, we will update it. For now,
>>> if you take a look at the help text of the stress tool by running
>>> `/usr/share/clearwater/bin/run_stress --help` you can find the
>>> explanation:
>>>
>>> o *--multiplier MULTIPLIER: Multiplier for the VoLTE load profile (e.g.
>>> passing 2 here will mean 2.6 calls and 4 re-registers per subs per hour)*
>>>
>>> For more information about the VoLTE load profile you can have a look at
>>> the documentation for the stress tool at http://clearwater.readthedocs.
>>> io/en/stable/Clearwater_stress_testing.html, specifically:
>>>
>>> o *[The stress tool will] send traffic, using a fixed load profile of
>>> 1.3 calls/hour for each subscriber (split equally between incoming and
>>> outgoing calls) and 2 re-registrations per hour*
>>>
>>>
>>>
>>> Good luck with your next steps using Clearwater! Hope you won?t hit any
>>> other problems, but if you do, please let us know!
>>>
>>>
>>>
>>> Kind regards,
>>>
>>> Michael
>>>
>>>
>>>
>>>
>>>
>>> *From:* Jim Page [mailto:jim.page at redmatter.com]
>>> *Sent:* 23 March 2018 09:20
>>> *To:* clearwater at lists.projectclearwater.org
>>> *Cc:* Michael Duppr? <Michael.Duppre at metaswitch.com>
>>>
>>>
>>> *Subject:* Re: [Project Clearwater] CW team please help - stress testing
>>>
>>>
>>>
>>> Dude, this is a free service. I am sure if you buy some of their
>>> commercial licenses they will prioritise your request, but if you are on
>>> here you need to understand that service given here is ?best effort?, and
>>> in my view they perform a fantastic job. So calm down.
>>>
>>>
>>>
>>> *RedMatter Ltd*
>>>
>>> *Jim Page*
>>> *VP Mobile Services*
>>>
>>> +44 (0)333 150 1666 <+44%20333%20150%201666>
>>> +44 (0)7870 361412 <+44%207870%20361412>
>>>
>>> jim.page at redmatter.com
>>>
>>>
>>>
>>> On 23 Mar 2018, at 02:29, Sunil Kumar <skgola1997 at gmail.com> wrote:
>>>
>>>
>>>
>>> Hi,
>>>
>>> Thanks for replying, but you guys are replying very late, its not good,
>>> I have been waiting for your reply from last 3 days :-( .
>>>
>>> Anyway, I thought the script hast other problem also, May be you will
>>> check it and fix it so that others wouldn't got that problem. Somehow I fix
>>> the problem, though it takes lot of of time to read the script and make
>>> some changes.
>>>
>>>
>>>
>>> I want ask few questions and *expecting reply within a day* :-)
>>>
>>> 1. when I use 1000 subscriber and running for 10 min duration, only few
>>> call are successful (around 300) and no calls are failed. How can I
>>> increase no. of calls.
>>>
>>> 2. Can you explain the exact use of* --multiplier *parameter in detail.
>>> I request you to add all the parameter in doc itself so other would not get
>>> problem while finding.
>>>
>>>
>>>
>>> Thanks,
>>>
>>> Sunil
>>>
>>>
>>>
>>>
>>>
>>> On Thu, Mar 22, 2018 at 8:09 PM, Michael Duppr? <
>>> Michael.Duppre at metaswitch.com> wrote:
>>>
>>> Hello Sunil,
>>>
>>>
>>>
>>> Sorry about the stress tool not working properly with a lower number of
>>> subscribers, that looks like a bug in the tool. I have raised issue
>>> https://github.com/Metaswitch/project-clearwater-issues/issues/30 to
>>> track and fix this problem, feel free to provide any other information on
>>> that ticket if you hit similar problems. Thanks for your help finding this
>>> bug!
>>>
>>>
>>>
>>> Looks like you?ve done the right things and went back to a slightly
>>> higher number of subscribers (50) and looked at the stress log file and the
>>> sprout log file. Unfortunately it looks like you?ve copied out the wrong
>>> time period from the sprout logs: In your email below, the stress tool logs
>>> are from 17:48, however the sprout logs that you?ve sent are from 5 hours
>>> before at 12:27.
>>>
>>> Similar for your tcpdump ? a good idea to have a look at this, but
>>> unfortunately what you?ve copied into your email is only the register flow,
>>> which is successful! :-)
>>>
>>>
>>>
>>> You?re probably pretty close finding the reason why the calls are
>>> failing in the sprout logs, could you please make sure you have a look at
>>> the timestamp of the time you ran the stress tool?
>>>
>>>
>>>
>>> Good luck and kind regards,
>>>
>>> Michael
>>>
>>>
>>>
>>>
>>>
>>> *From:* Clearwater [mailto:clearwater-bounces at lis
>>> ts.projectclearwater.org] *On Behalf Of *Sunil Kumar
>>> *Sent:* 20 March 2018 14:57
>>> *To:* clearwater at lists.projectclearwater.org; Bennett Allen <
>>> Bennett.Allen at metaswitch.com>
>>> *Subject:* Re: [Project Clearwater] CW team please help - stress testing
>>>
>>>
>>>
>>> Hi,
>>>
>>> I am facing problem in stress testing, Please look into the log. I am
>>> not able to debug the problem.
>>>
>>>
>>>
>>> I have taken this from wireshark, actually i use tcpdump.
>>>
>>>
>>>
>>> REGISTER sip:ims.com SIP/2.0
>>>
>>> Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-0
>>>
>>> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>>>
>>> Content-Length: 0
>>>
>>> Require: Path
>>>
>>> Path: <sip:127.0.1.1:5082;transport=tcp;lr>
>>>
>>> P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
>>>
>>> Supported: path
>>>
>>> To: <sip:2010000039 at ims.com>
>>>
>>> Route: <sip:icscf at sprout.ims.com;lr>
>>>
>>> Max-Forwards: 70
>>>
>>> Contact: <sip:2010000039 at 127.0.1.1:34768>;reg-id=1;+sip.instance="<ur
>>> n:uuid:00000000-0000-0000-0000-000000000001>"
>>>
>>> Call-ID: 1-784 at 127.0.1.1
>>>
>>> CSeq: 1 REGISTER
>>>
>>> Expires: 3600
>>>
>>> Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY,
>>> MESSAGE, PRACK
>>>
>>> Supported: path, gruu
>>>
>>> Authorization: Digest username="2010000039 at ims.com",realm="ims.com
>>> ",uri="sip:ims.com",nonce="",response="",algorithm=Digest-MD5
>>>
>>> User-Agent: 00-00000-0000000000000 Phone IMS 10.0
>>>
>>> P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000
>>> 000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
>>>
>>> P-Visited-Network-ID: ims.com
>>>
>>>
>>>
>>> SIP/2.0 401 Unauthorized
>>>
>>> Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.22
>>> 4.61.13;branch=z9hG4bK-784-1-0
>>>
>>> Call-ID: 1-784 at 127.0.1.1
>>>
>>> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>>>
>>> To: <sip:2010000039 at ims.com>;tag=z9hG4bKPjDBaGZjqTrLQiDSlHihO36o
>>> MPm7fxz2sQ
>>>
>>> CSeq: 1 REGISTER
>>>
>>> P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
>>>
>>> WWW-Authenticate: Digest  realm="ims.com",nonce="0e07c1b
>>> 77b566f37",opaque="5171f001504c2c3a",algorithm=MD5,qop="auth"
>>>
>>> Content-Length:  0
>>>
>>>
>>>
>>> REGISTER sip:ims.com SIP/2.0
>>>
>>> Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-2
>>>
>>> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>>>
>>> Content-Length: 0
>>>
>>> Require: Path
>>>
>>> Path: <sip:127.0.1.1:5082;transport=tcp;lr>
>>>
>>> P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
>>>
>>> Supported: path
>>>
>>> To: <sip:2010000039 at ims.com>
>>>
>>> Route: <sip:icscf at sprout.ims.com;lr>
>>>
>>> Max-Forwards: 70
>>>
>>> Contact: <sip:2010000039 at 127.0.1.1:34768>;reg-id=1;+sip.instance="<ur
>>> n:uuid:00000000-0000-0000-0000-000000000001>"
>>>
>>> Call-ID: 1-784 at 127.0.1.1
>>>
>>> CSeq: 1 REGISTER
>>>
>>> Expires: 3600
>>>
>>> Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY,
>>> MESSAGE, PRACK
>>>
>>> Supported: path, gruu
>>>
>>> Authorization: Digest username="2010000039 at ims.com",realm="ims.com
>>> ",cnonce="66334873",nc=00000001,qop=auth,uri="sip:sprout.ims.com:5052
>>> ",nonce="0e07c1b77b566f37",response="788d4520717e4e7b29f7fab43fdc44
>>> 8f",algorithm=MD5,opaque="5171f001504c2c3a"
>>>
>>> User-Agent: 00-00000-0000000000000 Phone IMS 10.0
>>>
>>> P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000
>>> 000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
>>>
>>> P-Visited-Network-ID: ims.com
>>>
>>>
>>>
>>> SIP/2.0 200 OK
>>>
>>> Service-Route: <sip:scscf.sprout.ims.com;tran
>>> sport=TCP;lr;orig;username=2010000039%40ims.com;nonce=0e07c1b77b566f37>
>>>
>>> Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.22
>>> 4.61.13;branch=z9hG4bK-784-1-2
>>>
>>> Call-ID: 1-784 at 127.0.1.1
>>>
>>> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>>>
>>> To: <sip:2010000039 at ims.com>;tag=z9hG4bKPjIvjh2DjwvU.vVNEv.nOiYA
>>> fsZRgMjHDF
>>>
>>> CSeq: 1 REGISTER
>>>
>>> P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
>>>
>>> Supported: outbound
>>>
>>> Contact: <sip:2010000039 at 127.0.1.1:34768>;expires=1800;+sip.instance=
>>> "<urn:uuid:00000000-0000-0000-0000-000000000001>";reg-id=1;pub-gruu="
>>> sip:2010000039 at ims.com;gr=urn:uuid:00000000-0000-0000-0000-000000000001"
>>>
>>> Require: outbound
>>>
>>> Path: <sip:127.0.1.1:5082;transport=tcp;lr>
>>>
>>> P-Associated-URI: <sip:2010000039 at ims.com>
>>>
>>> Content-Length:  0
>>>
>>>
>>>
>>>
>>>
>>> thanks in advance, Please resply.
>>>
>>>
>>>
>>> cheers,
>>>
>>> sunil
>>>
>>>
>>>
>>>
>>>
>>> On Tue, Mar 20, 2018 at 6:53 PM, Sunil Kumar <skgola1997 at gmail.com>
>>> wrote:
>>>
>>> Hi,
>>>
>>> It is using some other port on stress node not 5082. Is this a problem,
>>> if yes how can I fix this i have tried to open 5082 port on stress node
>>> using *sudo ufw allow 5082/tcp, *but no effect.
>>>
>>> Please check the wireshark log:
>>>
>>>
>>>
>>> Frame 2406: 703 bytes on wire (5624 bits), 703 bytes captured (5624 bits)
>>>
>>> Ethernet II, Src: PcsCompu_ff:d2:88 (08:00:27:ff:d2:88), Dst:
>>> PcsCompu_ab:71:0f (08:00:27:ab:71:0f)
>>>
>>> Internet Protocol Version 4, Src: 10.224.61.22, Dst: 10.224.61.13
>>>
>>> Transmission Control Protocol, Src Port: rlm-admin (5054), Dst Port:
>>> 34312 (34312), Seq: 349, Ack: 2199, Len: 637
>>>
>>> Session Initiation Protocol (503)
>>>
>>>
>>>
>>>
>>>
>>> cheers,
>>>
>>> sunil
>>>
>>>
>>>
>>> On Tue, Mar 20, 2018 at 5:21 PM, Sunil Kumar <skgola1997 at gmail.com>
>>> wrote:
>>>
>>> Hi all,
>>>
>>> I have taken tcpdump also but there is no SIP message. Please through
>>> some light on this problem. I am trying from last weak, not able to catch
>>> the problem. Thanks in advance.
>>>
>>>
>>>
>>> cheers,
>>>
>>> Sunil
>>>
>>>
>>>
>>> On Tue, Mar 20, 2018 at 10:30 AM, Sunil Kumar <skgola1997 at gmail.com>
>>> wrote:
>>>
>>> Hi CW team,
>>>
>>> Anyone out there please help me. I am facing problem in stress testing.
>>> I have installed CW manually. whenever I was running 1 or less than 20 it
>>> give some errors like:
>>>
>>>
>>>
>>> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
>>> <http://ims.com/> 1 2*
>>>
>>> [sudo] password for ubuntu:
>>>
>>> Starting initial registration, will take 0 seconds
>>>
>>> Initial registration succeeded
>>>
>>> Starting test
>>>
>>> Test complete
>>>
>>> Traceback (most recent call last):
>>>
>>>   File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
>>>
>>>     with open(CALLER_STATS) as f:
>>>
>>> IOError: [Errno 2] No such file or directory:
>>> '/var/log/clearwater-sip-stress/18065_caller_stats.log'
>>>
>>>
>>>
>>>
>>>
>>> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
>>> <http://ims.com/> 10 5*
>>>
>>> Starting initial registration, will take 0 seconds
>>>
>>> Initial registration succeeded
>>>
>>> Starting test
>>>
>>> Test complete
>>>
>>> Traceback (most recent call last):
>>>
>>>   File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
>>>
>>>     call_success_rate = 100 * float(row['SuccessfulCall(C)']) /
>>> float(row['TotalCallCreated'])
>>>
>>> ZeroDivisionError: float division by zero
>>>
>>>
>>>
>>>
>>>
>>> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress
>>> iind.intel.com <http://iind.intel.com/> 50 5*
>>>
>>> Starting initial registration, will take 0 seconds
>>>
>>> Initial registration succeeded
>>>
>>> Starting test
>>>
>>> Test complete
>>>
>>>
>>>
>>> Elapsed time: 00:03:41
>>>
>>> Start: 2018-03-20 17:46:43.268136
>>>
>>> End: 2018-03-20 17:51:31.363406
>>>
>>>
>>>
>>> Total calls: 2
>>>
>>> Successful calls: 0 (0.0%)
>>>
>>> Failed calls: 2 (100.0%)
>>>
>>> Unfinished calls: 0
>>>
>>>
>>>
>>> Retransmissions: 0
>>>
>>>
>>>
>>> Average time from INVITE to 180 Ringing: 0.0ms
>>>
>>> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
>>>
>>> Failed: call success rate 0.0% is lower than target 100.0%!
>>>
>>>
>>>
>>> Total re-REGISTERs: 8
>>>
>>> Successful re-REGISTERs: 8 (100.0%)
>>>
>>> Failed re-REGISTERS: 0 (0.0%)
>>>
>>>
>>>
>>> REGISTER retransmissions: 0
>>>
>>>
>>>
>>> Average time from REGISTER to 200 OK: 86.0ms
>>>
>>>
>>>
>>> Log files at /var/log/clearwater-sip-stress/18566_*
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> *[]ubuntu at stress:~$ cat
>>> /var/log/clearwater-sip-stress/18566_caller_errors.log*
>>>
>>> sipp: The following events occured:
>>>
>>> 2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on
>>> unexpected message for Call-Id '1-18576 at 127.0.1.1': while expecting
>>> '183' (index 2), received '*SIP/2.0 503 Service Unavailable*
>>>
>>> Via: SIP/2.0/TCP 127.0.1.1:42276;received=10.22
>>> 4.61.13;branch=z9hG4bK-18576-1-0
>>>
>>> Record-Route: <sip:scscf.sprout.ims.com;tran
>>> sport=TCP;lr;billing-role=charge-term>
>>>
>>> Record-Route: <sip:scscf.sprout. ims.com ;transport=TCP;lr;billing-role
>>> =charge-orig>
>>>
>>> Call-ID: 1-18576 at 127.0.1.1
>>>
>>> From: <sip:2010000042@ ims.com >;tag=18576SIPpTag001
>>>
>>> To: <sip:2010000015@ ims.co>;tag=z9hG4bKPj1Lm9whhQM
>>> slKrcZxnN6qCH0tb9Lj5Neu
>>>
>>> CSeq: 1 INVITE
>>>
>>> P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com
>>> ;term-ioi= ims.com
>>>
>>> P-Charging-Function-Addresses: ccf=0.0.0.0
>>>
>>> Content-Length:  0
>>>
>>>
>>>
>>>
>>>
>>> *[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt*
>>>
>>> --start msg--
>>>
>>>
>>>
>>> SIP/2.0 200 OK
>>>
>>> Via: SIP/2.0/TCP 10.224.61.22;rport=49294;recei
>>> ved=10.224.61.22;branch=z9hG4bK-670172
>>>
>>> Call-ID: poll-sip-670172
>>>
>>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670172
>>>
>>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670172
>>>
>>> CSeq: 670172 OPTIONS
>>>
>>> Content-Length:  0
>>>
>>>
>>>
>>>
>>>
>>> --end msg--
>>>
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>>>
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b
>>> Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
>>>
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> thread_dispatcher.cpp:270: Worker thread completed processing message
>>> 0x7f34ec34a3e8
>>>
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> thread_dispatcher.cpp:284: Request latency = 254us
>>>
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
>>>
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
>>>
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217:
>>> Rate adjustment calculation inputs: err -0.981500, smoothed latency 185,
>>> target latency 10000
>>>
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302:
>>> Maximum incoming request rate/second unchanged at 2000.000000 (current
>>> request rate is 0.200000 requests/sec, minimum threshold for a change is
>>> 1000.000000 requests/sec).
>>>
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
>>> 2000ui into continuous accumulator statistic
>>>
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
>>> 2000ui into continuous accumulator statistic
>>>
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed
>>> IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
>>>
>>> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
>>> thread_dispatcher.cpp:158: Attempting to process queue element
>>>
>>> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP connection closed
>>>
>>> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug
>>> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>>>
>>> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>>>
>>> 20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning:
>>> Failed to connect to the agentx master agent ([NIL]):
>>>
>>> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054
>>> TCP listener 10.224.61.22:5054: got incoming TCP connection from
>>> 10.224.61.22:42848, sock=573
>>>
>>> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> tcp->base.local_name: 10.224.61.22
>>>
>>> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP server transport created
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
>>> Processing incoming message: Request msg OPTIONS/cseq=670180
>>> (rdata0x7f34ec027690)
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose
>>> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180
>>> (rdata0x7f34ec027690) from TCP 10.224.61.22:42848:
>>>
>>> --start msg--
>>>
>>>
>>>
>>> OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
>>>
>>> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
>>>
>>> Max-Forwards: 2
>>>
>>> To: <sip:poll-sip at 10.224.61.22:5054>
>>>
>>> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670180
>>>
>>> Call-ID: poll-sip-670180
>>>
>>> CSeq: 670180 OPTIONS
>>>
>>> Contact: <sip:10.224.61.22>
>>>
>>> Accept: application/sdp
>>>
>>> Content-Length: 0
>>>
>>> User-Agent: poll-sip
>>>
>>>
>>>
>>>
>>>
>>> --end msg--
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
>>> home domain: false, local_to_node: true, is_gruu: false,
>>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
>>> Classified URI as 3
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
>>> 0x7f34ec34a3e8
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
>>> for worker threads with priority 15
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added
>>> IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> thread_dispatcher.cpp:183: Request latency so far = 57us
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c
>>> Distributing rdata to modules: Request msg OPTIONS/cseq=670180
>>> (rdata0x7f34ec34a3e8)
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139:
>>> home domain: false, local_to_node: true, is_gruu: false,
>>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172:
>>> Classified URI as 3
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint
>>> Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose
>>> common_sip_processing.cpp:103: TX 282 bytes Response msg
>>> 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848:
>>>
>>> --start msg--
>>>
>>>
>>>
>>> SIP/2.0 200 OK
>>>
>>> Via: SIP/2.0/TCP 10.224.61.22;rport=42848;recei
>>> ved=10.224.61.22;branch=z9hG4bK-670180
>>>
>>> Call-ID: poll-sip-670180
>>>
>>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670180
>>>
>>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670180
>>>
>>> CSeq: 670180 OPTIONS
>>>
>>> Content-Length:  0
>>>
>>>
>>>
>>>
>>>
>>> --end msg--
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809
>>> Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> thread_dispatcher.cpp:270: Worker thread completed processing message
>>> 0x7f34ec34a3e8
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> thread_dispatcher.cpp:284: Request latency = 129us
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341:
>>> Not recalculating rate as we haven't processed 20 requests yet (only 1).
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed
>>> IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
>>>
>>> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
>>> thread_dispatcher.cpp:158: Attempting to process queue element
>>>
>>> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327:
>>> Process request for URL /ping, args (null)
>>>
>>> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68:
>>> Sending response 200 to request for URL /ping, args (null)
>>>
>>> 20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP connection closed
>>>
>>> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug
>>> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>>>
>>> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>>>
>>> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053
>>> TCP listener 10.224.61.22:5053: got incoming TCP connection from
>>> 10.224.61.22:49356, sock=573
>>>
>>> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> tcp->base.local_name: 10.224.61.22
>>>
>>> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP server transport created
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
>>> Processing incoming message: Request msg OPTIONS/cseq=670182
>>> (rdata0x7f34ec027690)
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose
>>> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182
>>> (rdata0x7f34ec027690) from TCP 10.224.61.22:49356:
>>>
>>> --start msg--
>>>
>>>
>>>
>>> OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
>>>
>>> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
>>>
>>> Max-Forwards: 2
>>>
>>> To: <sip:poll-sip at 10.224.61.22:5053>
>>>
>>> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670182
>>>
>>> Call-ID: poll-sip-670182
>>>
>>> CSeq: 670182 OPTIONS
>>>
>>> Contact: <sip:10.224.61.22>
>>>
>>> Accept: application/sdp
>>>
>>> Content-Length: 0
>>>
>>> User-Agent: poll-sip
>>>
>>>
>>>
>>>
>>>
>>> --end msg--
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
>>> home domain: false, local_to_node: true, is_gruu: false,
>>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
>>> Classified URI as 3
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
>>> 0x7f34ec34a3e8
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
>>> for worker threads with priority 15
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added
>>> IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> thread_dispatcher.cpp:183: Request latency so far = 102us
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c
>>> Distributing rdata to modules: Request msg OPTIONS/cseq=670182
>>> (rdata0x7f34ec34a3e8)
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139:
>>> home domain: false, local_to_node: true, is_gruu: false,
>>> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172:
>>> Classified URI as 3
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint
>>> Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose
>>> common_sip_processing.cpp:103: TX 282 bytes Response msg
>>> 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356:
>>>
>>> --start msg--
>>>
>>>
>>>
>>> SIP/2.0 200 OK
>>>
>>> Via: SIP/2.0/TCP 10.224.61.22;rport=49356;recei
>>> ved=10.224.61.22;branch=z9hG4bK-670182
>>>
>>> Call-ID: poll-sip-670182
>>>
>>> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670182
>>>
>>> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670182
>>>
>>> CSeq: 670182 OPTIONS
>>>
>>> Content-Length:  0
>>>
>>>
>>>
>>>
>>>
>>> --end msg--
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00
>>> Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> thread_dispatcher.cpp:270: Worker thread completed processing message
>>> 0x7f34ec34a3e8
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> thread_dispatcher.cpp:284: Request latency = 232us
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341:
>>> Not recalculating rate as we haven't processed 20 requests yet (only 2).
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed
>>> IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
>>>
>>> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
>>> thread_dispatcher.cpp:158: Attempting to process queue element
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244:
>>> Reraising all alarms with a known state
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1001.1 alarm
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1005.1 alarm
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1011.1 alarm
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1012.1 alarm
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1013.1 alarm
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1004.1 alarm
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1002.1 alarm
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1009.1 alarm
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
>>> AlarmReqAgent: queue overflowed
>>>
>>> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
>>> issued 1010.1 alarm
>>>
>>> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP connection closed
>>>
>>> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug
>>> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>>>
>>> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
>>> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> all calls are failing I don't know what is going on, I am newbie to CW
>>> please guide some solution it will be great help.
>>>
>>>
>>>
>>>
>>>
>>> Thanks,
>>>
>>> Sunil
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Clearwater mailing list
>>> Clearwater at lists.projectclearwater.org
>>> http://lists.projectclearwater.org/mailman/listinfo/clearwat
>>> er_lists.projectclearwater.org
>>>
>>>
>>>
>>> _______________________________________________
>>> Clearwater mailing list
>>> Clearwater at lists.projectclearwater.org
>>> http://lists.projectclearwater.org/mailman/listinfo/clearwat
>>> er_lists.projectclearwater.org
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Clearwater mailing list
>>> Clearwater at lists.projectclearwater.org
>>> http://lists.projectclearwater.org/mailman/listinfo/clearwat
>>> er_lists.projectclearwater.org
>>>
>>>
>>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180328/297e05ea/attachment.html>

From wangwulin at hotmail.com  Wed Mar 28 02:40:00 2018
From: wangwulin at hotmail.com (wang wulin)
Date: Wed, 28 Mar 2018 06:40:00 +0000
Subject: [Project Clearwater] [clearwater] Some calls failed with 480 when
 do SIPp testing
Message-ID: <HK0PR03MB2755EBE5561B41362DC44420B9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>

Hi clearwater team,


The successful call rate is quite low when I do SIPp testing via the command: /usr/share/clearwater/bin/run_stress clearwater.opnfv 100 30 --sipp-output --icscf-target sprout.clearwater.local:5052 --scscf-target sprout.clearwater.local:5054

Actually after 7 successful calls, the 480 error occurs.

Here is one capture when runing:

------------------------------ Scenario Screen -------- [1-9]: Change Screen --
  Call-rate(length)   Port   Total-time  Total-calls  Remote-host
0.0(5000 ms)/1.000s   5062    1234.12 s           22  10.67.79.12:5054(TCP)

  0 new calls during 1.004 s period      1 ms scheduler resolution
  0 calls (limit 1)                      Peak was 1 calls, after 55 s
  1 Running, 3 Paused, 3 Woken up
  0 dead call msg (discarded)            0 out-of-call msg (discarded)
  2 open sockets

                                 Messages  Retrans   Timeout   Unexpected-Msg
      INVITE ---------->         22        0         0
         100 <----------         22        0         0         0
         183 <----------         12        0         0         10
       PRACK ---------->         12        0
         200 <----------         12        0         0         0
      UPDATE ---------->         12        0
         201 <----------         12        0         0         0
         180 <----------  E-RTD1 12        0         0         0
         201 <----------         0         0         0         0
         200 <----------         12        0         0         0
         ACK ---------->         12        0
       Pause [   5000ms]         12                            0
         BYE ---------->         12        0         0
         200 <----------         12        0         0         0

------ [+|-|*|/]: Adjust rate ---- [q]: Soft exit ---- [p]: Pause traffic -----

Last Error: Aborting call on unexpected message for Call-Id '21-14814 at 10...


2018-03-28      06:33:54.190938 1522218834.190938: Aborting call on unexpected message for Call-Id '17-14814 at 10.67.79.24': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.24:29014;received=10.67.79.24;branch=z9hG4bK-14814-17-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 17-14814 at 10.67.79.24
From: <sip:2010000012 at clearwater.opnfv>;tag=14814SIPpTag0017
To: <sip:2010000015 at clearwater.opnfv>;tag=z9hG4bK-14814-17-0
CSeq: 1 INVITE
P-Charging-Vector: icid-value="14814SIPpTag0017";orig-ioi=clearwater.opnfv;term-ioi=clearwater.opnfv
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0

'.



1) I deoloyed clearwater via one testcase named "cloudify_ims" from opnfv/functest project, where 3 steps are run:

   * deploy a VNF orchestrator (Cloudify)

   * deploy a Clearwater vIMS (IP Multimedia Subsystem) VNF from this orchestrator based on a TOSCA blueprint defined in [1]

   * run suite of signaling tests on top of this VNF

[1]: https://github.com/Orange-OpenSource/opnfv-cloudify-clearwater/archive/master.zip


8 instances are created and I also created a new instance named "stress-node" according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html-

bash-4.4# openstack server list
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+
| ID                                   | Name                                                  | Status | Networks                                                                              | Image                | Flavor    |
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+
| df2d9f82-8aa2-4514-9f07-27974267b590 | stress-node                                           | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.24                 | ubuntu_14.04         | m1.small  |
| 48a41da3-c55d-410c-a8f3-ef26bc409aef | server_clearwater-opnfv_bono_host_llp4mt              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.23, 192.168.36.113 | ubuntu_14.04         | m1.small  |
| a992435c-a397-4622-b0d3-b8e515ebab51 | server_clearwater-opnfv_homer_host_vz97vi             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.19                 | ubuntu_14.04         | m1.small  |
| 283fef46-78e4-4d0a-9e91-ffeb14e7bfb9 | server_clearwater-opnfv_sprout_host_nnoxye            | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.12                 | ubuntu_14.04         | m1.small  |
| bfe5111d-ed7d-4c65-80c2-20144935ee8c | server_clearwater-opnfv_ellis_host_bo7auh             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.14, 192.168.36.111 | ubuntu_14.04         | m1.small  |
| b83e1727-b5a6-430e-8b5e-b3f0e6421675 | server_clearwater-opnfv_vellum_host_sh0ocy            | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.6                  | ubuntu_14.04         | m1.small  |
| 8b03d8fa-5873-489b-994b-5f1de24a85c0 | server_clearwater-opnfv_bind_host_b9f49w              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.16, 192.168.36.110 | ubuntu_14.04         | m1.small  |
| 0ca72f55-7e05-4477-ad2e-2661ad49f7ce | server_clearwater-opnfv_dime_host_xryxen              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.11                 | ubuntu_14.04         | m1.small  |
| 7d51e479-24b6-4db4-9b8f-3ef42be0b87e | server_clearwater-opnfv_proxy_host_rqr7bo             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.5                  | ubuntu_14.04         | m1.small  |
| 82672984-b967-49ff-8f00-09ffe3f7fc87 | cloudify_manager-6c79129a-8384-4069-81f7-a024738102cd | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.13, 192.168.36.105 | cloudify_manager_4.0 | m1.medium |
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+


2) I checked  the ralf log

root at dime-au6gte:/var/log/ralf# vim ralf_20180328T060000Z.txt
28-03-2018 06:25:57.063 UTC Debug dnscachedresolver.cpp:543: Create and execute DNS query transaction

28-03-2018 06:25:57.063 UTC Debug dnscachedresolver.cpp:556: Wait for query responses
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:704: Received DNS response for _diameter._tcp.clearwater.opnfv type SRV
28-03-2018 06:25:57.064 UTC Warning dnscachedresolver.cpp:846: Failed to retrieve record for _diameter._tcp.clearwater.opnfv: Domain name not found
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:946: Adding _diameter._tcp.clearwater.opnfv to cache expiry list with deletion time of 1522218957
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:704: Received DNS response for _diameter._sctp.clearwater.opnfv type SRV
28-03-2018 06:25:57.064 UTC Warning dnscachedresolver.cpp:846: Failed to retrieve record for _diameter._sctp.clearwater.opnfv: Domain name not found
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:946: Adding _diameter._sctp.clearwater.opnfv to cache expiry list with deletion time of 1522218957
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:560: Received all query responses
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:588: Pulling 0 records from cache for _diameter._tcp.clearwater.opnfv SRV
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:588: Pulling 0 records from cache for _diameter._sctp.clearwater.opnfv SRV
28-03-2018 06:25:57.064 UTC Debug diameterresolver.cpp:131: TCP SRV record _diameter._tcp.clearwater.opnfv returned 0 records
28-03-2018 06:25:57.064 UTC Debug diameterresolver.cpp:134: SCTP SRV record _diameter._sctp.clearwater.opnfv returned 0 records
28-03-2018 06:25:57.064 UTC Error diameterstack.cpp:864: No Diameter peers have been found
28-03-2018 06:25:57.068 UTC Verbose httpstack.cpp:345: Process request for URL /ping, args (null)
28-03-2018 06:25:57.068 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /ping, args (null)
28-03-2018 06:25:58.027 UTC Verbose httpstack.cpp:345: Process request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:128: Handling request, body:
{
    "peers": {
        "ccf": [
            ""
        ]
    },
    "event": {
        "Accounting-Record-Type": 1,
        "Event-Timestamp": 1522218357,
        "Service-Information": {
            "Subscription-Id": [
                {
                    "Subscription-Id-Type": 2,
                    "Subscription-Id-Data": "sip:2010000020 at clearwater.opnfv"
                }
            ],
            "IMS-Information": {
                "Event-Type": {
                    "SIP-Method": "REGISTER",
                    "Expires": 3600
                },
                "Role-Of-Node": 0,
                "Node-Functionality": 1,
                "User-Session-Id": "2010000020///14990-8708 at 10.67.79.24",
                "Called-Party-Address": "sip:2010000020 at clearwater.opnfv",
                "Associated-URI": [
                    "sip:2010000020 at clearwater.opnfv"
                ],
                "Time-Stamps": {
                    "SIP-Request-Timestamp": 1522218357,
                    "SIP-Request-Timestamp-Fraction": 960,
                    "SIP-Response-Timestamp": 1522218357,
                    "SIP-Response-Timestamp-Fraction": 967
                },
                "IMS-Charging-Identifier": "",
                "Cause-Code": -1,
                "From-Address": "<sip:2010000020 at clearwater.opnfv>;tag=8708SIPpTag0014990",
                "Route-Header-Received": "<sip:clearwater.opnfv;transport=TCP;lr>",
                "Route-Header-Transmitted": "<sip:icscf.sprout.clearwater.local:5052;transport=TCP;lr;orig>",
                "Instance-Id": "<urn:uuid:00000000-0000-0000-0000-000000000001>"
            }
        }
    }
}
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:244: Adding CCF
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:82: Handle the received message
28-03-2018 06:25:58.027 UTC Debug peer_message_sender.cpp:84: Sending message to  (number 0)
28-03-2018 06:25:58.027 UTC Debug rf.cpp:63: Building an Accounting-Request
28-03-2018 06:25:58.027 UTC Debug freeDiameter: No Session-Id AVP found in message 0x7f764c008d50
28-03-2018 06:25:58.027 UTC Verbose diameterstack.cpp:1470: Sending Diameter message of type 271 on transaction 0x7f764c008d00
28-03-2018 06:25:58.027 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.027 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
28-03-2018 06:25:58.027 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm clearwater.opnfv
28-03-2018 06:25:58.027 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
28-03-2018 06:25:58.027 UTC Debug freeDiameter: Calling callback registered when query was sent (0x43cb30, 0x7f764c008d00)
28-03-2018 06:25:58.027 UTC Verbose diameterstack.cpp:1130: Got Diameter response of type 271 - calling callback on transaction 0x7f764c008d00
28-03-2018 06:25:58.027 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
28-03-2018 06:25:58.027 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
28-03-2018 06:25:58.027 UTC Warning session_manager.cpp:414: Session for 2010000020///14990-8708 at 10.67.79.24 received error from CDF
28-03-2018 06:25:58.032 UTC Verbose httpstack.cpp:345: Process request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:128: Handling request, body:
{
    "peers": {
        "ccf": [
            ""
        ]
    },
    "event": {
        "Accounting-Record-Type": 1,
        "Event-Timestamp": 1522218357,
        "Service-Information": {
            "Subscription-Id": [
                {
                    "Subscription-Id-Type": 2,
                    "Subscription-Id-Data": "sip:2010000021 at clearwater.opnfv"
                }
            ],
            "IMS-Information": {
                "Event-Type": {
                    "SIP-Method": "REGISTER",
                    "Expires": 3600
                },
                "Role-Of-Node": 0,
                "Node-Functionality": 1,
                "User-Session-Id": "2010000020///14990-8708 at 10.67.79.24",
                "Called-Party-Address": "sip:2010000021 at clearwater.opnfv",
                "Associated-URI": [
                    "sip:2010000021 at clearwater.opnfv"
                ],
                "Time-Stamps": {
                    "SIP-Request-Timestamp": 1522218357,
                    "SIP-Request-Timestamp-Fraction": 968,
                    "SIP-Response-Timestamp": 1522218357,
                    "SIP-Response-Timestamp-Fraction": 974
                },
                "IMS-Charging-Identifier": "",
                "Cause-Code": -1,
                "From-Address": "<sip:2010000021 at clearwater.opnfv>;tag=8708SIPpTag0014990",
                "Route-Header-Received": "<sip:clearwater.opnfv;transport=TCP;lr>",
                "Route-Header-Transmitted": "<sip:icscf.sprout.clearwater.local:5052;transport=TCP;lr;orig>",
                "Instance-Id": "<urn:uuid:00000000-0000-0000-0000-000000000001>"
            }
        }
    }
}
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:244: Adding CCF
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:82: Handle the received message
28-03-2018 06:25:58.032 UTC Debug peer_message_sender.cpp:84: Sending message to  (number 0)
28-03-2018 06:25:58.032 UTC Debug rf.cpp:63: Building an Accounting-Request
28-03-2018 06:25:58.032 UTC Debug freeDiameter: No Session-Id AVP found in message 0x7f764c001310
28-03-2018 06:25:58.032 UTC Verbose diameterstack.cpp:1470: Sending Diameter message of type 271 on transaction 0x7f764c006f30
28-03-2018 06:25:58.032 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.032 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
28-03-2018 06:25:58.032 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm clearwater.opnfv
28-03-2018 06:25:58.032 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
28-03-2018 06:25:58.032 UTC Debug freeDiameter: Calling callback registered when query was sent (0x43cb30, 0x7f764c006f30)
28-03-2018 06:25:58.032 UTC Verbose diameterstack.cpp:1130: Got Diameter response of type 271 - calling callback on transaction 0x7f764c006f30
28-03-2018 06:25:58.032 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
28-03-2018 06:25:58.032 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
28-03-2018 06:25:58.032 UTC Warning session_manager.cpp:414: Session for 2010000020///14990-8708 at 10.67.79.24 received error from CDF

Have I missed something in shared_config?

root at dime-au6gte:/var/log/ralf# cat /etc/clearwater/shared_config
# Deployment definitions
home_domain=clearwater.opnfv
sprout_hostname=sprout.clearwater.local
chronos_hostname=10.67.79.16:7253
hs_hostname=hs.clearwater.local:8888
hs_provisioning_hostname=hs-prov.clearwater.local:8889
sprout_impi_store=vellum.clearwater.local
sprout_registration_store=vellum.clearwater.local
cassandra_hostname=vellum.clearwater.local
chronos_hostname=vellum.clearwater.local
ralf_session_store=vellum.clearwater.local
ralf_hostname=ralf.clearwater.local:10888
xdms_hostname=homer.clearwater.local:7888
signaling_dns_server=10.67.79.16
snmp_ip=10.67.79.11
reg_max_expires=1800
bgcf=0



# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


root at dime-au6gte:~# cat /etc/clearwater/local_config
local_ip=10.67.79.11
public_ip=
public_hostname=dime-au6gte.clearwater.local



homestead seems fine except one line "Failed TWO read for get_row. Try ONE":

root at dime-au6gte:/var/log/homestead# vim homestead_20180328T060000Z.txt

28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:425: Attempt to parse vellum.clearwater.local as IP address
28-03-2018 06:30:58.045 UTC Verbose dnscachedresolver.cpp:486: Check cache for vellum.clearwater.local type 1
28-03-2018 06:30:58.045 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for vellum.clearwater.local A
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:231: Request for connection to IP: 10.67.79.6, port: 9160
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:244: Found existing connection 0x7fed8c0115a0 in pool
28-03-2018 06:30:58.045 UTC Debug cassandra_store.cpp:159: Generated Cassandra timestamp 1522218658045331
28-03-2018 06:30:58.045 UTC Debug cache.cpp:350: Issuing get for key sip:2010000021 at clearwater.opnfv
28-03-2018 06:30:58.045 UTC Debug cassandra_store.cpp:731: Failed TWO read for get_row. Try ONE
28-03-2018 06:30:58.045 UTC Debug cache.cpp:370: Retrieved XML column with TTL 0 and value <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription>
28-03-2018 06:30:58.045 UTC Debug cache.cpp:388: Retrieved is_registered column with value False and TTL 0
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1245: Got IMS subscription from cache
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1260: TTL for this database record is 0, IMS Subscription XML is not empty, registration state is UNREGISTERED, and the charging addresses are empty
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1510: Rejecting deregistration for user who was not registered
28-03-2018 06:30:58.045 UTC Verbose httpstack.cpp:93: Sending response 400 to request for URL /impu/sip%3A2010000021%40clearwater.opnfv/reg-data, args (null)
28-03-2018 06:30:58.046 UTC Debug cache.cpp:370: Retrieved XML column with TTL 0 and value <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription>
28-03-2018 06:30:58.046 UTC Debug cache.cpp:388: Retrieved is_registered column with value False and TTL 0
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1245: Got IMS subscription from cache
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1260: TTL for this database record is 0, IMS Subscription XML is not empty, registration state is UNREGISTERED, and the charging addresses are empty
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1286: Subscriber registering with new binding
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1457: Handling initial registration
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1654: Attempting to cache IMS subscription for public IDs
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1658: Got public IDs to cache against - doing it
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1663: Public ID sip:2010000021 at clearwater.opnfv
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:159: Generated Cassandra timestamp 1522218658046440
28-03-2018 06:30:58.046 UTC Debug a_record_resolver.cpp:80: ARecordResolver::resolve_iter for host vellum.clearwater.local, port 9160, family 2
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:425: Attempt to parse vellum.clearwater.local as IP address
28-03-2018 06:30:58.046 UTC Verbose dnscachedresolver.cpp:486: Check cache for vellum.clearwater.local type 1
28-03-2018 06:30:58.046 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for vellum.clearwater.local A
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:231: Request for connection to IP: 10.67.79.6, port: 9160
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:244: Found existing connection 0x7fed8c0115a0 in pool
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:612: Constructing cassandra put request with timestamp 1522218658046440 and per-column TTLs
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:629:   ims_subscription_xml => <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription> (TTL 0)
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:629:   is_registered =>  (TTL 0)
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:650: Executing put request operation
28-03-2018 06:30:58.047 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.047 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.047 UTC Debug handlers.cpp:1567: Sending 200 response (body was {"reqtype": "reg", "server_name": "sip:scscf.sprout.clearwater.local:5054;transport=TCP"})
28-03-2018 06:30:58.047 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /impu/sip%3A2010000021%40clearwater.opnfv/reg-data, args private_id=2010000021%40clearwater.opnfv



The error about "Could not get subscriber data from HSS" on Sprout node occurred sometimes.

root at sprout-2tl8g3:/var/log/sprout# vim sprout_20180328T060000Z.txt

28-03-2018 06:36:11.741 UTC Error httpclient.cpp:712: cURL failure with cURL error code 0 (see man 3 libcurl-errors) and HTTP error code 400
28-03-2018 06:36:11.741 UTC Error hssconnection.cpp:704: Could not get subscriber data from HSS
28-03-2018 06:36:25.875 UTC Status alarm.cpp:62: sprout issued 1004.1 alarm




root at dime-au6gte:/var/log/homestead# monit summary
Monit 5.18.1 uptime: 2h 27m
 Service Name                     Status                      Type
 node-dime-au6gte.clearwater....  Running                     System
 snmpd_process                    Running                     Process
 ralf_process                     Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 ralf_uptime                      Status ok                   Program
 poll_ralf                        Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program



Any help/suggestion would be much appreciated.



Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180328/ae0bb7b5/attachment.html>

From skgola1997 at gmail.com  Wed Mar 28 05:09:31 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Wed, 28 Mar 2018 14:39:31 +0530
Subject: [Project Clearwater] Very less call per sec
Message-ID: <CAHwYWpCBAi_cWW6CENdu9Rsz3swLEUZWFkO2hq0SA57WEegOeQ@mail.gmail.com>

Hi all,

I am running stress testing for 1000 subscribers and for 1 min duration.
When i am seeing the sipp output the c*all per sec is very less, and there
is showing limit 2 ( 0 calls (limit 2) ). *
Is there any way to increase the # of calls per sec.

------------------------------ Scenario Screen -------- [1-9]: Change
Screen --
  Call-rate(length)   Port   Total-time  Total-calls  Remote-host
0.2(5000 ms)/1.000s   5061      62.76 s           10  sprout.iind.:5054(TCP)

  Call limit reached (-m 10), 0.000 s period  0 ms scheduler resolution
  *0 calls (limit 2) *                     *Peak was 2 calls, after 11 s*
  0 Running, 9 Paused, 0 Woken up
  0 dead call msg (discarded)            0 out-of-call msg (discarded)
  1 open sockets

                                 Messages  Retrans   Timeout
 Unexpected-Msg
      INVITE ---------->         10        0         0
         100 <----------         10        0         0         0
         183 <----------         10        0         0         0
       PRACK ---------->         10        0
         200 <----------         10        0         0         0
      UPDATE ---------->         10        0
         201 <----------         9         0         0         0
         180 <----------  E-RTD1 10        0         0         0
         201 <----------         1         0         0         0
         200 <----------         10        0         0         0
         ACK ---------->         10        0
       Pause [   5000ms]         10                            0
         BYE ---------->         10        0         0
         200 <----------         10        0         0         0

------------------------------ Test Terminated
--------------------------------


----------------------------- Statistics Screen ------- [1-9]: Change
Screen --
  Start Time             | 2018-03-28   20:41:49.298290 1522249909.298290
  Last Reset Time        | 2018-03-28   20:42:52.067656 1522249972.067656
  Current Time           | 2018-03-28   20:42:52.069419 1522249972.069419
-------------------------+---------------------------+--------------------------
  Counter Name           | Periodic value            | Cumulative value
-------------------------+---------------------------+--------------------------
  Elapsed Time           | 00:00:00:001000           | 00:01:02:771000
  Call Rate              |    0.000 cps              |   * 0.159 cps*
-------------------------+---------------------------+--------------------------
  Incoming call created  |        0                  |        0
  OutGoing call created  |        0                  |       10
  Total Call created     |                          |       10
  Current Call           |        0                  |
-------------------------+---------------------------+--------------------------
  Successful call        |        0                  |       10
  Failed call            |        0                  |        0
-------------------------+---------------------------+--------------------------
  Response Time 1        | 00:00:00:000000           | 00:00:00:100000
  Call Length            | 00:00:00:000000           | 00:00:07:381000
------------------------------ Test Terminated
--------------------------------


Thanks,
Kumar
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180328/26dfe258/attachment.html>

From Robert.Day at metaswitch.com  Wed Mar 28 05:41:14 2018
From: Robert.Day at metaswitch.com (Robert Day)
Date: Wed, 28 Mar 2018 09:41:14 +0000
Subject: [Project Clearwater] Increasing the no. of nodes
In-Reply-To: <CAHwYWpAOhwWn+WSS6pTpu=4S9=Y5zvz+rD8gZ82cLPW_2ph+Pg@mail.gmail.com>
References: <CAHwYWpAOhwWn+WSS6pTpu=4S9=Y5zvz+rD8gZ82cLPW_2ph+Pg@mail.gmail.com>
Message-ID: <BY2PR02MB2149F417DC75A47924CCA430F4A30@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Sunil,

Instructions for installing large deployments are in our documentation at http://clearwater.readthedocs.io/en/stable/Manual_Install.html#larger-scale-deployments. You can either:

  *   Install a small deployment manually following http://clearwater.readthedocs.io/en/stable/Manual_Install.html (which it sounds like you?ve already done) then grow it following http://clearwater.readthedocs.io/en/stable/Clearwater_Elastic_Scaling.html#if-you-did-a-manual-install
  *   Alternatively, reinstall using our automated install process (http://clearwater.readthedocs.io/en/stable/Automated_Install.html) and then follow the simpler resize commands at http://clearwater.readthedocs.io/en/stable/Clearwater_Elastic_Scaling.html#if-you-did-an-automated-install.

Best,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sunil Kumar
Sent: 27 March 2018 19:19
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Increasing the no. of nodes

Hi all,
I have manually installed clearwater with single node. Now i want to increase the no. of nodes for sprout and for others also in the same setup, can you please guide me where i have to make changes. Is it possible or not?


Thanks
sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180328/40986526/attachment.html>

From wangwulin at huawei.com  Wed Mar 28 06:03:18 2018
From: wangwulin at huawei.com (Wangwulin (Linda))
Date: Wed, 28 Mar 2018 10:03:18 +0000
Subject: [Project Clearwater] [clearwater] Some calls failed with 480
 when do SIPp testing
In-Reply-To: <HK0PR03MB27553BFBF07E1FA23D56D81BB9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>
References: <HK0PR03MB2755EBE5561B41362DC44420B9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>,
	<HK0PR03MB2755B712AF0925DFBE3F0070B9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>
	<HK0PR03MB27553BFBF07E1FA23D56D81BB9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>
Message-ID: <3DE744999C06324789FFFC2A0230599701A01D09@dggemi502-mbs.china.huawei.com>

Hi,

When I set an additional param ?--multiplier 10? or other numbers (not the default 1), all the calls succeeded.

/usr/share/clearwater/bin/run_stress clearwater.opnfv 1000 10 --sipp-output --icscf-target 10.67.79.12:5052 --scscf-target 10.67.79.12:5054 --base-number 2010000000 --multiplier 10  --ccf 10.67.79.11

Total calls: 1083
Successful calls: 1083 (100.0%)
Failed calls: 0 (0.0%)
Unfinished calls: 0

Two questions:
1) What is ?multiplier? used for?
2) What does ?ccf? refer to? Actually the ip of ccf here is where the ralf service runs on my deployment. Is it necessary here?


Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com<mailto:wangwulin at hotmail.com>>
????: 2018?3?28? 14:40
???: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
??: [clearwater] Some calls failed with 480 when do SIPp testing


Hi clearwater team,



The successful call rate is quite low when I do SIPp testing via the command: /usr/share/clearwater/bin/run_stress clearwater.opnfv 100 30 --sipp-output --icscf-target sprout.clearwater.local:5052 --scscf-target sprout.clearwater.local:5054

Actually after 7 successful calls, the 480 error occurs.

Here is one capture when runing:

------------------------------ Scenario Screen -------- [1-9]: Change Screen --
  Call-rate(length)   Port   Total-time  Total-calls  Remote-host
0.0(5000 ms)/1.000s   5062    1234.12 s           22  10.67.79.12:5054(TCP)

  0 new calls during 1.004 s period      1 ms scheduler resolution
  0 calls (limit 1)                      Peak was 1 calls, after 55 s
  1 Running, 3 Paused, 3 Woken up
  0 dead call msg (discarded)            0 out-of-call msg (discarded)
  2 open sockets

                                 Messages  Retrans   Timeout   Unexpected-Msg
      INVITE ---------->         22        0         0
         100 <----------         22        0         0         0
         183 <----------         12        0         0         10
       PRACK ---------->         12        0
         200 <----------         12        0         0         0
      UPDATE ---------->         12        0
         201 <----------         12        0         0         0
         180 <----------  E-RTD1 12        0         0         0
         201 <----------         0         0         0         0
         200 <----------         12        0         0         0
         ACK ---------->         12        0
       Pause [   5000ms]         12                            0
         BYE ---------->         12        0         0
         200 <----------         12        0         0         0

------ [+|-|*|/]: Adjust rate ---- [q]: Soft exit ---- [p]: Pause traffic -----

Last Error: Aborting call on unexpected message for Call-Id '21-14814 at 10...


2018-03-28      06:33:54.190938 1522218834.190938: Aborting call on unexpected message for Call-Id '17-14814 at 10.67.79.24': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.24:29014;received=10.67.79.24;branch=z9hG4bK-14814-17-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 17-14814 at 10.67.79.24<mailto:17-14814 at 10.67.79.24>
From: <sip:2010000012 at clearwater.opnfv>;tag=14814SIPpTag0017
To: <sip:2010000015 at clearwater.opnfv>;tag=z9hG4bK-14814-17-0
CSeq: 1 INVITE
P-Charging-Vector: icid-value="14814SIPpTag0017";orig-ioi=clearwater.opnfv;term-ioi=clearwater.opnfv
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0

'.




1) I deoloyed clearwater via one testcase named "cloudify_ims" from opnfv/functest project, where 3 steps are run:

   * deploy a VNF orchestrator (Cloudify)

   * deploy a Clearwater vIMS (IP Multimedia Subsystem) VNF from this orchestrator based on a TOSCA blueprint defined in [1]
   * run suite of signaling tests on top of this VNF

[1]: https://github.com/Orange-OpenSource/opnfv-cloudify-clearwater/archive/master.zip



8 instances are created and I also created a new instance named "stress-node" according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html-
bash-4.4# openstack server list
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+
| ID                                   | Name                                                  | Status | Networks                                                                              | Image                | Flavor    |
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+
| df2d9f82-8aa2-4514-9f07-27974267b590 | stress-node                                           | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.24                 | ubuntu_14.04         | m1.small  |
| 48a41da3-c55d-410c-a8f3-ef26bc409aef | server_clearwater-opnfv_bono_host_llp4mt              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.23, 192.168.36.113 | ubuntu_14.04         | m1.small  |
| a992435c-a397-4622-b0d3-b8e515ebab51 | server_clearwater-opnfv_homer_host_vz97vi             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.19                 | ubuntu_14.04         | m1.small  |
| 283fef46-78e4-4d0a-9e91-ffeb14e7bfb9 | server_clearwater-opnfv_sprout_host_nnoxye            | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.12                 | ubuntu_14.04         | m1.small  |
| bfe5111d-ed7d-4c65-80c2-20144935ee8c | server_clearwater-opnfv_ellis_host_bo7auh             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.14, 192.168.36.111 | ubuntu_14.04         | m1.small  |
| b83e1727-b5a6-430e-8b5e-b3f0e6421675 | server_clearwater-opnfv_vellum_host_sh0ocy            | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.6                  | ubuntu_14.04         | m1.small  |
| 8b03d8fa-5873-489b-994b-5f1de24a85c0 | server_clearwater-opnfv_bind_host_b9f49w              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.16, 192.168.36.110 | ubuntu_14.04         | m1.small  |
| 0ca72f55-7e05-4477-ad2e-2661ad49f7ce | server_clearwater-opnfv_dime_host_xryxen              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.11                 | ubuntu_14.04         | m1.small  |
| 7d51e479-24b6-4db4-9b8f-3ef42be0b87e | server_clearwater-opnfv_proxy_host_rqr7bo             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.5                  | ubuntu_14.04         | m1.small  |
| 82672984-b967-49ff-8f00-09ffe3f7fc87 | cloudify_manager-6c79129a-8384-4069-81f7-a024738102cd | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.13, 192.168.36.105 | cloudify_manager_4.0 | m1.medium |
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+


2) I checked  the ralf log
root at dime-au6gte:/var/log/ralf# vim ralf_20180328T060000Z.txt
28-03-2018 06:25:57.063 UTC Debug dnscachedresolver.cpp:543: Create and execute DNS query transaction
28-03-2018 06:25:57.063 UTC Debug dnscachedresolver.cpp:556: Wait for query responses
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:704: Received DNS response for _diameter._tcp.clearwater.opnfv type SRV
28-03-2018 06:25:57.064 UTC Warning dnscachedresolver.cpp:846: Failed to retrieve record for _diameter._tcp.clearwater.opnfv: Domain name not found
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:946: Adding _diameter._tcp.clearwater.opnfv to cache expiry list with deletion time of 1522218957
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:704: Received DNS response for _diameter._sctp.clearwater.opnfv type SRV
28-03-2018 06:25:57.064 UTC Warning dnscachedresolver.cpp:846: Failed to retrieve record for _diameter._sctp.clearwater.opnfv: Domain name not found
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:946: Adding _diameter._sctp.clearwater.opnfv to cache expiry list with deletion time of 1522218957
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:560: Received all query responses
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:588: Pulling 0 records from cache for _diameter._tcp.clearwater.opnfv SRV
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:588: Pulling 0 records from cache for _diameter._sctp.clearwater.opnfv SRV
28-03-2018 06:25:57.064 UTC Debug diameterresolver.cpp:131: TCP SRV record _diameter._tcp.clearwater.opnfv returned 0 records
28-03-2018 06:25:57.064 UTC Debug diameterresolver.cpp:134: SCTP SRV record _diameter._sctp.clearwater.opnfv returned 0 records
28-03-2018 06:25:57.064 UTC Error diameterstack.cpp:864: No Diameter peers have been found
28-03-2018 06:25:57.068 UTC Verbose httpstack.cpp:345: Process request for URL /ping, args (null)
28-03-2018 06:25:57.068 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /ping, args (null)
28-03-2018 06:25:58.027 UTC Verbose httpstack.cpp:345: Process request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:128: Handling request, body:
{
    "peers": {
        "ccf": [
            ""
        ]
    },
    "event": {
        "Accounting-Record-Type": 1,
        "Event-Timestamp": 1522218357,
        "Service-Information": {
            "Subscription-Id": [
                {
                    "Subscription-Id-Type": 2,
                    "Subscription-Id-Data": "sip:2010000020 at clearwater.opnfv"
                }
            ],
            "IMS-Information": {
                "Event-Type": {
                    "SIP-Method": "REGISTER",
                    "Expires": 3600
                },
                "Role-Of-Node": 0,
                "Node-Functionality": 1,
                "User-Session-Id": "2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24>",
                "Called-Party-Address": "sip:2010000020 at clearwater.opnfv",
                "Associated-URI": [
                    "sip:2010000020 at clearwater.opnfv"
                ],
                "Time-Stamps": {
                    "SIP-Request-Timestamp": 1522218357,
                    "SIP-Request-Timestamp-Fraction": 960,
                    "SIP-Response-Timestamp": 1522218357,
                    "SIP-Response-Timestamp-Fraction": 967
                },
                "IMS-Charging-Identifier": "",
                "Cause-Code": -1,
                "From-Address": "<sip:2010000020 at clearwater.opnfv>;tag=8708SIPpTag0014990",
                "Route-Header-Received": "<sip:clearwater.opnfv;transport=TCP;lr>",
                "Route-Header-Transmitted": "<sip:icscf.sprout.clearwater.local:5052;transport=TCP;lr;orig>",
                "Instance-Id": "<urn:uuid:00000000-0000-0000-0000-000000000001>"
            }
        }
    }
}
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:244: Adding CCF
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:82: Handle the received message
28-03-2018 06:25:58.027 UTC Debug peer_message_sender.cpp:84: Sending message to  (number 0)
28-03-2018 06:25:58.027 UTC Debug rf.cpp:63: Building an Accounting-Request
28-03-2018 06:25:58.027 UTC Debug freeDiameter: No Session-Id AVP found in message 0x7f764c008d50
28-03-2018 06:25:58.027 UTC Verbose diameterstack.cpp:1470: Sending Diameter message of type 271 on transaction 0x7f764c008d00
28-03-2018 06:25:58.027 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.027 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
28-03-2018 06:25:58.027 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm clearwater.opnfv
28-03-2018 06:25:58.027 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
28-03-2018 06:25:58.027 UTC Debug freeDiameter: Calling callback registered when query was sent (0x43cb30, 0x7f764c008d00)
28-03-2018 06:25:58.027 UTC Verbose diameterstack.cpp:1130: Got Diameter response of type 271 - calling callback on transaction 0x7f764c008d00
28-03-2018 06:25:58.027 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
28-03-2018 06:25:58.027 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
28-03-2018 06:25:58.027 UTC Warning session_manager.cpp:414: Session for 2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24> received error from CDF
28-03-2018 06:25:58.032 UTC Verbose httpstack.cpp:345: Process request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:128: Handling request, body:
{
    "peers": {
        "ccf": [
            ""
        ]
    },
    "event": {
        "Accounting-Record-Type": 1,
        "Event-Timestamp": 1522218357,
        "Service-Information": {
            "Subscription-Id": [
                {
                    "Subscription-Id-Type": 2,
                    "Subscription-Id-Data": "sip:2010000021 at clearwater.opnfv"
                }
            ],
            "IMS-Information": {
                "Event-Type": {
                    "SIP-Method": "REGISTER",
                    "Expires": 3600
                },
                "Role-Of-Node": 0,
                "Node-Functionality": 1,
                "User-Session-Id": "2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24>",
                "Called-Party-Address": "sip:2010000021 at clearwater.opnfv",
                "Associated-URI": [
                    "sip:2010000021 at clearwater.opnfv"
                ],
                "Time-Stamps": {
                    "SIP-Request-Timestamp": 1522218357,
                    "SIP-Request-Timestamp-Fraction": 968,
                    "SIP-Response-Timestamp": 1522218357,
                    "SIP-Response-Timestamp-Fraction": 974
                },
                "IMS-Charging-Identifier": "",
                "Cause-Code": -1,
                "From-Address": "<sip:2010000021 at clearwater.opnfv>;tag=8708SIPpTag0014990",
                "Route-Header-Received": "<sip:clearwater.opnfv;transport=TCP;lr>",
                "Route-Header-Transmitted": "<sip:icscf.sprout.clearwater.local:5052;transport=TCP;lr;orig>",
                "Instance-Id": "<urn:uuid:00000000-0000-0000-0000-000000000001>"
            }
        }
    }
}
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:244: Adding CCF
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:82: Handle the received message
28-03-2018 06:25:58.032 UTC Debug peer_message_sender.cpp:84: Sending message to  (number 0)
28-03-2018 06:25:58.032 UTC Debug rf.cpp:63: Building an Accounting-Request
28-03-2018 06:25:58.032 UTC Debug freeDiameter: No Session-Id AVP found in message 0x7f764c001310
28-03-2018 06:25:58.032 UTC Verbose diameterstack.cpp:1470: Sending Diameter message of type 271 on transaction 0x7f764c006f30
28-03-2018 06:25:58.032 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.032 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
28-03-2018 06:25:58.032 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm clearwater.opnfv
28-03-2018 06:25:58.032 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
28-03-2018 06:25:58.032 UTC Debug freeDiameter: Calling callback registered when query was sent (0x43cb30, 0x7f764c006f30)
28-03-2018 06:25:58.032 UTC Verbose diameterstack.cpp:1130: Got Diameter response of type 271 - calling callback on transaction 0x7f764c006f30
28-03-2018 06:25:58.032 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
28-03-2018 06:25:58.032 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
28-03-2018 06:25:58.032 UTC Warning session_manager.cpp:414: Session for 2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24> received error from CDF

Have I missed something in shared_config?

root at dime-au6gte:/var/log/ralf# cat /etc/clearwater/shared_config
# Deployment definitions
home_domain=clearwater.opnfv
sprout_hostname=sprout.clearwater.local
chronos_hostname=10.67.79.16:7253
hs_hostname=hs.clearwater.local:8888
hs_provisioning_hostname=hs-prov.clearwater.local:8889
sprout_impi_store=vellum.clearwater.local
sprout_registration_store=vellum.clearwater.local
cassandra_hostname=vellum.clearwater.local
chronos_hostname=vellum.clearwater.local
ralf_session_store=vellum.clearwater.local
ralf_hostname=ralf.clearwater.local:10888
xdms_hostname=homer.clearwater.local:7888
signaling_dns_server=10.67.79.16
snmp_ip=10.67.79.11
reg_max_expires=1800
bgcf=0



# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


root at dime-au6gte:~# cat /etc/clearwater/local_config
local_ip=10.67.79.11
public_ip=
public_hostname=dime-au6gte.clearwater.local



homestead seems fine except one line "Failed TWO read for get_row. Try ONE":

root at dime-au6gte:/var/log/homestead# vim homestead_20180328T060000Z.txt
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:425: Attempt to parse vellum.clearwater.local as IP address
28-03-2018 06:30:58.045 UTC Verbose dnscachedresolver.cpp:486: Check cache for vellum.clearwater.local type 1
28-03-2018 06:30:58.045 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for vellum.clearwater.local A
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:231: Request for connection to IP: 10.67.79.6, port: 9160
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:244: Found existing connection 0x7fed8c0115a0 in pool
28-03-2018 06:30:58.045 UTC Debug cassandra_store.cpp:159: Generated Cassandra timestamp 1522218658045331
28-03-2018 06:30:58.045 UTC Debug cache.cpp:350: Issuing get for key sip:2010000021 at clearwater.opnfv
28-03-2018 06:30:58.045 UTC Debug cassandra_store.cpp:731: Failed TWO read for get_row. Try ONE
28-03-2018 06:30:58.045 UTC Debug cache.cpp:370: Retrieved XML column with TTL 0 and value <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>>
28-03-2018 06:30:58.045 UTC Debug cache.cpp:388: Retrieved is_registered column with value False and TTL 0
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1245: Got IMS subscription from cache
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1260: TTL for this database record is 0, IMS Subscription XML is not empty, registration state is UNREGISTERED, and the charging addresses are empty
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1510: Rejecting deregistration for user who was not registered
28-03-2018 06:30:58.045 UTC Verbose httpstack.cpp:93: Sending response 400 to request for URL /impu/sip%3A2010000021%40clearwater.opnfv/reg-data, args (null)
28-03-2018 06:30:58.046 UTC Debug cache.cpp:370: Retrieved XML column with TTL 0 and value <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>>
28-03-2018 06:30:58.046 UTC Debug cache.cpp:388: Retrieved is_registered column with value False and TTL 0
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1245: Got IMS subscription from cache
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1260: TTL for this database record is 0, IMS Subscription XML is not empty, registration state is UNREGISTERED, and the charging addresses are empty
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1286: Subscriber registering with new binding
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1457: Handling initial registration
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1654: Attempting to cache IMS subscription for public IDs
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1658: Got public IDs to cache against - doing it
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1663: Public ID sip:2010000021 at clearwater.opnfv
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:159: Generated Cassandra timestamp 1522218658046440
28-03-2018 06:30:58.046 UTC Debug a_record_resolver.cpp:80: ARecordResolver::resolve_iter for host vellum.clearwater.local, port 9160, family 2
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:425: Attempt to parse vellum.clearwater.local as IP address
28-03-2018 06:30:58.046 UTC Verbose dnscachedresolver.cpp:486: Check cache for vellum.clearwater.local type 1
28-03-2018 06:30:58.046 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for vellum.clearwater.local A
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:231: Request for connection to IP: 10.67.79.6, port: 9160
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:244: Found existing connection 0x7fed8c0115a0 in pool
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:612: Constructing cassandra put request with timestamp 1522218658046440 and per-column TTLs
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:629:   ims_subscription_xml => <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>> (TTL 0)
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:629:   is_registered =>  (TTL 0)
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:650: Executing put request operation
28-03-2018 06:30:58.047 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.047 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.047 UTC Debug handlers.cpp:1567: Sending 200 response (body was {"reqtype": "reg", "server_name": "sip:scscf.sprout.clearwater.local:5054;transport=TCP"})
28-03-2018 06:30:58.047 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /impu/sip%3A2010000021%40clearwater.opnfv/reg-data, args private_id=2010000021%40clearwater.opnfv



The error about "Could not get subscriber data from HSS" on Sprout node occurred sometimes.

root at sprout-2tl8g3:/var/log/sprout# vim sprout_20180328T060000Z.txt
28-03-2018 06:36:11.741 UTC Error httpclient.cpp:712: cURL failure with cURL error code 0 (see man 3 libcurl-errors) and HTTP error code 400
28-03-2018 06:36:11.741 UTC Error hssconnection.cpp:704: Could not get subscriber data from HSS
28-03-2018 06:36:25.875 UTC Status alarm.cpp:62: sprout issued 1004.1 alarm




root at dime-au6gte:/var/log/homestead# monit summary
Monit 5.18.1 uptime: 2h 27m
 Service Name                     Status                      Type
 node-dime-au6gte.clearwater....  Running                     System
 snmpd_process                    Running                     Process
 ralf_process                     Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 ralf_uptime                      Status ok                   Program
 poll_ralf                        Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


Any help/suggestion would be much appreciated.




Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180328/7c0e9acc/attachment.html>

From rob at projectclearwater.org  Wed Mar 28 07:25:17 2018
From: rob at projectclearwater.org (Robert Day (projectclearwater.org))
Date: Wed, 28 Mar 2018 11:25:17 +0000
Subject: [Project Clearwater] CW team please help - stress testing
In-Reply-To: <CAHwYWpBPa94n1we8-B=8JEpFKd6uXZ-i=RNTsG_CsW9HC5pAYQ@mail.gmail.com>
References: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
	<CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>
	<CAHwYWpB4WqEVnG69B_BazQC6yJVHW7A3pMPOWmRoVX_gYJhPgw@mail.gmail.com>
	<CAHwYWpBH8qdZbAEuFoE61Y13s1s9QGku912aBb5w8XpX+ZmAkQ@mail.gmail.com>
	<SN1PR02MB16779E13A341C6567440B42CF5A90@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpCj9xAoiE=eLe7+GMCrX7RjPDgMHXVazVhX4evT8AbTUw@mail.gmail.com>
	<C29325B9-84E3-49B8-96B4-044A1AB88CB8@redmatter.com>
	<SN1PR02MB1677A0FE6650898235E6F58FF5A80@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpDCMqp5yXafz-3zuzHQJbKNFBvfzJhYbofBGc2dUqKiyA@mail.gmail.com>
	<BY2PR02MB21491BAF112161741FC5D912F4AC0@BY2PR02MB2149.namprd02.prod.outlook.com>
	<CAHwYWpBPa94n1we8-B=8JEpFKd6uXZ-i=RNTsG_CsW9HC5pAYQ@mail.gmail.com>
Message-ID: <CY1PR02MB21546055F46EF97504A93CA4F4A30@CY1PR02MB2154.namprd02.prod.outlook.com>

Hi Sunil,

/var/log/sprout/sprout_err.log only contains stack traces if Sprout crashes, which isn?t what?s happening here. What I?d like to see is any Error logs in /var/log/sprout/sprout_current.txt, or /var/log/sprout/sprout_TIMESTAMP.txt (where TIMESTAMP matches the hour in which you saw failed calls). Are there any logs like that?

Best,
Rob

From: Sunil Kumar [mailto:skgola1997 at gmail.com]
Sent: 27 March 2018 12:57
To: Robert Day <Robert.Day at metaswitch.com>
Subject: Re: [Project Clearwater] CW team please help - stress testing

Thanks Robert for replying,

Yes, the file is there but it is empty..

[sprout]ubuntu at sprout:/var/log/sprout$ ls
access_20180326T180000Z.txt  access_20180327T000000Z.txt  access_20180327T060000Z.txt  access_20180327T120000Z.txt  access_20180327T180000Z.txt  sprout_20180327T150000Z.txt  sprout_err.log
access_20180326T190000Z.txt  access_20180327T010000Z.txt  access_20180327T070000Z.txt  access_20180327T130000Z.txt  access_20180327T190000Z.txt  sprout_20180327T160000Z.txt  sprout_out.log
access_20180326T200000Z.txt  access_20180327T020000Z.txt  access_20180327T080000Z.txt  access_20180327T140000Z.txt  access_current.txt           sprout_20180327T170000Z.txt
access_20180326T210000Z.txt  access_20180327T030000Z.txt  access_20180327T090000Z.txt  access_20180327T150000Z.txt  analytics.log                sprout_20180327T180000Z.txt
access_20180326T220000Z.txt  access_20180327T040000Z.txt  access_20180327T100000Z.txt  access_20180327T160000Z.txt  sprout_20180327T130000Z.txt  sprout_20180327T190000Z.txt
access_20180326T230000Z.txt  access_20180327T050000Z.txt  access_20180327T110000Z.txt  access_20180327T170000Z.txt  sprout_20180327T140000Z.txt  sprout_current.txt

[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_err.log
[sprout]ubuntu at sprout:/var/log/sprout$

Regards,
Sunil


On Tue, Mar 27, 2018 at 5:15 PM, Robert Day <Robert.Day at metaswitch.com<mailto:Robert.Day at metaswitch.com>> wrote:
Hi Sunil,

Are there any ?Error?-level logs in /var/log/sprout or /var/log/homestead on the VMs? That might suggest what?s causing the 503 Service Unavailable error.

Best,
Rob

From: Sunil Kumar [mailto:skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>]
Sent: 26 March 2018 12:28
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Michael Duppr? <Michael.Duppre at metaswitch.com<mailto:Michael.Duppre at metaswitch.com>>
Subject: Re: [Project Clearwater] CW team please help - stress testing

Hi Michael,
I am not able to make call got 503 service unavailable. can you please check and guide me some quick solution

[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress ims.com 100 2
Starting initial registration, will take 1 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:01:50
Start: 2018-03-26 23:18:41.605000
End: 2018-03-26 23:20:32.453238

Total calls: 2
Successful calls: 0 (0.0%)
Failed calls: 2 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 6
Successful re-REGISTERs: 6 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 59.0ms

Log files at /var/log/clearwater-sip-stress/12890_*






[]ubuntu at stress:~$ cat /var/log/clearwater-sip-stress/12890_caller_errors.log
sipp: The following events occured:
2018-03-26      23:19:37.070794 1522086577.070794: Aborting call on unexpected message for Call-Id '1-12900 at 127.0.1.1<mailto:1-12900 at 127.0.1.1>': while expecting '183' (index 2), received 'SIP/2.0 503 Service Unavailable
Via: SIP/2.0/TCP 127.0.1.1:34112;received=10.224.61.34;branch=z9hG4bK-12900-1-0
Record-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com>;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com>;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 1-12900 at 127.0.1.1<mailto:1-12900 at 127.0.1.1>
From: <sip:2010000002 at ims.com<mailto:sip%3A2010000002 at ims.com>>;tag=12900SIPpTag001
To: <sip:2010000041@ ims .com<sip:2010000041@%20ims%20.com>>;tag=z9hG4bKPjRhUbQK8NJ5Blp4qIYX7f1YWhM2FQZe2M
CSeq: 1 INVITE
P-Charging-Vector: icid-value="12900SIPpTag001";orig-ioi= ims.com;term-ioi= ims.com
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0


PFA the sprout log

thanks,
 sunil



On Fri, Mar 23, 2018 at 11:26 PM, Michael Duppr? <Michael.Duppre at metaswitch.com<mailto:Michael.Duppre at metaswitch.com>> wrote:
Hi Sunil,

Sorry to hear that you?ve found our reply too slow to be helpful this time, but I?m glad to hear that you seem to have made some progress on your own. As Jim mentioned, our mailing list is `best effort` while we focus on development work to keep the quality of Clearwater as high as possible ? thanks, Jim, I?m happy to hear that you like our free service!

Regarding your questions:
?  You?ve mentioned that you have hit other problems with stress tool, could you please head over to https://github.com/Metaswitch/project-clearwater-issues/issues and open up an issue with some specific diagnostics? That way we can track and prioritize the problem to get it fixed as soon as possible.
?  It sounds like you now got some calls working, that?s good news! Could you please share your stress tool and sprout logs to pinpoint if this is a problem with the generation of load or with Clearwater handling the calls? Also, you?ve mentioned that you?ve made some changes to the tool, what exactly did you change? I?d just like to be sure that none of your changes are causing this behaviour.
?  About the `multiplier` argument: Thanks for pointing out that our documentation is missing something useful here, we will update it. For now, if you take a look at the help text of the stress tool by running `/usr/share/clearwater/bin/run_stress --help` you can find the explanation:
o --multiplier MULTIPLIER: Multiplier for the VoLTE load profile (e.g. passing 2 here will mean 2.6 calls and 4 re-registers per subs per hour)

For more information about the VoLTE load profile you can have a look at the documentation for the stress tool at http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html, specifically:
o [The stress tool will] send traffic, using a fixed load profile of 1.3 calls/hour for each subscriber (split equally between incoming and outgoing calls) and 2 re-registrations per hour

Good luck with your next steps using Clearwater! Hope you won?t hit any other problems, but if you do, please let us know!

Kind regards,
Michael


From: Jim Page [mailto:jim.page at redmatter.com<mailto:jim.page at redmatter.com>]
Sent: 23 March 2018 09:20
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Michael Duppr? <Michael.Duppre at metaswitch.com<mailto:Michael.Duppre at metaswitch.com>>

Subject: Re: [Project Clearwater] CW team please help - stress testing

Dude, this is a free service. I am sure if you buy some of their commercial licenses they will prioritise your request, but if you are on here you need to understand that service given here is ?best effort?, and in my view they perform a fantastic job. So calm down.

RedMatter Ltd
Jim Page
VP Mobile Services
+44 (0)333 150 1666
+44 (0)7870 361412
jim.page at redmatter.com<mailto:jim.page at redmatter.com>

On 23 Mar 2018, at 02:29, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:

Hi,
Thanks for replying, but you guys are replying very late, its not good, I have been waiting for your reply from last 3 days :-( .
Anyway, I thought the script hast other problem also, May be you will check it and fix it so that others wouldn't got that problem. Somehow I fix the problem, though it takes lot of of time to read the script and make some changes.

I want ask few questions and expecting reply within a day :-)
1. when I use 1000 subscriber and running for 10 min duration, only few call are successful (around 300) and no calls are failed. How can I increase no. of calls.
2. Can you explain the exact use of --multiplier parameter in detail. I request you to add all the parameter in doc itself so other would not get problem while finding.

Thanks,
Sunil


On Thu, Mar 22, 2018 at 8:09 PM, Michael Duppr? <Michael.Duppre at metaswitch.com<mailto:Michael.Duppre at metaswitch.com>> wrote:
Hello Sunil,

Sorry about the stress tool not working properly with a lower number of subscribers, that looks like a bug in the tool. I have raised issue https://github.com/Metaswitch/project-clearwater-issues/issues/30 to track and fix this problem, feel free to provide any other information on that ticket if you hit similar problems. Thanks for your help finding this bug!

Looks like you?ve done the right things and went back to a slightly higher number of subscribers (50) and looked at the stress log file and the sprout log file. Unfortunately it looks like you?ve copied out the wrong time period from the sprout logs: In your email below, the stress tool logs are from 17:48, however the sprout logs that you?ve sent are from 5 hours before at 12:27.
Similar for your tcpdump ? a good idea to have a look at this, but unfortunately what you?ve copied into your email is only the register flow, which is successful! :-)

You?re probably pretty close finding the reason why the calls are failing in the sprout logs, could you please make sure you have a look at the timestamp of the time you ran the stress tool?

Good luck and kind regards,
Michael


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Sunil Kumar
Sent: 20 March 2018 14:57
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Bennett Allen <Bennett.Allen at metaswitch.com<mailto:Bennett.Allen at metaswitch.com>>
Subject: Re: [Project Clearwater] CW team please help - stress testing

Hi,
I am facing problem in stress testing, Please look into the log. I am not able to debug the problem.

I have taken this from wireshark, actually i use tcpdump.

REGISTER sip:ims.com<http://ims.com/> SIP/2.0
Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-0
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
Content-Length: 0
Require: Path
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
Supported: path
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Route: <sip:icscf at sprout.ims.com<mailto:sip%3Aicscf at sprout.ims.com>;lr>
Max-Forwards: 70
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768/>>;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
CSeq: 1 REGISTER
Expires: 3600
Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY, MESSAGE, PRACK
Supported: path, gruu
Authorization: Digest username="2010000039 at ims.com<mailto:2010000039 at ims.com>",realm="ims.com<http://ims.com/>",uri="sip:ims.com<http://ims.com/>",nonce="",response="",algorithm=Digest-MD5
User-Agent: 00-00000-0000000000000 Phone IMS 10.0
P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
P-Visited-Network-ID: ims.com<http://ims.com/>

SIP/2.0 401 Unauthorized
Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-1-0
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=z9hG4bKPjDBaGZjqTrLQiDSlHihO36oMPm7fxz2sQ
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
WWW-Authenticate: Digest  realm="ims.com<http://ims.com/>",nonce="0e07c1b77b566f37",opaque="5171f001504c2c3a",algorithm=MD5,qop="auth"
Content-Length:  0

REGISTER sip:ims.com<http://ims.com/> SIP/2.0
Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-2
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
Content-Length: 0
Require: Path
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
Supported: path
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Route: <sip:icscf at sprout.ims.com<mailto:sip%3Aicscf at sprout.ims.com>;lr>
Max-Forwards: 70
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768/>>;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
CSeq: 1 REGISTER
Expires: 3600
Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY, MESSAGE, PRACK
Supported: path, gruu
Authorization: Digest username="2010000039 at ims.com<mailto:2010000039 at ims.com>",realm="ims.com<http://ims.com/>",cnonce="66334873",nc=00000001,qop=auth,uri="sip:sprout.ims.com:5052<http://sprout.ims.com:5052/>",nonce="0e07c1b77b566f37",response="788d4520717e4e7b29f7fab43fdc448f",algorithm=MD5,opaque="5171f001504c2c3a"
User-Agent: 00-00000-0000000000000 Phone IMS 10.0
P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
P-Visited-Network-ID: ims.com<http://ims.com/>

SIP/2.0 200 OK
Service-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com/>;transport=TCP;lr;orig;username=2010000039%40ims.com<http://40ims.com/>;nonce=0e07c1b77b566f37>
Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-1-2
Call-ID: 1-784 at 127.0.1.1<mailto:1-784 at 127.0.1.1>
From: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=784SIPpTag001
To: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>;tag=z9hG4bKPjIvjh2DjwvU.vVNEv.nOiYAfsZRgMjHDF
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
Supported: outbound
Contact: <sip:2010000039 at 127.0.1.1:34768<http://sip:2010000039 at 127.0.1.1:34768/>>;expires=1800;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>";reg-id=1;pub-gruu="sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>;gr=urn:uuid:00000000-0000-0000-0000-000000000001"
Require: outbound
Path: <sip:127.0.1.1:5082;transport=tcp;lr>
P-Associated-URI: <sip:2010000039 at ims.com<mailto:sip%3A2010000039 at ims.com>>
Content-Length:  0


thanks in advance, Please resply.

cheers,
sunil


On Tue, Mar 20, 2018 at 6:53 PM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi,
It is using some other port on stress node not 5082. Is this a problem, if yes how can I fix this i have tried to open 5082 port on stress node using sudo ufw allow 5082/tcp, but no effect.
Please check the wireshark log:

Frame 2406: 703 bytes on wire (5624 bits), 703 bytes captured (5624 bits)
Ethernet II, Src: PcsCompu_ff:d2:88 (08:00:27:ff:d2:88), Dst: PcsCompu_ab:71:0f (08:00:27:ab:71:0f)
Internet Protocol Version 4, Src: 10.224.61.22, Dst: 10.224.61.13
Transmission Control Protocol, Src Port: rlm-admin (5054), Dst Port: 34312 (34312), Seq: 349, Ack: 2199, Len: 637
Session Initiation Protocol (503)


cheers,
sunil

On Tue, Mar 20, 2018 at 5:21 PM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi all,
I have taken tcpdump also but there is no SIP message. Please through some light on this problem. I am trying from last weak, not able to catch the problem. Thanks in advance.

cheers,
Sunil

On Tue, Mar 20, 2018 at 10:30 AM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi CW team,
Anyone out there please help me. I am facing problem in stress testing. I have installed CW manually. whenever I was running 1 or less than 20 it give some errors like:

[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com<http://ims.com/> 1 2
[sudo] password for ubuntu:
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
    with open(CALLER_STATS) as f:
IOError: [Errno 2] No such file or directory: '/var/log/clearwater-sip-stress/18065_caller_stats.log'


[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com<http://ims.com/> 10 5
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
    call_success_rate = 100 * float(row['SuccessfulCall(C)']) / float(row['TotalCallCreated'])
ZeroDivisionError: float division by zero


[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress iind.intel.com<http://iind.intel.com/> 50 5
Starting initial registration, will take 0 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:03:41
Start: 2018-03-20 17:46:43.268136
End: 2018-03-20 17:51:31.363406

Total calls: 2
Successful calls: 0 (0.0%)
Failed calls: 2 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 8
Successful re-REGISTERs: 8 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 86.0ms

Log files at /var/log/clearwater-sip-stress/18566_*



[]ubuntu at stress:~$ cat /var/log/clearwater-sip-stress/18566_caller_errors.log
sipp: The following events occured:
2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on unexpected message for Call-Id '1-18576 at 127.0.1.1<mailto:1-18576 at 127.0.1.1>': while expecting '183' (index 2), received 'SIP/2.0 503 Service Unavailable
Via: SIP/2.0/TCP 127.0.1.1:42276;received=10.224.61.13;branch=z9hG4bK-18576-1-0
Record-Route: <sip:scscf.sprout.ims.com<http://scscf.sprout.ims.com/>;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout. ims.com<http://ims.com/> ;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 1-18576 at 127.0.1.1<mailto:1-18576 at 127.0.1.1>
From: <sip:2010000042@ ims.com<http://ims.com/> >;tag=18576SIPpTag001
To: <sip:2010000015@ ims.co<http://ims.co/>>;tag=z9hG4bKPj1Lm9whhQMslKrcZxnN6qCH0tb9Lj5Neu
CSeq: 1 INVITE
P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com<http://ims.com/> ;term-ioi= ims.com<http://ims.com/>
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0


[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=49294;received=10.224.61.22;branch=z9hG4bK-670172
Call-ID: poll-sip-670172
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670172
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670172
CSeq: 670172 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:284: Request latency = 254us
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217: Rate adjustment calculation inputs: err -0.981500, smoothed latency 185, target latency 10000
20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302: Maximum incoming request rate/second unchanged at 2000.000000 (current request rate is 0.200000 requests/sec, minimum threshold for a change is 1000.000000 requests/sec).
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054<http://10.224.61.22:5054/>: got incoming TCP connection from 10.224.61.22:42848<http://10.224.61.22:42848/>, sock=573
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 tcp->base.local_name: 10.224.61.22
20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP server transport created
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=670180 (rdata0x7f34ec027690)
20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180 (rdata0x7f34ec027690) from TCP 10.224.61.22:42848<http://10.224.61.22:42848/>:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054<http://sip:poll-sip at 10.224.61.22:5054/> SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054<http://sip:poll-sip at 10.224.61.22:5054/>>
From: poll-sip <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670180
Call-ID: poll-sip-670180
CSeq: 670180 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8 for worker threads with priority 15
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:183: Request latency so far = 57us
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=670180 (rdata0x7f34ec34a3e8)
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848<http://10.224.61.22:42848/>:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=42848;received=10.224.61.22;branch=z9hG4bK-670180
Call-ID: poll-sip-670180
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670180
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670180
CSeq: 670180 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809 Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:284: Request latency = 129us
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 1).
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053<http://10.224.61.22:5053/>: got incoming TCP connection from 10.224.61.22:49356<http://10.224.61.22:49356/>, sock=573
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 tcp->base.local_name: 10.224.61.22
20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP server transport created
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=670182 (rdata0x7f34ec027690)
20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182 (rdata0x7f34ec027690) from TCP 10.224.61.22:49356<http://10.224.61.22:49356/>:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053<http://sip:poll-sip at 10.224.61.22:5053/> SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053<http://sip:poll-sip at 10.224.61.22:5053/>>
From: poll-sip <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670182
Call-ID: poll-sip-670182
CSeq: 670182 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8 for worker threads with priority 15
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:183: Request latency so far = 102us
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=670182 (rdata0x7f34ec34a3e8)
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172: Classified URI as 3
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose common_sip_processing.cpp:103: TX 282 bytes Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356<http://10.224.61.22:49356/>:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=49356;received=10.224.61.22;branch=z9hG4bK-670182
Call-ID: poll-sip-670182
From: "poll-sip" <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=670182
To: <sip:poll-sip at 10.224.61.22<mailto:sip%3Apoll-sip at 10.224.61.22>>;tag=z9hG4bK-670182
CSeq: 670182 OPTIONS
Content-Length:  0


--end msg--
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00 Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7f34ec34a3e8
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:284: Request latency = 232us
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 2).
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1001.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1005.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1011.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1012.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1013.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1004.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1002.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1009.1 alarm
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303: AlarmReqAgent: queue overflowed
20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout issued 1010.1 alarm
20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP connection closed
20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)



all calls are failing I don't know what is going on, I am newbie to CW please guide some solution it will be great help.


Thanks,
Sunil





_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180328/5539f32a/attachment.html>

From skgola1997 at gmail.com  Wed Mar 28 08:54:02 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Wed, 28 Mar 2018 18:24:02 +0530
Subject: [Project Clearwater] Clearwater, ellis freezing
In-Reply-To: <BY2PR02MB2149E0B6776DB17C397F44CDF4AC0@BY2PR02MB2149.namprd02.prod.outlook.com>
References: <CAJpo2SrM2rViuhGy86MHv9sZ-QVXjS2J89KZzGbeh=UZ0e_1AQ@mail.gmail.com>
	<BY2PR02MB2149E0B6776DB17C397F44CDF4AC0@BY2PR02MB2149.namprd02.prod.outlook.com>
Message-ID: <CAHwYWpA8kdDzewXNNxGdzN4Eq7othKvpGi+7GV8MgXQZ3qKxow@mail.gmail.com>

Hi Rob,
Thanks for replying.

I am running stress testing for 1000 subscribers and for 1 min duration.
When i am seeing the sipp output the c*all per sec is very less, and there
is showing limit 2 ( 0 calls (limit 2) ). *

*Is there any way to increase the # of calls per sec.?*
*Do I need to set something or do I need to make changes in script
(run_stress) for that like-*

CPS = OUTGOING_BHCA * args.subscriber_count * args.multiplier / 3600
*instead of 3600 can i reduced it?*

please let me know, i haven't set anything yet in any other node.

*Please find sipp log below:*


------------------------------ Scenario Screen -------- [1-9]: Change
Screen --
  Call-rate(length)   Port   Total-time  Total-calls  Remote-host
0.2(5000 ms)/1.000s   5061      62.76 s           10  sprout.iind.:5054(TCP)

  Call limit reached (-m 10), 0.000 s period  0 ms scheduler resolution
  *0 calls (limit 2) *                     *Peak was 2 calls, after 11 s*
  0 Running, 9 Paused, 0 Woken up
  0 dead call msg (discarded)            0 out-of-call msg (discarded)
  1 open sockets

                                 Messages  Retrans   Timeout
 Unexpected-Msg
      INVITE ---------->         10        0         0
         100 <----------         10        0         0         0
         183 <----------         10        0         0         0
       PRACK ---------->         10        0
         200 <----------         10        0         0         0
      UPDATE ---------->         10        0
         201 <----------         9         0         0         0
         180 <----------  E-RTD1 10        0         0         0
         201 <----------         1         0         0         0
         200 <----------         10        0         0         0
         ACK ---------->         10        0
       Pause [   5000ms]         10                            0
         BYE ---------->         10        0         0
         200 <----------         10        0         0         0

------------------------------ Test Terminated
--------------------------------


----------------------------- Statistics Screen ------- [1-9]: Change
Screen --
  Start Time             | 2018-03-28   20:41:49.298290 1522249909.298290
  Last Reset Time        | 2018-03-28   20:42:52.067656 1522249972.067656
  Current Time           | 2018-03-28   20:42:52.069419 1522249972.069419
-------------------------+---------------------------+------
--------------------
  Counter Name           | Periodic value            | Cumulative value
-------------------------+---------------------------+------
--------------------
  Elapsed Time           | 00:00:00:001000           | 00:01:02:771000
  Call Rate              |    0.000 cps              |   * 0.159 cps*
-------------------------+---------------------------+------
--------------------
  Incoming call created  |        0                  |        0
  OutGoing call created  |        0                  |       10
  Total Call created     |                          |       10
  Current Call           |        0                  |
-------------------------+---------------------------+------
--------------------
  Successful call        |        0                  |       10
  Failed call            |        0                  |        0
-------------------------+---------------------------+------
--------------------
  Response Time 1        | 00:00:00:000000           | 00:00:00:100000
  Call Length            | 00:00:00:000000           | 00:00:07:381000
------------------------------ Test Terminated
--------------------------------



*Please let me know how can I increase the # of call per second in stress
testing. *


Thanks,
sunil


On Tue, Mar 27, 2018 at 8:16 PM, Robert Day <Robert.Day at metaswitch.com>
wrote:

> Hi D?vid,
>
>
>
> ?19-03-2018 08:42:49.839 UTC Error main.cpp:1016: Failed to initialize
> the Cassandra store with error$? suggests the error is in Cassandra. Is
> there anything in /var/log/cassandra/system.log?
>
>
>
> Best,
>
> Rob
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *D?vid Kyselica
> *Sent:* 19 March 2018 08:56
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Clearwater, ellis freezing
>
>
>
> Hi,
>
> I`m new to clearwater project. I installed all-in-one AMI into amazon ec2.
> After trying to access web interface, I was asked by web browser to reload
> the page by this message:Failed to update the server (see detailed
> diagnostics in developer console). Please refresh the page.
>
> I`m really confused what it can by caused by. I will be thankful for any
> type of tips. It`s a fresh installation with no important configuration
> changes.
>
>
>
> *log file of homestead: *
>
> 19-03-2018 08:42:49.718 UTC Status http_connection_pool.cpp:35: Connection
> pool will use calculated $
>
> 19-03-2018 08:42:49.837 UTC Status httpconnection.h:58: Configuring HTTP
> Connection
>
> 19-03-2018 08:42:49.837 UTC Status httpconnection.h:59:   Connection
> created for server ec2-52-91-24$
>
> 19-03-2018 08:42:49.837 UTC Status main.cpp:973: No HSS configured - using
> Homestead-prov
>
> 19-03-2018 08:42:49.837 UTC Status a_record_resolver.cpp:29: Created
> ARecordResolver
>
> 19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:266: Configuring
> store connection
>
> 19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:267:   Hostname:
> 127.0.0.1
>
> 19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:268:   Port:
> 9160
>
> 19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:296: Configuring
> store worker pool
>
> 19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:297:   Threads:   10
>
> 19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:298:   Max Queue: 0
>
> 19-03-2018 08:42:49.839 UTC Error main.cpp:1016: Failed to initialize the
> Cassandra store with error$
>
> 19-03-2018 08:42:49.839 UTC Status main.cpp:1017: Homestead is shutting
> down
>
>
>
> *log file of ellis:*
>
> 19-03-2018 08:31:30.662 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.64ms
>
> 19-03-2018 08:31:46.424 UTC INFO web.py:1447: 200 GET / (0.0.0.0) 616.39ms
>
> 19-03-2018 08:31:46.753 UTC INFO web.py:1447: 200 GET
> /css/bootstrap.min.css (0.0.0.0) 86.74ms
>
> 19-03-2018 08:31:46.793 UTC INFO web.py:1447: 200 GET
> /css/bootstrap-responsive.css (0.0.0.0) 39.08ms
>
> 19-03-2018 08:31:46.834 UTC INFO web.py:1447: 200 GET /css/style.css
> (0.0.0.0) 1.16ms
>
> 19-03-2018 08:31:46.835 UTC INFO web.py:1447: 200 GET
> /css/jquery.miniColors.css (0.0.0.0) 1.03ms
>
> 19-03-2018 08:31:46.836 UTC INFO web.py:1447: 200 GET
> /css/fileuploader.css (0.0.0.0) 0.94ms
>
> 19-03-2018 08:31:47.695 UTC INFO web.py:1447: 200 GET /js/jquery.js
> (0.0.0.0) 573.28ms
>
> 19-03-2018 08:31:47.740 UTC INFO web.py:1447: 200 GET
> /js/jquery.ba-bbq.min.js (0.0.0.0) 42.90ms
>
> 19-03-2018 08:31:48.104 UTC INFO web.py:1447: 200 GET /js/fileuploader.js
> (0.0.0.0) 364.63ms
>
> 19-03-2018 08:31:48.106 UTC INFO web.py:1447: 200 GET
> /js/jquery.miniColors.min.js (0.0.0.0) 1.58ms
>
> 19-03-2018 08:31:48.107 UTC INFO web.py:1447: 200 GET /js/jquery.cookie.js
> (0.0.0.0) 1.02ms
>
> 19-03-2018 08:31:48.149 UTC INFO web.py:1447: 200 GET
> /js/jquery.total-storage.min.js (0.0.0.0) 41.65ms
>
> 19-03-2018 08:31:48.314 UTC INFO web.py:1447: 200 GET /js/bootstrap.min.js
> (0.0.0.0) 2.34ms
>
> 19-03-2018 08:31:48.354 UTC INFO web.py:1447: 200 GET /js/common.js
> (0.0.0.0) 1.91ms
>
> 19-03-2018 08:31:48.561 UTC INFO web.py:1447: 200 GET /js/app.js (0.0.0.0)
> 168.24ms
>
> 19-03-2018 08:31:49.012 UTC WARNING web.py:1447: 404 GET
> /images/favicon.ico (0.0.0.0) 81.57ms
>
> 19-03-2018 08:31:50.286 UTC INFO web.py:1447: 200 GET /accounts/david%
> 40hmz.sk/numbers/?cb=2b0a26dea4K0 (0.0.0.0) 40.51ms
>
> 19-03-2018 08:32:20.995 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.21ms
>
> 19-03-2018 08:32:35.655 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 162.92ms
>
> 19-03-2018 08:33:21.845 UTC INFO web.py:1447: 200 GET /addressbook.html
> (0.0.0.0) 329.69ms
>
> 19-03-2018 08:33:22.052 UTC INFO web.py:1447: 200 GET /css/addressbook.css
> (0.0.0.0) 1.18ms
>
> 19-03-2018 08:33:22.709 UTC INFO web.py:1447: 200 GET /js/backbone.js
> (0.0.0.0) 455.22ms
>
> 19-03-2018 08:33:22.752 UTC INFO web.py:1447: 200 GET /js/underscore.js
> (0.0.0.0) 43.11ms
>
> 19-03-2018 08:33:22.874 UTC INFO web.py:1447: 200 GET /js/addressbook.js
> (0.0.0.0) 43.49ms
>
> 19-03-2018 08:33:23.121 UTC INFO web.py:1447: 200 GET
> /js/templates/addressbook-contacts.html (0.0.0.0) 2.48ms
>
> 19-03-2018 08:33:23.405 UTC INFO web.py:1447: 200 GET /gab (0.0.0.0) 2.09ms
>
> 19-03-2018 08:33:27.915 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.18ms
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180328/ea49a652/attachment.html>

From skgola1997 at gmail.com  Wed Mar 28 13:12:22 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Wed, 28 Mar 2018 22:42:22 +0530
Subject: [Project Clearwater] Very less call per sec
In-Reply-To: <CAHwYWpCBAi_cWW6CENdu9Rsz3swLEUZWFkO2hq0SA57WEegOeQ@mail.gmail.com>
References: <CAHwYWpCBAi_cWW6CENdu9Rsz3swLEUZWFkO2hq0SA57WEegOeQ@mail.gmail.com>
Message-ID: <CAHwYWpBH10AMHxWv+WFeE-f=o09HJ8zFYn1Z_hPUGVzE3bk0Sg@mail.gmail.com>

Hi,

I want to increase the # of calls per seconds. Please help me out. I have
few doubt Please reply:

1) *How many max calls per sec can be make* using 1 vCPU and 2 GB RAM, 10
GB HDD, manual setup with stress test.

2)* How can i increase the no. of calls per sec*, Is there any specific
parameter for that or Do I need to change in script (run_stress) or
anywhere else please let me know.

3) I am using the stress node for making calls, I want to ask, *On which
nodes the calls or stress test is depends other than sprout and stress node*.
I have created the subscribers on vellum node.

4) *# of calls is limited by stress node or sprout node or any other node?*

5) *What is the maximum limitation of --multiplier*, i have checked up to
20 only?

6) *Should I increase the CPUs, RAM and HDD on single node or Do I create
new node for larger deployments* (multiple nodes), which is better way?

Please reply, i am waiting for it eagerly. *Thanks in advance.*

Regards,
suni

On Wed, Mar 28, 2018 at 2:39 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi all,
>
> I am running stress testing for 1000 subscribers and for 1 min duration.
> When i am seeing the sipp output the c*all per sec is very less, and
> there is showing limit 2 ( 0 calls (limit 2) ). *
> Is there any way to increase the # of calls per sec.
>
> ------------------------------ Scenario Screen -------- [1-9]: Change
> Screen --
>   Call-rate(length)   Port   Total-time  Total-calls  Remote-host
> 0.2(5000 ms)/1.000s   5061      62.76 s           10
> sprout.iind.:5054(TCP)
>
>   Call limit reached (-m 10), 0.000 s period  0 ms scheduler resolution
>   *0 calls (limit 2) *                     *Peak was 2 calls, after 11 s*
>   0 Running, 9 Paused, 0 Woken up
>   0 dead call msg (discarded)            0 out-of-call msg (discarded)
>   1 open sockets
>
>                                  Messages  Retrans   Timeout
>  Unexpected-Msg
>       INVITE ---------->         10        0         0
>          100 <----------         10        0         0         0
>          183 <----------         10        0         0         0
>        PRACK ---------->         10        0
>          200 <----------         10        0         0         0
>       UPDATE ---------->         10        0
>          201 <----------         9         0         0         0
>          180 <----------  E-RTD1 10        0         0         0
>          201 <----------         1         0         0         0
>          200 <----------         10        0         0         0
>          ACK ---------->         10        0
>        Pause [   5000ms]         10                            0
>          BYE ---------->         10        0         0
>          200 <----------         10        0         0         0
>
> ------------------------------ Test Terminated
> --------------------------------
>
>
> ----------------------------- Statistics Screen ------- [1-9]: Change
> Screen --
>   Start Time             | 2018-03-28   20:41:49.298290 1522249909.298290
>   Last Reset Time        | 2018-03-28   20:42:52.067656 1522249972.067656
>   Current Time           | 2018-03-28   20:42:52.069419 1522249972.069419
> -------------------------+---------------------------+------
> --------------------
>   Counter Name           | Periodic value            | Cumulative value
> -------------------------+---------------------------+------
> --------------------
>   Elapsed Time           | 00:00:00:001000           | 00:01:02:771000
>   Call Rate              |    0.000 cps              |   * 0.159 cps*
> -------------------------+---------------------------+------
> --------------------
>   Incoming call created  |        0                  |        0
>   OutGoing call created  |        0                  |       10
>   Total Call created     |                          |       10
>   Current Call           |        0                  |
> -------------------------+---------------------------+------
> --------------------
>   Successful call        |        0                  |       10
>   Failed call            |        0                  |        0
> -------------------------+---------------------------+------
> --------------------
>   Response Time 1        | 00:00:00:000000           | 00:00:00:100000
>   Call Length            | 00:00:00:000000           | 00:00:07:381000
> ------------------------------ Test Terminated
> --------------------------------
>
>
> Thanks,
> Kumar
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180328/86da033e/attachment.html>

From anthonynlee at gmail.com  Wed Mar 28 15:23:48 2018
From: anthonynlee at gmail.com (Anthony Lee)
Date: Wed, 28 Mar 2018 15:23:48 -0400
Subject: [Project Clearwater] Is there tutorial guide to use OpenIMS Core's
 HSS to configure service profile?
Message-ID: <CA+pBo5Ex8B3Zh+rv-9MiqLTr9MqGZnfnPESCzwoum0j76mo69w@mail.gmail.com>

Hi,

Is there anyone also try to integrate Clearwater with OpenIMS HSS?
I found it difficult to use web interface to setup a service profile for a
subscribe.
Is there guide I could follow? Is there any example?


Thanks
Anthony
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180328/cb695bfe/attachment.html>

From skgola1997 at gmail.com  Thu Mar 29 10:06:08 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Thu, 29 Mar 2018 19:36:08 +0530
Subject: [Project Clearwater] Stress Testing Queries
Message-ID: <CAHwYWpB_y_vwKpaRHgOPbo4_8GciF6uk=KF1N-wO+U9G0fa=mA@mail.gmail.com>

 Hi all,

I want to increase the # of calls per seconds. Please help me out. I have
few doubt Please reply:

1) *How many max calls per sec can be make* using 1 vCPU and 2 GB RAM, 10
GB HDD, manual setup with stress test.

2)* How can i increase the no. of calls per sec*, Is there any specific
parameter for that or Do I need to change in script (run_stress) or
anywhere else please let me know.

3) I am using the stress node for making calls, I want to ask, *On which
nodes the calls or stress test is depends other than sprout and stress node*.
I have created the subscribers on vellum node.

4) *# of calls is limited by stress node or sprout node or any other node?*

5) *What is the maximum limitation of --multiplier*, i have checked up to
20 only?

6) *Should I increase the CPUs, RAM and HDD on single node or Do I create
new node for larger deployments* (multiple nodes), which is better way?

7) I have not use any* throttling options *yet. Do I need to use them, if
yes please let me know which for increasing the CPS, how can i use (just
mentioned in shared_config?)

Please reply, i am waiting for it eagerly. *Thanks in advance.*

Regards,
suni
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180329/93f70f87/attachment.html>

From nagen_kr at yahoo.co.in  Sat Mar 31 07:07:38 2018
From: nagen_kr at yahoo.co.in (Nagendra Kumar)
Date: Sat, 31 Mar 2018 11:07:38 +0000 (UTC)
Subject: [Project Clearwater] Input required for High Availability use cases
 of clearwater
References: <372055626.178624.1522494458227.ref@mail.yahoo.com>
Message-ID: <372055626.178624.1522494458227@mail.yahoo.com>

Dear Team,
I have a high availability use case below, please provide your inputs.I have 6 nodes running for clearwater services on 6 different nodes and 1 for DNS(on Node7) as below:(I am using AWS AMI ubuntu 14 image and have manually installed as per instruction at Manual Install Instructions ? Project Clearwater 1.0 documentation)Desired configuration :
Node1 : EllisNode2 : BonoNode3 : SproutNode4 : HomerNode5 : DimeNode6 : Vellum

Now, we have a requirement of scale up and scale down.Scale Down:
1. if Node1 fails, Ellis should start working on Node2. That means Node2 hosts Ellis and Bono both running. 2. After that if Node2 fails, Ellis and Bono should shift to Node3. That means Node3 hosts Ellis, Bono and Sprout.3. And so on..... till Node5 fails and Node6 hosts Ellis, Bono, Sprout, Homer, Dime and Vellum.

Will it need any source code change or work with just configuration change? Please send me instruction for making changes.I want to try the following for the above use case:1. I install all software on each nodes. That means Ellis is installed on all nodes, sprout is installed on all nodes, etc.2. But, in the beginning, Ellis is started on Node1, Bono is started on Node2 and so on as shown above (desired configuration).3. When Node1? fails, then I start Ellis on Node 2(say manually, I will automate it later). That means Node2 has Ellis and Bono running.4. When Node2 fails, I start Ellis and Bono on Node2...And the like...5. In the end, I want Node6 to have all software running.
Scale UP: 1. After step 5 above, where Node6 are running all software.2. Now, Node 5 has started and came up, I want to stop Dime on Node6 and want to start on Node5. That means Node5 has Dime running and Node6 has Ellis, Bono, Sprout, Homer, and Vellum running.
3. Node Node 4 has come up and running. I want to stop Homer on Node 6 and start it on Node4. That means Node 4 has Homer running, Node5 has Dime running and Node 6 has Ellis, Bono, Sprout and Vellum running.4. And so on till I get the desired configuration running as shown above.

It should happen when call is running and call shouldn't drop.
Any help will be deeply appreciated.
Thanks-Nagu

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180331/5328a5de/attachment.html>

From skgola1997 at gmail.com  Mon Mar 26 07:28:32 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Mon, 26 Mar 2018 11:28:32 -0000
Subject: [Project Clearwater] CW team please help - stress testing
In-Reply-To: <SN1PR02MB1677A0FE6650898235E6F58FF5A80@SN1PR02MB1677.namprd02.prod.outlook.com>
References: <CAHwYWpC3fgHbjWJvbk1sM3hVxkU=pnA4hh+W-y9M2tZVR7q3-g@mail.gmail.com>
	<CAHwYWpBh5Q9zdbVCnK80gqovAHvOfAhG=Rj16iK_xwx5LgCs5A@mail.gmail.com>
	<CAHwYWpB4WqEVnG69B_BazQC6yJVHW7A3pMPOWmRoVX_gYJhPgw@mail.gmail.com>
	<CAHwYWpBH8qdZbAEuFoE61Y13s1s9QGku912aBb5w8XpX+ZmAkQ@mail.gmail.com>
	<SN1PR02MB16779E13A341C6567440B42CF5A90@SN1PR02MB1677.namprd02.prod.outlook.com>
	<CAHwYWpCj9xAoiE=eLe7+GMCrX7RjPDgMHXVazVhX4evT8AbTUw@mail.gmail.com>
	<C29325B9-84E3-49B8-96B4-044A1AB88CB8@redmatter.com>
	<SN1PR02MB1677A0FE6650898235E6F58FF5A80@SN1PR02MB1677.namprd02.prod.outlook.com>
Message-ID: <CAHwYWpDCMqp5yXafz-3zuzHQJbKNFBvfzJhYbofBGc2dUqKiyA@mail.gmail.com>

Hi Michael,
I am not able to make call got 503 service unavailable. can you please
check and guide me some quick solution

[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress ims.com 100 2
Starting initial registration, will take 1 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:01:50
Start: 2018-03-26 23:18:41.605000
End: 2018-03-26 23:20:32.453238

Total calls: 2
Successful calls: 0 (0.0%)
Failed calls: 2 (100.0%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 0.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 0.0% is lower than target 100.0%!

Total re-REGISTERs: 6
Successful re-REGISTERs: 6 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 59.0ms

Log files at /var/log/clearwater-sip-stress/12890_*






[]ubuntu at stress:~$ cat
/var/log/clearwater-sip-stress/12890_caller_errors.log
sipp: The following events occured:
2018-03-26      23:19:37.070794 1522086577.070794: Aborting call on
unexpected message for Call-Id '1-12900 at 127.0.1.1': while expecting '183'
(index 2), received 'SIP/2.0 503 Service Unavailable
Via: SIP/2.0/TCP 127.0.1.1:34112
;received=10.224.61.34;branch=z9hG4bK-12900-1-0
Record-Route: <sip:scscf.sprout.ims.com
;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.ims.com
;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 1-12900 at 127.0.1.1
From: <sip:2010000002 at ims.com>;tag=12900SIPpTag001
To: <sip:2010000041@ ims .com>;tag=z9hG4bKPjRhUbQK8NJ5Blp4qIYX7f1YWhM2FQZe2M
CSeq: 1 INVITE
P-Charging-Vector: icid-value="12900SIPpTag001";orig-ioi= ims.com;term-ioi=
ims.com
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0


PFA the sprout log

thanks,
 sunil



On Fri, Mar 23, 2018 at 11:26 PM, Michael Duppr? <
Michael.Duppre at metaswitch.com> wrote:

> Hi Sunil,
>
>
>
> Sorry to hear that you?ve found our reply too slow to be helpful this
> time, but I?m glad to hear that you seem to have made some progress on your
> own. As Jim mentioned, our mailing list is `best effort` while we focus on
> development work to keep the quality of Clearwater as high as possible ?
> thanks, Jim, I?m happy to hear that you like our free service!
>
>
>
> Regarding your questions:
>
>    - You?ve mentioned that you have hit other problems with stress tool,
>    could you please head over to https://github.com/Metaswitch/
>    project-clearwater-issues/issues
>    <https://github.com/Metaswitch/project-clearwater-issues/issues> and
>    open up an issue with some specific diagnostics? That way we can track and
>    prioritize the problem to get it fixed as soon as possible.
>    - It sounds like you now got some calls working, that?s good news!
>    Could you please share your stress tool and sprout logs to pinpoint if this
>    is a problem with the generation of load or with Clearwater handling the
>    calls? Also, you?ve mentioned that you?ve made some changes to the tool,
>    what exactly did you change? I?d just like to be sure that none of your
>    changes are causing this behaviour.
>    - About the `multiplier` argument: Thanks for pointing out that our
>    documentation is missing something useful here, we will update it. For now,
>    if you take a look at the help text of the stress tool by running `
>    /usr/share/clearwater/bin/run_stress --help` you can find the
>    explanation:
>       - *--multiplier MULTIPLIER: Multiplier for the VoLTE load profile
>       (e.g. passing 2 here will mean 2.6 calls and 4 re-registers per subs per
>       hour)*
>
> For more information about the VoLTE load profile you can have a look at
> the documentation for the stress tool at http://clearwater.readthedocs.
> io/en/stable/Clearwater_stress_testing.html, specifically:
>
>    - *[The stress tool will] send traffic, using a fixed load profile of
>       1.3 calls/hour for each subscriber (split equally between incoming and
>       outgoing calls) and 2 re-registrations per hour*
>
>
>
> Good luck with your next steps using Clearwater! Hope you won?t hit any
> other problems, but if you do, please let us know!
>
>
>
> Kind regards,
>
> Michael
>
>
>
>
>
> *From:* Jim Page [mailto:jim.page at redmatter.com]
> *Sent:* 23 March 2018 09:20
> *To:* clearwater at lists.projectclearwater.org
> *Cc:* Michael Duppr? <Michael.Duppre at metaswitch.com>
>
> *Subject:* Re: [Project Clearwater] CW team please help - stress testing
>
>
>
> Dude, this is a free service. I am sure if you buy some of their
> commercial licenses they will prioritise your request, but if you are on
> here you need to understand that service given here is ?best effort?, and
> in my view they perform a fantastic job. So calm down.
>
>
>
> *RedMatter Ltd*
>
> *Jim Page*
> *VP Mobile Services*
>
> +44 (0)333 150 1666
> +44 (0)7870 361412
>
> jim.page at redmatter.com
>
>
>
> On 23 Mar 2018, at 02:29, Sunil Kumar <skgola1997 at gmail.com> wrote:
>
>
>
> Hi,
>
> Thanks for replying, but you guys are replying very late, its not good, I
> have been waiting for your reply from last 3 days :-( .
>
> Anyway, I thought the script hast other problem also, May be you will
> check it and fix it so that others wouldn't got that problem. Somehow I fix
> the problem, though it takes lot of of time to read the script and make
> some changes.
>
>
>
> I want ask few questions and *expecting reply within a day* :-)
>
> 1. when I use 1000 subscriber and running for 10 min duration, only few
> call are successful (around 300) and no calls are failed. How can I
> increase no. of calls.
>
> 2. Can you explain the exact use of* --multiplier *parameter in detail. I
> request you to add all the parameter in doc itself so other would not get
> problem while finding.
>
>
>
> Thanks,
>
> Sunil
>
>
>
>
>
> On Thu, Mar 22, 2018 at 8:09 PM, Michael Duppr? <
> Michael.Duppre at metaswitch.com> wrote:
>
> Hello Sunil,
>
>
>
> Sorry about the stress tool not working properly with a lower number of
> subscribers, that looks like a bug in the tool. I have raised issue
> https://github.com/Metaswitch/project-clearwater-issues/issues/30 to
> track and fix this problem, feel free to provide any other information on
> that ticket if you hit similar problems. Thanks for your help finding this
> bug!
>
>
>
> Looks like you?ve done the right things and went back to a slightly higher
> number of subscribers (50) and looked at the stress log file and the sprout
> log file. Unfortunately it looks like you?ve copied out the wrong time
> period from the sprout logs: In your email below, the stress tool logs are
> from 17:48, however the sprout logs that you?ve sent are from 5 hours
> before at 12:27.
>
> Similar for your tcpdump ? a good idea to have a look at this, but
> unfortunately what you?ve copied into your email is only the register flow,
> which is successful! :-)
>
>
>
> You?re probably pretty close finding the reason why the calls are failing
> in the sprout logs, could you please make sure you have a look at the
> timestamp of the time you ran the stress tool?
>
>
>
> Good luck and kind regards,
>
> Michael
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sunil Kumar
> *Sent:* 20 March 2018 14:57
> *To:* clearwater at lists.projectclearwater.org; Bennett Allen <
> Bennett.Allen at metaswitch.com>
> *Subject:* Re: [Project Clearwater] CW team please help - stress testing
>
>
>
> Hi,
>
> I am facing problem in stress testing, Please look into the log. I am not
> able to debug the problem.
>
>
>
> I have taken this from wireshark, actually i use tcpdump.
>
>
>
> REGISTER sip:ims.com SIP/2.0
>
> Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-0
>
> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>
> Content-Length: 0
>
> Require: Path
>
> Path: <sip:127.0.1.1:5082;transport=tcp;lr>
>
> P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
>
> Supported: path
>
> To: <sip:2010000039 at ims.com>
>
> Route: <sip:icscf at sprout.ims.com;lr>
>
> Max-Forwards: 70
>
> Contact: <sip:2010000039 at 127.0.1.1:34768>;reg-id=1;+sip.instance=
> "<urn:uuid:00000000-0000-0000-0000-000000000001>"
>
> Call-ID: 1-784 at 127.0.1.1
>
> CSeq: 1 REGISTER
>
> Expires: 3600
>
> Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY,
> MESSAGE, PRACK
>
> Supported: path, gruu
>
> Authorization: Digest username="2010000039 at ims.com",realm="ims.com
> ",uri="sip:ims.com",nonce="",response="",algorithm=Digest-MD5
>
> User-Agent: 00-00000-0000000000000 Phone IMS 10.0
>
> P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=
> 000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
>
> P-Visited-Network-ID: ims.com
>
>
>
> SIP/2.0 401 Unauthorized
>
> Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-
> 1-0
>
> Call-ID: 1-784 at 127.0.1.1
>
> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>
> To: <sip:2010000039 at ims.com>;tag=z9hG4bKPjDBaGZjqTrLQiDSlHihO36oMPm7fxz2sQ
>
> CSeq: 1 REGISTER
>
> P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
>
> WWW-Authenticate: Digest  realm="ims.com",nonce="
> 0e07c1b77b566f37",opaque="5171f001504c2c3a",algorithm=MD5,qop="auth"
>
> Content-Length:  0
>
>
>
> REGISTER sip:ims.com SIP/2.0
>
> Via: SIP/2.0/TCP 127.0.1.1:34768;branch=z9hG4bK-784-1-2
>
> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>
> Content-Length: 0
>
> Require: Path
>
> Path: <sip:127.0.1.1:5082;transport=tcp;lr>
>
> P-Charging-Vector: icid-value=d4511351a7e24c5ff16243bac827fc3f1
>
> Supported: path
>
> To: <sip:2010000039 at ims.com>
>
> Route: <sip:icscf at sprout.ims.com;lr>
>
> Max-Forwards: 70
>
> Contact: <sip:2010000039 at 127.0.1.1:34768>;reg-id=1;+sip.instance=
> "<urn:uuid:00000000-0000-0000-0000-000000000001>"
>
> Call-ID: 1-784 at 127.0.1.1
>
> CSeq: 1 REGISTER
>
> Expires: 3600
>
> Allow: INVITE, ACK, OPTIONS, CANCEL, BYE, UPDATE, INFO, REFER, NOTIFY,
> MESSAGE, PRACK
>
> Supported: path, gruu
>
> Authorization: Digest username="2010000039 at ims.com",realm="ims.com
> ",cnonce="66334873",nc=00000001,qop=auth,uri="sip:sprout.ims.com:5052
> ",nonce="0e07c1b77b566f37",response="788d4520717e4e7b29f7fab43fdc44
> 8f",algorithm=MD5,opaque="5171f001504c2c3a"
>
> User-Agent: 00-00000-0000000000000 Phone IMS 10.0
>
> P-Access-Network-Info: IEEE-802.11;i-wlan-node-id=
> 000000000000;country=GB;local-time-zone="2016-01-01T00:00:00-00:00"
>
> P-Visited-Network-ID: ims.com
>
>
>
> SIP/2.0 200 OK
>
> Service-Route: <sip:scscf.sprout.ims.com;transport=TCP;lr;orig;
> username=2010000039%40ims.com;nonce=0e07c1b77b566f37>
>
> Via: SIP/2.0/TCP 127.0.1.1:34768;received=10.224.61.13;branch=z9hG4bK-784-
> 1-2
>
> Call-ID: 1-784 at 127.0.1.1
>
> From: <sip:2010000039 at ims.com>;tag=784SIPpTag001
>
> To: <sip:2010000039 at ims.com>;tag=z9hG4bKPjIvjh2DjwvU.vVNEv.nOiYAfsZRgMjHDF
>
> CSeq: 1 REGISTER
>
> P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"
>
> Supported: outbound
>
> Contact: <sip:2010000039 at 127.0.1.1:34768>;expires=1800;+sip.
> instance="<urn:uuid:00000000-0000-0000-0000-000000000001>";
> reg-id=1;pub-gruu="sip:2010000039 at ims.com;gr=urn:
> uuid:00000000-0000-0000-0000-000000000001"
>
> Require: outbound
>
> Path: <sip:127.0.1.1:5082;transport=tcp;lr>
>
> P-Associated-URI: <sip:2010000039 at ims.com>
>
> Content-Length:  0
>
>
>
>
>
> thanks in advance, Please resply.
>
>
>
> cheers,
>
> sunil
>
>
>
>
>
> On Tue, Mar 20, 2018 at 6:53 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:
>
> Hi,
>
> It is using some other port on stress node not 5082. Is this a problem, if
> yes how can I fix this i have tried to open 5082 port on stress node using *sudo
> ufw allow 5082/tcp, *but no effect.
>
> Please check the wireshark log:
>
>
>
> Frame 2406: 703 bytes on wire (5624 bits), 703 bytes captured (5624 bits)
>
> Ethernet II, Src: PcsCompu_ff:d2:88 (08:00:27:ff:d2:88), Dst:
> PcsCompu_ab:71:0f (08:00:27:ab:71:0f)
>
> Internet Protocol Version 4, Src: 10.224.61.22, Dst: 10.224.61.13
>
> Transmission Control Protocol, Src Port: rlm-admin (5054), Dst Port: 34312
> (34312), Seq: 349, Ack: 2199, Len: 637
>
> Session Initiation Protocol (503)
>
>
>
>
>
> cheers,
>
> sunil
>
>
>
> On Tue, Mar 20, 2018 at 5:21 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:
>
> Hi all,
>
> I have taken tcpdump also but there is no SIP message. Please through some
> light on this problem. I am trying from last weak, not able to catch the
> problem. Thanks in advance.
>
>
>
> cheers,
>
> Sunil
>
>
>
> On Tue, Mar 20, 2018 at 10:30 AM, Sunil Kumar <skgola1997 at gmail.com>
> wrote:
>
> Hi CW team,
>
> Anyone out there please help me. I am facing problem in stress testing. I
> have installed CW manually. whenever I was running 1 or less than 20 it
> give some errors like:
>
>
>
> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
> <http://ims.com/> 1 2*
>
> [sudo] password for ubuntu:
>
> Starting initial registration, will take 0 seconds
>
> Initial registration succeeded
>
> Starting test
>
> Test complete
>
> Traceback (most recent call last):
>
>   File "/usr/share/clearwater/bin/run_stress", line 340, in <module>
>
>     with open(CALLER_STATS) as f:
>
> IOError: [Errno 2] No such file or directory: '/var/log/clearwater-sip-
> stress/18065_caller_stats.log'
>
>
>
>
>
> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress ims.com
> <http://ims.com/> 10 5*
>
> Starting initial registration, will take 0 seconds
>
> Initial registration succeeded
>
> Starting test
>
> Test complete
>
> Traceback (most recent call last):
>
>   File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
>
>     call_success_rate = 100 * float(row['SuccessfulCall(C)']) /
> float(row['TotalCallCreated'])
>
> ZeroDivisionError: float division by zero
>
>
>
>
>
> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress
> iind.intel.com <http://iind.intel.com/> 50 5*
>
> Starting initial registration, will take 0 seconds
>
> Initial registration succeeded
>
> Starting test
>
> Test complete
>
>
>
> Elapsed time: 00:03:41
>
> Start: 2018-03-20 17:46:43.268136
>
> End: 2018-03-20 17:51:31.363406
>
>
>
> Total calls: 2
>
> Successful calls: 0 (0.0%)
>
> Failed calls: 2 (100.0%)
>
> Unfinished calls: 0
>
>
>
> Retransmissions: 0
>
>
>
> Average time from INVITE to 180 Ringing: 0.0ms
>
> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
>
> Failed: call success rate 0.0% is lower than target 100.0%!
>
>
>
> Total re-REGISTERs: 8
>
> Successful re-REGISTERs: 8 (100.0%)
>
> Failed re-REGISTERS: 0 (0.0%)
>
>
>
> REGISTER retransmissions: 0
>
>
>
> Average time from REGISTER to 200 OK: 86.0ms
>
>
>
> Log files at /var/log/clearwater-sip-stress/18566_*
>
>
>
>
>
>
>
> *[]ubuntu at stress:~$ cat
> /var/log/clearwater-sip-stress/18566_caller_errors.log*
>
> sipp: The following events occured:
>
> 2018-03-20      17:48:34.125945 1521548314.125945: Aborting call on
> unexpected message for Call-Id '1-18576 at 127.0.1.1': while expecting '183'
> (index 2), received '*SIP/2.0 503 Service Unavailable*
>
> Via: SIP/2.0/TCP 127.0.1.1:42276;received=10.224.61.13;branch=z9hG4bK-
> 18576-1-0
>
> Record-Route: <sip:scscf.sprout.ims.com;transport=TCP;lr;billing-role=
> charge-term>
>
> Record-Route: <sip:scscf.sprout. ims.com ;transport=TCP;lr;billing-
> role=charge-orig>
>
> Call-ID: 1-18576 at 127.0.1.1
>
> From: <sip:2010000042@ ims.com >;tag=18576SIPpTag001
>
> To: <sip:2010000015@ ims.co>;tag=z9hG4bKPj1Lm9whhQMslKrcZxnN6qCH0tb9Lj5Neu
>
> CSeq: 1 INVITE
>
> P-Charging-Vector: icid-value="18576SIPpTag001";orig-ioi= ims.com
> ;term-ioi= ims.com
>
> P-Charging-Function-Addresses: ccf=0.0.0.0
>
> Content-Length:  0
>
>
>
>
>
> *[sprout]ubuntu at sprout:/var/log/sprout$ cat sprout_current.txt*
>
> --start msg--
>
>
>
> SIP/2.0 200 OK
>
> Via: SIP/2.0/TCP 10.224.61.22;rport=49294;received=10.224.61.22;branch=
> z9hG4bK-670172
>
> Call-ID: poll-sip-670172
>
> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670172
>
> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670172
>
> CSeq: 670172 OPTIONS
>
> Content-Length:  0
>
>
>
>
>
> --end msg--
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug pjsip: tdta0x7f35841b
> Destroying txdata Response msg 200/OPTIONS/cseq=670172 (tdta0x7f35841bfe80)
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> thread_dispatcher.cpp:270: Worker thread completed processing message
> 0x7f34ec34a3e8
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> thread_dispatcher.cpp:284: Request latency = 254us
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f778
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 254 for 0x1d8f820
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:217: Rate
> adjustment calculation inputs: err -0.981500, smoothed latency 185, target
> latency 10000
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Info load_monitor.cpp:302:
> Maximum incoming request rate/second unchanged at 2000.000000 (current
> request rate is 0.200000 requests/sec, minimum threshold for a change is
> 1000.000000 requests/sec).
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
> 2000ui into continuous accumulator statistic
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample
> 2000ui into continuous accumulator statistic
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug utils.cpp:878: Removed
> IOHook 0x7f35577d5e30 to stack. There are now 0 hooks
>
> 20-03-2018 12:27:23.234 UTC [7f35577d6700] Debug
> thread_dispatcher.cpp:158: Attempting to process queue element
>
> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP connection closed
>
> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Debug
> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>
> 20-03-2018 12:27:25.235 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>
> 20-03-2018 12:27:28.790 UTC [7f3573109700] Warning (Net-SNMP): Warning:
> Failed to connect to the agentx master agent ([NIL]):
>
> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip:    tcplis:5054
> TCP listener 10.224.61.22:5054: got incoming TCP connection from
> 10.224.61.22:42848, sock=573
>
> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> tcp->base.local_name: 10.224.61.22
>
> 20-03-2018 12:27:31.277 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP server transport created
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
> Processing incoming message: Request msg OPTIONS/cseq=670180
> (rdata0x7f34ec027690)
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Verbose
> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670180
> (rdata0x7f34ec027690) from TCP 10.224.61.22:42848:
>
> --start msg--
>
>
>
> OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
>
> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670180
>
> Max-Forwards: 2
>
> To: <sip:poll-sip at 10.224.61.22:5054>
>
> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670180
>
> Call-ID: poll-sip-670180
>
> CSeq: 670180 OPTIONS
>
> Contact: <sip:10.224.61.22>
>
> Accept: application/sdp
>
> Content-Length: 0
>
> User-Agent: poll-sip
>
>
>
>
>
> --end msg--
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
> Classified URI as 3
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
> 0x7f34ec34a3e8
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
> for worker threads with priority 15
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
>
> 20-03-2018 12:27:31.314 UTC [7f34f170a700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:872: Added
> IOHook 0x7f353ffa6e30 to stack. There are now 1 hooks
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:183: Request latency so far = 57us
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: sip_endpoint.c
> Distributing rdata to modules: Request msg OPTIONS/cseq=670180
> (rdata0x7f34ec34a3e8)
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug uri_classifier.cpp:172:
> Classified URI as 3
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip:       endpoint
> Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) created
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Verbose
> common_sip_processing.cpp:103: TX 282 bytes Response msg
> 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300) to TCP 10.224.61.22:42848:
>
> --start msg--
>
>
>
> SIP/2.0 200 OK
>
> Via: SIP/2.0/TCP 10.224.61.22;rport=42848;received=10.224.61.22;branch=
> z9hG4bK-670180
>
> Call-ID: poll-sip-670180
>
> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670180
>
> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670180
>
> CSeq: 670180 OPTIONS
>
> Content-Length:  0
>
>
>
>
>
> --end msg--
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug pjsip: tdta0x7f34d809
> Destroying txdata Response msg 200/OPTIONS/cseq=670180 (tdta0x7f34d8091300)
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:270: Worker thread completed processing message
> 0x7f34ec34a3e8
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:284: Request latency = 129us
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f778
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 129 for 0x1d8f820
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug load_monitor.cpp:341: Not
> recalculating rate as we haven't processed 20 requests yet (only 1).
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug utils.cpp:878: Removed
> IOHook 0x7f353ffa6e30 to stack. There are now 0 hooks
>
> 20-03-2018 12:27:31.314 UTC [7f353ffa7700] Debug
> thread_dispatcher.cpp:158: Attempting to process queue element
>
> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:327:
> Process request for URL /ping, args (null)
>
> 20-03-2018 12:27:31.333 UTC [7f34f0f09700] Verbose httpstack.cpp:68:
> Sending response 200 to request for URL /ping, args (null)
>
> 20-03-2018 12:27:33.315 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP connection closed
>
> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Debug
> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>
> 20-03-2018 12:27:33.316 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>
> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip:    tcplis:5053
> TCP listener 10.224.61.22:5053: got incoming TCP connection from
> 10.224.61.22:49356, sock=573
>
> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> tcp->base.local_name: 10.224.61.22
>
> 20-03-2018 12:27:33.328 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP server transport created
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug pjsip: sip_endpoint.c
> Processing incoming message: Request msg OPTIONS/cseq=670182
> (rdata0x7f34ec027690)
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Verbose
> common_sip_processing.cpp:87: RX 351 bytes Request msg OPTIONS/cseq=670182
> (rdata0x7f34ec027690) from TCP 10.224.61.22:49356:
>
> --start msg--
>
>
>
> OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
>
> Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-670182
>
> Max-Forwards: 2
>
> To: <sip:poll-sip at 10.224.61.22:5053>
>
> From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=670182
>
> Call-ID: poll-sip-670182
>
> CSeq: 670182 OPTIONS
>
> Contact: <sip:10.224.61.22>
>
> Accept: application/sdp
>
> Content-Length: 0
>
> User-Agent: poll-sip
>
>
>
>
>
> --end msg--
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug uri_classifier.cpp:172:
> Classified URI as 3
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:554: Recieved message 0x7f34ec027690 on worker thread
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:571: Admitted request 0x7f34ec027690 on worker thread
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:606: Incoming message 0x7f34ec027690 cloned to
> 0x7f34ec34a3e8
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> thread_dispatcher.cpp:625: Queuing cloned received message 0x7f34ec34a3e8
> for worker threads with priority 15
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d93708
>
> 20-03-2018 12:27:33.329 UTC [7f34f170a700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 0 for 0x1d937b0
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:872: Added
> IOHook 0x7f354d7c1e30 to stack. There are now 1 hooks
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f34ec34a3e8
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:183: Request latency so far = 102us
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: sip_endpoint.c
> Distributing rdata to modules: Request msg OPTIONS/cseq=670182
> (rdata0x7f34ec34a3e8)
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:139:
> home domain: false, local_to_node: true, is_gruu: false,
> enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug uri_classifier.cpp:172:
> Classified URI as 3
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip:       endpoint
> Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) created
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Verbose
> common_sip_processing.cpp:103: TX 282 bytes Response msg
> 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350) to TCP 10.224.61.22:49356:
>
> --start msg--
>
>
>
> SIP/2.0 200 OK
>
> Via: SIP/2.0/TCP 10.224.61.22;rport=49356;received=10.224.61.22;branch=
> z9hG4bK-670182
>
> Call-ID: poll-sip-670182
>
> From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=670182
>
> To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-670182
>
> CSeq: 670182 OPTIONS
>
> Content-Length:  0
>
>
>
>
>
> --end msg--
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug pjsip: tdta0x7f34ec00
> Destroying txdata Response msg 200/OPTIONS/cseq=670182 (tdta0x7f34ec004350)
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:270: Worker thread completed processing message
> 0x7f34ec34a3e8
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:284: Request latency = 232us
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f778
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> event_statistic_accumulator.cpp:32: Accumulate 232 for 0x1d8f820
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug load_monitor.cpp:341: Not
> recalculating rate as we haven't processed 20 requests yet (only 2).
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug utils.cpp:878: Removed
> IOHook 0x7f354d7c1e30 to stack. There are now 0 hooks
>
> 20-03-2018 12:27:33.329 UTC [7f354d7c2700] Debug
> thread_dispatcher.cpp:158: Attempting to process queue element
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:244: Reraising
> all alarms with a known state
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1001.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1005.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1011.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1012.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1013.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1004.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1002.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1009.1 alarm
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Debug alarm.cpp:303:
> AlarmReqAgent: queue overflowed
>
> 20-03-2018 12:27:34.994 UTC [7f358b7fe700] Status alarm.cpp:37: sprout
> issued 1010.1 alarm
>
> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP connection closed
>
> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Debug
> connection_tracker.cpp:67: Connection 0x7f34ec027358 has been destroyed
>
> 20-03-2018 12:27:35.330 UTC [7f34f170a700] Verbose pjsip: tcps0x7f34ec02
> TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
>
>
>
>
>
>
>
> all calls are failing I don't know what is going on, I am newbie to CW
> please guide some solution it will be great help.
>
>
>
>
>
> Thanks,
>
> Sunil
>
>
>
>
>
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180326/e3cc1d9e/attachment.html>
-------------- next part --------------
26-03-2018 18:29:55.575 UTC [7fcd35f3f780] Status mmtelasplugin.cpp:59: MMTel AS plugin enabled
26-03-2018 18:29:55.575 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table mmtel_as_incoming_sip_transactions
26-03-2018 18:29:55.575 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table mmtel_as_outgoing_sip_transactions
26-03-2018 18:29:55.576 UTC [7fcd35f3f780] Status mmtelasplugin.cpp:68: Creating connection to XDMS homer.ims.com:7888
26-03-2018 18:29:55.576 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table homer-ip-count
26-03-2018 18:29:55.576 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table homer-latency
26-03-2018 18:29:55.576 UTC [7fcd35f3f780] Status http_connection_pool.cpp:35: Connection pool will use calculated response timeout of 100ms
26-03-2018 18:29:55.576 UTC [7fcd35f3f780] Status httpconnection.h:58: Configuring HTTP Connection
26-03-2018 18:29:55.576 UTC [7fcd35f3f780] Status httpconnection.h:59:   Connection created for server homer.ims.com:7888
26-03-2018 18:29:55.576 UTC [7fcd35f3f780] Debug pluginloader.cpp:119: Sproutlet mmtel using API version 1
26-03-2018 18:29:55.576 UTC [7fcd35f3f780] Status pluginloader.cpp:126: Loaded sproutlet mmtel using API version 1
26-03-2018 18:29:55.576 UTC [7fcd35f3f780] Status pluginloader.cpp:68: Attempt to load plug-in /usr/share/clearwater/sprout/plugins/sprout_scscf.so
26-03-2018 18:29:55.576 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table scscf_incoming_sip_transactions
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table scscf_outgoing_sip_transactions
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table no_matching_fallback_ifcs
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table no_matching_ifcs
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Status scscfplugin.cpp:99: S-CSCF plugin enabled
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Debug utils.cpp:405: Malformed host/port 10.224.61.22
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Status alarm.cpp:81: Alarm severity changed from NULL to 1009.1
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Status alarm.cpp:37: sprout issued 1009.1 alarm
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Status alarm.cpp:81: Alarm severity changed from NULL to 1010.1
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Status alarm.cpp:37: sprout issued 1010.1 alarm
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table scscf_routed_by_preloaded_route
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table invites_cancelled_before_1xx
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table invites_cancelled_after_1xx
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table scscf_audio_session_setup_time
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table scscf_video_session_setup_time
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table scscf_forked_invites
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table scscf_barred_calls
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Debug scscfsproutlet.cpp:118: Creating S-CSCF Sproutlet
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Debug scscfsproutlet.cpp:119:   S-CSCF cluster URI = sip:scscf.sprout.ims.com;transport=TCP
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Debug scscfsproutlet.cpp:120:   S-CSCF node URI    = sip:10.224.61.22:5054
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Debug scscfsproutlet.cpp:121:   I-CSCF URI         = sip:icscf.sprout.ims.com;transport=TCP
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Debug scscfsproutlet.cpp:122:   BGCF URI           = sip:bgcf.sprout.ims.com;transport=TCP
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table initial_reg_success_fail_count
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table re_reg_success_fail_count
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table de_reg_success_fail_count
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table third_party_initial_reg_success_fail_count
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table third_party_re_reg_success_fail_count
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table third_party_de_reg_success_fail_count
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table sip_digest_auth_success_fail_count
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table ims_aka_auth_success_fail_count
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table non_register_auth_success_fail_count
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Debug pluginloader.cpp:119: Sproutlet authentication using API version 1
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Status pluginloader.cpp:126: Loaded sproutlet authentication using API version 1
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Debug pluginloader.cpp:119: Sproutlet registrar using API version 1
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Status pluginloader.cpp:126: Loaded sproutlet registrar using API version 1
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Debug pluginloader.cpp:119: Sproutlet subscription using API version 1
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Status pluginloader.cpp:126: Loaded sproutlet subscription using API version 1
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Debug pluginloader.cpp:119: Sproutlet scscf-proxy using API version 1
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Status pluginloader.cpp:126: Loaded sproutlet scscf-proxy using API version 1
26-03-2018 18:29:55.577 UTC [7fcd35f3f780] Status pluginloader.cpp:68: Attempt to load plug-in /usr/share/clearwater/sprout/plugins/sprout_icscf.so
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table icscf_incoming_sip_transactions
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table icscf_outgoing_sip_transactions
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Status icscfplugin.cpp:67: I-CSCF plugin enabled
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Debug updater.h:31: Created updater
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Status scscfselector.cpp:52: Loading S-CSCF configuration from ./s-cscf.json
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Warning scscfselector.cpp:141: The S-CSCF json file is empty/invalid. Using default values
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Debug acr.cpp:1782: Created RalfACR factory for node type I-CSCF
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table icscf_session_establishment
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table icscf_session_establishment_network
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Debug pluginloader.cpp:119: Sproutlet icscf using API version 1
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Status pluginloader.cpp:126: Loaded sproutlet icscf using API version 1
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Debug pluginloader.cpp:64: Skipping /usr/share/clearwater/sprout/plugins/.. - doesn't have .so extension
26-03-2018 18:29:55.578 UTC [7fcd35f3f780] Status pluginloader.cpp:68: Attempt to load plug-in /usr/share/clearwater/sprout/plugins/sprout_bgcf.so
26-03-2018 18:29:55.579 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table bgcf_incoming_sip_transactions
26-03-2018 18:29:55.579 UTC [7fcd35f3f780] Info snmp_table.h:63: Registering SNMP table bgcf_outgoing_sip_transactions
26-03-2018 18:29:55.579 UTC [7fcd35f3f780] Status bgcfplugin.cpp:71: BGCF plugin enabled
26-03-2018 18:29:55.579 UTC [7fcd35f3f780] Debug updater.h:31: Created updater
26-03-2018 18:29:55.579 UTC [7fcd35f3f780] Debug bgcfservice.cpp:38: stat(./bgcf.json) returns 0
26-03-2018 18:29:55.579 UTC [7fcd35f3f780] Status bgcfservice.cpp:48: Loading BGCF configuration from ./bgcf.json
26-03-2018 18:29:55.579 UTC [7fcd35f3f780] Debug acr.cpp:1782: Created RalfACR factory for node type BGCF
26-03-2018 18:29:55.579 UTC [7fcd35f3f780] Debug pluginloader.cpp:119: Sproutlet bgcf using API version 1
26-03-2018 18:29:55.579 UTC [7fcd35f3f780] Status pluginloader.cpp:126: Loaded sproutlet bgcf using API version 1
26-03-2018 18:29:55.579 UTC [7fcd35f3f780] Status pluginloader.cpp:150: Finished loading plug-ins
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://client_count statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://connected_homers statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://connected_homesteads statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://connected_sprouts statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://latency_us statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://hss_latency_us statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://hss_digest_latency_us statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://hss_subscription_latency_us statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://xdm_latency_us statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://incoming_requests statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://rejected_overload statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://queue_size statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://hss_user_auth_latency_us statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://hss_location_latency_us statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://connected_ralfs statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://cdiv_total statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://cdiv_unconditional statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://cdiv_busy statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://cdiv_not_registered statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://cdiv_no_answer statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://cdiv_not_reachable statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://memento_completed_calls statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://memento_failed_calls statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://memento_not_recorded_overload statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://memento_cassandra_read_latency statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:111: Initializing inproc://memento_cassandra_write_latency statistic listener
26-03-2018 18:29:55.580 UTC [7fcd1ffff700] Debug zmq_lvc.cpp:119: Enabled XPUB_VERBOSE mode
26-03-2018 18:29:55.580 UTC [7fcd12fe5700] Debug updater.h:66: Started updater thread
26-03-2018 18:29:55.580 UTC [7fcd127e4700] Debug updater.h:66: Started updater thread
26-03-2018 18:29:55.580 UTC [7fcd11fe3700] Debug updater.h:66: Started updater thread
26-03-2018 18:29:55.580 UTC [7fcd11120700] Debug updater.h:66: Started updater thread
26-03-2018 18:29:55.580 UTC [7fcd106eb700] Debug updater.h:66: Started updater thread
26-03-2018 18:29:55.580 UTC [7fcd35f3f780] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
26-03-2018 18:29:55.580 UTC [7fcd35f3f780] Verbose pjsip: sip_endpoint.c Module "mod-sproutlet-controller" registered
26-03-2018 18:29:55.580 UTC [7fcd35f3f780] Verbose pjsip: sip_endpoint.c Module "mod-sproutlet-controller-tu" registered
26-03-2018 18:29:55.580 UTC [7fcd35f3f780] Debug sproutletproxy.cpp:51: Root Record-Route URI = sprout.ims.com
26-03-2018 18:29:55.580 UTC [7fcd35f3f780] Verbose pjsip: sip_endpoint.c Module "mod-common-processing" registered
26-03-2018 18:29:55.580 UTC [7fcd35f3f780] Verbose pjsip: sip_endpoint.c Module "mod-thread-dispatcher" registered
26-03-2018 18:29:55.583 UTC [7fcd0cee4700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd0cee4700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd0d6e5700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd0d6e5700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd0dee6700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd0dee6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd0e6e7700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd0e6e7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd07fff700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd07fff700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd077fe700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd077fe700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd06ffd700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd06ffd700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd067fc700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd067fc700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd05ffb700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd05ffb700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd057fa700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd057fa700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd04ff9700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd04ff9700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd047f8700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd047f8700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd03ff7700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd03ff7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.583 UTC [7fcd037f6700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.583 UTC [7fcd037f6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcd02ff5700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcd02ff5700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcd027f4700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcd027f4700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcd01ff3700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcd01ff3700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcd017f2700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcd017f2700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcd00ff1700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcd00ff1700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcd007f0700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcd007f0700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccfffef700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccfffef700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcd0eee8700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcd0eee8700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccff7ee700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccff7ee700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccfefed700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccfefed700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccfe7ec700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccfe7ec700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccfdfeb700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccfdfeb700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccec7c8700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccec7c8700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccea7c4700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccea7c4700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce47b8700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce47b8700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce77be700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce77be700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcceefcd700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcceefcd700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccebfc7700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccebfc7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce8fc1700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce8fc1700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf6fdd700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf6fdd700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf1fd3700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf1fd3700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce97c2700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce97c2700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce3fb7700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce3fb7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf5fdb700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf5fdb700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf27d4700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf27d4700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccef7ce700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccef7ce700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcceafc5700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcceafc5700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce5fbb700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce5fbb700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce4fb9700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce4fb9700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccfc7e8700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccfc7e8700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccfb7e6700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccfb7e6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf87e0700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf87e0700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf57da700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf57da700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf4fd9700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf4fd9700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf9fe3700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf9fe3700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccfa7e4700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccfa7e4700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf97e2700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf97e2700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf77de700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf77de700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf47d8700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf47d8700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf2fd5700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf2fd5700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf17d2700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf17d2700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf07d0700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf07d0700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcceffcf700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcceffcf700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccecfc9700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccecfc9700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce87c0700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce87c0700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce7fbf700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce7fbf700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce67bc700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce67bc700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccfafe5700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccfafe5700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf8fe1700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf8fe1700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce57ba700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce57ba700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccfd7ea700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccfd7ea700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccfcfe9700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccfcfe9700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccfbfe7700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccfbfe7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf7fdf700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf7fdf700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf67dc700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf67dc700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf3fd7700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf3fd7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf37d6700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf37d6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccf0fd1700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccf0fd1700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccee7cc700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccee7cc700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccedfcb700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccedfcb700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcced7ca700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcced7ca700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcceb7c6700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcceb7c6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce9fc3700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce9fc3700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce6fbd700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce6fbd700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce37b6700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce37b6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce2fb5700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce2fb5700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce27b4700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce27b4700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce1fb3700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce1fb3700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce17b2700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce17b2700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce0fb1700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce0fb1700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fcce07b0700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fcce07b0700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccdffaf700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccdffaf700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccdf7ae700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccdf7ae700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccdefad700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccdefad700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccde7ac700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccde7ac700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccddfab700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccddfab700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccdd7aa700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccdd7aa700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccdcfa9700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccdcfa9700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccdc7a8700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccdc7a8700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccdbfa7700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccdbfa7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccdb7a6700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccdb7a6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccdafa5700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccdafa5700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccda7a4700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccda7a4700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccd9fa3700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccd9fa3700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccd97a2700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccd97a2700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccd8fa1700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccd8fa1700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccd87a0700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccd87a0700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccd7f9f700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccd7f9f700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccd779e700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccd779e700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.584 UTC [7fccd6f9d700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.584 UTC [7fccd6f9d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd679c700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd679c700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd5f9b700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd5f9b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd579a700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd579a700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd4f99700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd4f99700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd4798700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd4798700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd3f97700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd3f97700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fcd0f6e9700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fcd0f6e9700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd3796700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd3796700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd2f95700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd2f95700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd2794700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd2794700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd1f93700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd1f93700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd1792700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd1792700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd0f91700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd0f91700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccd0790700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccd0790700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fcccff8f700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fcccff8f700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fcccf78e700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fcccf78e700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fcccef8d700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fcccef8d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccce78c700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccce78c700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fcccdf8b700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fcccdf8b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fcccd78a700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fcccd78a700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccccf89700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccccf89700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fcccc788700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fcccc788700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fcccbf87700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fcccbf87700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fcccb786700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fcccb786700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fcccaf85700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fcccaf85700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccca784700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccca784700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc9f83700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc9f83700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc9782700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc9782700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc8f81700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc8f81700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc8780700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc8780700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc7f7f700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc7f7f700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc777e700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc777e700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc6f7d700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc6f7d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc677c700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc677c700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc5f7b700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc5f7b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc577a700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc577a700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc4f79700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc4f79700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc4778700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc4778700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc3f77700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc3f77700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc3776700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc3776700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc2f75700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc2f75700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc2774700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc2774700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc1f73700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc1f73700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc1772700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc1772700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc0f71700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc0f71700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccc0770700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccc0770700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccbff6f700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccbff6f700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccbf76e700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccbf76e700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccbef6d700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccbef6d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccbe76c700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccbe76c700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccbdf6b700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccbdf6b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccbd76a700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccbd76a700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccbcf69700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccbcf69700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccbc768700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccbc768700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccbbf67700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccbbf67700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccbb766700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccbb766700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccbaf65700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccbaf65700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccba764700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccba764700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccb9f63700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccb9f63700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccb9762700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccb9762700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccb8f61700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccb8f61700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccb8760700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccb8760700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccb7f5f700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccb7f5f700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccb775e700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccb775e700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccb6f5d700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccb6f5d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccb675c700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccb675c700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccb5f5b700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccb5f5b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccb575a700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccb575a700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.585 UTC [7fccb4f59700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.585 UTC [7fccb4f59700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.586 UTC [7fcd35f3f780] Debug thread_dispatcher.cpp:708: Worker threads started
26-03-2018 18:29:55.586 UTC [7fcd35f3f780] Status httpstack.cpp:163: Binding HTTP TCP socket: address=10.224.61.22, port=9888
26-03-2018 18:29:55.587 UTC [7fcca0730700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca0730700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9b726700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9b726700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9f72e700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9f72e700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9bf27700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9bf27700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc98f21700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc98f21700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca5f3b700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca5f3b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca4f39700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca4f39700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca1f33700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca1f33700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9ef2d700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9ef2d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9e72c700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9e72c700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca3736700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca3736700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccadf4b700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccadf4b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccad74a700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccad74a700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccacf49700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccacf49700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccac748700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccac748700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccab746700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccab746700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca9f43700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca9f43700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca8740700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca8740700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca7f3f700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca7f3f700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca3f37700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca3f37700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca2f35700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca2f35700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca0f31700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca0f31700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9df2b700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9df2b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9c728700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9c728700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9af25700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9af25700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc99f23700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc99f23700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc99722700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc99722700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccb1f53700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccb1f53700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccb1752700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccb1752700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccaf74e700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccaf74e700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca8f41700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca8f41700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca4738700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca4738700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca2734700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca2734700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccb0f51700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccb0f51700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccb0750700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccb0750700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccaff4f700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccaff4f700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccaef4d700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccaef4d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccae74c700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccae74c700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccabf47700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccabf47700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccaaf45700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccaaf45700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccaa744700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccaa744700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca9742700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca9742700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca773e700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca773e700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca6f3d700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca6f3d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca673c700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca673c700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca573a700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca573a700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcca1732700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcca1732700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9ff2f700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9ff2f700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9d72a700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9d72a700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9cf29700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9cf29700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9a724700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9a724700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc98720700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc98720700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc97f1f700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc97f1f700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9771e700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9771e700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc96f1d700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc96f1d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9671c700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9671c700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc95f1b700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc95f1b700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fccb2754700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fccb2754700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc9571a700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc9571a700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc94f19700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc94f19700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc94718700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc94718700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc93f17700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc93f17700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc93716700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc93716700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc92f15700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc92f15700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc92714700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc92714700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc91f13700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc91f13700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.587 UTC [7fcc91712700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.587 UTC [7fcc91712700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.588 UTC [7fcc90f11700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.588 UTC [7fcc90f11700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.588 UTC [7fcc90710700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.588 UTC [7fcc90710700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.588 UTC [7fccb2f55700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.588 UTC [7fccb2f55700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.588 UTC [7fcc8ff0f700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.588 UTC [7fcc8ff0f700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.588 UTC [7fcc8f70e700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.588 UTC [7fcc8f70e700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.588 UTC [7fcc8ef0d700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.588 UTC [7fcc8ef0d700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.588 UTC [7fcc8e70c700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.588 UTC [7fcc8e70c700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.588 UTC [7fcc8df0b700] Status stack.cpp:164: PJSIP transport thread started with kernel thread ID 17952
26-03-2018 18:29:55.588 UTC [7fcc8df0b700] Debug utils.cpp:872: Added IOHook 0x7fcc8df0ae30 to stack. There are now 1 hooks
26-03-2018 18:29:55.588 UTC [7fccb3756700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.588 UTC [7fccb3756700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.588 UTC [7fccb3f57700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.588 UTC [7fccb3f57700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.588 UTC [7fccb4758700] Debug thread_dispatcher.cpp:354: Worker thread started
26-03-2018 18:29:55.588 UTC [7fccb4758700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.730 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:54422, sock=285
26-03-2018 18:29:55.730 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.730 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:29:55.730 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210324 (rdata0x7fcc88003650)
26-03-2018 18:29:55.730 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210324 (rdata0x7fcc88003650) from TCP 10.224.61.22:54422:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210324
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210324
Call-ID: poll-sip-1210324
CSeq: 1210324 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:29:55.730 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:29:55.731 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:29:55.731 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:29:55.731 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc88003650 on worker thread
26-03-2018 18:29:55.731 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc88003650 on worker thread
26-03-2018 18:29:55.731 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc88003650 cloned to 0x7fcc88006018
26-03-2018 18:29:55.731 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc88006018 for worker threads with priority 15
26-03-2018 18:29:55.731 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:29:55.731 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1780
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug utils.cpp:872: Added IOHook 0x7fcd0cee3e30 to stack. There are now 1 hooks
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc88006018
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug thread_dispatcher.cpp:183: Request latency so far = 68us
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210324 (rdata0x7fcc88006018)
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210324 (tdta0x7fcc700038d0) created
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210324 (tdta0x7fcc700038d0) to TCP 10.224.61.22:54422:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=54422;received=10.224.61.22;branch=z9hG4bK-1210324
Call-ID: poll-sip-1210324
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210324
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210324
CSeq: 1210324 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug pjsip: tdta0x7fcc7000 Destroying txdata Response msg 200/OPTIONS/cseq=1210324 (tdta0x7fcc700038d0)
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc88006018
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug thread_dispatcher.cpp:284: Request latency = 149us
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug event_statistic_accumulator.cpp:32: Accumulate 149 for 0x12dd778
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug event_statistic_accumulator.cpp:32: Accumulate 149 for 0x12dd7f0
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 1).
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug utils.cpp:878: Removed IOHook 0x7fcd0cee3e30 to stack. There are now 0 hooks
26-03-2018 18:29:55.731 UTC [7fcd0cee4700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:55.749 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:38569, sock=298
26-03-2018 18:29:55.749 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.749 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:29:55.755 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:33493, sock=303
26-03-2018 18:29:55.755 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.755 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:29:55.756 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:36951, sock=304
26-03-2018 18:29:55.756 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.756 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:29:55.756 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:41936, sock=305
26-03-2018 18:29:55.756 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8801 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.756 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8801 TCP server transport created
26-03-2018 18:29:55.757 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:36639, sock=306
26-03-2018 18:29:55.757 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8801 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.757 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8801 TCP server transport created
26-03-2018 18:29:55.757 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:45521, sock=307
26-03-2018 18:29:55.757 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8801 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.757 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8801 TCP server transport created
26-03-2018 18:29:55.757 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:40455, sock=308
26-03-2018 18:29:55.757 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8801 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.757 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8801 TCP server transport created
26-03-2018 18:29:55.758 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:44470, sock=309
26-03-2018 18:29:55.758 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8801 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.758 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8801 TCP server transport created
26-03-2018 18:29:55.758 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:46763, sock=310
26-03-2018 18:29:55.758 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8801 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.758 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8801 TCP server transport created
26-03-2018 18:29:55.759 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:37203, sock=311
26-03-2018 18:29:55.759 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8802 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.759 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8802 TCP server transport created
26-03-2018 18:29:55.759 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:34452, sock=312
26-03-2018 18:29:55.759 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8802 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.759 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8802 TCP server transport created
26-03-2018 18:29:55.760 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:34191, sock=313
26-03-2018 18:29:55.760 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8802 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.760 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8802 TCP server transport created
26-03-2018 18:29:55.760 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:39309, sock=314
26-03-2018 18:29:55.761 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8802 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.761 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8802 TCP server transport created
26-03-2018 18:29:55.761 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:44654, sock=319
26-03-2018 18:29:55.761 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8802 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.761 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8802 TCP server transport created
26-03-2018 18:29:55.761 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:35062, sock=320
26-03-2018 18:29:55.761 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8803 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.761 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8803 TCP server transport created
26-03-2018 18:29:55.762 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:42010, sock=321
26-03-2018 18:29:55.762 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8803 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.762 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8803 TCP server transport created
26-03-2018 18:29:55.762 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:40619, sock=322
26-03-2018 18:29:55.762 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8803 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.762 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8803 TCP server transport created
26-03-2018 18:29:55.763 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:37745, sock=323
26-03-2018 18:29:55.763 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8803 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.763 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8803 TCP server transport created
26-03-2018 18:29:55.763 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:34340, sock=324
26-03-2018 18:29:55.763 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8803 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.763 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8803 TCP server transport created
26-03-2018 18:29:55.763 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:46467, sock=325
26-03-2018 18:29:55.763 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8803 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.763 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8803 TCP server transport created
26-03-2018 18:29:55.764 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:37189, sock=326
26-03-2018 18:29:55.764 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8804 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.764 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8804 TCP server transport created
26-03-2018 18:29:55.764 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:42300, sock=327
26-03-2018 18:29:55.764 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8804 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.764 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8804 TCP server transport created
26-03-2018 18:29:55.765 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:38539, sock=328
26-03-2018 18:29:55.765 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8804 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.765 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8804 TCP server transport created
26-03-2018 18:29:55.765 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:34073, sock=329
26-03-2018 18:29:55.765 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8804 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.765 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8804 TCP server transport created
26-03-2018 18:29:55.765 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:42370, sock=330
26-03-2018 18:29:55.765 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8804 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.765 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8804 TCP server transport created
26-03-2018 18:29:55.766 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:36903, sock=335
26-03-2018 18:29:55.766 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8804 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.766 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8804 TCP server transport created
26-03-2018 18:29:55.766 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:44061, sock=336
26-03-2018 18:29:55.766 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8805 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.766 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8805 TCP server transport created
26-03-2018 18:29:55.767 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:35915, sock=337
26-03-2018 18:29:55.767 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8805 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.767 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8805 TCP server transport created
26-03-2018 18:29:55.767 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:36989, sock=338
26-03-2018 18:29:55.767 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8805 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.767 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8805 TCP server transport created
26-03-2018 18:29:55.767 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:38047, sock=339
26-03-2018 18:29:55.768 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8805 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.768 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8805 TCP server transport created
26-03-2018 18:29:55.768 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:44473, sock=340
26-03-2018 18:29:55.768 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8805 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.768 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8805 TCP server transport created
26-03-2018 18:29:55.769 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:46113, sock=341
26-03-2018 18:29:55.769 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8806 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.769 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8806 TCP server transport created
26-03-2018 18:29:55.769 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:36518, sock=342
26-03-2018 18:29:55.769 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8806 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.769 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8806 TCP server transport created
26-03-2018 18:29:55.769 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5052 TCP listener 10.224.61.22:5052: got incoming TCP connection from 10.224.61.8:46384, sock=343
26-03-2018 18:29:55.769 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8806 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:55.769 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8806 TCP server transport created
26-03-2018 18:29:55.899 UTC [7fcd35f3f780] Status httpstack.cpp:234: Binding HTTP unix socket: path=/tmp/sprout-http-mgmt-socket
26-03-2018 18:29:55.900 UTC [7fcc8d70a700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
26-03-2018 18:29:55.900 UTC [7fcc8d70a700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
26-03-2018 18:29:57.731 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:29:57.731 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc88003318 has been destroyed
26-03-2018 18:29:57.731 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:29:57.744 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:60946, sock=285
26-03-2018 18:29:57.744 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:29:57.744 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:29:57.745 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210326 (rdata0x7fcc88003650)
26-03-2018 18:29:57.745 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210326 (rdata0x7fcc88003650) from TCP 10.224.61.22:60946:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210326
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210326
Call-ID: poll-sip-1210326
CSeq: 1210326 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:29:57.745 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:29:57.745 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:29:57.745 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:29:57.745 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc88003650 on worker thread
26-03-2018 18:29:57.745 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc88003650 on worker thread
26-03-2018 18:29:57.745 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc88003650 cloned to 0x7fcc8806e5b8
26-03-2018 18:29:57.745 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806e5b8 for worker threads with priority 15
26-03-2018 18:29:57.745 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:29:57.745 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1780
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug utils.cpp:872: Added IOHook 0x7fcd0d6e4e30 to stack. There are now 1 hooks
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806e5b8
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug thread_dispatcher.cpp:183: Request latency so far = 101us
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210326 (rdata0x7fcc8806e5b8)
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210326 (tdta0x7fcd280111a0) created
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210326 (tdta0x7fcd280111a0) to TCP 10.224.61.22:60946:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=60946;received=10.224.61.22;branch=z9hG4bK-1210326
Call-ID: poll-sip-1210326
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210326
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210326
CSeq: 1210326 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug pjsip: tdta0x7fcd2801 Destroying txdata Response msg 200/OPTIONS/cseq=1210326 (tdta0x7fcd280111a0)
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806e5b8
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug thread_dispatcher.cpp:284: Request latency = 247us
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug event_statistic_accumulator.cpp:32: Accumulate 247 for 0x12dd778
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug event_statistic_accumulator.cpp:32: Accumulate 247 for 0x12dd7f0
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 2).
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug utils.cpp:878: Removed IOHook 0x7fcd0d6e4e30 to stack. There are now 0 hooks
26-03-2018 18:29:57.745 UTC [7fcd0d6e5700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:29:59.746 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:29:59.746 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc88003318 has been destroyed
26-03-2018 18:29:59.746 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:30:05.804 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:54480, sock=285
26-03-2018 18:30:05.805 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:30:05.805 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:30:05.827 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210334 (rdata0x7fcc88003650)
26-03-2018 18:30:05.827 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210334 (rdata0x7fcc88003650) from TCP 10.224.61.22:54480:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210334
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210334
Call-ID: poll-sip-1210334
CSeq: 1210334 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:30:05.827 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:05.827 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:05.827 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:30:05.827 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc88003650 on worker thread
26-03-2018 18:30:05.827 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc88003650 on worker thread
26-03-2018 18:30:05.827 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc88003650 cloned to 0x7fcc8806e5b8
26-03-2018 18:30:05.827 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806e5b8 for worker threads with priority 15
26-03-2018 18:30:05.827 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:30:05.827 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug utils.cpp:872: Added IOHook 0x7fcd0dee5e30 to stack. There are now 1 hooks
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806e5b8
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug thread_dispatcher.cpp:183: Request latency so far = 60us
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210334 (rdata0x7fcc8806e5b8)
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210334 (tdta0x22edc60) created
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210334 (tdta0x22edc60) to TCP 10.224.61.22:54480:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=54480;received=10.224.61.22;branch=z9hG4bK-1210334
Call-ID: poll-sip-1210334
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210334
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210334
CSeq: 1210334 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug pjsip:  tdta0x22edc60 Destroying txdata Response msg 200/OPTIONS/cseq=1210334 (tdta0x22edc60)
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806e5b8
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug thread_dispatcher.cpp:284: Request latency = 155us
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug event_statistic_accumulator.cpp:32: Accumulate 155 for 0x12dd778
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug event_statistic_accumulator.cpp:32: Accumulate 155 for 0x12dd820
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 3).
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug utils.cpp:878: Removed IOHook 0x7fcd0dee5e30 to stack. There are now 0 hooks
26-03-2018 18:30:05.827 UTC [7fcd0dee6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:30:05.849 UTC [7fcc8d70a700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
26-03-2018 18:30:05.849 UTC [7fcc8d70a700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
26-03-2018 18:30:07.827 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:30:07.827 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc88003318 has been destroyed
26-03-2018 18:30:07.827 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:30:07.839 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:32774, sock=285
26-03-2018 18:30:07.839 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:30:07.839 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:30:07.840 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210336 (rdata0x7fcc88003650)
26-03-2018 18:30:07.840 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210336 (rdata0x7fcc88003650) from TCP 10.224.61.22:32774:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210336
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210336
Call-ID: poll-sip-1210336
CSeq: 1210336 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:30:07.840 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:07.840 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:07.840 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:30:07.840 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc88003650 on worker thread
26-03-2018 18:30:07.840 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc88003650 on worker thread
26-03-2018 18:30:07.840 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc88003650 cloned to 0x7fcc8806e5b8
26-03-2018 18:30:07.840 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806e5b8 for worker threads with priority 15
26-03-2018 18:30:07.840 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:30:07.840 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug utils.cpp:872: Added IOHook 0x7fcd0e6e6e30 to stack. There are now 1 hooks
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806e5b8
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug thread_dispatcher.cpp:183: Request latency so far = 112us
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210336 (rdata0x7fcc8806e5b8)
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210336 (tdta0x7fcc70008060) created
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210336 (tdta0x7fcc70008060) to TCP 10.224.61.22:32774:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=32774;received=10.224.61.22;branch=z9hG4bK-1210336
Call-ID: poll-sip-1210336
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210336
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210336
CSeq: 1210336 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug pjsip: tdta0x7fcc7000 Destroying txdata Response msg 200/OPTIONS/cseq=1210336 (tdta0x7fcc70008060)
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806e5b8
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug thread_dispatcher.cpp:284: Request latency = 274us
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug event_statistic_accumulator.cpp:32: Accumulate 274 for 0x12dd778
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug event_statistic_accumulator.cpp:32: Accumulate 274 for 0x12dd820
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 4).
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug utils.cpp:878: Removed IOHook 0x7fcd0e6e6e30 to stack. There are now 0 hooks
26-03-2018 18:30:07.840 UTC [7fcd0e6e7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:30:09.841 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:30:09.841 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc88003318 has been destroyed
26-03-2018 18:30:09.841 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:30:10.594 UTC [7fcd0feea700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
26-03-2018 18:30:15.882 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:54548, sock=460
26-03-2018 18:30:15.882 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:30:15.882 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:30:15.895 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210344 (rdata0x7fcc88003650)
26-03-2018 18:30:15.895 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210344 (rdata0x7fcc88003650) from TCP 10.224.61.22:54548:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210344
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210344
Call-ID: poll-sip-1210344
CSeq: 1210344 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:30:15.895 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:15.895 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:15.895 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:30:15.895 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc88003650 on worker thread
26-03-2018 18:30:15.895 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc88003650 on worker thread
26-03-2018 18:30:15.895 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc88003650 cloned to 0x7fcc8806e5b8
26-03-2018 18:30:15.895 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806e5b8 for worker threads with priority 15
26-03-2018 18:30:15.895 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:30:15.895 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug utils.cpp:872: Added IOHook 0x7fcd07ffee30 to stack. There are now 1 hooks
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806e5b8
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug thread_dispatcher.cpp:183: Request latency so far = 52us
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210344 (rdata0x7fcc8806e5b8)
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210344 (tdta0x7fcc8400aa40) created
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210344 (tdta0x7fcc8400aa40) to TCP 10.224.61.22:54548:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=54548;received=10.224.61.22;branch=z9hG4bK-1210344
Call-ID: poll-sip-1210344
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210344
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210344
CSeq: 1210344 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug pjsip: tdta0x7fcc8400 Destroying txdata Response msg 200/OPTIONS/cseq=1210344 (tdta0x7fcc8400aa40)
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806e5b8
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug thread_dispatcher.cpp:284: Request latency = 122us
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug event_statistic_accumulator.cpp:32: Accumulate 122 for 0x12dd778
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug event_statistic_accumulator.cpp:32: Accumulate 122 for 0x12dd820
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 5).
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug utils.cpp:878: Removed IOHook 0x7fcd07ffee30 to stack. There are now 0 hooks
26-03-2018 18:30:15.895 UTC [7fcd07fff700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:30:15.926 UTC [7fcc8d70a700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
26-03-2018 18:30:15.926 UTC [7fcc8d70a700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
26-03-2018 18:30:17.896 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:30:17.896 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc88003318 has been destroyed
26-03-2018 18:30:17.896 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:30:17.909 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:32836, sock=285
26-03-2018 18:30:17.909 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:30:17.909 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:30:17.910 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210346 (rdata0x7fcc88003650)
26-03-2018 18:30:17.910 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210346 (rdata0x7fcc88003650) from TCP 10.224.61.22:32836:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210346
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210346
Call-ID: poll-sip-1210346
CSeq: 1210346 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:30:17.910 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:17.910 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:17.910 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:30:17.910 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc88003650 on worker thread
26-03-2018 18:30:17.910 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc88003650 on worker thread
26-03-2018 18:30:17.910 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc88003650 cloned to 0x7fcc8806e5b8
26-03-2018 18:30:17.910 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806e5b8 for worker threads with priority 15
26-03-2018 18:30:17.910 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:30:17.910 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug utils.cpp:872: Added IOHook 0x7fcd077fde30 to stack. There are now 1 hooks
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806e5b8
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug thread_dispatcher.cpp:183: Request latency so far = 93us
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210346 (rdata0x7fcc8806e5b8)
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210346 (tdta0x7fcc8000e660) created
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210346 (tdta0x7fcc8000e660) to TCP 10.224.61.22:32836:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=32836;received=10.224.61.22;branch=z9hG4bK-1210346
Call-ID: poll-sip-1210346
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210346
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210346
CSeq: 1210346 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug pjsip: tdta0x7fcc8000 Destroying txdata Response msg 200/OPTIONS/cseq=1210346 (tdta0x7fcc8000e660)
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806e5b8
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug thread_dispatcher.cpp:284: Request latency = 242us
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug event_statistic_accumulator.cpp:32: Accumulate 242 for 0x12dd778
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug event_statistic_accumulator.cpp:32: Accumulate 242 for 0x12dd820
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 6).
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug utils.cpp:878: Removed IOHook 0x7fcd077fde30 to stack. There are now 0 hooks
26-03-2018 18:30:17.910 UTC [7fcd077fe700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:30:19.911 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:30:19.911 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc88003318 has been destroyed
26-03-2018 18:30:19.911 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:30:25.554 UTC [7fcd2c84f700] Status alarm.cpp:244: Reraising all alarms with a known state
26-03-2018 18:30:25.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1011.1 alarm
26-03-2018 18:30:25.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1012.1 alarm
26-03-2018 18:30:25.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1013.1 alarm
26-03-2018 18:30:25.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1009.1 alarm
26-03-2018 18:30:25.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1010.1 alarm
26-03-2018 18:30:25.607 UTC [7fcd0feea700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
26-03-2018 18:30:25.961 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:54616, sock=285
26-03-2018 18:30:25.961 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:30:25.961 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:30:25.964 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210354 (rdata0x7fcc88003820)
26-03-2018 18:30:25.964 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210354 (rdata0x7fcc88003820) from TCP 10.224.61.22:54616:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210354
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210354
Call-ID: poll-sip-1210354
CSeq: 1210354 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:30:25.964 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:25.964 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:25.964 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:30:25.964 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc88003820 on worker thread
26-03-2018 18:30:25.964 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc88003820 on worker thread
26-03-2018 18:30:25.964 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc88003820 cloned to 0x7fcc8806e7c8
26-03-2018 18:30:25.964 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806e7c8 for worker threads with priority 15
26-03-2018 18:30:25.964 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:30:25.964 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug utils.cpp:872: Added IOHook 0x7fcd06ffce30 to stack. There are now 1 hooks
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806e7c8
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug thread_dispatcher.cpp:183: Request latency so far = 54us
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210354 (rdata0x7fcc8806e7c8)
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210354 (tdta0x7fcd080827a0) created
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210354 (tdta0x7fcd080827a0) to TCP 10.224.61.22:54616:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=54616;received=10.224.61.22;branch=z9hG4bK-1210354
Call-ID: poll-sip-1210354
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210354
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210354
CSeq: 1210354 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug pjsip: tdta0x7fcd0808 Destroying txdata Response msg 200/OPTIONS/cseq=1210354 (tdta0x7fcd080827a0)
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806e7c8
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug thread_dispatcher.cpp:284: Request latency = 129us
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0x12dd778
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug event_statistic_accumulator.cpp:32: Accumulate 129 for 0x12dd820
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 7).
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug utils.cpp:878: Removed IOHook 0x7fcd06ffce30 to stack. There are now 0 hooks
26-03-2018 18:30:25.964 UTC [7fcd06ffd700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:30:26.008 UTC [7fcc8d70a700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
26-03-2018 18:30:26.008 UTC [7fcc8d70a700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
26-03-2018 18:30:27.964 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:30:27.964 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880034e8 has been destroyed
26-03-2018 18:30:27.964 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:30:27.977 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:32898, sock=285
26-03-2018 18:30:27.977 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:30:27.977 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:30:27.978 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210356 (rdata0x7fcc88003820)
26-03-2018 18:30:27.978 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210356 (rdata0x7fcc88003820) from TCP 10.224.61.22:32898:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210356
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210356
Call-ID: poll-sip-1210356
CSeq: 1210356 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:30:27.978 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:27.978 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:27.978 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:30:27.978 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc88003820 on worker thread
26-03-2018 18:30:27.978 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc88003820 on worker thread
26-03-2018 18:30:27.978 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc88003820 cloned to 0x7fcc8806e7c8
26-03-2018 18:30:27.978 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806e7c8 for worker threads with priority 15
26-03-2018 18:30:27.978 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:30:27.978 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug utils.cpp:872: Added IOHook 0x7fcd067fbe30 to stack. There are now 1 hooks
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806e7c8
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug thread_dispatcher.cpp:183: Request latency so far = 126us
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210356 (rdata0x7fcc8806e7c8)
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210356 (tdta0x7fcd200093f0) created
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210356 (tdta0x7fcd200093f0) to TCP 10.224.61.22:32898:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=32898;received=10.224.61.22;branch=z9hG4bK-1210356
Call-ID: poll-sip-1210356
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210356
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210356
CSeq: 1210356 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug pjsip: tdta0x7fcd2000 Destroying txdata Response msg 200/OPTIONS/cseq=1210356 (tdta0x7fcd200093f0)
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806e7c8
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug thread_dispatcher.cpp:284: Request latency = 327us
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug event_statistic_accumulator.cpp:32: Accumulate 327 for 0x12dd778
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug event_statistic_accumulator.cpp:32: Accumulate 327 for 0x12dd820
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 8).
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug utils.cpp:878: Removed IOHook 0x7fcd067fbe30 to stack. There are now 0 hooks
26-03-2018 18:30:27.978 UTC [7fcd067fc700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:30:29.979 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:30:29.980 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880034e8 has been destroyed
26-03-2018 18:30:29.980 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:30:36.035 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:54682, sock=285
26-03-2018 18:30:36.035 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:30:36.035 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:30:36.042 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210365 (rdata0x7fcc88003820)
26-03-2018 18:30:36.042 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210365 (rdata0x7fcc88003820) from TCP 10.224.61.22:54682:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210365
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210365
Call-ID: poll-sip-1210365
CSeq: 1210365 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:30:36.042 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:36.042 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:36.042 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:30:36.042 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc88003820 on worker thread
26-03-2018 18:30:36.042 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc88003820 on worker thread
26-03-2018 18:30:36.042 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc88003820 cloned to 0x7fcc8806e7c8
26-03-2018 18:30:36.042 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806e7c8 for worker threads with priority 15
26-03-2018 18:30:36.042 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:30:36.042 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug utils.cpp:872: Added IOHook 0x7fcd05ffae30 to stack. There are now 1 hooks
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806e7c8
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug thread_dispatcher.cpp:183: Request latency so far = 59us
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210365 (rdata0x7fcc8806e7c8)
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210365 (tdta0x7fcd280111f0) created
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210365 (tdta0x7fcd280111f0) to TCP 10.224.61.22:54682:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=54682;received=10.224.61.22;branch=z9hG4bK-1210365
Call-ID: poll-sip-1210365
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210365
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210365
CSeq: 1210365 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug pjsip: tdta0x7fcd2801 Destroying txdata Response msg 200/OPTIONS/cseq=1210365 (tdta0x7fcd280111f0)
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806e7c8
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug thread_dispatcher.cpp:284: Request latency = 147us
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug event_statistic_accumulator.cpp:32: Accumulate 147 for 0x12dd778
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug event_statistic_accumulator.cpp:32: Accumulate 147 for 0x12dd820
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 9).
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug utils.cpp:878: Removed IOHook 0x7fcd05ffae30 to stack. There are now 0 hooks
26-03-2018 18:30:36.042 UTC [7fcd05ffb700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:30:36.081 UTC [7fcc8d70a700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
26-03-2018 18:30:36.081 UTC [7fcc8d70a700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
26-03-2018 18:30:38.043 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:30:38.043 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880034e8 has been destroyed
26-03-2018 18:30:38.043 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:30:38.056 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:32958, sock=285
26-03-2018 18:30:38.056 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:30:38.056 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:30:38.057 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210367 (rdata0x7fcc88003820)
26-03-2018 18:30:38.057 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210367 (rdata0x7fcc88003820) from TCP 10.224.61.22:32958:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210367
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210367
Call-ID: poll-sip-1210367
CSeq: 1210367 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:30:38.057 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:38.057 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:38.057 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:30:38.057 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc88003820 on worker thread
26-03-2018 18:30:38.057 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc88003820 on worker thread
26-03-2018 18:30:38.057 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc88003820 cloned to 0x7fcc8806e7c8
26-03-2018 18:30:38.057 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806e7c8 for worker threads with priority 15
26-03-2018 18:30:38.057 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:30:38.057 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug utils.cpp:872: Added IOHook 0x7fcd057f9e30 to stack. There are now 1 hooks
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806e7c8
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug thread_dispatcher.cpp:183: Request latency so far = 131us
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210367 (rdata0x7fcc8806e7c8)
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210367 (tdta0x22edc60) created
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210367 (tdta0x22edc60) to TCP 10.224.61.22:32958:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=32958;received=10.224.61.22;branch=z9hG4bK-1210367
Call-ID: poll-sip-1210367
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210367
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210367
CSeq: 1210367 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug pjsip:  tdta0x22edc60 Destroying txdata Response msg 200/OPTIONS/cseq=1210367 (tdta0x22edc60)
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806e7c8
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug thread_dispatcher.cpp:284: Request latency = 367us
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug event_statistic_accumulator.cpp:32: Accumulate 367 for 0x12dd778
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug event_statistic_accumulator.cpp:32: Accumulate 367 for 0x12dd820
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 10).
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug utils.cpp:878: Removed IOHook 0x7fcd057f9e30 to stack. There are now 0 hooks
26-03-2018 18:30:38.057 UTC [7fcd057fa700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:30:40.058 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:30:40.059 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880034e8 has been destroyed
26-03-2018 18:30:40.059 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:30:40.619 UTC [7fcd0feea700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
26-03-2018 18:30:46.115 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:54744, sock=285
26-03-2018 18:30:46.115 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:30:46.115 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:30:46.130 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210375 (rdata0x7fcc88003820)
26-03-2018 18:30:46.130 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210375 (rdata0x7fcc88003820) from TCP 10.224.61.22:54744:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210375
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210375
Call-ID: poll-sip-1210375
CSeq: 1210375 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:30:46.130 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:46.130 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:46.130 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:30:46.130 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc88003820 on worker thread
26-03-2018 18:30:46.130 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc88003820 on worker thread
26-03-2018 18:30:46.130 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc88003820 cloned to 0x7fcc8806e7c8
26-03-2018 18:30:46.130 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806e7c8 for worker threads with priority 15
26-03-2018 18:30:46.130 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:30:46.130 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug utils.cpp:872: Added IOHook 0x7fcd04ff8e30 to stack. There are now 1 hooks
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806e7c8
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug thread_dispatcher.cpp:183: Request latency so far = 62us
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210375 (rdata0x7fcc8806e7c8)
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210375 (tdta0x7fcc700080b0) created
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210375 (tdta0x7fcc700080b0) to TCP 10.224.61.22:54744:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=54744;received=10.224.61.22;branch=z9hG4bK-1210375
Call-ID: poll-sip-1210375
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210375
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210375
CSeq: 1210375 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug pjsip: tdta0x7fcc7000 Destroying txdata Response msg 200/OPTIONS/cseq=1210375 (tdta0x7fcc700080b0)
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806e7c8
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug thread_dispatcher.cpp:284: Request latency = 142us
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug event_statistic_accumulator.cpp:32: Accumulate 142 for 0x12dd778
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug event_statistic_accumulator.cpp:32: Accumulate 142 for 0x12dd820
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 11).
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug utils.cpp:878: Removed IOHook 0x7fcd04ff8e30 to stack. There are now 0 hooks
26-03-2018 18:30:46.130 UTC [7fcd04ff9700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:30:46.163 UTC [7fcc8d70a700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
26-03-2018 18:30:46.163 UTC [7fcc8d70a700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
26-03-2018 18:30:48.131 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:30:48.131 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880034e8 has been destroyed
26-03-2018 18:30:48.131 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:30:48.143 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:33020, sock=285
26-03-2018 18:30:48.143 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:30:48.143 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:30:48.144 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210377 (rdata0x7fcc88003820)
26-03-2018 18:30:48.144 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210377 (rdata0x7fcc88003820) from TCP 10.224.61.22:33020:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210377
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210377
Call-ID: poll-sip-1210377
CSeq: 1210377 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:30:48.144 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:48.144 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:48.144 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:30:48.144 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc88003820 on worker thread
26-03-2018 18:30:48.144 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc88003820 on worker thread
26-03-2018 18:30:48.144 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc88003820 cloned to 0x7fcc8806e7c8
26-03-2018 18:30:48.144 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806e7c8 for worker threads with priority 15
26-03-2018 18:30:48.144 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:30:48.144 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug utils.cpp:872: Added IOHook 0x7fcd047f7e30 to stack. There are now 1 hooks
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806e7c8
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug thread_dispatcher.cpp:183: Request latency so far = 93us
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210377 (rdata0x7fcc8806e7c8)
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210377 (tdta0x7fcc7800a1f0) created
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210377 (tdta0x7fcc7800a1f0) to TCP 10.224.61.22:33020:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=33020;received=10.224.61.22;branch=z9hG4bK-1210377
Call-ID: poll-sip-1210377
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210377
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210377
CSeq: 1210377 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug pjsip: tdta0x7fcc7800 Destroying txdata Response msg 200/OPTIONS/cseq=1210377 (tdta0x7fcc7800a1f0)
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806e7c8
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug thread_dispatcher.cpp:284: Request latency = 245us
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug event_statistic_accumulator.cpp:32: Accumulate 245 for 0x12dd778
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug event_statistic_accumulator.cpp:32: Accumulate 245 for 0x12dd820
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 12).
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug utils.cpp:878: Removed IOHook 0x7fcd047f7e30 to stack. There are now 0 hooks
26-03-2018 18:30:48.144 UTC [7fcd047f8700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:30:50.145 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:30:50.145 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880034e8 has been destroyed
26-03-2018 18:30:50.146 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:30:55.554 UTC [7fcd2c84f700] Status alarm.cpp:244: Reraising all alarms with a known state
26-03-2018 18:30:55.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1011.1 alarm
26-03-2018 18:30:55.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1012.1 alarm
26-03-2018 18:30:55.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1013.1 alarm
26-03-2018 18:30:55.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1009.1 alarm
26-03-2018 18:30:55.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1010.1 alarm
26-03-2018 18:30:55.628 UTC [7fcd0feea700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
26-03-2018 18:30:56.212 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:54808, sock=285
26-03-2018 18:30:56.212 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:30:56.212 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:30:56.223 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210385 (rdata0x7fcc880039f0)
26-03-2018 18:30:56.223 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210385 (rdata0x7fcc880039f0) from TCP 10.224.61.22:54808:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210385
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210385
Call-ID: poll-sip-1210385
CSeq: 1210385 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:30:56.223 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:56.223 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:56.223 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:30:56.223 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc880039f0 on worker thread
26-03-2018 18:30:56.223 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc880039f0 on worker thread
26-03-2018 18:30:56.223 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc880039f0 cloned to 0x7fcc8806f5c8
26-03-2018 18:30:56.223 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806f5c8 for worker threads with priority 15
26-03-2018 18:30:56.223 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:30:56.223 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug utils.cpp:872: Added IOHook 0x7fcd03ff6e30 to stack. There are now 1 hooks
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806f5c8
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug thread_dispatcher.cpp:183: Request latency so far = 189us
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210385 (rdata0x7fcc8806f5c8)
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210385 (tdta0x7fcc840087a0) created
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210385 (tdta0x7fcc840087a0) to TCP 10.224.61.22:54808:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=54808;received=10.224.61.22;branch=z9hG4bK-1210385
Call-ID: poll-sip-1210385
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210385
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210385
CSeq: 1210385 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug pjsip: tdta0x7fcc8400 Destroying txdata Response msg 200/OPTIONS/cseq=1210385 (tdta0x7fcc840087a0)
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806f5c8
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug thread_dispatcher.cpp:284: Request latency = 282us
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug event_statistic_accumulator.cpp:32: Accumulate 282 for 0x12dd778
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug event_statistic_accumulator.cpp:32: Accumulate 282 for 0x12dd820
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 13).
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug utils.cpp:878: Removed IOHook 0x7fcd03ff6e30 to stack. There are now 0 hooks
26-03-2018 18:30:56.223 UTC [7fcd03ff7700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:30:56.236 UTC [7fcc8d70a700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
26-03-2018 18:30:56.236 UTC [7fcc8d70a700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
26-03-2018 18:30:58.224 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:30:58.224 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880036b8 has been destroyed
26-03-2018 18:30:58.224 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:33082, sock=285
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210387 (rdata0x7fcc880039f0)
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210387 (rdata0x7fcc880039f0) from TCP 10.224.61.22:33082:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210387
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210387
Call-ID: poll-sip-1210387
CSeq: 1210387 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc880039f0 on worker thread
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc880039f0 on worker thread
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc880039f0 cloned to 0x7fcc8806f5c8
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806f5c8 for worker threads with priority 15
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:30:58.236 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug utils.cpp:872: Added IOHook 0x7fcd037f5e30 to stack. There are now 1 hooks
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806f5c8
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug thread_dispatcher.cpp:183: Request latency so far = 87us
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210387 (rdata0x7fcc8806f5c8)
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210387 (tdta0x7fcc8000e660) created
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210387 (tdta0x7fcc8000e660) to TCP 10.224.61.22:33082:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=33082;received=10.224.61.22;branch=z9hG4bK-1210387
Call-ID: poll-sip-1210387
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210387
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210387
CSeq: 1210387 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug pjsip: tdta0x7fcc8000 Destroying txdata Response msg 200/OPTIONS/cseq=1210387 (tdta0x7fcc8000e660)
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806f5c8
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug thread_dispatcher.cpp:284: Request latency = 214us
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug event_statistic_accumulator.cpp:32: Accumulate 214 for 0x12dd778
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug event_statistic_accumulator.cpp:32: Accumulate 214 for 0x12dd820
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 14).
26-03-2018 18:30:58.236 UTC [7fcd037f6700] Debug utils.cpp:878: Removed IOHook 0x7fcd037f5e30 to stack. There are now 0 hooks
26-03-2018 18:30:58.237 UTC [7fcd037f6700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:31:00.237 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:31:00.238 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880036b8 has been destroyed
26-03-2018 18:31:00.238 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:31:06.267 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:54866, sock=285
26-03-2018 18:31:06.267 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:31:06.267 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:31:06.285 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210395 (rdata0x7fcc880039f0)
26-03-2018 18:31:06.285 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210395 (rdata0x7fcc880039f0) from TCP 10.224.61.22:54866:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210395
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210395
Call-ID: poll-sip-1210395
CSeq: 1210395 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:31:06.285 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:31:06.285 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:31:06.285 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:31:06.285 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc880039f0 on worker thread
26-03-2018 18:31:06.285 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc880039f0 on worker thread
26-03-2018 18:31:06.285 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc880039f0 cloned to 0x7fcc8806f5c8
26-03-2018 18:31:06.285 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806f5c8 for worker threads with priority 15
26-03-2018 18:31:06.285 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:31:06.285 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug utils.cpp:872: Added IOHook 0x7fcd02ff4e30 to stack. There are now 1 hooks
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806f5c8
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug thread_dispatcher.cpp:183: Request latency so far = 66us
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210395 (rdata0x7fcc8806f5c8)
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210395 (tdta0x7fcc880714e0) created
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210395 (tdta0x7fcc880714e0) to TCP 10.224.61.22:54866:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=54866;received=10.224.61.22;branch=z9hG4bK-1210395
Call-ID: poll-sip-1210395
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210395
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210395
CSeq: 1210395 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug pjsip: tdta0x7fcc8807 Destroying txdata Response msg 200/OPTIONS/cseq=1210395 (tdta0x7fcc880714e0)
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806f5c8
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug thread_dispatcher.cpp:284: Request latency = 176us
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug event_statistic_accumulator.cpp:32: Accumulate 176 for 0x12dd778
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug event_statistic_accumulator.cpp:32: Accumulate 176 for 0x12dd820
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 15).
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug utils.cpp:878: Removed IOHook 0x7fcd02ff4e30 to stack. There are now 0 hooks
26-03-2018 18:31:06.285 UTC [7fcd02ff5700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:31:06.318 UTC [7fcc8d70a700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
26-03-2018 18:31:06.318 UTC [7fcc8d70a700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
26-03-2018 18:31:08.286 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:31:08.286 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880036b8 has been destroyed
26-03-2018 18:31:08.286 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:31:08.300 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:33142, sock=285
26-03-2018 18:31:08.300 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:31:08.301 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:31:08.301 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210397 (rdata0x7fcc880037e0)
26-03-2018 18:31:08.301 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210397 (rdata0x7fcc880037e0) from TCP 10.224.61.22:33142:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210397
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210397
Call-ID: poll-sip-1210397
CSeq: 1210397 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:31:08.301 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:31:08.301 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:31:08.301 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:31:08.301 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc880037e0 on worker thread
26-03-2018 18:31:08.301 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc880037e0 on worker thread
26-03-2018 18:31:08.301 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc880037e0 cloned to 0x7fcc8806f5c8
26-03-2018 18:31:08.301 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806f5c8 for worker threads with priority 15
26-03-2018 18:31:08.301 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:31:08.301 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:31:08.301 UTC [7fcd027f4700] Debug utils.cpp:872: Added IOHook 0x7fcd027f3e30 to stack. There are now 1 hooks
26-03-2018 18:31:08.301 UTC [7fcd027f4700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806f5c8
26-03-2018 18:31:08.301 UTC [7fcd027f4700] Debug thread_dispatcher.cpp:183: Request latency so far = 161us
26-03-2018 18:31:08.301 UTC [7fcd027f4700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210397 (rdata0x7fcc8806f5c8)
26-03-2018 18:31:08.301 UTC [7fcd027f4700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:31:08.301 UTC [7fcd027f4700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:31:08.302 UTC [7fcd027f4700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210397 (tdta0x7fcd080827f0) created
26-03-2018 18:31:08.302 UTC [7fcd027f4700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210397 (tdta0x7fcd080827f0) to TCP 10.224.61.22:33142:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=33142;received=10.224.61.22;branch=z9hG4bK-1210397
Call-ID: poll-sip-1210397
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210397
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210397
CSeq: 1210397 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:31:08.302 UTC [7fcd027f4700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:31:08.302 UTC [7fcd027f4700] Debug pjsip: tdta0x7fcd0808 Destroying txdata Response msg 200/OPTIONS/cseq=1210397 (tdta0x7fcd080827f0)
26-03-2018 18:31:08.302 UTC [7fcd027f4700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806f5c8
26-03-2018 18:31:08.302 UTC [7fcd027f4700] Debug thread_dispatcher.cpp:284: Request latency = 350us
26-03-2018 18:31:08.302 UTC [7fcd027f4700] Debug event_statistic_accumulator.cpp:32: Accumulate 350 for 0x12dd778
26-03-2018 18:31:08.302 UTC [7fcd027f4700] Debug event_statistic_accumulator.cpp:32: Accumulate 350 for 0x12dd820
26-03-2018 18:31:08.302 UTC [7fcd027f4700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 16).
26-03-2018 18:31:08.302 UTC [7fcd027f4700] Debug utils.cpp:878: Removed IOHook 0x7fcd027f3e30 to stack. There are now 0 hooks
26-03-2018 18:31:08.302 UTC [7fcd027f4700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:31:10.303 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:31:10.303 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880034a8 has been destroyed
26-03-2018 18:31:10.303 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:31:10.641 UTC [7fcd0feea700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:54928, sock=285
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210405 (rdata0x7fcc880037e0)
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210405 (rdata0x7fcc880037e0) from TCP 10.224.61.22:54928:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210405
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210405
Call-ID: poll-sip-1210405
CSeq: 1210405 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc880037e0 on worker thread
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc880037e0 on worker thread
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc880037e0 cloned to 0x7fcc8806f5c8
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806f5c8 for worker threads with priority 15
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:31:16.344 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug utils.cpp:872: Added IOHook 0x7fcd01ff2e30 to stack. There are now 1 hooks
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806f5c8
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug thread_dispatcher.cpp:183: Request latency so far = 58us
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210405 (rdata0x7fcc8806f5c8)
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210405 (tdta0x7fcd20009420) created
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210405 (tdta0x7fcd20009420) to TCP 10.224.61.22:54928:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=54928;received=10.224.61.22;branch=z9hG4bK-1210405
Call-ID: poll-sip-1210405
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210405
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210405
CSeq: 1210405 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug pjsip: tdta0x7fcd2000 Destroying txdata Response msg 200/OPTIONS/cseq=1210405 (tdta0x7fcd20009420)
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806f5c8
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug thread_dispatcher.cpp:284: Request latency = 131us
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug event_statistic_accumulator.cpp:32: Accumulate 131 for 0x12dd778
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug event_statistic_accumulator.cpp:32: Accumulate 131 for 0x12dd820
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 17).
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug utils.cpp:878: Removed IOHook 0x7fcd01ff2e30 to stack. There are now 0 hooks
26-03-2018 18:31:16.345 UTC [7fcd01ff3700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:31:16.390 UTC [7fcc8d70a700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
26-03-2018 18:31:16.390 UTC [7fcc8d70a700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
26-03-2018 18:31:18.345 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:31:18.345 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880034a8 has been destroyed
26-03-2018 18:31:18.345 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:31:18.357 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:33204, sock=285
26-03-2018 18:31:18.357 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:31:18.357 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:31:18.358 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210407 (rdata0x7fcc880037e0)
26-03-2018 18:31:18.358 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210407 (rdata0x7fcc880037e0) from TCP 10.224.61.22:33204:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210407
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210407
Call-ID: poll-sip-1210407
CSeq: 1210407 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:31:18.358 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:31:18.358 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:31:18.358 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:31:18.358 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc880037e0 on worker thread
26-03-2018 18:31:18.358 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc880037e0 on worker thread
26-03-2018 18:31:18.358 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc880037e0 cloned to 0x7fcc8806f5c8
26-03-2018 18:31:18.358 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806f5c8 for worker threads with priority 15
26-03-2018 18:31:18.358 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:31:18.358 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug utils.cpp:872: Added IOHook 0x7fcd017f1e30 to stack. There are now 1 hooks
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806f5c8
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug thread_dispatcher.cpp:183: Request latency so far = 110us
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210407 (rdata0x7fcc8806f5c8)
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210407 (tdta0x7fcd28011240) created
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210407 (tdta0x7fcd28011240) to TCP 10.224.61.22:33204:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=33204;received=10.224.61.22;branch=z9hG4bK-1210407
Call-ID: poll-sip-1210407
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210407
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210407
CSeq: 1210407 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug pjsip: tdta0x7fcd2801 Destroying txdata Response msg 200/OPTIONS/cseq=1210407 (tdta0x7fcd28011240)
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806f5c8
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug thread_dispatcher.cpp:284: Request latency = 281us
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug event_statistic_accumulator.cpp:32: Accumulate 281 for 0x12dd778
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug event_statistic_accumulator.cpp:32: Accumulate 281 for 0x12dd820
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 18).
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug utils.cpp:878: Removed IOHook 0x7fcd017f1e30 to stack. There are now 0 hooks
26-03-2018 18:31:18.358 UTC [7fcd017f2700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:31:20.359 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:31:20.359 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880034a8 has been destroyed
26-03-2018 18:31:20.359 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:31:25.554 UTC [7fcd2c84f700] Status alarm.cpp:244: Reraising all alarms with a known state
26-03-2018 18:31:25.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1011.1 alarm
26-03-2018 18:31:25.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1012.1 alarm
26-03-2018 18:31:25.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1013.1 alarm
26-03-2018 18:31:25.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1009.1 alarm
26-03-2018 18:31:25.554 UTC [7fcd2c84f700] Status alarm.cpp:37: sprout issued 1010.1 alarm
26-03-2018 18:31:25.655 UTC [7fcd0feea700] Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5054 TCP listener 10.224.61.22:5054: got incoming TCP connection from 10.224.61.22:54992, sock=460
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210415 (rdata0x7fcc880037e0)
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210415 (rdata0x7fcc880037e0) from TCP 10.224.61.22:54992:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5054 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210415
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5054>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210415
Call-ID: poll-sip-1210415
CSeq: 1210415 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc880037e0 on worker thread
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc880037e0 on worker thread
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc880037e0 cloned to 0x7fcc8806f5c8
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806f5c8 for worker threads with priority 15
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:31:26.421 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug utils.cpp:872: Added IOHook 0x7fcd00ff0e30 to stack. There are now 1 hooks
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806f5c8
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug thread_dispatcher.cpp:183: Request latency so far = 55us
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210415 (rdata0x7fcc8806f5c8)
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210415 (tdta0x22edc60) created
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210415 (tdta0x22edc60) to TCP 10.224.61.22:54992:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=54992;received=10.224.61.22;branch=z9hG4bK-1210415
Call-ID: poll-sip-1210415
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210415
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210415
CSeq: 1210415 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug pjsip:  tdta0x22edc60 Destroying txdata Response msg 200/OPTIONS/cseq=1210415 (tdta0x22edc60)
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806f5c8
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug thread_dispatcher.cpp:284: Request latency = 125us
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug event_statistic_accumulator.cpp:32: Accumulate 125 for 0x12dd778
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug event_statistic_accumulator.cpp:32: Accumulate 125 for 0x12dd820
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug load_monitor.cpp:341: Not recalculating rate as we haven't processed 20 requests yet (only 19).
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug utils.cpp:878: Removed IOHook 0x7fcd00ff0e30 to stack. There are now 0 hooks
26-03-2018 18:31:26.421 UTC [7fcd00ff1700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:31:26.463 UTC [7fcc8d70a700] Verbose httpstack.cpp:327: Process request for URL /ping, args (null)
26-03-2018 18:31:26.463 UTC [7fcc8d70a700] Verbose httpstack.cpp:68: Sending response 200 to request for URL /ping, args (null)
26-03-2018 18:31:28.422 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:31:28.422 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880034a8 has been destroyed
26-03-2018 18:31:28.422 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Verbose pjsip:    tcplis:5053 TCP listener 10.224.61.22:5053: got incoming TCP connection from 10.224.61.22:33266, sock=285
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 tcp->base.local_name: 10.224.61.22
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP server transport created
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1210417 (rdata0x7fcc880037e0)
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Verbose common_sip_processing.cpp:87: RX 355 bytes Request msg OPTIONS/cseq=1210417 (rdata0x7fcc880037e0) from TCP 10.224.61.22:33266:
--start msg--

OPTIONS sip:poll-sip at 10.224.61.22:5053 SIP/2.0
Via: SIP/2.0/TCP 10.224.61.22;rport;branch=z9hG4bK-1210417
Max-Forwards: 2
To: <sip:poll-sip at 10.224.61.22:5053>
From: poll-sip <sip:poll-sip at 10.224.61.22>;tag=1210417
Call-ID: poll-sip-1210417
CSeq: 1210417 OPTIONS
Contact: <sip:10.224.61.22>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Debug common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:554: Recieved message 0x7fcc880037e0 on worker thread
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:571: Admitted request 0x7fcc880037e0 on worker thread
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:606: Incoming message 0x7fcc880037e0 cloned to 0x7fcc8806f5c8
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7fcc8806f5c8 for worker threads with priority 15
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e1708
26-03-2018 18:31:28.435 UTC [7fcc8df0b700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x12e17b0
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug utils.cpp:872: Added IOHook 0x7fcd007efe30 to stack. There are now 1 hooks
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7fcc8806f5c8
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug thread_dispatcher.cpp:183: Request latency so far = 90us
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1210417 (rdata0x7fcc8806f5c8)
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug uri_classifier.cpp:172: Classified URI as 3
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=1210417 (tdta0x7fcc70008100) created
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Verbose common_sip_processing.cpp:103: TX 287 bytes Response msg 200/OPTIONS/cseq=1210417 (tdta0x7fcc70008100) to TCP 10.224.61.22:33266:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.224.61.22;rport=33266;received=10.224.61.22;branch=z9hG4bK-1210417
Call-ID: poll-sip-1210417
From: "poll-sip" <sip:poll-sip at 10.224.61.22>;tag=1210417
To: <sip:poll-sip at 10.224.61.22>;tag=z9hG4bK-1210417
CSeq: 1210417 OPTIONS
Content-Length:  0


--end msg--
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug pjsip: tdta0x7fcc7000 Destroying txdata Response msg 200/OPTIONS/cseq=1210417 (tdta0x7fcc70008100)
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug thread_dispatcher.cpp:270: Worker thread completed processing message 0x7fcc8806f5c8
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug thread_dispatcher.cpp:284: Request latency = 215us
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug event_statistic_accumulator.cpp:32: Accumulate 215 for 0x12dd778
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug event_statistic_accumulator.cpp:32: Accumulate 215 for 0x12dd820
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Info load_monitor.cpp:217: Rate adjustment calculation inputs: err -0.978900, smoothed latency 211, target latency 10000
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Info load_monitor.cpp:302: Maximum incoming request rate/second unchanged at 2000.000000 (current request rate is 6.099999 requests/sec, minimum threshold for a change is 1000.000000 requests/sec).
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug snmp_continuous_accumulator_by_scope_table.cpp:86: Accumulating sample 2000ui into continuous accumulator statistic
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug utils.cpp:878: Removed IOHook 0x7fcd007efe30 to stack. There are now 0 hooks
26-03-2018 18:31:28.435 UTC [7fcd007f0700] Debug thread_dispatcher.cpp:158: Attempting to process queue element
26-03-2018 18:31:30.436 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP connection closed
26-03-2018 18:31:30.436 UTC [7fcc8df0b700] Debug connection_tracker.cpp:67: Connection 0x7fcc880034a8 has been destroyed
26-03-2018 18:31:30.436 UTC [7fcc8df0b700] Verbose pjsip: tcps0x7fcc8800 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)

From daniemarques at outlook.com  Thu Mar 29 09:50:40 2018
From: daniemarques at outlook.com (Daniel Marques)
Date: Thu, 29 Mar 2018 13:50:40 -0000
Subject: [Project Clearwater] Sprout sending OPTIONS to sprout and not AS
Message-ID: <HE1PR08MB28101C9B25EC9F0DCB61F6CFD7A20@HE1PR08MB2810.eurprd08.prod.outlook.com>

Hi


I am setting up one clearwater environment using docker.


I have successfully deployed the ims containers and two AS with IFCs.


The REGISTER requests work with success and reach the AS.


The OPTIONS requests don't work ( i have copied the IFCs from an older installation of clearwater, so I believe that the format of the IFCs is OK).


I took a pcap in sprout, and what I observe is that sprout receives the OPTIONS and executes a NAPTR dns query to the user domain. The resulting dns query points the request to sprout again.


I believe that instead of sprout attempting to execute the above dns query, should check the IFCs and send the OPTIONS towards the AS.


Can anyone help me with this?


I send here a part of the logs.


29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1 (rdata0x7f21b4068500)
29-03-2018 13:45:19.173 UTC [7f21b8819700] Verbose common_sip_processing.cpp:87: RX 1686 bytes Request msg OPTIONS/cseq=1 (rdata0x7f21b4068500) from TCP 172.50.0.9:38087:
--start msg--

OPTIONS sip:+917722000104 at ims.mnc874.mcc405.3gppnetwork.org SIP/2.0
Record-Route: <sip:172.50.0.9:5058;transport=TCP;lr>
...
Route: <sip:sprout:5054;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f>
Content-Length:  0
...

--end msg--
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug uri_classifier.cpp:172: Classified URI as 5
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug pjutils.cpp:1771: Logging SAS Call-ID marker, Call-ID mHkJ0rybfT
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f21b4068500 on worker thread
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f21b4068500 on worker thread
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f21b4068500 cloned to 0x7f21b406ba18
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f21b406ba18 for worker threads with priority 15
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x21b2ad8
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x21b2b20
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug utils.cpp:872: Added IOHook 0x7f25927cbe30 to stack. There are now 1 hooks
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f21b406ba18
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug thread_dispatcher.cpp:183: Request latency so far = 278us
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1 (rdata0x7f21b406ba18)
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug uri_classifier.cpp:172: Classified URI as 5
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug basicproxy.cpp:62: Process OPTIONS request
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug sproutletproxy.cpp:631: Sproutlet Proxy transaction (0x7f219c021810) created. There are now 1 instances
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug basicproxy.cpp:1318: Report SAS start marker - trail (f)
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug pjutils.cpp:719: Cloned Request msg OPTIONS/cseq=1 (rdata0x7f21b406ba18) to tdta0x7f219c021c60
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug pjsip: tsx0x7f219c025 Transaction created for Request msg OPTIONS/cseq=1 (rdata0x7f21b406ba18)
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug pjsip: tsx0x7f219c025 Incoming Request msg OPTIONS/cseq=1 (rdata0x7f21b406ba18) in state Null
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug pjsip: tsx0x7f219c025 State changed from Null to Trying, event=RX_MSG
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug basicproxy.cpp:183: tsx0x7f219c025098 - tu_on_tsx_state UAS, TSX_STATE RX_MSG state=Trying
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug pjsip:       endpoint Response msg 408/OPTIONS/cseq=1 (tdta0x7f219c025800) created
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:sprout:5054;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug sproutletproxy.cpp:237: No Sproutlet found using service name or host
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug sproutletproxy.cpp:243: Find default service for port 5054
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:429: Creating URI for service registrar
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:455: Constructed URI sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=registrar
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:1276: Remove top Route header Route: <sip:sprout:5054;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f>
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=registrar
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:302: Found services param - registrar
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:172: Classified URI as 5
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:429: Creating URI for service subscription
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:302: Found services param - registrar
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:455: Constructed URI sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=subscription
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:1276: Remove top Route header Route: <sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=registrar>
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=subscription
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:302: Found services param - subscription
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:429: Creating URI for service scscf-proxy
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:302: Found services param - subscription
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:455: Constructed URI sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=scscf-proxy
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:1276: Remove top Route header Route: <sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=subscription>
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=scscf-proxy
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:302: Found services param - scscf-proxy
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug scscfsproutlet.cpp:424: S-CSCF Transaction (0x7f219c0278f0) created

29-03-2018 13:45:19.174 UTC [7f25927cc700] Verbose sproutletproxy.cpp:1384: Created Sproutlet scscf-proxy-0x7f219c0278f0 for Request msg OPTIONS/cseq=1 (tdta0x7f219c021c60)
29-03-2018 13:45:19.174 UTC [7f25927cc700] Verbose sproutletproxy.cpp:2487: Routing Request msg OPTIONS/cseq=1 (tdta0x7f219c021c60) (1727 bytes) to downstream sproutlet scscf-proxy:
--start msg--
OPTIONS sip:+917722000104 at ims.mnc874.mcc405.3gppnetwork.org SIP/2.0
Route: <sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=scscf-proxy>
Record-Route: <sip:172.50.0.9:5058;transport=TCP;lr>

...
--end msg--
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('EXTERNAL'->'scscf'/'scscf-proxy')
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('EXTERNAL'->'scscf'/'scscf-proxy')
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2517: Internal network function boundary: no
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug pjutils.cpp:736: Cloned tdta0x7f219c021c60 to tdta0x7f219c028430
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:1450: Remove top Route header Route: <sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=scscf-proxy>
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2115: Adding message 0x7f219c028a40 => txdata 0x7f219c0284d8 mapping
29-03-2018 13:45:19.174 UTC [7f25927cc700] Verbose sproutletproxy.cpp:1946: scscf-proxy-0x7f219c0278f0 pass initial request Request msg OPTIONS/cseq=1 (tdta0x7f219c028430) to Sproutlet
29-03-2018 13:45:19.174 UTC [7f25927cc700] Info scscfsproutlet.cpp:471: S-CSCF received initial request
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:172: Classified URI as 3
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug scscfsproutlet.cpp:945: Route header references this system
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug scscfsproutlet.cpp:998: No ODI token, or invalid ODI token, on request, and no P-Charging-Vector header (so can't log ICID for correlation)
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug scscfsproutlet.cpp:1004: Got our Route header, session case orig, OD=None
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug pjutils.cpp:294: Served user from P-Asserted-Identity header
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:172: Classified URI as 5
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:172: Classified URI as 5
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug scscfsproutlet.cpp:1348: URI is not locally hosted
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug acr.cpp:1797: Create RalfACR for node type S-CSCF with role Originating
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug acr.cpp:24: Created ACR (0x7f219c02b3a0)
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug acr.cpp:170: Created S-CSCF Ralf ACR
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug acr.cpp:29: Destroyed ACR (0x7f219c02b3a0)
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:172: Classified URI as 5
29-03-2018 13:45:19.174 UTC [7f25927cc700] Info scscfsproutlet.cpp:656: Route request without applying services
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:1621: Sproutlet send_request 0x7f219c028a40
29-03-2018 13:45:19.174 UTC [7f25927cc700] Verbose sproutletproxy.cpp:1662: scscf-proxy-0x7f219c0278f0 sending Request msg OPTIONS/cseq=1 (tdta0x7f219c028430) on fork 0
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2130: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2170: Processing request 0x7f219c0284d8, fork = 0
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2334: scscf-proxy-0x7f219c0278f0 transmitting request on fork 0
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2349: scscf-proxy-0x7f219c0278f0 store reference to non-ACK request Request msg OPTIONS/cseq=1 (tdta0x7f219c028430) on fork 0
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2122: Removing message 0x7f219c028a40 => txdata 0x7f219c0284d8 mapping
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:+917722000104 at ims.mnc874.mcc405.3gppnetwork.org
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:342: Possible service name ims will be used if mnc874.mcc405.3gppnetwork.org is a local hostname
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:365: Found user part - +917722000104
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:1007: No local sproutlet matches request
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug pjsip: tsx0x7f219c02b Transaction created for Request msg OPTIONS/cseq=1 (tdta0x7f219c028430)
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug basicproxy.cpp:1669: Added trail identifier 15 to UAC transaction
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug pjutils.cpp:510: Next hop node is encoded in Request-URI
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sipresolver.cpp:84: SIPResolver::resolve for name ims.mnc874.mcc405.3gppnetwork.org, port 0, transport -1, family 2
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug utils.cpp:446: Attempt to parse ims.mnc874.mcc405.3gppnetwork.org as IP address
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug sipresolver.cpp:147: Do NAPTR look-up for ims.mnc874.mcc405.3gppnetwork.org
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug ttlcache.h:200: Time now is 1522331119, expiry time of entry at head of expiry list is 1522330274
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug ttlcache.h:93: Entry not in cache, so create new entry
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug baseresolver.cpp:252: NAPTR cache factory called for ims.mnc874.mcc405.3gppnetwork.org
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug baseresolver.cpp:264: Sending DNS NAPTR query for ims.mnc874.mcc405.3gppnetwork.org
29-03-2018 13:45:19.175 UTC [7f25927cc700] Verbose dnscachedresolver.cpp:468: Check cache for ims.mnc874.mcc405.3gppnetwork.org type 35
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug dnscachedresolver.cpp:474: No entry found in cache
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug dnscachedresolver.cpp:477: Create cache entry pending query
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug dnscachedresolver.cpp:525: Create and execute DNS query transaction
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug dnscachedresolver.cpp:538: Wait for query responses
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug thread_dispatcher.cpp:117: Pausing stopwatch due to DNS query




Thanks


Daniel


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180329/5ed548c2/attachment.html>

From joehary at gmail.com  Sat Mar 31 08:02:28 2018
From: joehary at gmail.com (joehary ar)
Date: Sat, 31 Mar 2018 12:02:28 -0000
Subject: [Project Clearwater] Deploying Clearwater using Heat Template
Message-ID: <CAKHfQt7+g8thaq7+7os6jZWfD+kZX7o0VE3K-ctF-6VnBQfhig@mail.gmail.com>

Hi,

I would like to seek support and guidance on how I can deploy clearwater
using heat template based on the yaml file
https://github.com/Metaswitch/clearwater-heat

May I know anybody successful try this template?

1. Do i need to download all the file in git onto my openstack folder?
2. Just create heat stack with only one yaml file --> clearwater.yaml ?

Please assist me further.

Thank you.

Regards,
Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180331/692020ad/attachment.html>

