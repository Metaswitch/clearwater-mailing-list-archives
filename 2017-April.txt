From ahmed.eldeeb at 4gtss.com  Mon Apr  3 08:27:59 2017
From: ahmed.eldeeb at 4gtss.com (Ahmed Eldeeb)
Date: Mon, 3 Apr 2017 14:27:59 +0200
Subject: [Project Clearwater] Registration failed (Internal Server Error
 (500))
Message-ID: <8589c05a-2642-dcc4-9e40-19c71d64f494@4gtss.com>

Hello,

After downloading clearwater latest release on ubuntu server 14.04 i 
cannot register using zoiper i keep receiving Internal Server Error 500 
and it seems that a problem in sprout which appears in sprout logs

i Attached the logs to the mail so you can help me. However, this 
problem didnot appear in the latest version of clearwater 1.0-170307.151028

Thanks in advance


-------------- next part --------------
03-04-2017 12:16:18.052 UTC Verbose sproutletproxy.cpp:538: Sproutlet Proxy transaction (0x7f983400b310) destroyed
03-04-2017 12:16:18.052 UTC Debug basicproxy.cpp:494: BasicProxy::UASTsx destructor (0x7f983400b310)
03-04-2017 12:16:18.052 UTC Debug basicproxy.cpp:511: Disconnect UAC transactions from UAS transaction
03-04-2017 12:16:18.052 UTC Debug basicproxy.cpp:525: Free original request
03-04-2017 12:16:18.052 UTC Debug pjsip: tdta0x7f983406 Destroying txdata Request msg REGISTER/cseq=1 (tdta0x7f98340608c0)
03-04-2017 12:16:18.052 UTC Debug basicproxy.cpp:534: Free un-used best response
03-04-2017 12:16:18.052 UTC Debug pjsip: tdta0x7f983403 Destroying txdata Response msg 408/REGISTER/cseq=1 (tdta0x7f9834030140)
03-04-2017 12:16:18.052 UTC Debug basicproxy.cpp:555: BasicProxy::UASTsx destructor completed
03-04-2017 12:16:18.052 UTC Debug pjsip: tdta0x7f983407 Destroying txdata Response msg 500/REGISTER/cseq=1 (tdta0x7f9834070e90)
03-04-2017 12:16:18.052 UTC Debug pjsip: tsx0x7f983400e Transaction destroyed!
03-04-2017 12:16:18.970 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg REGISTER/cseq=1 (rdata0x7f98640f3180)
03-04-2017 12:16:18.970 UTC Verbose common_sip_processing.cpp:120: RX 781 bytes Request msg REGISTER/cseq=1 (rdata0x7f98640f3180) from TCP 192.168.0.210:34676:
--start msg--

REGISTER sip:ims.cw.4gtss.com:5060;transport=TCP SIP/2.0
Via: SIP/2.0/TCP 192.168.0.210:5058;rport;branch=z9hG4bKPj7OjKdLfzTihQz6Rf6JLTREZbtt3I978D
Path: <sip:LluSNt2I9m at 192.168.0.210:5058;transport=TCP;lr;ob>
Via: SIP/2.0/TCP 192.168.0.207:44446;received=192.168.0.207;branch=z9hG4bK-524287-1---b4bccf7baf143257
Max-Forwards: 70
Contact: <sip:6505550392 at 192.168.0.207:44446;transport=tcp;rinstance=e5dd172bc47340ed>
To: <sip:6505550392 at ims.cw.4gtss.com>
From: <sip:6505550392 at ims.cw.4gtss.com>;tag=9542d61f
Call-ID: gKQzo8Y5VOUJVN__WEpSBA..
CSeq: 1 REGISTER
Expires: 600
User-Agent: Zoiper rv2.8.30
Allow-Events: presence, kpml, talk
P-Visited-Network-ID: ims.cw.4gtss.com
Route: <sip:icscf.sprout.ims.cw.4gtss.com:5052;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
03-04-2017 12:16:18.970 UTC Debug pjutils.cpp:1714: Logging SAS Call-ID marker, Call-ID gKQzo8Y5VOUJVN__WEpSBA..
03-04-2017 12:16:18.970 UTC Debug thread_dispatcher.cpp:264: Queuing cloned received message 0x7f98640b0ae8 for worker threads
03-04-2017 12:16:18.970 UTC Debug thread_dispatcher.cpp:150: Worker thread dequeue message 0x7f98640b0ae8
03-04-2017 12:16:18.970 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg REGISTER/cseq=1 (rdata0x7f98640b0ae8)
03-04-2017 12:16:18.970 UTC Debug uri_classifier.cpp:174: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
03-04-2017 12:16:18.970 UTC Debug uri_classifier.cpp:204: Classified URI as 4
03-04-2017 12:16:18.970 UTC Debug basicproxy.cpp:92: Process REGISTER request
03-04-2017 12:16:18.970 UTC Verbose sproutletproxy.cpp:507: Sproutlet Proxy transaction (0x7f985c01c2d0) created
03-04-2017 12:16:18.970 UTC Debug basicproxy.cpp:1298: Report SAS start marker - trail (190)
03-04-2017 12:16:18.970 UTC Debug pjutils.cpp:724: Cloned Request msg REGISTER/cseq=1 (rdata0x7f98640b0ae8) to tdta0x7f985c00d0f0
03-04-2017 12:16:18.971 UTC Debug pjsip: tsx0x7f985c010 Transaction created for Request msg REGISTER/cseq=1 (rdata0x7f98640b0ae8)
03-04-2017 12:16:18.971 UTC Debug pjsip: tsx0x7f985c010 Incoming Request msg REGISTER/cseq=1 (rdata0x7f98640b0ae8) in state Null
03-04-2017 12:16:18.971 UTC Debug pjsip: tsx0x7f985c010 State changed from Null to Trying, event=RX_MSG
03-04-2017 12:16:18.971 UTC Debug basicproxy.cpp:213: tsx0x7f985c010708 - tu_on_tsx_state UAS, TSX_STATE RX_MSG state=Trying
03-04-2017 12:16:18.971 UTC Debug pjsip:       endpoint Response msg 408/REGISTER/cseq=1 (tdta0x7f985c059940) created
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:154: Found next routable URI: sip:icscf.sprout.ims.cw.4gtss.com:5052;transport=TCP;lr;orig
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:289: Possible service name icscf will be used if sprout.ims.cw.4gtss.com is a local hostname
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:293: Adding possible service name icscf based on domain
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:289: Possible service name icscf will be used if sprout.ims.cw.4gtss.com is a local hostname
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:293: Adding possible service name icscf based on domain
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:289: Possible service name icscf will be used if sprout.ims.cw.4gtss.com is a local hostname
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:293: Adding possible service name icscf based on domain
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:289: Possible service name icscf will be used if sprout.ims.cw.4gtss.com is a local hostname
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:293: Adding possible service name icscf based on domain
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:289: Possible service name icscf will be used if sprout.ims.cw.4gtss.com is a local hostname
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:293: Adding possible service name icscf based on domain
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:289: Possible service name icscf will be used if sprout.ims.cw.4gtss.com is a local hostname
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:293: Adding possible service name icscf based on domain
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:289: Possible service name icscf will be used if sprout.ims.cw.4gtss.com is a local hostname
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:293: Adding possible service name icscf based on domain
03-04-2017 12:16:18.971 UTC Verbose sproutletproxy.cpp:1163: Created Sproutlet icscf-0x7f985c0008e0 for Request msg REGISTER/cseq=1 (tdta0x7f985c00d0f0)
03-04-2017 12:16:18.971 UTC Verbose sproutletproxy.cpp:2095: Routing Request msg REGISTER/cseq=1 (tdta0x7f985c00d0f0) (810 bytes) to downstream sproutlet icscf:
--start msg--

REGISTER sip:ims.cw.4gtss.com:5060;transport=TCP SIP/2.0
Via: SIP/2.0/TCP 192.168.0.210:5058;rport=34676;received=192.168.0.210;branch=z9hG4bKPj7OjKdLfzTihQz6Rf6JLTREZbtt3I978D
Path: <sip:LluSNt2I9m at 192.168.0.210:5058;transport=TCP;lr;ob>
Via: SIP/2.0/TCP 192.168.0.207:44446;received=192.168.0.207;branch=z9hG4bK-524287-1---b4bccf7baf143257
Max-Forwards: 70
Contact: <sip:6505550392 at 192.168.0.207:44446;transport=tcp;rinstance=e5dd172bc47340ed>
To: <sip:6505550392 at ims.cw.4gtss.com>
From: <sip:6505550392 at ims.cw.4gtss.com>;tag=9542d61f
Call-ID: gKQzo8Y5VOUJVN__WEpSBA..
CSeq: 1 REGISTER
Expires: 600
User-Agent: Zoiper rv2.8.30
Allow-Events: presence, kpml, talk
P-Visited-Network-ID: ims.cw.4gtss.com
Route: <sip:icscf.sprout.ims.cw.4gtss.com:5052;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
03-04-2017 12:16:18.971 UTC Debug pjutils.cpp:741: Cloned tdta0x7f985c00d0f0 to tdta0x7f985c05a950
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:1224: Remove top Route header Route: <sip:icscf.sprout.ims.cw.4gtss.com:5052;transport=TCP;lr;orig>
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:1768: Adding message 0x7f985c05af60 => txdata 0x7f985c05a9f8 mapping
03-04-2017 12:16:18.971 UTC Verbose sproutletproxy.cpp:1613: icscf-0x7f985c0008e0 pass initial request Request msg REGISTER/cseq=1 (tdta0x7f985c05a950) to Sproutlet
03-04-2017 12:16:18.971 UTC Debug acr.cpp:1812: Create RalfACR for node type I-CSCF with role Terminating
03-04-2017 12:16:18.971 UTC Debug acr.cpp:49: Created ACR (0x7f985c04d290)
03-04-2017 12:16:18.971 UTC Debug acr.cpp:189: Created I-CSCF Ralf ACR
03-04-2017 12:16:18.971 UTC Debug acr.cpp:269: Set record type for I-CSCF, BGCF, IBCF, AS to EVENT_RECORD
03-04-2017 12:16:18.971 UTC Debug icscfsproutlet.cpp:194: I-CSCF initialize transaction for REGISTER request
03-04-2017 12:16:18.971 UTC Debug icscfrouter.cpp:362: Perform UAR - impi 6505550392 at ims.cw.4gtss.com, impu sip:6505550392 at ims.cw.4gtss.com, vn ims.cw.4gtss.com, auth_type REG
03-04-2017 12:16:18.971 UTC Debug a_record_resolver.cpp:80: ARecordResolver::resolve_iter for host hs.ims.cw.4gtss.com, port 8888, family 2
03-04-2017 12:16:18.971 UTC Debug baseresolver.cpp:425: Attempt to parse hs.ims.cw.4gtss.com as IP address
03-04-2017 12:16:18.971 UTC Verbose dnscachedresolver.cpp:486: Check cache for hs.ims.cw.4gtss.com type 1
03-04-2017 12:16:18.971 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for hs.ims.cw.4gtss.com A
03-04-2017 12:16:18.971 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
03-04-2017 12:16:18.971 UTC Debug baseresolver.cpp:425: Attempt to parse hs.ims.cw.4gtss.com as IP address
03-04-2017 12:16:18.971 UTC Debug baseresolver.cpp:819: 192.168.0.212:8888 transport 6 has state: WHITE
03-04-2017 12:16:18.971 UTC Debug baseresolver.cpp:819: 192.168.0.212:8888 transport 6 has state: WHITE
03-04-2017 12:16:18.971 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
03-04-2017 12:16:18.971 UTC Debug connection_pool.h:231: Request for connection to IP: 192.168.0.212, port: 8888
03-04-2017 12:16:18.971 UTC Debug connection_pool.h:244: Found existing connection 0x7f986c0216d0 in pool
03-04-2017 12:16:18.971 UTC Debug httpclient.cpp:478: Set CURLOPT_RESOLVE: hs.ims.cw.4gtss.com:8888:192.168.0.212
03-04-2017 12:16:18.971 UTC Debug httpclient.cpp:505: Sending HTTP request : http://hs.ims.cw.4gtss.com:8888/impi/6505550392%40ims.cw.4gtss.com/registration-status?impu=sip%3A6505550392%40ims.cw.4gtss.com&visited-network=ims.cw.4gtss.com&auth-type=REG (trying 192.168.0.212)
03-04-2017 12:16:18.971 UTC Debug httpclient.cpp:832: Received header http/1.1200ok with value 
03-04-2017 12:16:18.971 UTC Debug httpclient.cpp:833: Header pointer: 0x7f98547ef340
03-04-2017 12:16:18.971 UTC Debug httpclient.cpp:832: Received header content-length with value 83
03-04-2017 12:16:18.971 UTC Debug httpclient.cpp:833: Header pointer: 0x7f98547ef340
03-04-2017 12:16:18.971 UTC Debug httpclient.cpp:832: Received header content-type with value text/plain
03-04-2017 12:16:18.971 UTC Debug httpclient.cpp:833: Header pointer: 0x7f98547ef340
03-04-2017 12:16:18.971 UTC Debug httpclient.cpp:832: Received header  with value 
03-04-2017 12:16:18.971 UTC Debug httpclient.cpp:833: Header pointer: 0x7f98547ef340
03-04-2017 12:16:18.971 UTC Debug httpclient.cpp:538: Received HTTP response: status=200, doc={"result-code":2001,"scscf":"sip:scscf.sprout.ims.cw.4gtss.com:5054;transport=TCP"}
03-04-2017 12:16:18.971 UTC Debug baseresolver.cpp:830: Successful response from  192.168.0.212:8888 transport 6
03-04-2017 12:16:18.971 UTC Debug connection_pool.h:267: Release connection to IP: 192.168.0.212, port: 8888 to pool
03-04-2017 12:16:18.971 UTC Debug icscfrouter.cpp:245: HSS returned S-CSCF sip:scscf.sprout.ims.cw.4gtss.com:5054;transport=TCP as target
03-04-2017 12:16:18.971 UTC Debug acr.cpp:653: Storing Server-Capabilities
03-04-2017 12:16:18.971 UTC Debug icscfrouter.cpp:120: SCSCF specified by HSS: sip:scscf.sprout.ims.cw.4gtss.com:5054;transport=TCP
03-04-2017 12:16:18.971 UTC Debug uri_classifier.cpp:174: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
03-04-2017 12:16:18.971 UTC Debug uri_classifier.cpp:204: Classified URI as 3
03-04-2017 12:16:18.971 UTC Debug icscfsproutlet.cpp:283: Found SCSCF for REGISTER
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:1364: Sproutlet send_request 0x7f985c05af60
03-04-2017 12:16:18.971 UTC Verbose sproutletproxy.cpp:1400: icscf-0x7f985c0008e0 sending Request msg REGISTER/cseq=1 (tdta0x7f985c05a950) on fork 0
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:1783: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:1823: Processing request 0x7f985c05a9f8, fork = 0
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:1947: icscf-0x7f985c0008e0 transmitting request on fork 0
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:1961: icscf-0x7f985c0008e0 store reference to non-ACK request Request msg REGISTER/cseq=1 (tdta0x7f985c05a950) on fork 0
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:1775: Removing message 0x7f985c05af60 => txdata 0x7f985c05a9f8 mapping
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:154: Found next routable URI: sip:scscf.sprout.ims.cw.4gtss.com:5054;transport=TCP
03-04-2017 12:16:18.971 UTC Debug sproutletproxy.cpp:289: Possible service name scscf will be used if sprout.ims.cw.4gtss.com is a local hostname
03-04-2017 12:16:18.972 UTC Debug sproutletproxy.cpp:293: Adding possible service name scscf based on domain
03-04-2017 12:16:18.972 UTC Verbose sproutletproxy.cpp:1163: Created Sproutlet authentication-0x7f985c03da90 for Request msg REGISTER/cseq=1 (tdta0x7f985c05a950)
03-04-2017 12:16:18.972 UTC Verbose sproutletproxy.cpp:2095: Routing Request msg REGISTER/cseq=1 (tdta0x7f985c05a950) (752 bytes) to downstream sproutlet authentication:
--start msg--

REGISTER sip:scscf.sprout.ims.cw.4gtss.com:5054;transport=TCP SIP/2.0
Via: SIP/2.0/TCP 192.168.0.210:5058;rport=34676;received=192.168.0.210;branch=z9hG4bKPj7OjKdLfzTihQz6Rf6JLTREZbtt3I978D
Path: <sip:LluSNt2I9m at 192.168.0.210:5058;transport=TCP;lr;ob>
Via: SIP/2.0/TCP 192.168.0.207:44446;received=192.168.0.207;branch=z9hG4bK-524287-1---b4bccf7baf143257
Max-Forwards: 69
Contact: <sip:6505550392 at 192.168.0.207:44446;transport=tcp;rinstance=e5dd172bc47340ed>
To: <sip:6505550392 at ims.cw.4gtss.com>
From: <sip:6505550392 at ims.cw.4gtss.com>;tag=9542d61f
Call-ID: gKQzo8Y5VOUJVN__WEpSBA..
CSeq: 1 REGISTER
Expires: 600
User-Agent: Zoiper rv2.8.30
Allow-Events: presence, kpml, talk
P-Visited-Network-ID: ims.cw.4gtss.com
Content-Length:  0


--end msg--
03-04-2017 12:16:18.972 UTC Debug pjutils.cpp:741: Cloned tdta0x7f985c05a950 to tdta0x7f985c068d10
03-04-2017 12:16:18.972 UTC Debug sproutletproxy.cpp:1768: Adding message 0x7f985c069320 => txdata 0x7f985c068db8 mapping
03-04-2017 12:16:18.972 UTC Verbose sproutletproxy.cpp:1613: authentication-0x7f985c03da90 pass initial request Request msg REGISTER/cseq=1 (tdta0x7f985c068d10) to Sproutlet
03-04-2017 12:16:18.972 UTC Debug authenticationsproutlet.cpp:829: Authentication module invoked
03-04-2017 12:16:18.972 UTC Debug authenticationsproutlet.cpp:841: Request needs authentication
03-04-2017 12:16:18.972 UTC Debug acr.cpp:1812: Create RalfACR for node type S-CSCF with role Originating
03-04-2017 12:16:18.972 UTC Debug acr.cpp:49: Created ACR (0x7f985c0110a0)
03-04-2017 12:16:18.972 UTC Debug acr.cpp:189: Created S-CSCF Ralf ACR
03-04-2017 12:16:18.972 UTC Debug acr.cpp:229: Set record type for P/S-CSCF
03-04-2017 12:16:18.972 UTC Debug acr.cpp:237: Non-dialog message => EVENT_RECORD
03-04-2017 12:16:18.972 UTC Debug acr.cpp:1540: Stored 0 subscription identifiers
03-04-2017 12:16:18.972 UTC Debug authenticationsproutlet.cpp:1150: No authentication information in request or stale nonce, so reject with challenge
03-04-2017 12:16:18.972 UTC Debug sproutletproxy.cpp:1768: Adding message 0x7f985c06b2e0 => txdata 0x7f985c06ad78 mapping
03-04-2017 12:16:18.972 UTC Debug pjutils.cpp:448: Private identity defaulted from public identity = 6505550392 at ims.cw.4gtss.com
03-04-2017 12:16:18.972 UTC Debug a_record_resolver.cpp:80: ARecordResolver::resolve_iter for host hs.ims.cw.4gtss.com, port 8888, family 2
03-04-2017 12:16:18.972 UTC Debug baseresolver.cpp:425: Attempt to parse hs.ims.cw.4gtss.com as IP address
03-04-2017 12:16:18.972 UTC Verbose dnscachedresolver.cpp:486: Check cache for hs.ims.cw.4gtss.com type 1
03-04-2017 12:16:18.972 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for hs.ims.cw.4gtss.com A
03-04-2017 12:16:18.972 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
03-04-2017 12:16:18.972 UTC Debug baseresolver.cpp:425: Attempt to parse hs.ims.cw.4gtss.com as IP address
03-04-2017 12:16:18.972 UTC Debug baseresolver.cpp:819: 192.168.0.212:8888 transport 6 has state: WHITE
03-04-2017 12:16:18.972 UTC Debug baseresolver.cpp:819: 192.168.0.212:8888 transport 6 has state: WHITE
03-04-2017 12:16:18.972 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
03-04-2017 12:16:18.972 UTC Debug connection_pool.h:231: Request for connection to IP: 192.168.0.212, port: 8888
03-04-2017 12:16:18.972 UTC Debug connection_pool.h:244: Found existing connection 0x7f986c0216d0 in pool
03-04-2017 12:16:18.972 UTC Debug httpclient.cpp:478: Set CURLOPT_RESOLVE: hs.ims.cw.4gtss.com:8888:192.168.0.212
03-04-2017 12:16:18.972 UTC Debug httpclient.cpp:505: Sending HTTP request : http://hs.ims.cw.4gtss.com:8888/impi/6505550392%40ims.cw.4gtss.com/av?impu=sip%3A6505550392%40ims.cw.4gtss.com (trying 192.168.0.212)
03-04-2017 12:16:18.982 UTC Debug httpclient.cpp:832: Received header http/1.1200ok with value 
03-04-2017 12:16:18.982 UTC Debug httpclient.cpp:833: Header pointer: 0x7f98547ef060
03-04-2017 12:16:18.982 UTC Debug httpclient.cpp:832: Received header content-length with value 93
03-04-2017 12:16:18.982 UTC Debug httpclient.cpp:833: Header pointer: 0x7f98547ef060
03-04-2017 12:16:18.982 UTC Debug httpclient.cpp:832: Received header content-type with value text/plain
03-04-2017 12:16:18.982 UTC Debug httpclient.cpp:833: Header pointer: 0x7f98547ef060
03-04-2017 12:16:18.982 UTC Debug httpclient.cpp:832: Received header  with value 
03-04-2017 12:16:18.982 UTC Debug httpclient.cpp:833: Header pointer: 0x7f98547ef060
03-04-2017 12:16:18.982 UTC Debug httpclient.cpp:538: Received HTTP response: status=200, doc={"digest":{"ha1":"f18910d1d4f10d6e232b49add87a697e","realm":"ims.cw.4gtss.com","qop":"auth"}}
03-04-2017 12:16:18.982 UTC Debug baseresolver.cpp:830: Successful response from  192.168.0.212:8888 transport 6
03-04-2017 12:16:18.982 UTC Debug connection_pool.h:267: Release connection to IP: 192.168.0.212, port: 8888 to pool
03-04-2017 12:16:18.982 UTC Debug authenticationsproutlet.cpp:219: Verifying AV: {"digest":{"ha1":"f18910d1d4f10d6e232b49add87a697e","realm":"ims.cw.4gtss.com","qop":"auth"}}
03-04-2017 12:16:18.982 UTC Debug authenticationsproutlet.cpp:246: Digest specified
03-04-2017 12:16:18.982 UTC Debug authenticationsproutlet.cpp:411: Valid AV - generate challenge
03-04-2017 12:16:18.982 UTC Debug authenticationsproutlet.cpp:420: Create WWW-Authenticate header
03-04-2017 12:16:18.982 UTC Debug authenticationsproutlet.cpp:536: Add Digest information
03-04-2017 12:16:18.982 UTC Debug authenticationsproutlet.cpp:591: Write authentication challenge to IMPI store
03-04-2017 12:16:18.982 UTC Debug memcachedstore.cpp:1128: Start GET from table impi for key 6505550392 at ims.cw.4gtss.com
03-04-2017 12:16:18.982 UTC Debug astaire_resolver.cpp:72: AstaireResolver::resolve for host sprout.ims.cw.4gtss.com, family 2
03-04-2017 12:16:18.982 UTC Debug utils.cpp:353: Malformed host/port sprout.ims.cw.4gtss.com
03-04-2017 12:16:18.982 UTC Debug baseresolver.cpp:425: Attempt to parse sprout.ims.cw.4gtss.com as IP address
03-04-2017 12:16:18.982 UTC Verbose dnscachedresolver.cpp:486: Check cache for sprout.ims.cw.4gtss.com type 1
03-04-2017 12:16:18.982 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for sprout.ims.cw.4gtss.com A
03-04-2017 12:16:18.982 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
03-04-2017 12:16:18.982 UTC Debug baseresolver.cpp:819: 192.168.0.212:11311 transport 6 has state: BLACK
03-04-2017 12:16:18.982 UTC Debug baseresolver.cpp:819: 192.168.0.212:11311 transport 6 has state: BLACK
03-04-2017 12:16:18.982 UTC Debug baseresolver.cpp:1032: Added an unhealthy server, now have 1 of 2
03-04-2017 12:16:18.982 UTC Debug memcachedstore.cpp:1469: Found 1 targets for sprout.ims.cw.4gtss.com
03-04-2017 12:16:18.982 UTC Debug memcachedstore.cpp:1494: Duplicate target IP=192.168.0.212, port= 11311 as it is the only target
03-04-2017 12:16:18.982 UTC Debug memcachedstore.cpp:1082: Try server IP 192.168.0.212, port 11311
03-04-2017 12:16:18.982 UTC Debug connection_pool.h:231: Request for connection to IP: 192.168.0.212, port: 11311
03-04-2017 12:16:18.982 UTC Debug connection_pool.h:244: Found existing connection 0x7f986c059740 in pool
03-04-2017 12:16:18.983 UTC Debug memcachedstore.cpp:107: Fetch result
03-04-2017 12:16:18.983 UTC Debug memcachedstore.cpp:1093: libmemcached returned 16
03-04-2017 12:16:18.983 UTC Debug memcachedstore.cpp:1103: Return code means the request should not be retried
03-04-2017 12:16:18.983 UTC Debug connection_pool.h:267: Release connection to IP: 192.168.0.212, port: 11311 to pool
03-04-2017 12:16:18.983 UTC Debug memcachedstore.cpp:1200: Key not found
03-04-2017 12:16:18.983 UTC Debug communicationmonitor.cpp:82: Checking communication changes - successful attempts 4, failures 4
03-04-2017 12:16:18.983 UTC Debug impistore.cpp:648: Storing IMPI for 6505550392 at ims.cw.4gtss.com
{"authChallenges":[{"type":"digest","nonce":"4884896d5d032f4b","nc":1,"expires":1491221818,"correlator":"z9hG4bKPj7OjKdLfzTihQz6Rf6JLTREZbtt3I978D","realm":"ims.cw.4gtss.com","qop":"auth","ha1":"f18910d1d4f10d6e232b49add87a697e"}]}
03-04-2017 12:16:18.983 UTC Debug memcachedstore.cpp:1251: Writing 231 bytes to table impi key 6505550392 at ims.cw.4gtss.com, CAS = 0, expiry = 40
03-04-2017 12:16:18.983 UTC Debug astaire_resolver.cpp:72: AstaireResolver::resolve for host sprout.ims.cw.4gtss.com, family 2
03-04-2017 12:16:18.983 UTC Debug utils.cpp:353: Malformed host/port sprout.ims.cw.4gtss.com
03-04-2017 12:16:18.983 UTC Debug baseresolver.cpp:425: Attempt to parse sprout.ims.cw.4gtss.com as IP address
03-04-2017 12:16:18.983 UTC Verbose dnscachedresolver.cpp:486: Check cache for sprout.ims.cw.4gtss.com type 1
03-04-2017 12:16:18.983 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for sprout.ims.cw.4gtss.com A
03-04-2017 12:16:18.983 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
03-04-2017 12:16:18.983 UTC Debug baseresolver.cpp:819: 192.168.0.212:11311 transport 6 has state: BLACK
03-04-2017 12:16:18.983 UTC Debug baseresolver.cpp:819: 192.168.0.212:11311 transport 6 has state: BLACK
03-04-2017 12:16:18.983 UTC Debug baseresolver.cpp:1032: Added an unhealthy server, now have 1 of 2
03-04-2017 12:16:18.983 UTC Debug memcachedstore.cpp:1469: Found 1 targets for sprout.ims.cw.4gtss.com
03-04-2017 12:16:18.983 UTC Debug memcachedstore.cpp:1494: Duplicate target IP=192.168.0.212, port= 11311 as it is the only target
03-04-2017 12:16:18.983 UTC Debug memcachedstore.cpp:1082: Try server IP 192.168.0.212, port 11311
03-04-2017 12:16:18.983 UTC Debug connection_pool.h:231: Request for connection to IP: 192.168.0.212, port: 11311
03-04-2017 12:16:18.983 UTC Debug connection_pool.h:244: Found existing connection 0x7f986c059740 in pool
03-04-2017 12:16:18.983 UTC Debug memcachedstore.cpp:144: Attempting to add data for key impi\\6505550392 at ims.cw.4gtss.com
03-04-2017 12:16:18.983 UTC Debug memcachedstore.cpp:154: Attempting memcached ADD command
03-04-2017 12:16:18.984 UTC Debug memcachedstore.cpp:244: ADD/CAS returned rc = 7 (UNKNOWN READ FAILURE)
(140292624120432) UNKNOWN READ FAILURE,  host: 192.168.0.212:11311 -> libmemcached/response.cc:782
03-04-2017 12:16:18.984 UTC Debug memcachedstore.cpp:1093: libmemcached returned 7
03-04-2017 12:16:18.984 UTC Debug memcachedstore.cpp:1110: Blacklisting target
03-04-2017 12:16:18.984 UTC Debug baseresolver.cpp:400: Add 192.168.0.212:11311 transport 6 to blacklist for 30 seconds, graylist for 0 seconds
03-04-2017 12:16:18.984 UTC Debug connection_pool.h:267: Release connection to IP: 192.168.0.212, port: 11311 to pool
03-04-2017 12:16:18.984 UTC Debug memcachedstore.cpp:1082: Try server IP 192.168.0.212, port 11311
03-04-2017 12:16:18.984 UTC Debug connection_pool.h:231: Request for connection to IP: 192.168.0.212, port: 11311
03-04-2017 12:16:18.984 UTC Debug connection_pool.h:244: Found existing connection 0x7f986c059740 in pool
03-04-2017 12:16:18.984 UTC Debug memcachedstore.cpp:144: Attempting to add data for key impi\\6505550392 at ims.cw.4gtss.com
03-04-2017 12:16:18.984 UTC Debug memcachedstore.cpp:154: Attempting memcached ADD command
03-04-2017 12:16:18.985 UTC Debug memcachedstore.cpp:244: ADD/CAS returned rc = 7 (UNKNOWN READ FAILURE)
(140292624120432) UNKNOWN READ FAILURE,  host: 192.168.0.212:11311 -> libmemcached/response.cc:782
03-04-2017 12:16:18.985 UTC Debug memcachedstore.cpp:1093: libmemcached returned 7
03-04-2017 12:16:18.985 UTC Debug memcachedstore.cpp:1110: Blacklisting target
03-04-2017 12:16:18.985 UTC Debug baseresolver.cpp:400: Add 192.168.0.212:11311 transport 6 to blacklist for 30 seconds, graylist for 0 seconds
03-04-2017 12:16:18.985 UTC Debug connection_pool.h:267: Release connection to IP: 192.168.0.212, port: 11311 to pool
03-04-2017 12:16:18.985 UTC Debug memcachedstore.cpp:1366: Failed to write data for impi\\6505550392 at ims.cw.4gtss.com to store with error UNKNOWN READ FAILURE
03-04-2017 12:16:18.985 UTC Error impistore.cpp:664: Failed to write IMPI for private_id 6505550392 at ims.cw.4gtss.com
03-04-2017 12:16:18.985 UTC Debug authenticationsproutlet.cpp:675: Failed to store nonce in memcached
03-04-2017 12:16:18.985 UTC Info acr.cpp:690: No CCF or ECF to send ACR for session gKQzo8Y5VOUJVN__WEpSBA.. to - dropping!
03-04-2017 12:16:18.985 UTC Verbose sproutletproxy.cpp:1427: authentication-0x7f985c03da90 sending Response msg 500/REGISTER/cseq=1 (tdta0x7f985c06acd0)
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1775: Removing message 0x7f985c069320 => txdata 0x7f985c068db8 mapping
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1504: Free message tdta0x7f985c068d10
03-04-2017 12:16:18.985 UTC Debug pjsip: tdta0x7f985c06 Destroying txdata Request msg REGISTER/cseq=1 (tdta0x7f985c068d10)
03-04-2017 12:16:18.985 UTC Debug acr.cpp:54: Destroyed ACR (0x7f985c0110a0)
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1783: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1869: Aggregating response with status code 500
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1919: 3xx/4xx/5xx/6xx response
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1923: Best 3xx/4xx/5xx/6xx response so far
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1810: All UAC responded
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1775: Removing message 0x7f985c06b2e0 => txdata 0x7f985c06ad78 mapping
03-04-2017 12:16:18.985 UTC Verbose sproutletproxy.cpp:2095: Routing Response msg 500/REGISTER/cseq=1 (tdta0x7f985c06acd0) (602 bytes) to upstream sproutlet icscf:
--start msg--

SIP/2.0 500 Internal Server Error
Via: SIP/2.0/TCP 192.168.0.210:5058;rport=34676;received=192.168.0.210;branch=z9hG4bKPj7OjKdLfzTihQz6Rf6JLTREZbtt3I978D
Via: SIP/2.0/TCP 192.168.0.207:44446;received=192.168.0.207;branch=z9hG4bK-524287-1---b4bccf7baf143257
Call-ID: gKQzo8Y5VOUJVN__WEpSBA..
From: <sip:6505550392 at ims.cw.4gtss.com>;tag=9542d61f
To: <sip:6505550392 at ims.cw.4gtss.com>;tag=z9hG4bKPj7OjKdLfzTihQz6Rf6JLTREZbtt3I978D
CSeq: 1 REGISTER
WWW-Authenticate: Digest  realm="ims.cw.4gtss.com",nonce="4884896d5d032f4b",opaque="6748a04c7f558e19",algorithm=MD5,qop="auth"
Content-Length:  0


--end msg--
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1768: Adding message 0x7f985c06b2e0 => txdata 0x7f985c06ad78 mapping
03-04-2017 12:16:18.985 UTC Verbose sproutletproxy.cpp:1666: icscf-0x7f985c0008e0 received final response Response msg 500/REGISTER/cseq=1 (tdta0x7f985c06acd0) on fork 0, state = Terminated
03-04-2017 12:16:18.985 UTC Debug acr.cpp:1540: Stored 1 subscription identifiers
03-04-2017 12:16:18.985 UTC Debug icscfsproutlet.cpp:333: Check retry conditions for REGISTER, status = 500, S-CSCF responsive
03-04-2017 12:16:18.985 UTC Verbose sproutletproxy.cpp:1427: icscf-0x7f985c0008e0 sending Response msg 500/REGISTER/cseq=1 (tdta0x7f985c06acd0)
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1783: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1869: Aggregating response with status code 500
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1919: 3xx/4xx/5xx/6xx response
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1923: Best 3xx/4xx/5xx/6xx response so far
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1810: All UAC responded
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1775: Removing message 0x7f985c06b2e0 => txdata 0x7f985c06ad78 mapping
03-04-2017 12:16:18.985 UTC Debug pjsip: tsx0x7f985c010 Sending Response msg 500/REGISTER/cseq=1 (tdta0x7f985c06acd0) in state Trying
03-04-2017 12:16:18.985 UTC Verbose common_sip_processing.cpp:136: TX 602 bytes Response msg 500/REGISTER/cseq=1 (tdta0x7f985c06acd0) to TCP 192.168.0.210:34676:
--start msg--

SIP/2.0 500 Internal Server Error
Via: SIP/2.0/TCP 192.168.0.210:5058;rport=34676;received=192.168.0.210;branch=z9hG4bKPj7OjKdLfzTihQz6Rf6JLTREZbtt3I978D
Via: SIP/2.0/TCP 192.168.0.207:44446;received=192.168.0.207;branch=z9hG4bK-524287-1---b4bccf7baf143257
Call-ID: gKQzo8Y5VOUJVN__WEpSBA..
From: <sip:6505550392 at ims.cw.4gtss.com>;tag=9542d61f
To: <sip:6505550392 at ims.cw.4gtss.com>;tag=z9hG4bKPj7OjKdLfzTihQz6Rf6JLTREZbtt3I978D
CSeq: 1 REGISTER
WWW-Authenticate: Digest  realm="ims.cw.4gtss.com",nonce="4884896d5d032f4b",opaque="6748a04c7f558e19",algorithm=MD5,qop="auth"
Content-Length:  0


--end msg--
03-04-2017 12:16:18.985 UTC Debug pjsip: tsx0x7f985c010 State changed from Trying to Completed, event=TX_MSG
03-04-2017 12:16:18.985 UTC Debug basicproxy.cpp:213: tsx0x7f985c010708 - tu_on_tsx_state UAS, TSX_STATE TX_MSG state=Completed
03-04-2017 12:16:18.985 UTC Verbose sproutletproxy.cpp:1861: icscf-0x7f985c0008e0 suiciding
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1169: Destroying SproutletWrapper 0x7f985c00cbf0
03-04-2017 12:16:18.985 UTC Info acr.cpp:690: No CCF or ECF to send ACR for session gKQzo8Y5VOUJVN__WEpSBA.. to - dropping!
03-04-2017 12:16:18.985 UTC Debug acr.cpp:54: Destroyed ACR (0x7f985c04d290)
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1178: Free original request Request msg REGISTER/cseq=1 (tdta0x7f985c00d0f0) (tdta0x7f985c00d0f0)
03-04-2017 12:16:18.985 UTC Verbose sproutletproxy.cpp:1861: authentication-0x7f985c03da90 suiciding
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1169: Destroying SproutletWrapper 0x7f985c03db00
03-04-2017 12:16:18.985 UTC Debug sproutletproxy.cpp:1178: Free original request Request msg REGISTER/cseq=1 (tdta0x7f985c05a950) (tdta0x7f985c05a950)
03-04-2017 12:16:18.985 UTC Debug pjsip: tdta0x7f985c05 Destroying txdata Request msg REGISTER/cseq=1 (tdta0x7f985c05a950)
03-04-2017 12:16:18.985 UTC Debug thread_dispatcher.cpp:200: Worker thread completed processing message 0x7f98640b0ae8
03-04-2017 12:16:18.985 UTC Debug thread_dispatcher.cpp:206: Request latency = 14704us
03-04-2017 12:16:18.985 UTC Info load_monitor.cpp:232: Accepted 100.000000% of requests, latency error = -0.915970, overload responses = 0
03-04-2017 12:16:18.985 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 25 requests in last 203671ms, minimum threshold for a change is 25458.876953
03-04-2017 12:16:18.985 UTC Debug snmp_continuous_accumulator_by_scope_table.cpp:111: Accumulating sample 250ui into continuous accumulator statistic
03-04-2017 12:16:18.985 UTC Debug snmp_continuous_accumulator_by_scope_table.cpp:111: Accumulating sample 250ui into continuous accumulator statistic
03-04-2017 12:16:18.989 UTC Debug pjsip: tsx0x7f985c010 Timeout timer event
03-04-2017 12:16:18.989 UTC Debug pjsip: tsx0x7f985c010 State changed from Completed to Terminated, event=TIMER
03-04-2017 12:16:18.989 UTC Debug basicproxy.cpp:213: tsx0x7f985c010708 - tu_on_tsx_state UAS, TSX_STATE TIMER state=Terminated
03-04-2017 12:16:18.989 UTC Debug basicproxy.cpp:1308: Report SAS end marker - trail (190)
03-04-2017 12:16:18.989 UTC Debug pjsip: tsx0x7f985c010 Timeout timer event
03-04-2017 12:16:18.989 UTC Debug pjsip: tsx0x7f985c010 State changed from Terminated to Destroyed, event=TIMER
03-04-2017 12:16:18.989 UTC Debug basicproxy.cpp:213: tsx0x7f985c010708 - tu_on_tsx_state UAS, TSX_STATE TIMER state=Destroyed
03-04-2017 12:16:18.989 UTC Debug sproutletproxy.cpp:750: tsx0x7f985c010708 - UAS tsx destroyed
03-04-2017 12:16:18.989 UTC Debug sproutletproxy.cpp:1090: Safe for UASTsx to suicide
03-04-2017 12:16:18.989 UTC Debug basicproxy.cpp:1483: Transaction ((nil)) suiciding
03-04-2017 12:16:18.989 UTC Verbose sproutletproxy.cpp:538: Sproutlet Proxy transaction (0x7f985c01c2d0) destroyed
03-04-2017 12:16:18.989 UTC Debug basicproxy.cpp:494: BasicProxy::UASTsx destructor (0x7f985c01c2d0)
03-04-2017 12:16:18.989 UTC Debug basicproxy.cpp:511: Disconnect UAC transactions from UAS transaction
03-04-2017 12:16:18.989 UTC Debug basicproxy.cpp:525: Free original request
03-04-2017 12:16:18.989 UTC Debug pjsip: tdta0x7f985c00 Destroying txdata Request msg REGISTER/cseq=1 (tdta0x7f985c00d0f0)
03-04-2017 12:16:18.989 UTC Debug basicproxy.cpp:534: Free un-used best response
03-04-2017 12:16:18.989 UTC Debug pjsip: tdta0x7f985c05 Destroying txdata Response msg 408/REGISTER/cseq=1 (tdta0x7f985c059940)
03-04-2017 12:16:18.989 UTC Debug basicproxy.cpp:555: BasicProxy::UASTsx destructor completed
03-04-2017 12:16:18.989 UTC Debug pjsip: tdta0x7f985c06 Destroying txdata Response msg 500/REGISTER/cseq=1 (tdta0x7f985c06acd0)
03-04-2017 12:16:18.989 UTC Debug pjsip: tsx0x7f985c010 Transaction destroyed!


From Andrew.Edmonds at metaswitch.com  Tue Apr  4 13:06:06 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Tue, 4 Apr 2017 17:06:06 +0000
Subject: [Project Clearwater] Registration failed (Internal Server Error
 (500))
In-Reply-To: <8589c05a-2642-dcc4-9e40-19c71d64f494@4gtss.com>
References: <8589c05a-2642-dcc4-9e40-19c71d64f494@4gtss.com>
Message-ID: <BLUPR02MB437603F43D72815430A1FB0E50B0@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Ahmed,

Thank you for your continued use of Clearwater, we appreciate the feedback and logs you have provided here. A 500 error typically means that something has gone wrong with a node's internal processing. I've confirmed that this is the case by having a look through the Sprout logs you have attached, the first point at which we see something that looks like an error is the point at which Sprout attempts to save the authentication challenge for the subscriber in its local memory store, memcached:

03-04-2017 12:16:18.984 UTC Debug memcachedstore.cpp:144: Attempting to add data for key impi\\6505550392 at ims.cw.4gtss.com
03-04-2017 12:16:18.984 UTC Debug memcachedstore.cpp:154: Attempting memcached ADD command
03-04-2017 12:16:18.985 UTC Debug memcachedstore.cpp:244: ADD/CAS returned rc = 7 (UNKNOWN READ FAILURE)
(140292624120432) UNKNOWN READ FAILURE,  host: 192.168.0.212:11311 -> libmemcached/response.cc:782
03-04-2017 12:16:18.985 UTC Debug memcachedstore.cpp:1093: libmemcached returned 7
03-04-2017 12:16:18.985 UTC Debug memcachedstore.cpp:1110: Blacklisting target
03-04-2017 12:16:18.985 UTC Debug baseresolver.cpp:400: Add 192.168.0.212:11311 transport 6 to blacklist for 30 seconds, graylist for 0 seconds
03-04-2017 12:16:18.985 UTC Debug connection_pool.h:267: Release connection to IP: 192.168.0.212, port: 11311 to pool
03-04-2017 12:16:18.985 UTC Debug memcachedstore.cpp:1366: Failed to write data for impi\\6505550392 at ims.cw.4gtss.com to store with error UNKNOWN READ FAILURE
03-04-2017 12:16:18.985 UTC Error impistore.cpp:664: Failed to write IMPI for private_id 6505550392 at ims.cw.4gtss.com
03-04-2017 12:16:18.985 UTC Debug authenticationsproutlet.cpp:675: Failed to store nonce in memcached

This means that Sprout would have no means by which to authenticate the subscriber and has to send out a 500 Internal Server Error.

To try and determine the cause behind this issue can you answer the following questions:

- Is memcached running? There are a few tools you can use to determine this: running "sudo monit summary" on your Sprout node should give the status of the memcached process, likewise " ps aux | grep memcached" will show memcached's process id if it is running.
- Is the IP address that Sprout is attempting to use to communicate with memcached correct? It is shown as 192.168.0.212 in the above logs, this should be the IP address of one of your Sprout nodes. You can verify whether it is by logging on to each of your Sprout nodes, running "ifconfig" and taking note of the inet addr property of each node.

Thanks,

Andrew

From ahmed.eldeeb at 4gtss.com  Wed Apr  5 03:33:12 2017
From: ahmed.eldeeb at 4gtss.com (Ahmed Eldeeb)
Date: Wed, 5 Apr 2017 09:33:12 +0200
Subject: [Project Clearwater] Registration failed (Internal Server Error
 (500))
In-Reply-To: <BLUPR02MB437603F43D72815430A1FB0E50B0@BLUPR02MB437.namprd02.prod.outlook.com>
References: <8589c05a-2642-dcc4-9e40-19c71d64f494@4gtss.com>
	<BLUPR02MB437603F43D72815430A1FB0E50B0@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <4482a74d-a508-b2a6-a347-3da44f4860d3@4gtss.com>

The memcached process is running and all processes running fine and the 
ip of sprout is correct as 192.168.0.212 however,The old version i have 
installed of sprout-node (1.0-170307) is running without this problem

I have found that the new version installed is 1.0-170313


On 04/04/17 19:06, Andrew Edmonds wrote:
> Hi Ahmed,
>
> Thank you for your continued use of Clearwater, we appreciate the feedback and logs you have provided here. A 500 error typically means that something has gone wrong with a node's internal processing. I've confirmed that this is the case by having a look through the Sprout logs you have attached, the first point at which we see something that looks like an error is the point at which Sprout attempts to save the authentication challenge for the subscriber in its local memory store, memcached:
>
> 03-04-2017 12:16:18.984 UTC Debug memcachedstore.cpp:144: Attempting to add data for key impi\\6505550392 at ims.cw.4gtss.com
> 03-04-2017 12:16:18.984 UTC Debug memcachedstore.cpp:154: Attempting memcached ADD command
> 03-04-2017 12:16:18.985 UTC Debug memcachedstore.cpp:244: ADD/CAS returned rc = 7 (UNKNOWN READ FAILURE)
> (140292624120432) UNKNOWN READ FAILURE,  host: 192.168.0.212:11311 -> libmemcached/response.cc:782
> 03-04-2017 12:16:18.985 UTC Debug memcachedstore.cpp:1093: libmemcached returned 7
> 03-04-2017 12:16:18.985 UTC Debug memcachedstore.cpp:1110: Blacklisting target
> 03-04-2017 12:16:18.985 UTC Debug baseresolver.cpp:400: Add 192.168.0.212:11311 transport 6 to blacklist for 30 seconds, graylist for 0 seconds
> 03-04-2017 12:16:18.985 UTC Debug connection_pool.h:267: Release connection to IP: 192.168.0.212, port: 11311 to pool
> 03-04-2017 12:16:18.985 UTC Debug memcachedstore.cpp:1366: Failed to write data for impi\\6505550392 at ims.cw.4gtss.com to store with error UNKNOWN READ FAILURE
> 03-04-2017 12:16:18.985 UTC Error impistore.cpp:664: Failed to write IMPI for private_id 6505550392 at ims.cw.4gtss.com
> 03-04-2017 12:16:18.985 UTC Debug authenticationsproutlet.cpp:675: Failed to store nonce in memcached
>
> This means that Sprout would have no means by which to authenticate the subscriber and has to send out a 500 Internal Server Error.
>
> To try and determine the cause behind this issue can you answer the following questions:
>
> - Is memcached running? There are a few tools you can use to determine this: running "sudo monit summary" on your Sprout node should give the status of the memcached process, likewise " ps aux | grep memcached" will show memcached's process id if it is running.
> - Is the IP address that Sprout is attempting to use to communicate with memcached correct? It is shown as 192.168.0.212 in the above logs, this should be the IP address of one of your Sprout nodes. You can verify whether it is by logging on to each of your Sprout nodes, running "ifconfig" and taking note of the inet addr property of each node.
>
> Thanks,
>
> Andrew
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org




From Andrew.Edmonds at metaswitch.com  Wed Apr  5 06:37:33 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Wed, 5 Apr 2017 10:37:33 +0000
Subject: [Project Clearwater] CDIV with MRF Invite message bounce-back
 to TAS
In-Reply-To: <TU4PR84MB00647DA578239D9C2ABC65A2DC330@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>
References: <TU4PR84MB00647DA578239D9C2ABC65A2DC330@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>
Message-ID: <BLUPR02MB4376A66862AAD26BAD047B7E50A0@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Aravind,

Thanks for your email.

In order to diagnose the issue I need to understand a little more about your deployment. Could you please answer:


*         What does your deployment look like? What node types are you using and what are the IP addresses of your TAS and MRF node?

*         What is the expected call flow of the Communication Diversion scenario with MRF announcement that you are testing?

It would also be helpful if you could supply the Sprout log file (contained in /var/log/sprout) for the time at which you made the call.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shirabur, Aravind Ashok (CMS)
Sent: Monday, March 27, 2017 9:46 AM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] CDIV with MRF Invite message bounce-back to TAS

Hi,

We are testing the Communication Diversion scenario with MRF announcement, observed that the Announcement INVITE request initiated from the Application Server (TAS) is bouncing back. Do we need to configure the S-CSCF to forward the request to MRF ?

The expected Header in the Announcement INVITE message is missing ? Application server is setting the 'P-Served-User' as "P-Served-User: <sip:7406246264 at mshpe.com>;regstate=reg;sescase=term"


Find the attached wireshark trace for detail ...

[cid:image001.jpg at 01D2AE00.B1B492C0]



Clearwater-infrastructure version:  1.0-160518.173307

Thanks
Aravind

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170405/2247d032/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 64462 bytes
Desc: image001.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170405/2247d032/attachment.jpg>

From Sebastian.Rex at metaswitch.com  Wed Apr  5 12:36:54 2017
From: Sebastian.Rex at metaswitch.com (Sebastian Rex)
Date: Wed, 5 Apr 2017 16:36:54 +0000
Subject: [Project Clearwater] Release note for Celebrimbor
Message-ID: <SN1PR02MB1664DD90BBF2C02970CC5E798F0A0@SN1PR02MB1664.namprd02.prod.outlook.com>

The release for Project Clearwater sprint "Celebrimbor" has been cut. The code for this release is tagged as release-120 in GitHub.

This release includes the following bug fixes:


*         A fresh checkout of Ellis doesn't build first time

*         Unable to unregister subscriber with wildcard identity in implicit registration set

*         Chronos doesn't replicate tombstones

*         reg-info NOTIFY XML contains undefined "ere" namespace

*         A fresh checkout of Sprout doesn't build

To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.

Seb

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170405/6e2e2483/attachment.html>

From Muhammad.Shaikh at huawei.com  Mon Apr 10 18:16:43 2017
From: Muhammad.Shaikh at huawei.com (Muhammad Shaikh (Salman))
Date: Mon, 10 Apr 2017 22:16:43 +0000
Subject: [Project Clearwater] Clearwater IMS homers failed to start..
Message-ID: <BE94A67F9CCBC0409944CFAEE0FE797F013C56AA@SJCEML701-CHM.china.huawei.com>

I have net setup running and after the installation of homer it is failed to start with error message:

[homer]ubuntu at homer:~$ sudo service homer start
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named service_identity'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 218, in <module>
    standalone()
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 128, in standalone
    application = create_application()
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 98, in create_application
    application = cyclone.web.Application(api.get_routes(), **app_settings)
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 68, in get_routes
    return sum([load_module(m).ROUTES for m in settings.INSTALLED_HANDLERS], []) + ROUTES
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 62, in load_module
    fromlist=["ROUTES"])
ImportError: No module named homestead

I am using Python 2.7.6 and have already installed and updated the service_identity modue of python through easy_install and now I get the message that it is already up to date:

easy_install service_identity
Searching for service-identity
Best match: service-identity 16.0.0
Processing service_identity-16.0.0-py2.7.egg
service-identity 16.0.0 is already the active version in easy-install.pth

Using /usr/local/lib/python2.7/dist-packages/service_identity-16.0.0-py2.7.egg
Processing dependencies for service-identity
Finished processing dependencies for service-identity

----------------------------------------------------------
Salman Shaikh
NFV Open Lab Architect
Huawei R&D USA
FutureWei Technologies, Inc
Email: Muhammad.Shaikh at huawei.com<mailto:Muhammad.Shaikh at huawei.com>
Mobile: +1 (408) 421-6210
----------------------------------------------------------

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170410/5b447f26/attachment.html>

From Andrew.Edmonds at metaswitch.com  Tue Apr 11 12:37:55 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Tue, 11 Apr 2017 16:37:55 +0000
Subject: [Project Clearwater] Clearwater IMS homers failed to start..
In-Reply-To: <BE94A67F9CCBC0409944CFAEE0FE797F013C56AA@SJCEML701-CHM.china.huawei.com>
References: <BE94A67F9CCBC0409944CFAEE0FE797F013C56AA@SJCEML701-CHM.china.huawei.com>
Message-ID: <BLUPR02MB437331F47CBC7458C31DE4FE5000@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Salman,

Thanks for your question.

In order to better understand what is going on here could I please ask you to provide some more details about the deployment you are using:


*         What version of Clearwater are you using?

*         What topology are you using? What nodes make up your deployment?

On your homer node it would be useful if you could retrieve and send me the homer and monit logs, to do this try starting homer again and then download the most recent log files contained in /var/log/homer and /var/log/clearwater-monit.

On startup homer should read its configuration file and generate a local_settings.py file to mandate what handlers to install. One possibility is that this local_settings.py file was never generated. To verify this could you please run the command "sudo /usr/share/clearwater/infrastructure/scripts/homer" and let me know what the output is. You might find that after running this script you are able to start the homer process without any errors.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Muhammad Shaikh (Salman)
Sent: Monday, April 10, 2017 11:17 PM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Clearwater IMS homers failed to start..

I have net setup running and after the installation of homer it is failed to start with error message:

[homer]ubuntu at homer:~$ sudo service homer start
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named service_identity'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 218, in <module>
    standalone()
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 128, in standalone
    application = create_application()
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 98, in create_application
    application = cyclone.web.Application(api.get_routes(), **app_settings)
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 68, in get_routes
    return sum([load_module(m).ROUTES for m in settings.INSTALLED_HANDLERS], []) + ROUTES
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 62, in load_module
    fromlist=["ROUTES"])
ImportError: No module named homestead

I am using Python 2.7.6 and have already installed and updated the service_identity modue of python through easy_install and now I get the message that it is already up to date:

easy_install service_identity
Searching for service-identity
Best match: service-identity 16.0.0
Processing service_identity-16.0.0-py2.7.egg
service-identity 16.0.0 is already the active version in easy-install.pth

Using /usr/local/lib/python2.7/dist-packages/service_identity-16.0.0-py2.7.egg
Processing dependencies for service-identity
Finished processing dependencies for service-identity

----------------------------------------------------------
Salman Shaikh
NFV Open Lab Architect
Huawei R&D USA
FutureWei Technologies, Inc
Email: Muhammad.Shaikh at huawei.com<mailto:Muhammad.Shaikh at huawei.com>
Mobile: +1 (408) 421-6210
----------------------------------------------------------

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170411/86d9b0c4/attachment.html>

From Muhammad.Shaikh at huawei.com  Tue Apr 11 20:19:23 2017
From: Muhammad.Shaikh at huawei.com (Muhammad Shaikh (Salman))
Date: Wed, 12 Apr 2017 00:19:23 +0000
Subject: [Project Clearwater] Clearwater IMS homers failed to start..
In-Reply-To: <BLUPR02MB437331F47CBC7458C31DE4FE5000@BLUPR02MB437.namprd02.prod.outlook.com>
References: <BE94A67F9CCBC0409944CFAEE0FE797F013C56AA@SJCEML701-CHM.china.huawei.com>
	<BLUPR02MB437331F47CBC7458C31DE4FE5000@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <BE94A67F9CCBC0409944CFAEE0FE797F013C5EAF@SJCEML701-CHM.china.huawei.com>


Thanks Andrew,
I think it was likely be the installation issue. I did not install homer correctly. I have got it working by re-installing the nodes. But Now I have issue at Homestead node with Cassandra as:

homestead_current.txt -->

12-04-2017 00:14:36.797 UTC Status load_monitor.cpp:105: Constructing LoadMonitor
12-04-2017 00:14:36.797 UTC Status load_monitor.cpp:106:    Target latency (usecs)   : 100000
12-04-2017 00:14:36.797 UTC Status load_monitor.cpp:107:    Max bucket size          : 1000
12-04-2017 00:14:36.797 UTC Status load_monitor.cpp:108:    Initial token fill rate/s: 100.000000
12-04-2017 00:14:36.797 UTC Status load_monitor.cpp:109:    Min token fill rate/s    : 10.000000
12-04-2017 00:14:36.797 UTC Status dnscachedresolver.cpp:150: Creating Cached Resolver using servers:
12-04-2017 00:14:36.797 UTC Status dnscachedresolver.cpp:160:     127.0.0.1
12-04-2017 00:14:36.797 UTC Status a_record_resolver.cpp:54: Created ARecordResolver
12-04-2017 00:14:36.797 UTC Status a_record_resolver.cpp:54: Created ARecordResolver
12-04-2017 00:14:36.797 UTC Status cassandra_store.cpp:181: Configuring store connection
12-04-2017 00:14:36.797 UTC Status cassandra_store.cpp:182:   Hostname:  127.0.0.1
12-04-2017 00:14:36.797 UTC Status cassandra_store.cpp:183:   Port:      9160
12-04-2017 00:14:36.797 UTC Status cassandra_store.cpp:211: Configuring store worker pool
12-04-2017 00:14:36.797 UTC Status cassandra_store.cpp:212:   Threads:   10
12-04-2017 00:14:36.797 UTC Status cassandra_store.cpp:213:   Max Queue: 0
12-04-2017 00:14:36.798 UTC Error main.cpp:756: Failed to initialize the Cassandra cache with error code 1.
12-04-2017 00:14:36.798 UTC Status main.cpp:757: Homestead is shutting down



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Andrew Edmonds
Sent: Tuesday, April 11, 2017 9:38 AM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Clearwater IMS homers failed to start..

Hi Salman,

Thanks for your question.

In order to better understand what is going on here could I please ask you to provide some more details about the deployment you are using:


*         What version of Clearwater are you using?

*         What topology are you using? What nodes make up your deployment?

On your homer node it would be useful if you could retrieve and send me the homer and monit logs, to do this try starting homer again and then download the most recent log files contained in /var/log/homer and /var/log/clearwater-monit.

On startup homer should read its configuration file and generate a local_settings.py file to mandate what handlers to install. One possibility is that this local_settings.py file was never generated. To verify this could you please run the command "sudo /usr/share/clearwater/infrastructure/scripts/homer" and let me know what the output is. You might find that after running this script you are able to start the homer process without any errors.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Muhammad Shaikh (Salman)
Sent: Monday, April 10, 2017 11:17 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Clearwater IMS homers failed to start..

I have net setup running and after the installation of homer it is failed to start with error message:

[homer]ubuntu at homer:~$ sudo service homer start
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named service_identity'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 218, in <module>
    standalone()
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 128, in standalone
    application = create_application()
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 98, in create_application
    application = cyclone.web.Application(api.get_routes(), **app_settings)
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 68, in get_routes
    return sum([load_module(m).ROUTES for m in settings.INSTALLED_HANDLERS], []) + ROUTES
  File "/usr/share/clearwater/crest/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 62, in load_module
    fromlist=["ROUTES"])
ImportError: No module named homestead

I am using Python 2.7.6 and have already installed and updated the service_identity modue of python through easy_install and now I get the message that it is already up to date:

easy_install service_identity
Searching for service-identity
Best match: service-identity 16.0.0
Processing service_identity-16.0.0-py2.7.egg
service-identity 16.0.0 is already the active version in easy-install.pth

Using /usr/local/lib/python2.7/dist-packages/service_identity-16.0.0-py2.7.egg
Processing dependencies for service-identity
Finished processing dependencies for service-identity

----------------------------------------------------------
Salman Shaikh
NFV Open Lab Architect
Huawei R&D USA
FutureWei Technologies, Inc
Email: Muhammad.Shaikh at huawei.com<mailto:Muhammad.Shaikh at huawei.com>
Mobile: +1 (408) 421-6210
----------------------------------------------------------

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170412/8524d9b2/attachment.html>

From silvestro.ciampoli at hcl.com  Wed Apr 12 05:02:56 2017
From: silvestro.ciampoli at hcl.com (Silvestro Ciampoli)
Date: Wed, 12 Apr 2017 09:02:56 +0000
Subject: [Project Clearwater] Sprout failed: Error: No local site
 registration store specified
Message-ID: <HK2PR0401MB19543327896D0B1A72AF1D1A9F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>


Hi Clearwater Team,


I'm trying to (re-)install Clearwater on Openstack (6 VMs) but I'm encountering an issue on the Sprout node. It was failing with the following error : "Error main.cpp:1678: No local site registration store specified". Please see the .log in attachment for further details. Do you have any suggestions?


Below the status of other processes and the version information.



Thanks in advance,


Silvestro Ciampoli


------------------------------------


[sprout]ubuntu at 0:~$ sudo monit summary
Monit 5.18.1 uptime: 1h 0m
 Service Name                     Status                      Type
 node-0.sprout.hcllab.it          Running                     System
 sprout_process                   Execution failed | Does...  Process
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 sprout_uptime                    Wait parent                 Program
 poll_sprout_sip                  Wait parent                 Program
 poll_sprout_http                 Wait parent                 Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Progra



/usr/share/clearwater/bin/clearwater-version

astaire                                  1.0-170331.102803
chronos                                  1.0-170331.102347
clearwater-cluster-manager               1.0-170331.141119
clearwater-config-manager                1.0-170331.141119
clearwater-diags-monitor                 1.0-170327.103718
clearwater-etcd                          1.0-170331.141119
clearwater-infrastructure                1.0-170327.103718
clearwater-log-cleanup                   1.0-170327.103718
clearwater-management                    1.0-170331.141119
clearwater-memcached                     1.0-170327.103718
clearwater-monit                         5.18-170403.124108
clearwater-nginx                         1.0-170223.132518
clearwater-queue-manager                 1.0-170331.141119
clearwater-snmp-handler-astaire          1.0-170331.103056
clearwater-snmpd                         1.0-170327.103718
clearwater-socket-factory                1.0-170327.103718
clearwater-tcp-scalability               1.0-170327.103718
memcached                                1.6.00-0clearwater0.5
sprout                                   1.0-170403.142633
sprout-base                              1.0-170403.142633
sprout-bgcf                              1.0-170403.142633
sprout-icscf                             1.0-170403.142633
sprout-mmtel-as                          1.0-170403.142633
sprout-node                              1.0-170403.142633
sprout-scscf                             1.0-170403.142633





::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170412/0d99234d/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sprout_failure.log
Type: text/x-log
Size: 5590 bytes
Desc: sprout_failure.log
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170412/0d99234d/attachment.bin>

From silvestro.ciampoli at hcl.com  Wed Apr 12 06:18:00 2017
From: silvestro.ciampoli at hcl.com (Silvestro Ciampoli)
Date: Wed, 12 Apr 2017 10:18:00 +0000
Subject: [Project Clearwater] Sprout failed: Error: No local site
 registration store specified
In-Reply-To: <HK2PR0401MB19543327896D0B1A72AF1D1A9F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>
References: <HK2PR0401MB19543327896D0B1A72AF1D1A9F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>
Message-ID: <HK2PR0401MB1954ABE9136D0FEA2CED0BE79F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>


Hi,


I'm sorry I missed to specify that to deploy Clearwater in Openstack I used the Heat template available on github. The configuration foreseen the separation between management and signaling network.



BR,

Silvestro Ciampoli


________________________________
Da: Silvestro Ciampoli
Inviato: mercoled? 12 aprile 2017 11:02
A: clearwater at lists.projectclearwater.org
Oggetto: Sprout failed: Error: No local site registration store specified



Hi Clearwater Team,


I'm trying to (re-)install Clearwater on Openstack (6 VMs) but I'm encountering an issue on the Sprout node. It was failing with the following error : "Error main.cpp:1678: No local site registration store specified". Please see the .log in attachment for further details. Do you have any suggestions?


Below the status of other processes and the version information.



Thanks in advance,


Silvestro Ciampoli


------------------------------------


[sprout]ubuntu at 0:~$ sudo monit summary
Monit 5.18.1 uptime: 1h 0m
 Service Name                     Status                      Type
 node-0.sprout.hcllab.it          Running                     System
 sprout_process                   Execution failed | Does...  Process
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 sprout_uptime                    Wait parent                 Program
 poll_sprout_sip                  Wait parent                 Program
 poll_sprout_http                 Wait parent                 Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Progra



/usr/share/clearwater/bin/clearwater-version

astaire                                  1.0-170331.102803
chronos                                  1.0-170331.102347
clearwater-cluster-manager               1.0-170331.141119
clearwater-config-manager                1.0-170331.141119
clearwater-diags-monitor                 1.0-170327.103718
clearwater-etcd                          1.0-170331.141119
clearwater-infrastructure                1.0-170327.103718
clearwater-log-cleanup                   1.0-170327.103718
clearwater-management                    1.0-170331.141119
clearwater-memcached                     1.0-170327.103718
clearwater-monit                         5.18-170403.124108
clearwater-nginx                         1.0-170223.132518
clearwater-queue-manager                 1.0-170331.141119
clearwater-snmp-handler-astaire          1.0-170331.103056
clearwater-snmpd                         1.0-170327.103718
clearwater-socket-factory                1.0-170327.103718
clearwater-tcp-scalability               1.0-170327.103718
memcached                                1.6.00-0clearwater0.5
sprout                                   1.0-170403.142633
sprout-base                              1.0-170403.142633
sprout-bgcf                              1.0-170403.142633
sprout-icscf                             1.0-170403.142633
sprout-mmtel-as                          1.0-170403.142633
sprout-node                              1.0-170403.142633
sprout-scscf                             1.0-170403.142633





::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170412/b7da41e7/attachment.html>

From Andrew.Edmonds at metaswitch.com  Wed Apr 12 11:47:03 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Wed, 12 Apr 2017 15:47:03 +0000
Subject: [Project Clearwater] Sprout failed: Error: No local site
 registration store specified
In-Reply-To: <HK2PR0401MB1954ABE9136D0FEA2CED0BE79F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>
References: <HK2PR0401MB19543327896D0B1A72AF1D1A9F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>
	<HK2PR0401MB1954ABE9136D0FEA2CED0BE79F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>
Message-ID: <BLUPR02MB437C573C2DB0D6DF37370F3E5030@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Silvestro,

Thank you for your question and the information you have provided.

You were right to point out the error "Error main.cpp:1678: No local site registration store specified". This will be the cause of your Sprout process failing to start. In order to specify a local site registration store you need a sprout_registration_store option set in /etc/clearwater/shared_config, you can see here<https://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html> for more details.

Could you try the following steps:


?         add the line "sprout_registration_store=sprout.<domain name>" to /etc/clearwater/shared_config

?         run "sudo cw-upload_shared_config"

?         run "sudo monit summary"

and see if the Sprout process is able to start.

I believe the issue was caused by using a Heat template which is out-of-date in comparison to your Clearwater version. If you re-deploy using the latest stable version of the Heat template<https://github.com/Metaswitch/clearwater-heat/tree/stable> available on Github you should find that this is no longer an issue.

Let me know if this helps.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Silvestro Ciampoli
Sent: Wednesday, April 12, 2017 11:18 AM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Sprout failed: Error: No local site registration store specified




Hi,



I'm sorry I missed to specify that to deploy Clearwater in Openstack I used the Heat template available on github. The configuration foreseen the separation between management and signaling network.





BR,

Silvestro Ciampoli

________________________________
Da: Silvestro Ciampoli
Inviato: mercoled? 12 aprile 2017 11:02
A: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Oggetto: Sprout failed: Error: No local site registration store specified




Hi Clearwater Team,



I'm trying to (re-)install Clearwater on Openstack (6 VMs) but I'm encountering an issue on the Sprout node. It was failing with the following error : "Error main.cpp:1678: No local site registration store specified". Please see the .log in attachment for further details. Do you have any suggestions?



Below the status of other processes and the version information.





Thanks in advance,



Silvestro Ciampoli



------------------------------------


[sprout]ubuntu at 0:~$ sudo monit summary
Monit 5.18.1 uptime: 1h 0m
 Service Name                     Status                      Type
 node-0.sprout.hcllab.it          Running                     System
 sprout_process                   Execution failed | Does...  Process
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 sprout_uptime                    Wait parent                 Program
 poll_sprout_sip                  Wait parent                 Program
 poll_sprout_http                 Wait parent                 Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Progra




/usr/share/clearwater/bin/clearwater-version
astaire                                  1.0-170331.102803
chronos                                  1.0-170331.102347
clearwater-cluster-manager               1.0-170331.141119
clearwater-config-manager                1.0-170331.141119
clearwater-diags-monitor                 1.0-170327.103718
clearwater-etcd                          1.0-170331.141119
clearwater-infrastructure                1.0-170327.103718
clearwater-log-cleanup                   1.0-170327.103718
clearwater-management                    1.0-170331.141119
clearwater-memcached                     1.0-170327.103718
clearwater-monit                         5.18-170403.124108
clearwater-nginx                         1.0-170223.132518
clearwater-queue-manager                 1.0-170331.141119
clearwater-snmp-handler-astaire          1.0-170331.103056
clearwater-snmpd                         1.0-170327.103718
clearwater-socket-factory                1.0-170327.103718
clearwater-tcp-scalability               1.0-170327.103718
memcached                                1.6.00-0clearwater0.5
sprout                                   1.0-170403.142633
sprout-base                              1.0-170403.142633
sprout-bgcf                              1.0-170403.142633
sprout-icscf                             1.0-170403.142633
sprout-mmtel-as                          1.0-170403.142633
sprout-node                              1.0-170403.142633
sprout-scscf                             1.0-170403.142633







::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170412/27c7ef12/attachment.html>

From Muhammad.Shaikh at huawei.com  Wed Apr 12 13:32:52 2017
From: Muhammad.Shaikh at huawei.com (Muhammad Shaikh (Salman))
Date: Wed, 12 Apr 2017 17:32:52 +0000
Subject: [Project Clearwater] Homestead after re-install is not part of the
 etcd cluster and fail to start.
Message-ID: <BE94A67F9CCBC0409944CFAEE0FE797F013C6679@SJCEML701-CHM.china.huawei.com>


I have re-installed the homestead node from scratch and after that the etcd cluster is no longer showing the homestead part of it's list.

I have issued following command on ellis and see homestead is missing:

sudo clearwater-etcdctl member list
176d77d99abd6922: name=192-168-0-32 peerURLs=http://192.168.0.32:2380 clientURLs=http://192.168.0.32:4000
2c677e440fa18894: name=192-168-0-33 peerURLs=http://192.168.0.33:2380 clientURLs=http://192.168.0.33:4000
40ae69e640183094: name=192-168-0-34 peerURLs=http://192.168.0.34:2380 clientURLs=http://192.168.0.34:4000
722329d972aa3204: name=192-168-0-37 peerURLs=http://192.168.0.37:2380 clientURLs=http://192.168.0.37:4000
a9a4e28f06b02bd6: name=192-168-0-35 peerURLs=http://192.168.0.35:2380 clientURLs=http://192.168.0.35:4000

The local_config file do have the etcd cluster with "192.168.0.36", but it is missing from the member list.

[ellis]ubuntu at ellis:~$ cat /etc/clearwater/local_config
local_ip=192.168.0.32
public_ip=10.145.253.142
public_hostname=10.145.253.142
etcd_cluster="192.168.0.32,192.168.0.33,192.168.0.34,192.168.0.35,192.168.0.36,192.168.0.37"

When I logged in on homestead I also noticed that  homestead process is not started:

[homestead]ubuntu at homestead:~$ netstat -an | grep LIST
tcp        0      0 0.0.0.0:53              0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:2812          0.0.0.0:*               LISTEN
tcp6       0      0 :::80                   :::*                    LISTEN
tcp6       0      0 :::53                   :::*                    LISTEN
tcp6       0      0 :::22                   :::*                    LISTEN
unix  2      [ ACC ]     STREAM     LISTENING     9138     /tmp/clearwater_signaling_namespace_socket
unix  2      [ ACC ]     SEQPACKET  LISTENING     7222     /run/udev/control
unix  2      [ ACC ]     STREAM     LISTENING     8766     /var/run/dbus/system_bus_socket
unix  2      [ ACC ]     STREAM     LISTENING     6525     @/com/ubuntu/upstart
unix  2      [ ACC ]     STREAM     LISTENING     9164     /var/run/acpid.socket
unix  2      [ ACC ]     STREAM     LISTENING     9091     /tmp/clearwater_management_namespace_socket
unix  2      [ ACC ]     STREAM     LISTENING     9726     /var/agentx/master

It looks like that Casandra is failing to start with the error:

12-04-2017 17:30:02.492 UTC Status load_monitor.cpp:105: Constructing LoadMonitor
12-04-2017 17:30:02.492 UTC Status load_monitor.cpp:106:    Target latency (usecs)   : 100000
12-04-2017 17:30:02.492 UTC Status load_monitor.cpp:107:    Max bucket size          : 1000
12-04-2017 17:30:02.492 UTC Status load_monitor.cpp:108:    Initial token fill rate/s: 100.000000
12-04-2017 17:30:02.492 UTC Status load_monitor.cpp:109:    Min token fill rate/s    : 10.000000
12-04-2017 17:30:02.492 UTC Status dnscachedresolver.cpp:150: Creating Cached Resolver using servers:
12-04-2017 17:30:02.492 UTC Status dnscachedresolver.cpp:160:     127.0.0.1
12-04-2017 17:30:02.492 UTC Status a_record_resolver.cpp:54: Created ARecordResolver
12-04-2017 17:30:02.492 UTC Status a_record_resolver.cpp:54: Created ARecordResolver
12-04-2017 17:30:02.492 UTC Status cassandra_store.cpp:181: Configuring store connection
12-04-2017 17:30:02.492 UTC Status cassandra_store.cpp:182:   Hostname:  127.0.0.1
12-04-2017 17:30:02.492 UTC Status cassandra_store.cpp:183:   Port:      9160
12-04-2017 17:30:02.492 UTC Status cassandra_store.cpp:211: Configuring store worker pool
12-04-2017 17:30:02.492 UTC Status cassandra_store.cpp:212:   Threads:   10
12-04-2017 17:30:02.492 UTC Status cassandra_store.cpp:213:   Max Queue: 0
12-04-2017 17:30:02.493 UTC Error main.cpp:756: Failed to initialize the Cassandra cache with error code 3.
12-04-2017 17:30:02.493 UTC Status main.cpp:757: Homestead is shutting down

[homestead]ubuntu at homestead:/var/log/homestead$ sudo monit summary
Monit 5.18.1 uptime: 16h 5m
Service Name                     Status                      Type
 node-homestead                   Running                     System
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Execution failed | Does...  Process
 homestead-prov_process           Execution failed | Does...  Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Execution failed | Does...  Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 cassandra_process                Running                     Process
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Wait parent                 Program
 poll_homestead                   Wait parent                 Program
 check_cx_health                  Wait parent                 Program
 poll_homestead-prov              Wait parent                 Program
 clearwater_queue_manager_uptime  Status failed               Program
 etcd_uptime                      Wait parent                 Program
 poll_etcd_cluster                Wait parent                 Program
 poll_etcd                        Wait parent                 Program
 cassandra_uptime                 Status failed               Program
 poll_cassandra                   Status ok                   Program
 poll_cqlsh                       Status ok                   Program

In homestead_err log I also noticed the following error:

Thrift: Wed Apr 12 17:09:25 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:09:25 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:11:09 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:11:09 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:12:51 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:12:51 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:14:35 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:14:35 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:16:17 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:16:17 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:18:00 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:18:00 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:19:44 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:19:44 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:21:27 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:21:27 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:23:09 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:23:09 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:24:52 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:24:52 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:26:35 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:26:35 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:28:19 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:28:19 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:30:02 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:30:02 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:31:45 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused
Thrift: Wed Apr 12 17:31:45 2017 TSocket::open() error on socket (after THRIFT_POLL) <Host: 127.0.0.1 Port: 9160>Connection refused


----------------------------------------------------------
Salman Shaikh
NFV Open Lab Architect
Huawei R&D USA
FutureWei Technologies, Inc
Email: Muhammad.Shaikh at huawei.com<mailto:Muhammad.Shaikh at huawei.com>
Mobile: +1 (408) 421-6210
----------------------------------------------------------

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170412/8ac8c70a/attachment.html>

From jsepulveda at ipnexia.com  Tue Apr 18 04:02:29 2017
From: jsepulveda at ipnexia.com (Juan Sepulveda)
Date: Tue, 18 Apr 2017 08:02:29 +0000
Subject: [Project Clearwater] Sprout scratch
Message-ID: <105db6c500cd408fb1c9fdf5dec68c0b@exchange2013.toledo.be>

Hi,

Since Friday for an unknown reason, the sprout start to scratch permanently. We can see in the log internal loop and the poll_sprout_sip process restarting.

Logs are in attached file. Please can you debug this issue?

The only actions we performed last Friday is to fix a etcd process failure and we add 2 lines in the shared_config file:

billing_realm=ims02.as20650.net
cdf_identity=192.169.38.56

Currently, those 2 lines are commented.
In the attached file, you have also the version and the monit command output.

Let me know if you need more info.

Thx

Kr

Juan


IP Nexia, a new brand of Toledo Telecom.
[Description: http://www.ipnexia.com/images/pix.gif]

Juan Sepulveda
Voip and Network Engineer
M + 32 478 97 98 79   T +32 2 600 16 69   @ jsepulveda at ipnexia.com<mailto:jsepulveda at ipnexia.com>



IP Nexia Kouterveldstraat, 2 - 1831 Diegem
Tel +32 2 648 08 48   Fax +32 2 646 44 24   www.ipnexia.com<http://www.ipnexia.com/>
Disclaimer<http://www.ipnexia.com/disclaimer/disclaimer.html>

[Description: http://www.ipnexia.com/images/back-sign2_02.jpg]

[Description: http://www.ipnexia.com/images/back-sign2_03.jpg]



> Please consider the environmental impact of needlessly printing this e-mail.



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170418/992cb425/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 171 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170418/992cb425/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 4448 bytes
Desc: image002.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170418/992cb425/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.jpg
Type: image/jpeg
Size: 2618 bytes
Desc: image003.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170418/992cb425/attachment-0001.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image004.png
Type: image/png
Size: 170 bytes
Desc: image004.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170418/992cb425/attachment-0001.png>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sprout log 180417 - scratch sprout.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170418/992cb425/attachment.txt>

From silvestro.ciampoli at hcl.com  Thu Apr 13 07:14:28 2017
From: silvestro.ciampoli at hcl.com (Silvestro Ciampoli)
Date: Thu, 13 Apr 2017 11:14:28 +0000
Subject: [Project Clearwater] Sprout failed: Error: No local site
 registration store specified
In-Reply-To: <BLUPR02MB437C573C2DB0D6DF37370F3E5030@BLUPR02MB437.namprd02.prod.outlook.com>
References: <HK2PR0401MB19543327896D0B1A72AF1D1A9F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>
	<HK2PR0401MB1954ABE9136D0FEA2CED0BE79F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>,
	<BLUPR02MB437C573C2DB0D6DF37370F3E5030@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <HK2PR0401MB1954FCCD783D94B457FF706E9F020@HK2PR0401MB1954.apcprd04.prod.outlook.com>

Hi Andrew,


thanks a lot for your suggestion. Now it seems that all nodes have started correctly. The problem that I'm encountering with this deploy now is that I'm not able to perform a call by Zoiper anymore. I'm able to register correctly by Zoiper two clients (running on two different ubuntu desktop) but the call was failing.


In attachment you can find a .cap with registration and call_tentative gotten on one client interface. I attached also bono and sprout log.


Please let me know if you have any suggestions.


Thanks in advance,

Silvestro Ciampoli

________________________________
Da: Clearwater <clearwater-bounces at lists.projectclearwater.org> per conto di Andrew Edmonds <Andrew.Edmonds at metaswitch.com>
Inviato: mercoled? 12 aprile 2017 17:47:03
A: clearwater at lists.projectclearwater.org
Oggetto: Re: [Project Clearwater] Sprout failed: Error: No local site registration store specified

Hi Silvestro,

Thank you for your question and the information you have provided.

You were right to point out the error "Error main.cpp:1678: No local site registration store specified". This will be the cause of your Sprout process failing to start. In order to specify a local site registration store you need a sprout_registration_store option set in /etc/clearwater/shared_config, you can see here<https://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html> for more details.

Could you try the following steps:


?         add the line ?sprout_registration_store=sprout.<domain name>? to /etc/clearwater/shared_config

?         run ?sudo cw-upload_shared_config?

?         run ?sudo monit summary?

and see if the Sprout process is able to start.

I believe the issue was caused by using a Heat template which is out-of-date in comparison to your Clearwater version. If you re-deploy using the latest stable version of the Heat template<https://github.com/Metaswitch/clearwater-heat/tree/stable> available on Github you should find that this is no longer an issue.

Let me know if this helps.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Silvestro Ciampoli
Sent: Wednesday, April 12, 2017 11:18 AM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Sprout failed: Error: No local site registration store specified




Hi,



I'm sorry I missed to specify that to deploy Clearwater in Openstack I used the Heat template available on github. The configuration foreseen the separation between management and signaling network.





BR,

Silvestro Ciampoli

________________________________
Da: Silvestro Ciampoli
Inviato: mercoled? 12 aprile 2017 11:02
A: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Oggetto: Sprout failed: Error: No local site registration store specified




Hi Clearwater Team,



I'm trying to (re-)install Clearwater on Openstack (6 VMs) but I'm encountering an issue on the Sprout node. It was failing with the following error : "Error main.cpp:1678: No local site registration store specified". Please see the .log in attachment for further details. Do you have any suggestions?



Below the status of other processes and the version information.





Thanks in advance,



Silvestro Ciampoli



------------------------------------


[sprout]ubuntu at 0:~$ sudo monit summary
Monit 5.18.1 uptime: 1h 0m
 Service Name                     Status                      Type
 node-0.sprout.hcllab.it          Running                     System
 sprout_process                   Execution failed | Does...  Process
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 sprout_uptime                    Wait parent                 Program
 poll_sprout_sip                  Wait parent                 Program
 poll_sprout_http                 Wait parent                 Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Progra




/usr/share/clearwater/bin/clearwater-version
astaire                                  1.0-170331.102803
chronos                                  1.0-170331.102347
clearwater-cluster-manager               1.0-170331.141119
clearwater-config-manager                1.0-170331.141119
clearwater-diags-monitor                 1.0-170327.103718
clearwater-etcd                          1.0-170331.141119
clearwater-infrastructure                1.0-170327.103718
clearwater-log-cleanup                   1.0-170327.103718
clearwater-management                    1.0-170331.141119
clearwater-memcached                     1.0-170327.103718
clearwater-monit                         5.18-170403.124108
clearwater-nginx                         1.0-170223.132518
clearwater-queue-manager                 1.0-170331.141119
clearwater-snmp-handler-astaire          1.0-170331.103056
clearwater-snmpd                         1.0-170327.103718
clearwater-socket-factory                1.0-170327.103718
clearwater-tcp-scalability               1.0-170327.103718
memcached                                1.6.00-0clearwater0.5
sprout                                   1.0-170403.142633
sprout-base                              1.0-170403.142633
sprout-bgcf                              1.0-170403.142633
sprout-icscf                             1.0-170403.142633
sprout-mmtel-as                          1.0-170403.142633
sprout-node                              1.0-170403.142633
sprout-scscf                             1.0-170403.142633







::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170413/4a4bb564/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: log.tgz
Type: application/x-compressed-tar
Size: 528010 bytes
Desc: log.tgz
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170413/4a4bb564/attachment.bin>

From Andrew.Edmonds at metaswitch.com  Tue Apr 18 10:28:16 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Tue, 18 Apr 2017 14:28:16 +0000
Subject: [Project Clearwater] Sprout failed: Error: No local site
 registration store specified
In-Reply-To: <HK2PR0401MB1954FCCD783D94B457FF706E9F020@HK2PR0401MB1954.apcprd04.prod.outlook.com>
References: <HK2PR0401MB19543327896D0B1A72AF1D1A9F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>
	<HK2PR0401MB1954ABE9136D0FEA2CED0BE79F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>,
	<BLUPR02MB437C573C2DB0D6DF37370F3E5030@BLUPR02MB437.namprd02.prod.outlook.com>
	<HK2PR0401MB1954FCCD783D94B457FF706E9F020@HK2PR0401MB1954.apcprd04.prod.outlook.com>
Message-ID: <BL2PR02MB433467056088123212421F3E5190@BL2PR02MB433.namprd02.prod.outlook.com>

Hi Silvestro,

Thank you for your continued support of Clearwater, I am glad to hear you have all your nodes starting correctly now.

I have taken a look through the diagnostics you have provided to try and see why your Zoiper calls are not working correctly.

One thing you can do with the .cap trace you have sent is search for "sip", this allows you to view all the SIP traffic flowing through your deployment. If I do this you can see a REGISTER at 10:35:23.2997597 that does not receive any response. This indicates to me that there might be an issue with your Sprout node processing registrations, however the Sprout logs you have attached do not cover this time period and I can't see evidence of any calls being attempted in any of the diagnostics you've send me.

Could you please collect another .cap trace on your Sprout node (and let me know the IP address of this node). As this trace is running could you please do the following, keeping a note of the time as you do so:


?         Register two subscribers on your Zoiper clients

?         Attempt to make a call between them

Then could you send me that trace and the Sprout logs from the same time period.

Thanks,

Andrew


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Silvestro Ciampoli
Sent: Thursday, April 13, 2017 12:14 PM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Sprout failed: Error: No local site registration store specified


Hi Andrew,



thanks a lot for your suggestion. Now it seems that all nodes have started correctly. The problem that I'm encountering with this deploy now is that I'm not able to perform a call by Zoiper anymore. I'm able to register correctly by Zoiper two clients (running on two different ubuntu desktop) but the call was failing.



In attachment you can find a .cap with registration and call_tentative gotten on one client interface. I attached also bono and sprout log.



Please let me know if you have any suggestions.



Thanks in advance,

Silvestro Ciampoli

________________________________
Da: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> per conto di Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>>
Inviato: mercoled? 12 aprile 2017 17:47:03
A: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Oggetto: Re: [Project Clearwater] Sprout failed: Error: No local site registration store specified

Hi Silvestro,

Thank you for your question and the information you have provided.

You were right to point out the error "Error main.cpp:1678: No local site registration store specified". This will be the cause of your Sprout process failing to start. In order to specify a local site registration store you need a sprout_registration_store option set in /etc/clearwater/shared_config, you can see here<https://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html> for more details.

Could you try the following steps:


?         add the line "sprout_registration_store=sprout.<domain name>" to /etc/clearwater/shared_config

?         run "sudo cw-upload_shared_config"

?         run "sudo monit summary"

and see if the Sprout process is able to start.

I believe the issue was caused by using a Heat template which is out-of-date in comparison to your Clearwater version. If you re-deploy using the latest stable version of the Heat template<https://github.com/Metaswitch/clearwater-heat/tree/stable> available on Github you should find that this is no longer an issue.

Let me know if this helps.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Silvestro Ciampoli
Sent: Wednesday, April 12, 2017 11:18 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Sprout failed: Error: No local site registration store specified




Hi,



I'm sorry I missed to specify that to deploy Clearwater in Openstack I used the Heat template available on github. The configuration foreseen the separation between management and signaling network.





BR,

Silvestro Ciampoli

________________________________
Da: Silvestro Ciampoli
Inviato: mercoled? 12 aprile 2017 11:02
A: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Oggetto: Sprout failed: Error: No local site registration store specified




Hi Clearwater Team,



I'm trying to (re-)install Clearwater on Openstack (6 VMs) but I'm encountering an issue on the Sprout node. It was failing with the following error : "Error main.cpp:1678: No local site registration store specified". Please see the .log in attachment for further details. Do you have any suggestions?



Below the status of other processes and the version information.





Thanks in advance,



Silvestro Ciampoli



------------------------------------


[sprout]ubuntu at 0:~$ sudo monit summary
Monit 5.18.1 uptime: 1h 0m
 Service Name                     Status                      Type
 node-0.sprout.hcllab.it          Running                     System
 sprout_process                   Execution failed | Does...  Process
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 sprout_uptime                    Wait parent                 Program
 poll_sprout_sip                  Wait parent                 Program
 poll_sprout_http                 Wait parent                 Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Progra




/usr/share/clearwater/bin/clearwater-version
astaire                                  1.0-170331.102803
chronos                                  1.0-170331.102347
clearwater-cluster-manager               1.0-170331.141119
clearwater-config-manager                1.0-170331.141119
clearwater-diags-monitor                 1.0-170327.103718
clearwater-etcd                          1.0-170331.141119
clearwater-infrastructure                1.0-170327.103718
clearwater-log-cleanup                   1.0-170327.103718
clearwater-management                    1.0-170331.141119
clearwater-memcached                     1.0-170327.103718
clearwater-monit                         5.18-170403.124108
clearwater-nginx                         1.0-170223.132518
clearwater-queue-manager                 1.0-170331.141119
clearwater-snmp-handler-astaire          1.0-170331.103056
clearwater-snmpd                         1.0-170327.103718
clearwater-socket-factory                1.0-170327.103718
clearwater-tcp-scalability               1.0-170327.103718
memcached                                1.6.00-0clearwater0.5
sprout                                   1.0-170403.142633
sprout-base                              1.0-170403.142633
sprout-bgcf                              1.0-170403.142633
sprout-icscf                             1.0-170403.142633
sprout-mmtel-as                          1.0-170403.142633
sprout-node                              1.0-170403.142633
sprout-scscf                             1.0-170403.142633







::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170418/af441220/attachment.html>

From chessmancaryl at gmail.com  Wed Apr 19 05:32:30 2017
From: chessmancaryl at gmail.com (chess man)
Date: Wed, 19 Apr 2017 10:32:30 +0100
Subject: [Project Clearwater] Sprout processing time
Message-ID: <CAB9Pg_qBTPmzF+hx4WmepV7Vt5Nhx34Vn4Jvb0SB1k=Rgjxf4A@mail.gmail.com>

Hi clearwater team,

Can you provide me with the maximum processing time allowed for Sprout.


Many thanks,
Chess

<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Garanti
sans virus. www.avast.com
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170419/b42bdf46/attachment.html>

From jsepulveda at ipnexia.com  Fri Apr 21 07:35:37 2017
From: jsepulveda at ipnexia.com (Juan Sepulveda)
Date: Fri, 21 Apr 2017 11:35:37 +0000
Subject: [Project Clearwater] Sprout scratch
In-Reply-To: <105db6c500cd408fb1c9fdf5dec68c0b@exchange2013.toledo.be>
References: <105db6c500cd408fb1c9fdf5dec68c0b@exchange2013.toledo.be>
Message-ID: <75a22d33cf6042cca66135225b65950b@exchange2013.toledo.be>

Hi,

Could you come back to us regarding this problem?

Is it a known  issue?

Thx

Kr

Juan


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juan Sepulveda
Sent: mardi 18 avril 2017 10:02
To: 'clearwater at lists.projectclearwater.org'
Subject: [Project Clearwater] Sprout scratch

Hi,

Since Friday for an unknown reason, the sprout start to scratch permanently. We can see in the log internal loop and the poll_sprout_sip process restarting.

Logs are in attached file. Please can you debug this issue?

The only actions we performed last Friday is to fix a etcd process failure and we add 2 lines in the shared_config file:

billing_realm=ims02.as20650.net
cdf_identity=192.169.38.56

Currently, those 2 lines are commented.
In the attached file, you have also the version and the monit command output.

Let me know if you need more info.

Thx

Kr

Juan


IP Nexia, a new brand of Toledo Telecom.
[Description: http://www.ipnexia.com/images/pix.gif]

Juan Sepulveda
Voip and Network Engineer
M + 32 478 97 98 79   T +32 2 600 16 69   @ jsepulveda at ipnexia.com<mailto:jsepulveda at ipnexia.com>



IP Nexia Kouterveldstraat, 2 - 1831 Diegem
Tel +32 2 648 08 48   Fax +32 2 646 44 24   www.ipnexia.com<http://www.ipnexia.com/>
Disclaimer<http://www.ipnexia.com/disclaimer/disclaimer.html>

[Description: http://www.ipnexia.com/images/back-sign2_02.jpg]

[Description: http://www.ipnexia.com/images/back-sign2_03.jpg]



> Please consider the environmental impact of needlessly printing this e-mail.



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170421/4ee1e2af/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 171 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170421/4ee1e2af/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 4448 bytes
Desc: image002.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170421/4ee1e2af/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.jpg
Type: image/jpeg
Size: 2618 bytes
Desc: image003.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170421/4ee1e2af/attachment-0001.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image004.png
Type: image/png
Size: 170 bytes
Desc: image004.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170421/4ee1e2af/attachment-0001.png>

From Adam.Lindley at metaswitch.com  Fri Apr 21 09:08:37 2017
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Fri, 21 Apr 2017 13:08:37 +0000
Subject: [Project Clearwater] Release note for sprint Durin
Message-ID: <BN6PR02MB24669AA88C6977108924F94EE21A0@BN6PR02MB2466.namprd02.prod.outlook.com>

The release for Project Clearwater sprint "Durin" has been cut. The code for this release is tagged as release-121 in GitHub.

We have added a new feature, giving Project Clearwater the ability to support Wildcarded Public Identities.
This will allow Clearwater to work alongside an HSS which uses Wildcarded Public Identities, giving the benefit that service can be provided for multiple IMPUs without having to provision each identity explicitly in the HSS.

We have also made some minor tweaks and fixes.

To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.

Adam
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170421/b8db1b16/attachment.html>

From silvestro.ciampoli at hcl.com  Fri Apr 21 11:29:18 2017
From: silvestro.ciampoli at hcl.com (Silvestro Ciampoli)
Date: Fri, 21 Apr 2017 15:29:18 +0000
Subject: [Project Clearwater] Sprout failed: Error: No local site
 registration store specified
In-Reply-To: <HK2PR0401MB19547F81BD984D8AF42E4CE59F180@HK2PR0401MB1954.apcprd04.prod.outlook.com>
References: <HK2PR0401MB19543327896D0B1A72AF1D1A9F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>
	<HK2PR0401MB1954ABE9136D0FEA2CED0BE79F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>,
	<BLUPR02MB437C573C2DB0D6DF37370F3E5030@BLUPR02MB437.namprd02.prod.outlook.com>
	<HK2PR0401MB1954FCCD783D94B457FF706E9F020@HK2PR0401MB1954.apcprd04.prod.outlook.com>,
	<BL2PR02MB433467056088123212421F3E5190@BL2PR02MB433.namprd02.prod.outlook.com>,
	<HK2PR0401MB19547F81BD984D8AF42E4CE59F180@HK2PR0401MB1954.apcprd04.prod.outlook.com>
Message-ID: <HK2PR0401MB1954C3F7D32A02882CD457C79F1A0@HK2PR0401MB1954.apcprd04.prod.outlook.com>

Hi Andrew,

Eventually I have found out the problem. It was a mtu issue on signaling net. I'm now able to continue my testing activity on our setup.

Thanks a lot for your support!

BR,
Silvestro
________________________________
Da: Silvestro Ciampoli
Inviato: gioved? 20 aprile 2017 10:52:11
A: clearwater at lists.projectclearwater.org
Oggetto: Re: Sprout failed: Error: No local site registration store specified


Hi Andrew,


thank a lot for your help.


To ease (hopefully)your investigation I preferred to re-run the live test (just basic_call ones) suggested on clearwater site. All the tests, expect the registration steps failed. On sprout log I can see TRANSPORT_ERROR messages. Do you have any idea regarding the reason I'm observing these kind of message?

Please in attachment you can find the .pcap, log file of sprout and bono nodes and the output of the live test.


Here below the steps performed:

- 08:02:13: sudo service bono stop

- 08:02:31: sudo service sprout stop

- from an ubuntu desktop (10.0.0.230): rake test[example.com] SIGNUP_CODE=secret (it includes only basic call, registration and cancel tests)



Please below you can find the list of the nodes running on Openstack.



Thanks and regards,


Silvestro


-------------------------------------------------------


|bono-0.example.com | ACTIVE | - | Running | clearwater-private-signaling=192.168.1.6, 10.0.110.128; clearwater-private-management=192.168.0.7, 10.0.110.137 |


|ellis-0.example.com | ACTIVE | - | Running | clearwater-private-management=192.168.0.4, 10.0.110.193 |


| homer-0.example.com | ACTIVE | - | Running | clearwater-private-signaling=192.168.1.7, 10.0.110.130; clearwater-private-management=192.168.0.8, 10.0.110.139 |


| homestead-0.example.com | ACTIVE | - | Running | clearwater-private-signaling=192.168.1.5; clearwater-private-management=192.168.0.6, 10.0.110.198 |


| ns.example.com | ACTIVE | - | Running | clearwater-private-signaling=192.168.1.3; clearwater-private-management=192.168.0.3, 10.0.110.192 |


| ralf-0.example.com | ACTIVE | - | Running | clearwater-private-signaling=192.168.1.4; clearwater-private-management=192.168.0.5, 10.0.110.197 |


| sprout-0.example.com | ACTIVE | - | Running | clearwater-private-signaling=192.168.1.8; clearwater-private-management=192.168.0.9, 10.0.110.135 |





________________________________
Da: Clearwater <clearwater-bounces at lists.projectclearwater.org> per conto di Andrew Edmonds <Andrew.Edmonds at metaswitch.com>
Inviato: marted? 18 aprile 2017 16:28:16
A: clearwater at lists.projectclearwater.org
Oggetto: Re: [Project Clearwater] Sprout failed: Error: No local site registration store specified


Hi Silvestro,



Thank you for your continued support of Clearwater, I am glad to hear you have all your nodes starting correctly now.



I have taken a look through the diagnostics you have provided to try and see why your Zoiper calls are not working correctly.



One thing you can do with the .cap trace you have sent is search for ?sip?, this allows you to view all the SIP traffic flowing through your deployment. If I do this you can see a REGISTER at 10:35:23.2997597 that does not receive any response. This indicates to me that there might be an issue with your Sprout node processing registrations, however the Sprout logs you have attached do not cover this time period and I can?t see evidence of any calls being attempted in any of the diagnostics you?ve send me.



Could you please collect another .cap trace on your Sprout node (and let me know the IP address of this node). As this trace is running could you please do the following, keeping a note of the time as you do so:



?         Register two subscribers on your Zoiper clients

?         Attempt to make a call between them



Then could you send me that trace and the Sprout logs from the same time period.



Thanks,



Andrew





From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Silvestro Ciampoli
Sent: Thursday, April 13, 2017 12:14 PM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Sprout failed: Error: No local site registration store specified



Hi Andrew,



thanks a lot for your suggestion. Now it seems that all nodes have started correctly. The problem that I'm encountering with this deploy now is that I'm not able to perform a call by Zoiper anymore. I'm able to register correctly by Zoiper two clients (running on two different ubuntu desktop) but the call was failing.



In attachment you can find a .cap with registration and call_tentative gotten on one client interface. I attached also bono and sprout log.



Please let me know if you have any suggestions.



Thanks in advance,

Silvestro Ciampoli

________________________________

Da: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> per conto di Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>>
Inviato: mercoled? 12 aprile 2017 17:47:03
A: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Oggetto: Re: [Project Clearwater] Sprout failed: Error: No local site registration store specified



Hi Silvestro,



Thank you for your question and the information you have provided.



You were right to point out the error "Error main.cpp:1678: No local site registration store specified". This will be the cause of your Sprout process failing to start. In order to specify a local site registration store you need a sprout_registration_store option set in /etc/clearwater/shared_config, you can see here<https://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html> for more details.



Could you try the following steps:



?         add the line ?sprout_registration_store=sprout.<domain name>? to /etc/clearwater/shared_config

?         run ?sudo cw-upload_shared_config?

?         run ?sudo monit summary?



and see if the Sprout process is able to start.



I believe the issue was caused by using a Heat template which is out-of-date in comparison to your Clearwater version. If you re-deploy using the latest stable version of the Heat template<https://github.com/Metaswitch/clearwater-heat/tree/stable> available on Github you should find that this is no longer an issue.



Let me know if this helps.



Thanks,



Andrew



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Silvestro Ciampoli
Sent: Wednesday, April 12, 2017 11:18 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Sprout failed: Error: No local site registration store specified





Hi,



I'm sorry I missed to specify that to deploy Clearwater in Openstack I used the Heat template available on github. The configuration foreseen the separation between management and signaling network.





BR,

Silvestro Ciampoli



________________________________

Da: Silvestro Ciampoli
Inviato: mercoled? 12 aprile 2017 11:02
A: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Oggetto: Sprout failed: Error: No local site registration store specified





Hi Clearwater Team,



I'm trying to (re-)install Clearwater on Openstack (6 VMs) but I'm encountering an issue on the Sprout node. It was failing with the following error : "Error main.cpp:1678: No local site registration store specified". Please see the .log in attachment for further details. Do you have any suggestions?



Below the status of other processes and the version information.





Thanks in advance,



Silvestro Ciampoli



------------------------------------



[sprout]ubuntu at 0:~$ sudo monit summary
Monit 5.18.1 uptime: 1h 0m
 Service Name                     Status                      Type
 node-0.sprout.hcllab.it          Running                     System
 sprout_process                   Execution failed | Does...  Process
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 sprout_uptime                    Wait parent                 Program
 poll_sprout_sip                  Wait parent                 Program
 poll_sprout_http                 Wait parent                 Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Progra





/usr/share/clearwater/bin/clearwater-version

astaire                                  1.0-170331.102803
chronos                                  1.0-170331.102347
clearwater-cluster-manager               1.0-170331.141119
clearwater-config-manager                1.0-170331.141119
clearwater-diags-monitor                 1.0-170327.103718
clearwater-etcd                          1.0-170331.141119
clearwater-infrastructure                1.0-170327.103718
clearwater-log-cleanup                   1.0-170327.103718
clearwater-management                    1.0-170331.141119
clearwater-memcached                     1.0-170327.103718
clearwater-monit                         5.18-170403.124108
clearwater-nginx                         1.0-170223.132518
clearwater-queue-manager                 1.0-170331.141119
clearwater-snmp-handler-astaire          1.0-170331.103056
clearwater-snmpd                         1.0-170327.103718
clearwater-socket-factory                1.0-170327.103718
clearwater-tcp-scalability               1.0-170327.103718
memcached                                1.6.00-0clearwater0.5
sprout                                   1.0-170403.142633
sprout-base                              1.0-170403.142633
sprout-bgcf                              1.0-170403.142633
sprout-icscf                             1.0-170403.142633
sprout-mmtel-as                          1.0-170403.142633
sprout-node                              1.0-170403.142633
sprout-scscf                             1.0-170403.142633








::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170421/30506705/attachment.html>

From jsepulveda at ipnexia.com  Mon Apr 24 03:35:58 2017
From: jsepulveda at ipnexia.com (Juan Sepulveda)
Date: Mon, 24 Apr 2017 07:35:58 +0000
Subject: [Project Clearwater] Sprout scratch
In-Reply-To: <105db6c500cd408fb1c9fdf5dec68c0b@exchange2013.toledo.be>
References: <105db6c500cd408fb1c9fdf5dec68c0b@exchange2013.toledo.be>
Message-ID: <9a61709a8a784fab8c7466625d2cd141@exchange2013.toledo.be>

Hi,

please can you check why a loop is detected? We are in the max log level. No more info to explain the loop.

Please help us.


***********************************************************************************************************************************************
--start msg--

OPTIONS sip:poll-sip at 192.168.38.55:5054 SIP/2.0
Route: <sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=registrar>
Via: SIP/2.0/TCP 192.168.38.55;rport=34358;received=192.168.38.55;branch=z9hG4bK-319922
Max-Forwards: 1
To: <sip:poll-sip at 192.168.38.55>
From: "poll-sip" <sip:poll-sip at 192.168.38.55>;tag=319922
Call-ID: poll-sip-319922
CSeq: 319922 OPTIONS
Contact: <sip:192.168.38.55>
Accept: application/sdp
User-Agent: poll-sip
Content-Length:  0


--end msg--
24-04-2017 07:20:32.545 UTC Debug pjutils.cpp:741: Cloned tdta0x7f891800f080 to tdta0x7f8918013500
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1238: Remove top Route header Route: <sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=registrar>
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1813: Adding message 0x7f8918013b10 => txdata 0x7f89180135a8 mapping
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:1658: registrar-0x7f891800d180 pass initial request Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) to Sproutlet
24-04-2017 07:20:32.545 UTC Info registrarsproutlet.cpp:172: Registrar sproutlet received initial request
24-04-2017 07:20:32.545 UTC Debug uri_classifier.cpp:174: home domain: true, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
24-04-2017 07:20:32.545 UTC Debug uri_classifier.cpp:204: Classified URI as 4
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:384: Creating URI for service subscription
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:405: Constructed URI sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=subscription
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1409: Sproutlet send_request 0x7f8918013b10
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:1445: registrar-0x7f891800d180 sending Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1828: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1868: Processing request 0x7f89180135a8, fork = 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1992: registrar-0x7f891800d180 transmitting request on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:2006: registrar-0x7f891800d180 store reference to non-ACK request Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1820: Removing message 0x7f8918013b10 => txdata 0x7f89180135a8 mapping
24-04-2017 07:20:32.545 UTC Info sproutletproxy.cpp:818: Loop detected - rejecting request with 483 status code
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:2140: Routing Response msg 483/OPTIONS/cseq=319922 (tdta0x7f8918076070) (297 bytes) to upstream sproutlet registrar:
--start msg--

SIP/2.0 483 Too Many Hops
Via: SIP/2.0/TCP 192.168.38.55;rport=34358;received=192.168.38.55;branch=z9hG4bK-319922
Call-ID: poll-sip-319922
From: "poll-sip" <sip:poll-sip at 192.168.38.55>;tag=319922
To: <sip:poll-sip at 192.168.38.55>;tag=z9hG4bK-319922
CSeq: 319922 OPTIONS
Content-Length:  0


--end msg--



THx

Kr

Juan


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juan Sepulveda
Sent: mardi 18 avril 2017 10:02
To: 'clearwater at lists.projectclearwater.org'
Subject: [Project Clearwater] Sprout scratch

Hi,

Since Friday for an unknown reason, the sprout start to scratch permanently. We can see in the log internal loop and the poll_sprout_sip process restarting.

Logs are in attached file. Please can you debug this issue?

The only actions we performed last Friday is to fix a etcd process failure and we add 2 lines in the shared_config file:

billing_realm=ims02.as20650.net
cdf_identity=192.169.38.56

Currently, those 2 lines are commented.
In the attached file, you have also the version and the monit command output.

Let me know if you need more info.

Thx

Kr

Juan


IP Nexia, a new brand of Toledo Telecom.
[Description: http://www.ipnexia.com/images/pix.gif]

Juan Sepulveda
Voip and Network Engineer
M + 32 478 97 98 79   T +32 2 600 16 69   @ jsepulveda at ipnexia.com<mailto:jsepulveda at ipnexia.com>



IP Nexia Kouterveldstraat, 2 - 1831 Diegem
Tel +32 2 648 08 48   Fax +32 2 646 44 24   www.ipnexia.com<http://www.ipnexia.com/>
Disclaimer<http://www.ipnexia.com/disclaimer/disclaimer.html>

[Description: http://www.ipnexia.com/images/back-sign2_02.jpg]

[Description: http://www.ipnexia.com/images/back-sign2_03.jpg]



> Please consider the environmental impact of needlessly printing this e-mail.



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170424/30d3151a/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 171 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170424/30d3151a/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 4448 bytes
Desc: image002.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170424/30d3151a/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.jpg
Type: image/jpeg
Size: 2618 bytes
Desc: image003.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170424/30d3151a/attachment-0001.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image004.png
Type: image/png
Size: 170 bytes
Desc: image004.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170424/30d3151a/attachment-0001.png>

From silvestro.ciampoli at hcl.com  Thu Apr 20 04:52:11 2017
From: silvestro.ciampoli at hcl.com (Silvestro Ciampoli)
Date: Thu, 20 Apr 2017 08:52:11 +0000
Subject: [Project Clearwater] Sprout failed: Error: No local site
 registration store specified
In-Reply-To: <BL2PR02MB433467056088123212421F3E5190@BL2PR02MB433.namprd02.prod.outlook.com>
References: <HK2PR0401MB19543327896D0B1A72AF1D1A9F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>
	<HK2PR0401MB1954ABE9136D0FEA2CED0BE79F030@HK2PR0401MB1954.apcprd04.prod.outlook.com>,
	<BLUPR02MB437C573C2DB0D6DF37370F3E5030@BLUPR02MB437.namprd02.prod.outlook.com>
	<HK2PR0401MB1954FCCD783D94B457FF706E9F020@HK2PR0401MB1954.apcprd04.prod.outlook.com>,
	<BL2PR02MB433467056088123212421F3E5190@BL2PR02MB433.namprd02.prod.outlook.com>
Message-ID: <HK2PR0401MB19547F81BD984D8AF42E4CE59F180@HK2PR0401MB1954.apcprd04.prod.outlook.com>

Hi Andrew,


thank a lot for your help.


To ease (hopefully)your investigation I preferred to re-run the live test (just basic_call ones) suggested on clearwater site. All the tests, expect the registration steps failed. On sprout log I can see TRANSPORT_ERROR messages. Do you have any idea regarding the reason I'm observing these kind of message?

Please in attachment you can find the .pcap, log file of sprout and bono nodes and the output of the live test.


Here below the steps performed:

- 08:02:13: sudo service bono stop

- 08:02:31: sudo service sprout stop

- from an ubuntu desktop (10.0.0.230): rake test[example.com] SIGNUP_CODE=secret (it includes only basic call, registration and cancel tests)



Please below you can find the list of the nodes running on Openstack.



Thanks and regards,


Silvestro


-------------------------------------------------------


|bono-0.example.com | ACTIVE | - | Running | clearwater-private-signaling=192.168.1.6, 10.0.110.128; clearwater-private-management=192.168.0.7, 10.0.110.137 |


|ellis-0.example.com | ACTIVE | - | Running | clearwater-private-management=192.168.0.4, 10.0.110.193 |


| homer-0.example.com | ACTIVE | - | Running | clearwater-private-signaling=192.168.1.7, 10.0.110.130; clearwater-private-management=192.168.0.8, 10.0.110.139 |


| homestead-0.example.com | ACTIVE | - | Running | clearwater-private-signaling=192.168.1.5; clearwater-private-management=192.168.0.6, 10.0.110.198 |


| ns.example.com | ACTIVE | - | Running | clearwater-private-signaling=192.168.1.3; clearwater-private-management=192.168.0.3, 10.0.110.192 |


| ralf-0.example.com | ACTIVE | - | Running | clearwater-private-signaling=192.168.1.4; clearwater-private-management=192.168.0.5, 10.0.110.197 |


| sprout-0.example.com | ACTIVE | - | Running | clearwater-private-signaling=192.168.1.8; clearwater-private-management=192.168.0.9, 10.0.110.135 |





________________________________
Da: Clearwater <clearwater-bounces at lists.projectclearwater.org> per conto di Andrew Edmonds <Andrew.Edmonds at metaswitch.com>
Inviato: marted? 18 aprile 2017 16:28:16
A: clearwater at lists.projectclearwater.org
Oggetto: Re: [Project Clearwater] Sprout failed: Error: No local site registration store specified


Hi Silvestro,



Thank you for your continued support of Clearwater, I am glad to hear you have all your nodes starting correctly now.



I have taken a look through the diagnostics you have provided to try and see why your Zoiper calls are not working correctly.



One thing you can do with the .cap trace you have sent is search for ?sip?, this allows you to view all the SIP traffic flowing through your deployment. If I do this you can see a REGISTER at 10:35:23.2997597 that does not receive any response. This indicates to me that there might be an issue with your Sprout node processing registrations, however the Sprout logs you have attached do not cover this time period and I can?t see evidence of any calls being attempted in any of the diagnostics you?ve send me.



Could you please collect another .cap trace on your Sprout node (and let me know the IP address of this node). As this trace is running could you please do the following, keeping a note of the time as you do so:



?         Register two subscribers on your Zoiper clients

?         Attempt to make a call between them



Then could you send me that trace and the Sprout logs from the same time period.



Thanks,



Andrew





From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Silvestro Ciampoli
Sent: Thursday, April 13, 2017 12:14 PM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Sprout failed: Error: No local site registration store specified



Hi Andrew,



thanks a lot for your suggestion. Now it seems that all nodes have started correctly. The problem that I'm encountering with this deploy now is that I'm not able to perform a call by Zoiper anymore. I'm able to register correctly by Zoiper two clients (running on two different ubuntu desktop) but the call was failing.



In attachment you can find a .cap with registration and call_tentative gotten on one client interface. I attached also bono and sprout log.



Please let me know if you have any suggestions.



Thanks in advance,

Silvestro Ciampoli

________________________________

Da: Clearwater <clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>> per conto di Andrew Edmonds <Andrew.Edmonds at metaswitch.com<mailto:Andrew.Edmonds at metaswitch.com>>
Inviato: mercoled? 12 aprile 2017 17:47:03
A: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Oggetto: Re: [Project Clearwater] Sprout failed: Error: No local site registration store specified



Hi Silvestro,



Thank you for your question and the information you have provided.



You were right to point out the error "Error main.cpp:1678: No local site registration store specified". This will be the cause of your Sprout process failing to start. In order to specify a local site registration store you need a sprout_registration_store option set in /etc/clearwater/shared_config, you can see here<https://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html> for more details.



Could you try the following steps:



?         add the line ?sprout_registration_store=sprout.<domain name>? to /etc/clearwater/shared_config

?         run ?sudo cw-upload_shared_config?

?         run ?sudo monit summary?



and see if the Sprout process is able to start.



I believe the issue was caused by using a Heat template which is out-of-date in comparison to your Clearwater version. If you re-deploy using the latest stable version of the Heat template<https://github.com/Metaswitch/clearwater-heat/tree/stable> available on Github you should find that this is no longer an issue.



Let me know if this helps.



Thanks,



Andrew



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Silvestro Ciampoli
Sent: Wednesday, April 12, 2017 11:18 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Sprout failed: Error: No local site registration store specified





Hi,



I'm sorry I missed to specify that to deploy Clearwater in Openstack I used the Heat template available on github. The configuration foreseen the separation between management and signaling network.





BR,

Silvestro Ciampoli



________________________________

Da: Silvestro Ciampoli
Inviato: mercoled? 12 aprile 2017 11:02
A: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Oggetto: Sprout failed: Error: No local site registration store specified





Hi Clearwater Team,



I'm trying to (re-)install Clearwater on Openstack (6 VMs) but I'm encountering an issue on the Sprout node. It was failing with the following error : "Error main.cpp:1678: No local site registration store specified". Please see the .log in attachment for further details. Do you have any suggestions?



Below the status of other processes and the version information.





Thanks in advance,



Silvestro Ciampoli



------------------------------------



[sprout]ubuntu at 0:~$ sudo monit summary
Monit 5.18.1 uptime: 1h 0m
 Service Name                     Status                      Type
 node-0.sprout.hcllab.it          Running                     System
 sprout_process                   Execution failed | Does...  Process
 snmpd_process                    Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 memcached_process                Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 chronos_process                  Running                     Process
 astaire_process                  Running                     Process
 sprout_uptime                    Wait parent                 Program
 poll_sprout_sip                  Wait parent                 Program
 poll_sprout_http                 Wait parent                 Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 memcached_uptime                 Status ok                   Program
 poll_memcached                   Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 chronos_uptime                   Status ok                   Program
 poll_chronos                     Status ok                   Program
 astaire_uptime                   Status ok                   Progra





/usr/share/clearwater/bin/clearwater-version

astaire                                  1.0-170331.102803
chronos                                  1.0-170331.102347
clearwater-cluster-manager               1.0-170331.141119
clearwater-config-manager                1.0-170331.141119
clearwater-diags-monitor                 1.0-170327.103718
clearwater-etcd                          1.0-170331.141119
clearwater-infrastructure                1.0-170327.103718
clearwater-log-cleanup                   1.0-170327.103718
clearwater-management                    1.0-170331.141119
clearwater-memcached                     1.0-170327.103718
clearwater-monit                         5.18-170403.124108
clearwater-nginx                         1.0-170223.132518
clearwater-queue-manager                 1.0-170331.141119
clearwater-snmp-handler-astaire          1.0-170331.103056
clearwater-snmpd                         1.0-170327.103718
clearwater-socket-factory                1.0-170327.103718
clearwater-tcp-scalability               1.0-170327.103718
memcached                                1.6.00-0clearwater0.5
sprout                                   1.0-170403.142633
sprout-base                              1.0-170403.142633
sprout-bgcf                              1.0-170403.142633
sprout-icscf                             1.0-170403.142633
sprout-mmtel-as                          1.0-170403.142633
sprout-node                              1.0-170403.142633
sprout-scscf                             1.0-170403.142633








::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170420/6eebad34/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: reducedLiveTest.tgz
Type: application/x-compressed-tar
Size: 613269 bytes
Desc: reducedLiveTest.tgz
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170420/6eebad34/attachment.bin>

From arvindas at hpe.com  Mon Apr 24 08:33:16 2017
From: arvindas at hpe.com (Shirabur, Aravind Ashok (CMS))
Date: Mon, 24 Apr 2017 12:33:16 +0000
Subject: [Project Clearwater] I-CSCF configuration on All-in-one Images setup
Message-ID: <TU4PR84MB00645536D139EAB5C704C9F0DC1F0@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>

Hi,

Trying to REGISTER the X-Lite UE to clearwater (clearwater-infrastructure  1.0-160518.173307) IMS network, registration fails with error response -404. After analyzing the sprout logs I-CSCF is not involved in the REGISTER path, instead the REGISTER message delivered directly to S-CSCF.  How to configure the I-CSCF ? attaching the 'shared_config' file

"All-in-one Images" setup is trying to integrate with HPE-HSS


24-04-2017 11:41:57.508 UTC Debug basicproxy.cpp:92: Process REGISTER request
24-04-2017 11:41:57.508 UTC Verbose sproutletproxy.cpp:503: Sproutlet Proxy transaction (0x26ca080) created
24-04-2017 11:41:57.508 UTC Debug basicproxy.cpp:1271: Report SAS start marker - trail (c)
24-04-2017 11:41:57.508 UTC Debug pjutils.cpp:674: Cloned Request msg REGISTER/cseq=1 (rdata0x7f662408b3f8) to tdta0x26ca490
24-04-2017 11:41:57.508 UTC Debug pjsip:   tsx0x26cc918 Transaction created for Request msg REGISTER/cseq=1 (rdata0x7f662408b3f8)
24-04-2017 11:41:57.508 UTC Debug pjsip:   tsx0x26cc918 Incoming Request msg REGISTER/cseq=1 (rdata0x7f662408b3f8) in state Null
24-04-2017 11:41:57.508 UTC Debug pjsip:   tsx0x26cc918 State changed from Null to Trying, event=RX_MSG
24-04-2017 11:41:57.508 UTC Debug basicproxy.cpp:213: tsx0x26cc918 - tu_on_tsx_state UAS, TSX_STATE RX_MSG state=Trying
24-04-2017 11:41:57.508 UTC Debug pjsip:       endpoint Response msg 408/REGISTER/cseq=1 (tdta0x26cd080) created
24-04-2017 11:41:57.508 UTC Debug sproutletproxy.cpp:124: Find target Sproutlet for request
24-04-2017 11:41:57.508 UTC Debug sproutletproxy.cpp:163: Found next routable URI: sip:scscf.cwaio:5052;transport=TCP;lr;orig
24-04-2017 11:41:57.508 UTC Debug sproutletproxy.cpp:334: Possible service name - scscf
24-04-2017 11:41:57.508 UTC Debug sproutletproxy.cpp:340: Hostname - cwaio
24-04-2017 11:41:57.508 UTC Debug scscfsproutlet.cpp:381: S-CSCF Transaction (0x26ce1c0) created
24-04-2017 11:41:57.508 UTC Verbose sproutletproxy.cpp:1159: Created Sproutlet scscf-0x26ce1c0 for Request msg REGISTER/cseq=1 (tdta0x26ca490)
24-04-2017 11:41:57.508 UTC Verbose sproutletproxy.cpp:2067: Routing Request msg REGISTER/cseq=1 (tdta0x26ca490) (866 bytes) to downstream sproutlet scscf:
--start msg--

REGISTER sip:cmshpe.com SIP/2.0^M
Via: SIP/2.0/TCP 192.168.1.176:55337;rport=55337;received=192.168.1.176;branch=z9hG4bKPjOv5TCMYJtX7PPAJ9XwpHOz1vjbLgNO8N^M
Path: <sip:3uCCzSme1F at 192.168.1.176:5058;transport=TCP;lr;ob>^M
Via: SIP/2.0/TCP 192.168.1.14:9000;rport=65430;received=192.168.1.14;branch=z9hG4bK-524287-1---d346e05081a6b464^M
Max-Forwards: 70^M
Contact: <sip:7406246263 at 192.168.1.14:9000;transport=tcp;rinstance=bbeada83830e3779>^M
To: "Aravind" <sip:7406246263 at cmshpe.com>^M
From: "Aravind" <sip:7406246263 at cmshpe.com>;tag=64cd157d^M
Call-ID: 84253MzY0OGViMzc4YWMxYmU4NGM1YThjYTk4N2RjOWZiNWI^M
CSeq: 1 REGISTER^M
Expires: 3600^M
Allow: SUBSCRIBE, NOTIFY, INVITE, ACK, CANCEL, BYE, REFER, INFO, OPTIONS, MESSAGE^M
User-Agent: X-Lite release 4.9.8 stamp 84253^M
P-Visited-Network-ID: cmshpe.com^M
Route: <sip:scscf.cwaio:5052;transport=TCP;lr;orig>^M
Content-Length:  0^M

Thanks
Aravind

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170424/5a3e6c05/attachment.html>

From Andrew.Edmonds at metaswitch.com  Mon Apr 24 14:36:30 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Mon, 24 Apr 2017 18:36:30 +0000
Subject: [Project Clearwater] Sprout scratch
In-Reply-To: <9a61709a8a784fab8c7466625d2cd141@exchange2013.toledo.be>
References: <105db6c500cd408fb1c9fdf5dec68c0b@exchange2013.toledo.be>
	<9a61709a8a784fab8c7466625d2cd141@exchange2013.toledo.be>
Message-ID: <BLUPR02MB437C436D99DB7949E58C74DE51F0@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Juan

Thank you for your questions and the diagnostics you have provided.

I think there are two different bits of configuration here that are incorrect which are causing the symptoms you are seeing.

Looking through the Sprout log file you sent me I can see on line 773 "Signal 6 caught" after which Sprout crashes. This typically means that monit has killed the process.

It looks like the reason for monit killing the process comes from a routing error. Looking through the logs I can see:

18-04-2017 07:40:46.895 UTC Debug uri_classifier.cpp:204: Classified URI as 4

this suggests some incorrect configuration such as home_domain in /etc/clearwater/shared config being set to an IP address. home_domain should be the main SIP domain of the deployment, and determines which SIP URIs Clearwater will treat as local. Using an IP address here rather than a domain name could cause a SIP address to be routed as a home domain URI rather than a node-local one.

In addition I can also see in the logs:

18-04-2017 07:40:26.466 UTC Error bgcfservice.cpp:98: Failed to read BGCF configuration data: {
        "routes" : [
                { "name" : "route to GB 42",
                "domain" : "192.168.38.42",
                "route" : ["sip:192.168.38.42:5060;transport=UDP"]
                },
                   ]
}

This log suggests that there is something wrong with the BGCF routing record used above, if you look through it you can the final comma is unnecessary because there is only one routing rule. This probably isn't causing the loop you are seeing but it would cause off-net calls to fail.

Could you please:


*         Send me the contents of your /etc/clearwater/shared_config file

*         If home_domain is indeed an IP address could you change it to the domain name of your deployment and run "sudo cw-upload_shared_config"

*         Delete the comma in /etc/clearwater/bgcf.json and run "sudo cw-upload_bgcf_json"

Once you have done this you can try making a call, please let me know how this goes.

Thanks,

Andrew


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juan Sepulveda
Sent: Monday, April 24, 2017 8:36 AM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Sprout scratch

Hi,

please can you check why a loop is detected? We are in the max log level. No more info to explain the loop.

Please help us.


***********************************************************************************************************************************************
--start msg--

OPTIONS sip:poll-sip at 192.168.38.55:5054 SIP/2.0
Route: <sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=registrar>
Via: SIP/2.0/TCP 192.168.38.55;rport=34358;received=192.168.38.55;branch=z9hG4bK-319922
Max-Forwards: 1
To: <sip:poll-sip at 192.168.38.55>
From: "poll-sip" <sip:poll-sip at 192.168.38.55>;tag=319922
Call-ID: poll-sip-319922
CSeq: 319922 OPTIONS
Contact: <sip:192.168.38.55>
Accept: application/sdp
User-Agent: poll-sip
Content-Length:  0


--end msg--
24-04-2017 07:20:32.545 UTC Debug pjutils.cpp:741: Cloned tdta0x7f891800f080 to tdta0x7f8918013500
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1238: Remove top Route header Route: <sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=registrar>
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1813: Adding message 0x7f8918013b10 => txdata 0x7f89180135a8 mapping
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:1658: registrar-0x7f891800d180 pass initial request Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) to Sproutlet
24-04-2017 07:20:32.545 UTC Info registrarsproutlet.cpp:172: Registrar sproutlet received initial request
24-04-2017 07:20:32.545 UTC Debug uri_classifier.cpp:174: home domain: true, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
24-04-2017 07:20:32.545 UTC Debug uri_classifier.cpp:204: Classified URI as 4
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:384: Creating URI for service subscription
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:405: Constructed URI sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=subscription
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1409: Sproutlet send_request 0x7f8918013b10
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:1445: registrar-0x7f891800d180 sending Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1828: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1868: Processing request 0x7f89180135a8, fork = 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1992: registrar-0x7f891800d180 transmitting request on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:2006: registrar-0x7f891800d180 store reference to non-ACK request Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1820: Removing message 0x7f8918013b10 => txdata 0x7f89180135a8 mapping
24-04-2017 07:20:32.545 UTC Info sproutletproxy.cpp:818: Loop detected - rejecting request with 483 status code
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:2140: Routing Response msg 483/OPTIONS/cseq=319922 (tdta0x7f8918076070) (297 bytes) to upstream sproutlet registrar:
--start msg--

SIP/2.0 483 Too Many Hops
Via: SIP/2.0/TCP 192.168.38.55;rport=34358;received=192.168.38.55;branch=z9hG4bK-319922
Call-ID: poll-sip-319922
From: "poll-sip" <sip:poll-sip at 192.168.38.55>;tag=319922
To: <sip:poll-sip at 192.168.38.55>;tag=z9hG4bK-319922
CSeq: 319922 OPTIONS
Content-Length:  0


--end msg--



THx

Kr

Juan


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juan Sepulveda
Sent: mardi 18 avril 2017 10:02
To: 'clearwater at lists.projectclearwater.org'
Subject: [Project Clearwater] Sprout scratch

Hi,

Since Friday for an unknown reason, the sprout start to scratch permanently. We can see in the log internal loop and the poll_sprout_sip process restarting.

Logs are in attached file. Please can you debug this issue?

The only actions we performed last Friday is to fix a etcd process failure and we add 2 lines in the shared_config file:

billing_realm=ims02.as20650.net
cdf_identity=192.169.38.56

Currently, those 2 lines are commented.
In the attached file, you have also the version and the monit command output.

Let me know if you need more info.

Thx

Kr

Juan


IP Nexia, a new brand of Toledo Telecom.
[Description: http://www.ipnexia.com/images/pix.gif]

Juan Sepulveda
Voip and Network Engineer
M + 32 478 97 98 79   T +32 2 600 16 69   @ jsepulveda at ipnexia.com<mailto:jsepulveda at ipnexia.com>



IP Nexia Kouterveldstraat, 2 - 1831 Diegem
Tel +32 2 648 08 48   Fax +32 2 646 44 24   www.ipnexia.com<http://www.ipnexia.com/>
Disclaimer<http://www.ipnexia.com/disclaimer/disclaimer.html>

[Description: http://www.ipnexia.com/images/back-sign2_02.jpg]

[Description: http://www.ipnexia.com/images/back-sign2_03.jpg]



> Please consider the environmental impact of needlessly printing this e-mail.



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170424/625dc4ff/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 171 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170424/625dc4ff/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 4448 bytes
Desc: image002.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170424/625dc4ff/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.jpg
Type: image/jpeg
Size: 2618 bytes
Desc: image003.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170424/625dc4ff/attachment-0001.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image004.png
Type: image/png
Size: 170 bytes
Desc: image004.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170424/625dc4ff/attachment-0001.png>

From arvindas at hpe.com  Tue Apr 25 03:36:16 2017
From: arvindas at hpe.com (Shirabur, Aravind Ashok (CMS))
Date: Tue, 25 Apr 2017 07:36:16 +0000
Subject: [Project Clearwater] All-in-one Images setup supports I-CSCF
 functionality ?
Message-ID: <TU4PR84MB006408AEDEAAE47F1248F38BDC1E0@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>

Hi,

All-in-one Images setup supports  I-CSCF functionality ?

Trying to REGISTER the X-Lite UE to clearwater (clearwater-infrastructure  1.0-160518.173307) IMS network, registration fails with error response -404. After analyzing the sprout logs I-CSCF is not involved in the REGISTER path, instead the REGISTER message delivered directly to S-CSCF.  How to configure the I-CSCF ? attaching the 'shared_config' file

"All-in-one Images" setup is trying to integrate with HPE-HSS


24-04-2017 11:41:57.508 UTC Debug basicproxy.cpp:92: Process REGISTER request
24-04-2017 11:41:57.508 UTC Verbose sproutletproxy.cpp:503: Sproutlet Proxy transaction (0x26ca080) created
24-04-2017 11:41:57.508 UTC Debug basicproxy.cpp:1271: Report SAS start marker - trail (c)
24-04-2017 11:41:57.508 UTC Debug pjutils.cpp:674: Cloned Request msg REGISTER/cseq=1 (rdata0x7f662408b3f8) to tdta0x26ca490
24-04-2017 11:41:57.508 UTC Debug pjsip:   tsx0x26cc918 Transaction created for Request msg REGISTER/cseq=1 (rdata0x7f662408b3f8)
24-04-2017 11:41:57.508 UTC Debug pjsip:   tsx0x26cc918 Incoming Request msg REGISTER/cseq=1 (rdata0x7f662408b3f8) in state Null
24-04-2017 11:41:57.508 UTC Debug pjsip:   tsx0x26cc918 State changed from Null to Trying, event=RX_MSG
24-04-2017 11:41:57.508 UTC Debug basicproxy.cpp:213: tsx0x26cc918 - tu_on_tsx_state UAS, TSX_STATE RX_MSG state=Trying
24-04-2017 11:41:57.508 UTC Debug pjsip:       endpoint Response msg 408/REGISTER/cseq=1 (tdta0x26cd080) created
24-04-2017 11:41:57.508 UTC Debug sproutletproxy.cpp:124: Find target Sproutlet for request
24-04-2017 11:41:57.508 UTC Debug sproutletproxy.cpp:163: Found next routable URI: sip:scscf.cwaio:5052;transport=TCP;lr;orig
24-04-2017 11:41:57.508 UTC Debug sproutletproxy.cpp:334: Possible service name - scscf
24-04-2017 11:41:57.508 UTC Debug sproutletproxy.cpp:340: Hostname - cwaio
24-04-2017 11:41:57.508 UTC Debug scscfsproutlet.cpp:381: S-CSCF Transaction (0x26ce1c0) created
24-04-2017 11:41:57.508 UTC Verbose sproutletproxy.cpp:1159: Created Sproutlet scscf-0x26ce1c0 for Request msg REGISTER/cseq=1 (tdta0x26ca490)
24-04-2017 11:41:57.508 UTC Verbose sproutletproxy.cpp:2067: Routing Request msg REGISTER/cseq=1 (tdta0x26ca490) (866 bytes) to downstream sproutlet scscf:
--start msg--

REGISTER sip:cmshpe.com SIP/2.0^M
Via: SIP/2.0/TCP 192.168.1.176:55337;rport=55337;received=192.168.1.176;branch=z9hG4bKPjOv5TCMYJtX7PPAJ9XwpHOz1vjbLgNO8N^M
Path: <sip:3uCCzSme1F at 192.168.1.176:5058;transport=TCP;lr;ob>^M
Via: SIP/2.0/TCP 192.168.1.14:9000;rport=65430;received=192.168.1.14;branch=z9hG4bK-524287-1---d346e05081a6b464^M
Max-Forwards: 70^M
Contact: <sip:7406246263 at 192.168.1.14:9000;transport=tcp;rinstance=bbeada83830e3779>^M
To: "Aravind" <sip:7406246263 at cmshpe.com>^M
From: "Aravind" <sip:7406246263 at cmshpe.com>;tag=64cd157d^M
Call-ID: 84253MzY0OGViMzc4YWMxYmU4NGM1YThjYTk4N2RjOWZiNWI^M
CSeq: 1 REGISTER^M
Expires: 3600^M
Allow: SUBSCRIBE, NOTIFY, INVITE, ACK, CANCEL, BYE, REFER, INFO, OPTIONS, MESSAGE^M
User-Agent: X-Lite release 4.9.8 stamp 84253^M
P-Visited-Network-ID: cmshpe.com^M
Route: <sip:scscf.cwaio:5052;transport=TCP;lr;orig>^M
Content-Length:  0^M

Thanks
Aravind

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/4132c033/attachment.html>

From jsepulveda at ipnexia.com  Tue Apr 25 04:50:48 2017
From: jsepulveda at ipnexia.com (Juan Sepulveda)
Date: Tue, 25 Apr 2017 08:50:48 +0000
Subject: [Project Clearwater] Sprout scratch
In-Reply-To: <BLUPR02MB437C436D99DB7949E58C74DE51F0@BLUPR02MB437.namprd02.prod.outlook.com>
References: <105db6c500cd408fb1c9fdf5dec68c0b@exchange2013.toledo.be>
	<9a61709a8a784fab8c7466625d2cd141@exchange2013.toledo.be>
	<BLUPR02MB437C436D99DB7949E58C74DE51F0@BLUPR02MB437.namprd02.prod.outlook.com>
Message-ID: <6a54d1eda4fa44029d14950a3a3759c6@exchange2013.toledo.be>

Hi Andrew,

for the bgcf.json issue, we corrected it:

root at cw-sprout1:/etc/clearwater# cat bgcf.json
{
        "routes" : [
                { "name" : "route to GB 42",
                "domain" : "192.168.38.42",
                "route" : ["sip:192.168.38.42:5060;transport=UDP"]
                }
                   ]
}

For config issue, It is strange as we don't use IP address in the shared_confi file (only for the enum server):

root at cw-sprout1:/etc/clearwater# cat shared_config
# Deployment definitions
me_domain=ims02.as20650.net
sprout_hostname=cw-sprout1.ims02.as20650.net
sprout_registration_store=cw-sprout1.ims02.as20650.net
hs_hostname=cw-homestead1.ims02.as20650.net:8888
hs_provisioning_hostname=cw-homestead1.ims02.as20650.net:8889
ralf_hostname=cw-ralf1.ims02.as20650.net:10888
xdms_hostname=cw-homer1.ims02.as20650.net:7888

# Email server configuration
smtp_smarthost=relay.ipnexia.com
##smtp_username=<username>
#smtp_password=<password>
email_recovery_sender=clearwater at ipnexia.com

# Keys
signup_key="ims02!"
turn_workaround="ims02!"
ellis_api_key="ims02!"
ellis_cookie_key="ims02!"

# I-CSCF/S-CSCF configuration
icscf=5052
upstream_hostname=icscf.sprout.ims02.as20650.net
upstream_port=5052


# Application Servers
#gemini=<gemini port>
#memento=<memento port>

enum_server=192.168.38.57
#enum_file=/etc/clearwater/enum.json
#billing_realm=ims02.as20650.net
#cdf_identity=192.169.38.56
root at cw-sprout1:/etc/clearwater#


Juan


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Andrew Edmonds
Sent: lundi 24 avril 2017 20:37
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Sprout scratch

Hi Juan

Thank you for your questions and the diagnostics you have provided.

I think there are two different bits of configuration here that are incorrect which are causing the symptoms you are seeing.

Looking through the Sprout log file you sent me I can see on line 773 "Signal 6 caught" after which Sprout crashes. This typically means that monit has killed the process.

It looks like the reason for monit killing the process comes from a routing error. Looking through the logs I can see:

18-04-2017 07:40:46.895 UTC Debug uri_classifier.cpp:204: Classified URI as 4

this suggests some incorrect configuration such as home_domain in /etc/clearwater/shared config being set to an IP address. home_domain should be the main SIP domain of the deployment, and determines which SIP URIs Clearwater will treat as local. Using an IP address here rather than a domain name could cause a SIP address to be routed as a home domain URI rather than a node-local one.

In addition I can also see in the logs:

18-04-2017 07:40:26.466 UTC Error bgcfservice.cpp:98: Failed to read BGCF configuration data: {
        "routes" : [
                { "name" : "route to GB 42",
                "domain" : "192.168.38.42",
                "route" : ["sip:192.168.38.42:5060;transport=UDP"]
                },
                   ]
}

This log suggests that there is something wrong with the BGCF routing record used above, if you look through it you can the final comma is unnecessary because there is only one routing rule. This probably isn't causing the loop you are seeing but it would cause off-net calls to fail.

Could you please:


*         Send me the contents of your /etc/clearwater/shared_config file

*         If home_domain is indeed an IP address could you change it to the domain name of your deployment and run "sudo cw-upload_shared_config"

*         Delete the comma in /etc/clearwater/bgcf.json and run "sudo cw-upload_bgcf_json"

Once you have done this you can try making a call, please let me know how this goes.

Thanks,

Andrew


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juan Sepulveda
Sent: Monday, April 24, 2017 8:36 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Sprout scratch

Hi,

please can you check why a loop is detected? We are in the max log level. No more info to explain the loop.

Please help us.


***********************************************************************************************************************************************
--start msg--

OPTIONS sip:poll-sip at 192.168.38.55:5054 SIP/2.0
Route: <sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=registrar>
Via: SIP/2.0/TCP 192.168.38.55;rport=34358;received=192.168.38.55;branch=z9hG4bK-319922
Max-Forwards: 1
To: <sip:poll-sip at 192.168.38.55>
From: "poll-sip" <sip:poll-sip at 192.168.38.55>;tag=319922
Call-ID: poll-sip-319922
CSeq: 319922 OPTIONS
Contact: <sip:192.168.38.55>
Accept: application/sdp
User-Agent: poll-sip
Content-Length:  0


--end msg--
24-04-2017 07:20:32.545 UTC Debug pjutils.cpp:741: Cloned tdta0x7f891800f080 to tdta0x7f8918013500
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1238: Remove top Route header Route: <sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=registrar>
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1813: Adding message 0x7f8918013b10 => txdata 0x7f89180135a8 mapping
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:1658: registrar-0x7f891800d180 pass initial request Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) to Sproutlet
24-04-2017 07:20:32.545 UTC Info registrarsproutlet.cpp:172: Registrar sproutlet received initial request
24-04-2017 07:20:32.545 UTC Debug uri_classifier.cpp:174: home domain: true, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
24-04-2017 07:20:32.545 UTC Debug uri_classifier.cpp:204: Classified URI as 4
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:384: Creating URI for service subscription
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:405: Constructed URI sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=subscription
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1409: Sproutlet send_request 0x7f8918013b10
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:1445: registrar-0x7f891800d180 sending Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1828: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1868: Processing request 0x7f89180135a8, fork = 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1992: registrar-0x7f891800d180 transmitting request on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:2006: registrar-0x7f891800d180 store reference to non-ACK request Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1820: Removing message 0x7f8918013b10 => txdata 0x7f89180135a8 mapping
24-04-2017 07:20:32.545 UTC Info sproutletproxy.cpp:818: Loop detected - rejecting request with 483 status code
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:2140: Routing Response msg 483/OPTIONS/cseq=319922 (tdta0x7f8918076070) (297 bytes) to upstream sproutlet registrar:
--start msg--

SIP/2.0 483 Too Many Hops
Via: SIP/2.0/TCP 192.168.38.55;rport=34358;received=192.168.38.55;branch=z9hG4bK-319922
Call-ID: poll-sip-319922
From: "poll-sip" <sip:poll-sip at 192.168.38.55>;tag=319922
To: <sip:poll-sip at 192.168.38.55>;tag=z9hG4bK-319922
CSeq: 319922 OPTIONS
Content-Length:  0


--end msg--



THx

Kr

Juan


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juan Sepulveda
Sent: mardi 18 avril 2017 10:02
To: 'clearwater at lists.projectclearwater.org'
Subject: [Project Clearwater] Sprout scratch

Hi,

Since Friday for an unknown reason, the sprout start to scratch permanently. We can see in the log internal loop and the poll_sprout_sip process restarting.

Logs are in attached file. Please can you debug this issue?

The only actions we performed last Friday is to fix a etcd process failure and we add 2 lines in the shared_config file:

billing_realm=ims02.as20650.net
cdf_identity=192.169.38.56

Currently, those 2 lines are commented.
In the attached file, you have also the version and the monit command output.

Let me know if you need more info.

Thx

Kr

Juan


IP Nexia, a new brand of Toledo Telecom.
[Description: http://www.ipnexia.com/images/pix.gif]

Juan Sepulveda
Voip and Network Engineer
M + 32 478 97 98 79   T +32 2 600 16 69   @ jsepulveda at ipnexia.com<mailto:jsepulveda at ipnexia.com>



IP Nexia Kouterveldstraat, 2 - 1831 Diegem
Tel +32 2 648 08 48   Fax +32 2 646 44 24   www.ipnexia.com<http://www.ipnexia.com/>
Disclaimer<http://www.ipnexia.com/disclaimer/disclaimer.html>

[Description: http://www.ipnexia.com/images/back-sign2_02.jpg]

[Description: http://www.ipnexia.com/images/back-sign2_03.jpg]



> Please consider the environmental impact of needlessly printing this e-mail.



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/18c1b6cb/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 171 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/18c1b6cb/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 4448 bytes
Desc: image002.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/18c1b6cb/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.jpg
Type: image/jpeg
Size: 2618 bytes
Desc: image003.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/18c1b6cb/attachment-0001.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image004.png
Type: image/png
Size: 170 bytes
Desc: image004.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/18c1b6cb/attachment-0001.png>

From a6160107 at gmail.com  Tue Apr 25 05:43:01 2017
From: a6160107 at gmail.com (=?UTF-8?B?5rqr5L+K57at?=)
Date: Tue, 25 Apr 2017 17:43:01 +0800
Subject: [Project Clearwater] Call between two Clearwater Deployment
Message-ID: <CAOgshDuLDdFhoT_KtnCxXT2r=cPcESTNQv2+6nhRiJJ7+9cidQ@mail.gmail.com>

Hi,

I want to make a call between two clearwater deployment,

and I found two configure guide as following :

Multiple Domains
http://clearwater.readthedocs.io/en/stable/Multiple_Domains.html?highlight=multiple%20domain

Off-net Calling with BGCF and IBCF
http://clearwater.readthedocs.io/en/stable/IBCF.html?highlight=IBCF

I had following these step and set these configuration, and I make a call
from clearwater deployment "A" to deployment "B". Finally, I got "Service
Unavailable"  message return.

I installrf six component in a VM on VMware, and set IBCF and BGCF
configuration in this VM.

My configuration in "A" as following :

/etc/clearwater/bgcf.json :
{
    "routes" : [
        { "name" : "Routing to B.ims",

              "domain" : "B.ims",

              "route"  :
["sip:<ip of A>:5058","sip:<ip of B>:5060"]

            }
]
}


/etc/clearwater/user_settings :
trusted_peers="<ip of B>"


these are some questions about what cause fail:

1. Should I have to set ENUM configuration?  I don't want to make off-net
call, but external call between two clearwater deployment, just like IMS to
IMS call.

2. I had check these SIP mes. transmission with wireshark, I found INVITE
message did't pass to deployment B.

3.Should I set ibcf.a.ims in /etc/hosts file? or set in my dns file?

Please give me some suggestions,
I really want to successfully make a call between two deployment.

Thanks,
Eric
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/0e61c914/attachment.html>

From Andrew.Edmonds at metaswitch.com  Tue Apr 25 06:49:06 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Tue, 25 Apr 2017 10:49:06 +0000
Subject: [Project Clearwater] Sprout scratch
In-Reply-To: <6a54d1eda4fa44029d14950a3a3759c6@exchange2013.toledo.be>
References: <105db6c500cd408fb1c9fdf5dec68c0b@exchange2013.toledo.be>
	<9a61709a8a784fab8c7466625d2cd141@exchange2013.toledo.be>
	<BLUPR02MB437C436D99DB7949E58C74DE51F0@BLUPR02MB437.namprd02.prod.outlook.com>
	<6a54d1eda4fa44029d14950a3a3759c6@exchange2013.toledo.be>
Message-ID: <BL2PR02MB43351FC86A6B06601588BC1E51E0@BL2PR02MB433.namprd02.prod.outlook.com>

Hi Juan,

Thank you for the extra diagnostics.

I have taken a look through your shared config file. The line:

"me_domain=ims02.as20650.net"

should be replaced by:

"home_domain=ims02.as20650.net"

Could you try making this change and running the command "sudo cw-upload_shared_config". Once you have done this please let me know if calls start working.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juan Sepulveda
Sent: Tuesday, April 25, 2017 9:51 AM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Sprout scratch

Hi Andrew,

for the bgcf.json issue, we corrected it:

root at cw-sprout1:/etc/clearwater# cat bgcf.json
{
        "routes" : [
                { "name" : "route to GB 42",
                "domain" : "192.168.38.42",
                "route" : ["sip:192.168.38.42:5060;transport=UDP"]
                }
                   ]
}

For config issue, It is strange as we don't use IP address in the shared_confi file (only for the enum server):

root at cw-sprout1:/etc/clearwater# cat shared_config
# Deployment definitions
me_domain=ims02.as20650.net
sprout_hostname=cw-sprout1.ims02.as20650.net
sprout_registration_store=cw-sprout1.ims02.as20650.net
hs_hostname=cw-homestead1.ims02.as20650.net:8888
hs_provisioning_hostname=cw-homestead1.ims02.as20650.net:8889
ralf_hostname=cw-ralf1.ims02.as20650.net:10888
xdms_hostname=cw-homer1.ims02.as20650.net:7888

# Email server configuration
smtp_smarthost=relay.ipnexia.com
##smtp_username=<username>
#smtp_password=<password>
email_recovery_sender=clearwater at ipnexia.com<mailto:email_recovery_sender=clearwater at ipnexia.com>

# Keys
signup_key="ims02!"
turn_workaround="ims02!"
ellis_api_key="ims02!"
ellis_cookie_key="ims02!"

# I-CSCF/S-CSCF configuration
icscf=5052
upstream_hostname=icscf.sprout.ims02.as20650.net
upstream_port=5052


# Application Servers
#gemini=<gemini port>
#memento=<memento port>

enum_server=192.168.38.57
#enum_file=/etc/clearwater/enum.json
#billing_realm=ims02.as20650.net
#cdf_identity=192.169.38.56
root at cw-sprout1:/etc/clearwater#


Juan


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Andrew Edmonds
Sent: lundi 24 avril 2017 20:37
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Sprout scratch

Hi Juan

Thank you for your questions and the diagnostics you have provided.

I think there are two different bits of configuration here that are incorrect which are causing the symptoms you are seeing.

Looking through the Sprout log file you sent me I can see on line 773 "Signal 6 caught" after which Sprout crashes. This typically means that monit has killed the process.

It looks like the reason for monit killing the process comes from a routing error. Looking through the logs I can see:

18-04-2017 07:40:46.895 UTC Debug uri_classifier.cpp:204: Classified URI as 4

this suggests some incorrect configuration such as home_domain in /etc/clearwater/shared config being set to an IP address. home_domain should be the main SIP domain of the deployment, and determines which SIP URIs Clearwater will treat as local. Using an IP address here rather than a domain name could cause a SIP address to be routed as a home domain URI rather than a node-local one.

In addition I can also see in the logs:

18-04-2017 07:40:26.466 UTC Error bgcfservice.cpp:98: Failed to read BGCF configuration data: {
        "routes" : [
                { "name" : "route to GB 42",
                "domain" : "192.168.38.42",
                "route" : ["sip:192.168.38.42:5060;transport=UDP"]
                },
                   ]
}

This log suggests that there is something wrong with the BGCF routing record used above, if you look through it you can the final comma is unnecessary because there is only one routing rule. This probably isn't causing the loop you are seeing but it would cause off-net calls to fail.

Could you please:


*         Send me the contents of your /etc/clearwater/shared_config file

*         If home_domain is indeed an IP address could you change it to the domain name of your deployment and run "sudo cw-upload_shared_config"

*         Delete the comma in /etc/clearwater/bgcf.json and run "sudo cw-upload_bgcf_json"

Once you have done this you can try making a call, please let me know how this goes.

Thanks,

Andrew


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juan Sepulveda
Sent: Monday, April 24, 2017 8:36 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Sprout scratch

Hi,

please can you check why a loop is detected? We are in the max log level. No more info to explain the loop.

Please help us.


***********************************************************************************************************************************************
--start msg--

OPTIONS sip:poll-sip at 192.168.38.55:5054 SIP/2.0
Route: <sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=registrar>
Via: SIP/2.0/TCP 192.168.38.55;rport=34358;received=192.168.38.55;branch=z9hG4bK-319922
Max-Forwards: 1
To: <sip:poll-sip at 192.168.38.55>
From: "poll-sip" <sip:poll-sip at 192.168.38.55>;tag=319922
Call-ID: poll-sip-319922
CSeq: 319922 OPTIONS
Contact: <sip:192.168.38.55>
Accept: application/sdp
User-Agent: poll-sip
Content-Length:  0


--end msg--
24-04-2017 07:20:32.545 UTC Debug pjutils.cpp:741: Cloned tdta0x7f891800f080 to tdta0x7f8918013500
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1238: Remove top Route header Route: <sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=registrar>
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1813: Adding message 0x7f8918013b10 => txdata 0x7f89180135a8 mapping
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:1658: registrar-0x7f891800d180 pass initial request Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) to Sproutlet
24-04-2017 07:20:32.545 UTC Info registrarsproutlet.cpp:172: Registrar sproutlet received initial request
24-04-2017 07:20:32.545 UTC Debug uri_classifier.cpp:174: home domain: true, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
24-04-2017 07:20:32.545 UTC Debug uri_classifier.cpp:204: Classified URI as 4
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:384: Creating URI for service subscription
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:405: Constructed URI sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=subscription
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1409: Sproutlet send_request 0x7f8918013b10
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:1445: registrar-0x7f891800d180 sending Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1828: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1868: Processing request 0x7f89180135a8, fork = 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1992: registrar-0x7f891800d180 transmitting request on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:2006: registrar-0x7f891800d180 store reference to non-ACK request Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1820: Removing message 0x7f8918013b10 => txdata 0x7f89180135a8 mapping
24-04-2017 07:20:32.545 UTC Info sproutletproxy.cpp:818: Loop detected - rejecting request with 483 status code
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:2140: Routing Response msg 483/OPTIONS/cseq=319922 (tdta0x7f8918076070) (297 bytes) to upstream sproutlet registrar:
--start msg--

SIP/2.0 483 Too Many Hops
Via: SIP/2.0/TCP 192.168.38.55;rport=34358;received=192.168.38.55;branch=z9hG4bK-319922
Call-ID: poll-sip-319922
From: "poll-sip" <sip:poll-sip at 192.168.38.55>;tag=319922
To: <sip:poll-sip at 192.168.38.55>;tag=z9hG4bK-319922
CSeq: 319922 OPTIONS
Content-Length:  0


--end msg--



THx

Kr

Juan


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juan Sepulveda
Sent: mardi 18 avril 2017 10:02
To: 'clearwater at lists.projectclearwater.org'
Subject: [Project Clearwater] Sprout scratch

Hi,

Since Friday for an unknown reason, the sprout start to scratch permanently. We can see in the log internal loop and the poll_sprout_sip process restarting.

Logs are in attached file. Please can you debug this issue?

The only actions we performed last Friday is to fix a etcd process failure and we add 2 lines in the shared_config file:

billing_realm=ims02.as20650.net
cdf_identity=192.169.38.56

Currently, those 2 lines are commented.
In the attached file, you have also the version and the monit command output.

Let me know if you need more info.

Thx

Kr

Juan


IP Nexia, a new brand of Toledo Telecom.
[Description: http://www.ipnexia.com/images/pix.gif]

Juan Sepulveda
Voip and Network Engineer
M + 32 478 97 98 79   T +32 2 600 16 69   @ jsepulveda at ipnexia.com<mailto:jsepulveda at ipnexia.com>



IP Nexia Kouterveldstraat, 2 - 1831 Diegem
Tel +32 2 648 08 48   Fax +32 2 646 44 24   www.ipnexia.com<http://www.ipnexia.com/>
Disclaimer<http://www.ipnexia.com/disclaimer/disclaimer.html>

[Description: http://www.ipnexia.com/images/back-sign2_02.jpg]

[Description: http://www.ipnexia.com/images/back-sign2_03.jpg]



> Please consider the environmental impact of needlessly printing this e-mail.



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/88013bda/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 171 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/88013bda/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 4448 bytes
Desc: image002.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/88013bda/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.jpg
Type: image/jpeg
Size: 2618 bytes
Desc: image003.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/88013bda/attachment-0001.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image004.png
Type: image/png
Size: 170 bytes
Desc: image004.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/88013bda/attachment-0001.png>

From jsepulveda at ipnexia.com  Tue Apr 25 07:43:33 2017
From: jsepulveda at ipnexia.com (Juan Sepulveda)
Date: Tue, 25 Apr 2017 11:43:33 +0000
Subject: [Project Clearwater] Sprout scratch
In-Reply-To: <BL2PR02MB43351FC86A6B06601588BC1E51E0@BL2PR02MB433.namprd02.prod.outlook.com>
References: <105db6c500cd408fb1c9fdf5dec68c0b@exchange2013.toledo.be>
	<9a61709a8a784fab8c7466625d2cd141@exchange2013.toledo.be>
	<BLUPR02MB437C436D99DB7949E58C74DE51F0@BLUPR02MB437.namprd02.prod.outlook.com>
	<6a54d1eda4fa44029d14950a3a3759c6@exchange2013.toledo.be>
	<BL2PR02MB43351FC86A6B06601588BC1E51E0@BL2PR02MB433.namprd02.prod.outlook.com>
Message-ID: <4334cd15826541838d0b534862d719df@exchange2013.toledo.be>

Hi Andrew,

Yes, it correct the issue.  Again, strange, the original file we have is with home_domain. By mistake, we probably deleted some letters.

Thx

Kr

Juan


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Andrew Edmonds
Sent: mardi 25 avril 2017 12:49
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Sprout scratch

Hi Juan,

Thank you for the extra diagnostics.

I have taken a look through your shared config file. The line:

"me_domain=ims02.as20650.net"

should be replaced by:

"home_domain=ims02.as20650.net"

Could you try making this change and running the command "sudo cw-upload_shared_config". Once you have done this please let me know if calls start working.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juan Sepulveda
Sent: Tuesday, April 25, 2017 9:51 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Sprout scratch

Hi Andrew,

for the bgcf.json issue, we corrected it:

root at cw-sprout1:/etc/clearwater# cat bgcf.json
{
        "routes" : [
                { "name" : "route to GB 42",
                "domain" : "192.168.38.42",
                "route" : ["sip:192.168.38.42:5060;transport=UDP"]
                }
                   ]
}

For config issue, It is strange as we don't use IP address in the shared_confi file (only for the enum server):

root at cw-sprout1:/etc/clearwater# cat shared_config
# Deployment definitions
me_domain=ims02.as20650.net
sprout_hostname=cw-sprout1.ims02.as20650.net
sprout_registration_store=cw-sprout1.ims02.as20650.net
hs_hostname=cw-homestead1.ims02.as20650.net:8888
hs_provisioning_hostname=cw-homestead1.ims02.as20650.net:8889
ralf_hostname=cw-ralf1.ims02.as20650.net:10888
xdms_hostname=cw-homer1.ims02.as20650.net:7888

# Email server configuration
smtp_smarthost=relay.ipnexia.com
##smtp_username=<username>
#smtp_password=<password>
email_recovery_sender=clearwater at ipnexia.com<mailto:email_recovery_sender=clearwater at ipnexia.com>

# Keys
signup_key="ims02!"
turn_workaround="ims02!"
ellis_api_key="ims02!"
ellis_cookie_key="ims02!"

# I-CSCF/S-CSCF configuration
icscf=5052
upstream_hostname=icscf.sprout.ims02.as20650.net
upstream_port=5052


# Application Servers
#gemini=<gemini port>
#memento=<memento port>

enum_server=192.168.38.57
#enum_file=/etc/clearwater/enum.json
#billing_realm=ims02.as20650.net
#cdf_identity=192.169.38.56
root at cw-sprout1:/etc/clearwater#


Juan


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Andrew Edmonds
Sent: lundi 24 avril 2017 20:37
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Sprout scratch

Hi Juan

Thank you for your questions and the diagnostics you have provided.

I think there are two different bits of configuration here that are incorrect which are causing the symptoms you are seeing.

Looking through the Sprout log file you sent me I can see on line 773 "Signal 6 caught" after which Sprout crashes. This typically means that monit has killed the process.

It looks like the reason for monit killing the process comes from a routing error. Looking through the logs I can see:

18-04-2017 07:40:46.895 UTC Debug uri_classifier.cpp:204: Classified URI as 4

this suggests some incorrect configuration such as home_domain in /etc/clearwater/shared config being set to an IP address. home_domain should be the main SIP domain of the deployment, and determines which SIP URIs Clearwater will treat as local. Using an IP address here rather than a domain name could cause a SIP address to be routed as a home domain URI rather than a node-local one.

In addition I can also see in the logs:

18-04-2017 07:40:26.466 UTC Error bgcfservice.cpp:98: Failed to read BGCF configuration data: {
        "routes" : [
                { "name" : "route to GB 42",
                "domain" : "192.168.38.42",
                "route" : ["sip:192.168.38.42:5060;transport=UDP"]
                },
                   ]
}

This log suggests that there is something wrong with the BGCF routing record used above, if you look through it you can the final comma is unnecessary because there is only one routing rule. This probably isn't causing the loop you are seeing but it would cause off-net calls to fail.

Could you please:


*         Send me the contents of your /etc/clearwater/shared_config file

*         If home_domain is indeed an IP address could you change it to the domain name of your deployment and run "sudo cw-upload_shared_config"

*         Delete the comma in /etc/clearwater/bgcf.json and run "sudo cw-upload_bgcf_json"

Once you have done this you can try making a call, please let me know how this goes.

Thanks,

Andrew


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juan Sepulveda
Sent: Monday, April 24, 2017 8:36 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Sprout scratch

Hi,

please can you check why a loop is detected? We are in the max log level. No more info to explain the loop.

Please help us.


***********************************************************************************************************************************************
--start msg--

OPTIONS sip:poll-sip at 192.168.38.55:5054 SIP/2.0
Route: <sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=registrar>
Via: SIP/2.0/TCP 192.168.38.55;rport=34358;received=192.168.38.55;branch=z9hG4bK-319922
Max-Forwards: 1
To: <sip:poll-sip at 192.168.38.55>
From: "poll-sip" <sip:poll-sip at 192.168.38.55>;tag=319922
Call-ID: poll-sip-319922
CSeq: 319922 OPTIONS
Contact: <sip:192.168.38.55>
Accept: application/sdp
User-Agent: poll-sip
Content-Length:  0


--end msg--
24-04-2017 07:20:32.545 UTC Debug pjutils.cpp:741: Cloned tdta0x7f891800f080 to tdta0x7f8918013500
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1238: Remove top Route header Route: <sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=registrar>
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1813: Adding message 0x7f8918013b10 => txdata 0x7f89180135a8 mapping
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:1658: registrar-0x7f891800d180 pass initial request Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) to Sproutlet
24-04-2017 07:20:32.545 UTC Info registrarsproutlet.cpp:172: Registrar sproutlet received initial request
24-04-2017 07:20:32.545 UTC Debug uri_classifier.cpp:174: home domain: true, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
24-04-2017 07:20:32.545 UTC Debug uri_classifier.cpp:204: Classified URI as 4
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:384: Creating URI for service subscription
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:405: Constructed URI sip:cw-sprout1.ims02.as20650.net;transport=tcp;lr;service=subscription
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1409: Sproutlet send_request 0x7f8918013b10
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:1445: registrar-0x7f891800d180 sending Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1828: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1868: Processing request 0x7f89180135a8, fork = 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1992: registrar-0x7f891800d180 transmitting request on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:2006: registrar-0x7f891800d180 store reference to non-ACK request Request msg OPTIONS/cseq=319922 (tdta0x7f8918013500) on fork 0
24-04-2017 07:20:32.545 UTC Debug sproutletproxy.cpp:1820: Removing message 0x7f8918013b10 => txdata 0x7f89180135a8 mapping
24-04-2017 07:20:32.545 UTC Info sproutletproxy.cpp:818: Loop detected - rejecting request with 483 status code
24-04-2017 07:20:32.545 UTC Verbose sproutletproxy.cpp:2140: Routing Response msg 483/OPTIONS/cseq=319922 (tdta0x7f8918076070) (297 bytes) to upstream sproutlet registrar:
--start msg--

SIP/2.0 483 Too Many Hops
Via: SIP/2.0/TCP 192.168.38.55;rport=34358;received=192.168.38.55;branch=z9hG4bK-319922
Call-ID: poll-sip-319922
From: "poll-sip" <sip:poll-sip at 192.168.38.55>;tag=319922
To: <sip:poll-sip at 192.168.38.55>;tag=z9hG4bK-319922
CSeq: 319922 OPTIONS
Content-Length:  0


--end msg--



THx

Kr

Juan


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juan Sepulveda
Sent: mardi 18 avril 2017 10:02
To: 'clearwater at lists.projectclearwater.org'
Subject: [Project Clearwater] Sprout scratch

Hi,

Since Friday for an unknown reason, the sprout start to scratch permanently. We can see in the log internal loop and the poll_sprout_sip process restarting.

Logs are in attached file. Please can you debug this issue?

The only actions we performed last Friday is to fix a etcd process failure and we add 2 lines in the shared_config file:

billing_realm=ims02.as20650.net
cdf_identity=192.169.38.56

Currently, those 2 lines are commented.
In the attached file, you have also the version and the monit command output.

Let me know if you need more info.

Thx

Kr

Juan


IP Nexia, a new brand of Toledo Telecom.
[Description: http://www.ipnexia.com/images/pix.gif]

Juan Sepulveda
Voip and Network Engineer
M + 32 478 97 98 79   T +32 2 600 16 69   @ jsepulveda at ipnexia.com<mailto:jsepulveda at ipnexia.com>



IP Nexia Kouterveldstraat, 2 - 1831 Diegem
Tel +32 2 648 08 48   Fax +32 2 646 44 24   www.ipnexia.com<http://www.ipnexia.com/>
Disclaimer<http://www.ipnexia.com/disclaimer/disclaimer.html>

[Description: http://www.ipnexia.com/images/back-sign2_02.jpg]

[Description: http://www.ipnexia.com/images/back-sign2_03.jpg]



> Please consider the environmental impact of needlessly printing this e-mail.



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/940faa4c/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 171 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/940faa4c/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 4448 bytes
Desc: image002.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/940faa4c/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.jpg
Type: image/jpeg
Size: 2618 bytes
Desc: image003.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/940faa4c/attachment-0001.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image004.png
Type: image/png
Size: 170 bytes
Desc: image004.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170425/940faa4c/attachment-0001.png>

From Andrew.Edmonds at metaswitch.com  Wed Apr 26 10:55:30 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Wed, 26 Apr 2017 14:55:30 +0000
Subject: [Project Clearwater] All-in-one Images setup supports I-CSCF
 functionality ?
In-Reply-To: <TU4PR84MB006408AEDEAAE47F1248F38BDC1E0@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>
References: <TU4PR84MB006408AEDEAAE47F1248F38BDC1E0@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>
Message-ID: <BLUPR02MB437547202AFFE1859E9C2C9E5110@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Aravind,

Thank you for your question.

To understand why the I-CSCF processing does not happen on an All-in-one Clearwater deployment it would help to think about what the role of the I-CSCF is.

The I-CSCF is responsible for finding which S-SCSF is serving the requested subscriber. A real World IMS deployment may contain multiple S-SCSCFs, only one S-CSCF gets assigned to each registered subscriber and it is the I-CSCF's responsibility to find out which S-CSCF that is and to route messages on to it.

On a Clearwater All-in-one node we know that there is only one single S-CSCF, and that all subscribers must be served by this S-CSCF. This means there is no use in having any I-CSCF processing, all messages can simply be routed to the one single S-CSCF.

This is why you are not seeing any I-CSCF processing in the sprout log you have provided. This does not however explain why you are receiving a 404 error.

A 404 error typically means the subscriber which you are attempting to register has not been provisioned in the HSS. Can you log onto Ellis by going to http://<aio-identity> and verify the subscriber you are attempting to register does exist.

If everything does look okay and registers are still failing can you please provide the full sprout log for the time that the register was attempted.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shirabur, Aravind Ashok (CMS)
Sent: Tuesday, April 25, 2017 8:36 AM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] All-in-one Images setup supports I-CSCF functionality ?

Hi,

All-in-one Images setup supports  I-CSCF functionality ?

Trying to REGISTER the X-Lite UE to clearwater (clearwater-infrastructure  1.0-160518.173307) IMS network, registration fails with error response -404. After analyzing the sprout logs I-CSCF is not involved in the REGISTER path, instead the REGISTER message delivered directly to S-CSCF.  How to configure the I-CSCF ? attaching the 'shared_config' file

"All-in-one Images" setup is trying to integrate with HPE-HSS


24-04-2017 11:41:57.508 UTC Debug basicproxy.cpp:92: Process REGISTER request
24-04-2017 11:41:57.508 UTC Verbose sproutletproxy.cpp:503: Sproutlet Proxy transaction (0x26ca080) created
24-04-2017 11:41:57.508 UTC Debug basicproxy.cpp:1271: Report SAS start marker - trail (c)
24-04-2017 11:41:57.508 UTC Debug pjutils.cpp:674: Cloned Request msg REGISTER/cseq=1 (rdata0x7f662408b3f8) to tdta0x26ca490
24-04-2017 11:41:57.508 UTC Debug pjsip:   tsx0x26cc918 Transaction created for Request msg REGISTER/cseq=1 (rdata0x7f662408b3f8)
24-04-2017 11:41:57.508 UTC Debug pjsip:   tsx0x26cc918 Incoming Request msg REGISTER/cseq=1 (rdata0x7f662408b3f8) in state Null
24-04-2017 11:41:57.508 UTC Debug pjsip:   tsx0x26cc918 State changed from Null to Trying, event=RX_MSG
24-04-2017 11:41:57.508 UTC Debug basicproxy.cpp:213: tsx0x26cc918 - tu_on_tsx_state UAS, TSX_STATE RX_MSG state=Trying
24-04-2017 11:41:57.508 UTC Debug pjsip:       endpoint Response msg 408/REGISTER/cseq=1 (tdta0x26cd080) created
24-04-2017 11:41:57.508 UTC Debug sproutletproxy.cpp:124: Find target Sproutlet for request
24-04-2017 11:41:57.508 UTC Debug sproutletproxy.cpp:163: Found next routable URI: sip:scscf.cwaio:5052;transport=TCP;lr;orig
24-04-2017 11:41:57.508 UTC Debug sproutletproxy.cpp:334: Possible service name - scscf
24-04-2017 11:41:57.508 UTC Debug sproutletproxy.cpp:340: Hostname - cwaio
24-04-2017 11:41:57.508 UTC Debug scscfsproutlet.cpp:381: S-CSCF Transaction (0x26ce1c0) created
24-04-2017 11:41:57.508 UTC Verbose sproutletproxy.cpp:1159: Created Sproutlet scscf-0x26ce1c0 for Request msg REGISTER/cseq=1 (tdta0x26ca490)
24-04-2017 11:41:57.508 UTC Verbose sproutletproxy.cpp:2067: Routing Request msg REGISTER/cseq=1 (tdta0x26ca490) (866 bytes) to downstream sproutlet scscf:
--start msg--

REGISTER sip:cmshpe.com SIP/2.0^M
Via: SIP/2.0/TCP 192.168.1.176:55337;rport=55337;received=192.168.1.176;branch=z9hG4bKPjOv5TCMYJtX7PPAJ9XwpHOz1vjbLgNO8N^M
Path: <sip:3uCCzSme1F at 192.168.1.176:5058;transport=TCP;lr;ob>^M
Via: SIP/2.0/TCP 192.168.1.14:9000;rport=65430;received=192.168.1.14;branch=z9hG4bK-524287-1---d346e05081a6b464^M
Max-Forwards: 70^M
Contact: <sip:7406246263 at 192.168.1.14:9000;transport=tcp;rinstance=bbeada83830e3779>^M
To: "Aravind" <sip:7406246263 at cmshpe.com>^M
From: "Aravind" <sip:7406246263 at cmshpe.com>;tag=64cd157d^M
Call-ID: 84253MzY0OGViMzc4YWMxYmU4NGM1YThjYTk4N2RjOWZiNWI^M
CSeq: 1 REGISTER^M
Expires: 3600^M
Allow: SUBSCRIBE, NOTIFY, INVITE, ACK, CANCEL, BYE, REFER, INFO, OPTIONS, MESSAGE^M
User-Agent: X-Lite release 4.9.8 stamp 84253^M
P-Visited-Network-ID: cmshpe.com^M
Route: <sip:scscf.cwaio:5052;transport=TCP;lr;orig>^M
Content-Length:  0^M

Thanks
Aravind

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170426/7e6a29f2/attachment.html>

From maatoug.wassim at gmail.com  Mon Apr 24 09:20:23 2017
From: maatoug.wassim at gmail.com (maatoug wassim)
Date: Mon, 24 Apr 2017 15:20:23 +0200
Subject: [Project Clearwater] images and flavors for clearwater components
Message-ID: <CAM61SbxdbXr+DwNSBWuZSEEzyEsHhwKZJFEv3N=ciR_VuFoH9g@mail.gmail.com>

Hi,

What are the supported images and flavors for clearwater components please?

Regards,
Wassim
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170424/e787dfad/attachment.html>

From Andrew.Edmonds at metaswitch.com  Wed Apr 26 12:36:31 2017
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Wed, 26 Apr 2017 16:36:31 +0000
Subject: [Project Clearwater] Sprout processing time
In-Reply-To: <CAB9Pg_qBTPmzF+hx4WmepV7Vt5Nhx34Vn4Jvb0SB1k=Rgjxf4A@mail.gmail.com>
References: <CAB9Pg_qBTPmzF+hx4WmepV7Vt5Nhx34Vn4Jvb0SB1k=Rgjxf4A@mail.gmail.com>
Message-ID: <BLUPR02MB437AA8012A896CA9E95A0D1E5110@BLUPR02MB437.namprd02.prod.outlook.com>

Hi Chess,

There is a config option ?target_latency_us? which is maximum time for Sprout to process requests, after which Sprout will start rejecting some requests. This defaults to 100 milliseconds. You could change this though by entering into /etc/clearwater/shared_config the line:

target_latency_us=<new value>

and then running the command ?sudo cw-upload_shared_config?.

You can read more about Sprout performance and throttling at our blog post here<http://www.projectclearwater.org/clearwater-performance-and-our-load-monitor/>.

Please let me know if you have any further questions.

Thanks,

Andrew

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of chess man
Sent: Wednesday, April 19, 2017 10:33 AM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Sprout processing time

Hi clearwater team,
Can you provide me with the maximum processing time allowed for Sprout.

Many thanks,
Chess

[Image removed by sender.]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>

Garanti sans virus. www.avast.com<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170426/c01317ff/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 350 bytes
Desc: image001.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170426/c01317ff/attachment.jpg>

From rajeshkvs at gmail.com  Thu Apr 27 05:55:43 2017
From: rajeshkvs at gmail.com (rajeshkvs .)
Date: Thu, 27 Apr 2017 15:25:43 +0530
Subject: [Project Clearwater] using CW-AIO deployment
Message-ID: <CAMebKSOwgfi3Py5t8F5coUhmRbRbnPvCqOFQjvANzFx5NoC6mA@mail.gmail.com>

Hi all,

 I am trying to explore CW-AIO deployment. I did the setup successfully and
could make the successful call as well. But i am facing one issue,

I am using Oracle Virtual Box to deploy CW-AIO,

1. When VM deployed with Network as "NAT". I could register and make the
call between two clients.

2. But when i deploy VM with network as "Bridged". I am not able to
register, i am getting 403 Forbidden error. Even in Ellis i could not see
the proper dashboard as i view in NAT mode.

Looks like we may need to do some additional config to make it work with
"Bridged" mode.

As i am planning to integrate with another TAS server, i want this setup to
work in Bridged so that i can configure SIP trunk to this setup.

Kindly help.

Regards,
Rajesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170427/7a69e917/attachment.html>

From hkaranjikar at apm.com  Fri Apr 28 03:03:21 2017
From: hkaranjikar at apm.com (Hrishikesh Karanjikar)
Date: Fri, 28 Apr 2017 12:33:21 +0530
Subject: [Project Clearwater] Unable to register using Zoiper client with AIO
Message-ID: <CABmBNEa=uH6UyL_f5+XfOAd46g14zJcvkTyXzM12zEFxJSMZ5Q@mail.gmail.com>

Hi,

======================================================================
I have downloaded AIO from http://vm-images.cw-ngv.com/cw-aio.ova
I am using VirtualBox version 4.3.36_Ubuntu r105129 to run above image.
My operating system is Ubuntu 14.04 LTS 64 bit.
My Zopier 64 bit client (Version - 3.3.25608)
======================================================================

I am able to access Ellis as well as able to ssh to VM from my Ubuntu Host.
How ever I am unable to register using Zopier client.

The credentials provided by Ellis which I am using for registration are as
follows,

======================================================================
User / User @host : 6505550691 at example.com
Password : My Password
Domain /  Outbound Proxy - localhost:8060
======================================================================

The errors I am getting from sproute are as follows,

======================================================================
25-04-2017 12:35:19.526 UTC Status alarm.cpp:62: sprout issued 1001.1 alarm
25-04-2017 12:35:19.526 UTC Status alarm.cpp:62: sprout issued 1002.1 alarm
25-04-2017 12:35:27.935 UTC Error httpclient.cpp:712: cURL failure with
cURL error code 0 (see man 3 libcurl-errors) and HTTP error code 404
25-04-2017 12:35:27.935 UTC Error hssconnection.cpp:148: Failed to get
Authentication Vector for 6505550691
========================================================================

In my Zoiper client I see following message,

SIP 408 - Request Timeout.

When I capture packets on my host using wireshark,

I see Zoiper client sends register request.
But I get 401 Unauthorized as reply.
For 2nd register request I get 403 Forbiden as reply.

Please guide me what is wrong?

Thanks
Hrishikesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20170428/35790839/attachment.html>

