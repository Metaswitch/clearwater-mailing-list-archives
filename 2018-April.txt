From Robert.Day at metaswitch.com  Tue Apr  3 12:20:04 2018
From: Robert.Day at metaswitch.com (Robert Day)
Date: Tue, 3 Apr 2018 16:20:04 +0000
Subject: [Project Clearwater] Is there tutorial guide to use OpenIMS
 Core's HSS to configure service profile?
In-Reply-To: <CA+pBo5Ex8B3Zh+rv-9MiqLTr9MqGZnfnPESCzwoum0j76mo69w@mail.gmail.com>
References: <CA+pBo5Ex8B3Zh+rv-9MiqLTr9MqGZnfnPESCzwoum0j76mo69w@mail.gmail.com>
Message-ID: <BY2PR02MB2149C1CC4778890249E5A65BF4A50@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Anthony,

If the Clearwater integration documentation at http://clearwater.readthedocs.io/en/stable/OpenIMSCore_HSS_Integration.html#configuring-subscribers isn?t what you need, then maybe OpenIMSCore?s own documentation will help ? that?s at http://www.openimscore.com/documentation/installation-guide/#step7.

There are also some Youtube videos showing application servers being set up in OpenIMSCore HSS ? see https://youtu.be/3nVRpzkUbIg?t=3m47s, for example.

Best,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Anthony Lee
Sent: 28 March 2018 20:24
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Is there tutorial guide to use OpenIMS Core's HSS to configure service profile?

Hi,

Is there anyone also try to integrate Clearwater with OpenIMS HSS?
I found it difficult to use web interface to setup a service profile for a subscribe.
Is there guide I could follow? Is there any example?


Thanks
Anthony
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180403/42698700/attachment.html>

From Robert.Day at metaswitch.com  Tue Apr  3 12:20:03 2018
From: Robert.Day at metaswitch.com (Robert Day)
Date: Tue, 3 Apr 2018 16:20:03 +0000
Subject: [Project Clearwater] [clearwater] Some calls failed with 480
 when do SIPp testing
In-Reply-To: <3DE744999C06324789FFFC2A0230599701A01D09@dggemi502-mbs.china.huawei.com>
References: <HK0PR03MB2755EBE5561B41362DC44420B9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>,
	<HK0PR03MB2755B712AF0925DFBE3F0070B9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>
	<HK0PR03MB27553BFBF07E1FA23D56D81BB9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>
	<3DE744999C06324789FFFC2A0230599701A01D09@dggemi502-mbs.china.huawei.com>
Message-ID: <BY2PR02MB21492C7E1D2F45AEA4AA3ED5F4A50@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Linda,

The ?multiplier? parameter increases the rate of activity for each subscriber ? if each subscriber normally makes 0.65 calls per hour and re-registers every 30 minutes, --multiplier 10 will mean they make 6.5 calls per hour and re-register every 3 minutes. This is effectively a way to generate more load with fewer subscribers (decreasing the amount of provisioning time needed).

If --multiplier 10 is making your calls work, it may be that you haven?t configured Clearwater?s registration expiry time correctly ? as http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html#deploying-a-stress-node says, ?You should also ensure that the reg_max_expires setting is set to 1800 rather than the default of 300?. If you haven?t done that, then registrations will time out too early, causing 480 Temporarily Unavailable errors when we try and make calls to subscribers with expired registrations. Increasing the multiplier means subscribers re-register more often, so happens to avoid this problem.

?ccf? is an IMS Charging Collection Function ? the network element that Ralf reports billing events to, as described in http://www.projectclearwater.org/rf-billing/. If you don?t have one, you can ignore that parameter.

Hope that helps!

Best,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Wangwulin (Linda)
Sent: 28 March 2018 11:03
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] [clearwater] Some calls failed with 480 when do SIPp testing

Hi,

When I set an additional param ?--multiplier 10? or other numbers (not the default 1), all the calls succeeded.

/usr/share/clearwater/bin/run_stress clearwater.opnfv 1000 10 --sipp-output --icscf-target 10.67.79.12:5052 --scscf-target 10.67.79.12:5054 --base-number 2010000000 --multiplier 10  --ccf 10.67.79.11

Total calls: 1083
Successful calls: 1083 (100.0%)
Failed calls: 0 (0.0%)
Unfinished calls: 0

Two questions:
1) What is ?multiplier? used for?
2) What does ?ccf? refer to? Actually the ip of ccf here is where the ralf service runs on my deployment. Is it necessary here?


Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com<mailto:wangwulin at hotmail.com>>
????: 2018?3?28? 14:40
???: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
??: [clearwater] Some calls failed with 480 when do SIPp testing


Hi clearwater team,



The successful call rate is quite low when I do SIPp testing via the command: /usr/share/clearwater/bin/run_stress clearwater.opnfv 100 30 --sipp-output --icscf-target sprout.clearwater.local:5052 --scscf-target sprout.clearwater.local:5054

Actually after 7 successful calls, the 480 error occurs.

Here is one capture when runing:

------------------------------ Scenario Screen -------- [1-9]: Change Screen --
  Call-rate(length)   Port   Total-time  Total-calls  Remote-host
0.0(5000 ms)/1.000s   5062    1234.12 s           22  10.67.79.12:5054(TCP)

  0 new calls during 1.004 s period      1 ms scheduler resolution
  0 calls (limit 1)                      Peak was 1 calls, after 55 s
  1 Running, 3 Paused, 3 Woken up
  0 dead call msg (discarded)            0 out-of-call msg (discarded)
  2 open sockets

                                 Messages  Retrans   Timeout   Unexpected-Msg
      INVITE ---------->         22        0         0
         100 <----------         22        0         0         0
         183 <----------         12        0         0         10
       PRACK ---------->         12        0
         200 <----------         12        0         0         0
      UPDATE ---------->         12        0
         201 <----------         12        0         0         0
         180 <----------  E-RTD1 12        0         0         0
         201 <----------         0         0         0         0
         200 <----------         12        0         0         0
         ACK ---------->         12        0
       Pause [   5000ms]         12                            0
         BYE ---------->         12        0         0
         200 <----------         12        0         0         0

------ [+|-|*|/]: Adjust rate ---- [q]: Soft exit ---- [p]: Pause traffic -----

Last Error: Aborting call on unexpected message for Call-Id '21-14814 at 10...


2018-03-28      06:33:54.190938 1522218834.190938: Aborting call on unexpected message for Call-Id '17-14814 at 10.67.79.24': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.24:29014;received=10.67.79.24;branch=z9hG4bK-14814-17-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 17-14814 at 10.67.79.24<mailto:17-14814 at 10.67.79.24>
From: <sip:2010000012 at clearwater.opnfv>;tag=14814SIPpTag0017
To: <sip:2010000015 at clearwater.opnfv>;tag=z9hG4bK-14814-17-0
CSeq: 1 INVITE
P-Charging-Vector: icid-value="14814SIPpTag0017";orig-ioi=clearwater.opnfv;term-ioi=clearwater.opnfv
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0

'.




1) I deoloyed clearwater via one testcase named "cloudify_ims" from opnfv/functest project, where 3 steps are run:

   * deploy a VNF orchestrator (Cloudify)

   * deploy a Clearwater vIMS (IP Multimedia Subsystem) VNF from this orchestrator based on a TOSCA blueprint defined in [1]
   * run suite of signaling tests on top of this VNF

[1]: https://github.com/Orange-OpenSource/opnfv-cloudify-clearwater/archive/master.zip



8 instances are created and I also created a new instance named "stress-node" according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html-
bash-4.4# openstack server list
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+
| ID                                   | Name                                                  | Status | Networks                                                                              | Image                | Flavor    |
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+
| df2d9f82-8aa2-4514-9f07-27974267b590 | stress-node                                           | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.24                 | ubuntu_14.04         | m1.small  |
| 48a41da3-c55d-410c-a8f3-ef26bc409aef | server_clearwater-opnfv_bono_host_llp4mt              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.23, 192.168.36.113 | ubuntu_14.04         | m1.small  |
| a992435c-a397-4622-b0d3-b8e515ebab51 | server_clearwater-opnfv_homer_host_vz97vi             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.19                 | ubuntu_14.04         | m1.small  |
| 283fef46-78e4-4d0a-9e91-ffeb14e7bfb9 | server_clearwater-opnfv_sprout_host_nnoxye            | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.12                 | ubuntu_14.04         | m1.small  |
| bfe5111d-ed7d-4c65-80c2-20144935ee8c | server_clearwater-opnfv_ellis_host_bo7auh             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.14, 192.168.36.111 | ubuntu_14.04         | m1.small  |
| b83e1727-b5a6-430e-8b5e-b3f0e6421675 | server_clearwater-opnfv_vellum_host_sh0ocy            | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.6                  | ubuntu_14.04         | m1.small  |
| 8b03d8fa-5873-489b-994b-5f1de24a85c0 | server_clearwater-opnfv_bind_host_b9f49w              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.16, 192.168.36.110 | ubuntu_14.04         | m1.small  |
| 0ca72f55-7e05-4477-ad2e-2661ad49f7ce | server_clearwater-opnfv_dime_host_xryxen              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.11                 | ubuntu_14.04         | m1.small  |
| 7d51e479-24b6-4db4-9b8f-3ef42be0b87e | server_clearwater-opnfv_proxy_host_rqr7bo             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.5                  | ubuntu_14.04         | m1.small  |
| 82672984-b967-49ff-8f00-09ffe3f7fc87 | cloudify_manager-6c79129a-8384-4069-81f7-a024738102cd | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.13, 192.168.36.105 | cloudify_manager_4.0 | m1.medium |
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+


2) I checked  the ralf log
root at dime-au6gte:/var/log/ralf# vim ralf_20180328T060000Z.txt
28-03-2018 06:25:57.063 UTC Debug dnscachedresolver.cpp:543: Create and execute DNS query transaction
28-03-2018 06:25:57.063 UTC Debug dnscachedresolver.cpp:556: Wait for query responses
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:704: Received DNS response for _diameter._tcp.clearwater.opnfv type SRV
28-03-2018 06:25:57.064 UTC Warning dnscachedresolver.cpp:846: Failed to retrieve record for _diameter._tcp.clearwater.opnfv: Domain name not found
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:946: Adding _diameter._tcp.clearwater.opnfv to cache expiry list with deletion time of 1522218957
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:704: Received DNS response for _diameter._sctp.clearwater.opnfv type SRV
28-03-2018 06:25:57.064 UTC Warning dnscachedresolver.cpp:846: Failed to retrieve record for _diameter._sctp.clearwater.opnfv: Domain name not found
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:946: Adding _diameter._sctp.clearwater.opnfv to cache expiry list with deletion time of 1522218957
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:560: Received all query responses
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:588: Pulling 0 records from cache for _diameter._tcp.clearwater.opnfv SRV
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:588: Pulling 0 records from cache for _diameter._sctp.clearwater.opnfv SRV
28-03-2018 06:25:57.064 UTC Debug diameterresolver.cpp:131: TCP SRV record _diameter._tcp.clearwater.opnfv returned 0 records
28-03-2018 06:25:57.064 UTC Debug diameterresolver.cpp:134: SCTP SRV record _diameter._sctp.clearwater.opnfv returned 0 records
28-03-2018 06:25:57.064 UTC Error diameterstack.cpp:864: No Diameter peers have been found
28-03-2018 06:25:57.068 UTC Verbose httpstack.cpp:345: Process request for URL /ping, args (null)
28-03-2018 06:25:57.068 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /ping, args (null)
28-03-2018 06:25:58.027 UTC Verbose httpstack.cpp:345: Process request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:128: Handling request, body:
{
    "peers": {
        "ccf": [
            ""
        ]
    },
    "event": {
        "Accounting-Record-Type": 1,
        "Event-Timestamp": 1522218357,
        "Service-Information": {
            "Subscription-Id": [
                {
                    "Subscription-Id-Type": 2,
                    "Subscription-Id-Data": "sip:2010000020 at clearwater.opnfv"
                }
            ],
            "IMS-Information": {
                "Event-Type": {
                    "SIP-Method": "REGISTER",
                    "Expires": 3600
                },
                "Role-Of-Node": 0,
                "Node-Functionality": 1,
                "User-Session-Id": "2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24>",
                "Called-Party-Address": "sip:2010000020 at clearwater.opnfv",
                "Associated-URI": [
                    "sip:2010000020 at clearwater.opnfv"
                ],
                "Time-Stamps": {
                    "SIP-Request-Timestamp": 1522218357,
                    "SIP-Request-Timestamp-Fraction": 960,
                    "SIP-Response-Timestamp": 1522218357,
                    "SIP-Response-Timestamp-Fraction": 967
                },
                "IMS-Charging-Identifier": "",
                "Cause-Code": -1,
                "From-Address": "<sip:2010000020 at clearwater.opnfv>;tag=8708SIPpTag0014990",
                "Route-Header-Received": "<sip:clearwater.opnfv;transport=TCP;lr>",
                "Route-Header-Transmitted": "<sip:icscf.sprout.clearwater.local:5052;transport=TCP;lr;orig>",
                "Instance-Id": "<urn:uuid:00000000-0000-0000-0000-000000000001>"
            }
        }
    }
}
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:244: Adding CCF
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:82: Handle the received message
28-03-2018 06:25:58.027 UTC Debug peer_message_sender.cpp:84: Sending message to  (number 0)
28-03-2018 06:25:58.027 UTC Debug rf.cpp:63: Building an Accounting-Request
28-03-2018 06:25:58.027 UTC Debug freeDiameter: No Session-Id AVP found in message 0x7f764c008d50
28-03-2018 06:25:58.027 UTC Verbose diameterstack.cpp:1470: Sending Diameter message of type 271 on transaction 0x7f764c008d00
28-03-2018 06:25:58.027 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.027 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
28-03-2018 06:25:58.027 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm clearwater.opnfv
28-03-2018 06:25:58.027 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
28-03-2018 06:25:58.027 UTC Debug freeDiameter: Calling callback registered when query was sent (0x43cb30, 0x7f764c008d00)
28-03-2018 06:25:58.027 UTC Verbose diameterstack.cpp:1130: Got Diameter response of type 271 - calling callback on transaction 0x7f764c008d00
28-03-2018 06:25:58.027 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
28-03-2018 06:25:58.027 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
28-03-2018 06:25:58.027 UTC Warning session_manager.cpp:414: Session for 2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24> received error from CDF
28-03-2018 06:25:58.032 UTC Verbose httpstack.cpp:345: Process request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:128: Handling request, body:
{
    "peers": {
        "ccf": [
            ""
        ]
    },
    "event": {
        "Accounting-Record-Type": 1,
        "Event-Timestamp": 1522218357,
        "Service-Information": {
            "Subscription-Id": [
                {
                    "Subscription-Id-Type": 2,
                    "Subscription-Id-Data": "sip:2010000021 at clearwater.opnfv"
                }
            ],
            "IMS-Information": {
                "Event-Type": {
                    "SIP-Method": "REGISTER",
                    "Expires": 3600
                },
                "Role-Of-Node": 0,
                "Node-Functionality": 1,
                "User-Session-Id": "2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24>",
                "Called-Party-Address": "sip:2010000021 at clearwater.opnfv",
                "Associated-URI": [
                    "sip:2010000021 at clearwater.opnfv"
                ],
                "Time-Stamps": {
                    "SIP-Request-Timestamp": 1522218357,
                    "SIP-Request-Timestamp-Fraction": 968,
                    "SIP-Response-Timestamp": 1522218357,
                    "SIP-Response-Timestamp-Fraction": 974
                },
                "IMS-Charging-Identifier": "",
                "Cause-Code": -1,
                "From-Address": "<sip:2010000021 at clearwater.opnfv>;tag=8708SIPpTag0014990",
                "Route-Header-Received": "<sip:clearwater.opnfv;transport=TCP;lr>",
                "Route-Header-Transmitted": "<sip:icscf.sprout.clearwater.local:5052;transport=TCP;lr;orig>",
                "Instance-Id": "<urn:uuid:00000000-0000-0000-0000-000000000001>"
            }
        }
    }
}
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:244: Adding CCF
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:82: Handle the received message
28-03-2018 06:25:58.032 UTC Debug peer_message_sender.cpp:84: Sending message to  (number 0)
28-03-2018 06:25:58.032 UTC Debug rf.cpp:63: Building an Accounting-Request
28-03-2018 06:25:58.032 UTC Debug freeDiameter: No Session-Id AVP found in message 0x7f764c001310
28-03-2018 06:25:58.032 UTC Verbose diameterstack.cpp:1470: Sending Diameter message of type 271 on transaction 0x7f764c006f30
28-03-2018 06:25:58.032 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.032 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
28-03-2018 06:25:58.032 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm clearwater.opnfv
28-03-2018 06:25:58.032 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
28-03-2018 06:25:58.032 UTC Debug freeDiameter: Calling callback registered when query was sent (0x43cb30, 0x7f764c006f30)
28-03-2018 06:25:58.032 UTC Verbose diameterstack.cpp:1130: Got Diameter response of type 271 - calling callback on transaction 0x7f764c006f30
28-03-2018 06:25:58.032 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
28-03-2018 06:25:58.032 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
28-03-2018 06:25:58.032 UTC Warning session_manager.cpp:414: Session for 2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24> received error from CDF

Have I missed something in shared_config?

root at dime-au6gte:/var/log/ralf# cat /etc/clearwater/shared_config
# Deployment definitions
home_domain=clearwater.opnfv
sprout_hostname=sprout.clearwater.local
chronos_hostname=10.67.79.16:7253
hs_hostname=hs.clearwater.local:8888
hs_provisioning_hostname=hs-prov.clearwater.local:8889
sprout_impi_store=vellum.clearwater.local
sprout_registration_store=vellum.clearwater.local
cassandra_hostname=vellum.clearwater.local
chronos_hostname=vellum.clearwater.local
ralf_session_store=vellum.clearwater.local
ralf_hostname=ralf.clearwater.local:10888
xdms_hostname=homer.clearwater.local:7888
signaling_dns_server=10.67.79.16
snmp_ip=10.67.79.11
reg_max_expires=1800
bgcf=0



# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


root at dime-au6gte:~# cat /etc/clearwater/local_config
local_ip=10.67.79.11
public_ip=
public_hostname=dime-au6gte.clearwater.local



homestead seems fine except one line "Failed TWO read for get_row. Try ONE":

root at dime-au6gte:/var/log/homestead# vim homestead_20180328T060000Z.txt
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:425: Attempt to parse vellum.clearwater.local as IP address
28-03-2018 06:30:58.045 UTC Verbose dnscachedresolver.cpp:486: Check cache for vellum.clearwater.local type 1
28-03-2018 06:30:58.045 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for vellum.clearwater.local A
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:231: Request for connection to IP: 10.67.79.6, port: 9160
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:244: Found existing connection 0x7fed8c0115a0 in pool
28-03-2018 06:30:58.045 UTC Debug cassandra_store.cpp:159: Generated Cassandra timestamp 1522218658045331
28-03-2018 06:30:58.045 UTC Debug cache.cpp:350: Issuing get for key sip:2010000021 at clearwater.opnfv
28-03-2018 06:30:58.045 UTC Debug cassandra_store.cpp:731: Failed TWO read for get_row. Try ONE
28-03-2018 06:30:58.045 UTC Debug cache.cpp:370: Retrieved XML column with TTL 0 and value <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>>
28-03-2018 06:30:58.045 UTC Debug cache.cpp:388: Retrieved is_registered column with value False and TTL 0
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1245: Got IMS subscription from cache
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1260: TTL for this database record is 0, IMS Subscription XML is not empty, registration state is UNREGISTERED, and the charging addresses are empty
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1510: Rejecting deregistration for user who was not registered
28-03-2018 06:30:58.045 UTC Verbose httpstack.cpp:93: Sending response 400 to request for URL /impu/sip%3A2010000021%40clearwater.opnfv/reg-data, args (null)
28-03-2018 06:30:58.046 UTC Debug cache.cpp:370: Retrieved XML column with TTL 0 and value <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>>
28-03-2018 06:30:58.046 UTC Debug cache.cpp:388: Retrieved is_registered column with value False and TTL 0
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1245: Got IMS subscription from cache
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1260: TTL for this database record is 0, IMS Subscription XML is not empty, registration state is UNREGISTERED, and the charging addresses are empty
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1286: Subscriber registering with new binding
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1457: Handling initial registration
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1654: Attempting to cache IMS subscription for public IDs
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1658: Got public IDs to cache against - doing it
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1663: Public ID sip:2010000021 at clearwater.opnfv
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:159: Generated Cassandra timestamp 1522218658046440
28-03-2018 06:30:58.046 UTC Debug a_record_resolver.cpp:80: ARecordResolver::resolve_iter for host vellum.clearwater.local, port 9160, family 2
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:425: Attempt to parse vellum.clearwater.local as IP address
28-03-2018 06:30:58.046 UTC Verbose dnscachedresolver.cpp:486: Check cache for vellum.clearwater.local type 1
28-03-2018 06:30:58.046 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for vellum.clearwater.local A
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:231: Request for connection to IP: 10.67.79.6, port: 9160
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:244: Found existing connection 0x7fed8c0115a0 in pool
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:612: Constructing cassandra put request with timestamp 1522218658046440 and per-column TTLs
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:629:   ims_subscription_xml => <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>> (TTL 0)
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:629:   is_registered =>  (TTL 0)
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:650: Executing put request operation
28-03-2018 06:30:58.047 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.047 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.047 UTC Debug handlers.cpp:1567: Sending 200 response (body was {"reqtype": "reg", "server_name": "sip:scscf.sprout.clearwater.local:5054;transport=TCP"})
28-03-2018 06:30:58.047 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /impu/sip%3A2010000021%40clearwater.opnfv/reg-data, args private_id=2010000021%40clearwater.opnfv



The error about "Could not get subscriber data from HSS" on Sprout node occurred sometimes.

root at sprout-2tl8g3:/var/log/sprout# vim sprout_20180328T060000Z.txt
28-03-2018 06:36:11.741 UTC Error httpclient.cpp:712: cURL failure with cURL error code 0 (see man 3 libcurl-errors) and HTTP error code 400
28-03-2018 06:36:11.741 UTC Error hssconnection.cpp:704: Could not get subscriber data from HSS
28-03-2018 06:36:25.875 UTC Status alarm.cpp:62: sprout issued 1004.1 alarm




root at dime-au6gte:/var/log/homestead# monit summary
Monit 5.18.1 uptime: 2h 27m
 Service Name                     Status                      Type
 node-dime-au6gte.clearwater....  Running                     System
 snmpd_process                    Running                     Process
 ralf_process                     Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 ralf_uptime                      Status ok                   Program
 poll_ralf                        Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


Any help/suggestion would be much appreciated.




Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180403/4e37e17d/attachment.html>

From Robert.Day at metaswitch.com  Tue Apr  3 12:43:51 2018
From: Robert.Day at metaswitch.com (Robert Day)
Date: Tue, 3 Apr 2018 16:43:51 +0000
Subject: [Project Clearwater] Input required for High Availability use
 cases of clearwater
In-Reply-To: <372055626.178624.1522494458227@mail.yahoo.com>
References: <372055626.178624.1522494458227.ref@mail.yahoo.com>
	<372055626.178624.1522494458227@mail.yahoo.com>
Message-ID: <BY2PR02MB214907C243ADB037EBC12EB3F4A50@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Nagendra,

The high-availability approach you describe looks different from the one Clearwater is designed to support. Rather than having a Bono node become a combined Bono/Ellis node if the Ellis node fails, we have an approach where:

  *   You have pools of nodes of each type ? e.g. you might have 3 or 4 Sprout nodes, depending on load requirements
  *   Other components use DNS to balance load across those 4 Sprout nodes
  *   If one of the 4 Sprout nodes fails, it gets blacklisted ? and the other 3 Sprout nodes can handle the traffic meant for it, as the relevant state is stored in redundant data stores on Vellum

In this model, scaling in is as simple as removing a node from DNS and deleting it, and scaling out is as simple as adding a node and then adding it to DNS.

http://www.projectclearwater.org/technical/call-flows/ shows the call flows when Sprout, Dime etc. fail in more detail.

Best,
Rob


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Nagendra Kumar
Sent: 31 March 2018 12:08
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Input required for High Availability use cases of clearwater

Dear Team,

I have a high availability use case below, please provide your inputs.
I have 6 nodes running for clearwater services on 6 different nodes and 1 for DNS(on Node7) as below:
(I am using AWS AMI ubuntu 14 image and have manually installed as per instruction at Manual Install Instructions ? Project Clearwater 1.0 documentation<http://clearwater.readthedocs.io/en/stable/Manual_Install.html>)
Desired configuration :
Node1 : Ellis
Node2 : Bono
Node3 : Sprout
Node4 : Homer
Node5 : Dime
Node6 : Vellum


Now, we have a requirement of scale up and scale down.
Scale Down:
1. if Node1 fails, Ellis should start working on Node2. That means Node2 hosts Ellis and Bono both running.
2. After that if Node2 fails, Ellis and Bono should shift to Node3. That means Node3 hosts Ellis, Bono and Sprout.
3. And so on..... till Node5 fails and Node6 hosts Ellis, Bono, Sprout, Homer, Dime and Vellum.

Will it need any source code change or work with just configuration change? Please send me instruction for making changes.
I want to try the following for the above use case:
1. I install all software on each nodes. That means Ellis is installed on all nodes, sprout is installed on all nodes, etc.
2. But, in the beginning, Ellis is started on Node1, Bono is started on Node2 and so on as shown above (desired configuration).
3. When Node1  fails, then I start Ellis on Node 2(say manually, I will automate it later). That means Node2 has Ellis and Bono running.
4. When Node2 fails, I start Ellis and Bono on Node2...And the like...
5. In the end, I want Node6 to have all software running.

Scale UP:
1. After step 5 above, where Node6 are running all software.
2. Now, Node 5 has started and came up, I want to stop Dime on Node6 and want to start on Node5. That means Node5 has Dime running and Node6 has Ellis, Bono, Sprout, Homer, and Vellum running.
3. Node Node 4 has come up and running. I want to stop Homer on Node 6 and start it on Node4. That means Node 4 has Homer running, Node5 has Dime running and Node 6 has Ellis, Bono, Sprout and Vellum running.
4. And so on till I get the desired configuration running as shown above.
It should happen when call is running and call shouldn't drop.

Any help will be deeply appreciated.

Thanks
-Nagu

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180403/004968df/attachment.html>

From Robert.Day at metaswitch.com  Tue Apr  3 12:43:50 2018
From: Robert.Day at metaswitch.com (Robert Day)
Date: Tue, 3 Apr 2018 16:43:50 +0000
Subject: [Project Clearwater] Stress Testing Queries
In-Reply-To: <CAHwYWpB_y_vwKpaRHgOPbo4_8GciF6uk=KF1N-wO+U9G0fa=mA@mail.gmail.com>
References: <CAHwYWpB_y_vwKpaRHgOPbo4_8GciF6uk=KF1N-wO+U9G0fa=mA@mail.gmail.com>
Message-ID: <BY2PR02MB2149822CA8A1DA775EE6097FF4A50@BY2PR02MB2149.namprd02.prod.outlook.com>

Hi Sunil,

Answering your questions in order:


  1.  We don?t have fixed performance figures for Project Clearwater (as opposed to our Clearwater Core product) ? we just provide the tools to let the community do their own performance testing and analysis. This is partly because hardware varies so much (CPU architecture, speed, etc.) that it?s difficult to give precise figures.
  2.  When using the run_stress script, each emulated subscriber generates 1.3 calls/second (50% incoming, 50% outgoing). This can be increased with the ?multiplier? option (e.g. --multiplier 10 means 13 calls/second), but this also increases the registration rate ? to increase the calls per second independently of registration rate, you?d need to edit the script.
  3.  Call success depends on the stress node and all of Sprout, Dime and Vellum. http://www.projectclearwater.org/technical/call-flows/ has more information on how they all get involved in call flows.
  4.  It?s difficult to predict exactly where the bottlenecks would be, but in general, the Sprout node does the most work (and so either it will be the bottleneck or you?ll use more Sprout nodes than other nodes). Different configurations will affect this a lot, though ? if using homestead-prov rather than a HSS, the Cassandra database on Vellum will be doing more work, and if using Rf billing, the Dime node will be doing more work.
  5.  I don?t think we?ve tested a maximum value for --multiplier. My guess would be that it starts to break down if calls/registrations are happening faster than once per second per subscriber ? so 100 might be OK but 2000 probably wouldn?t be.
  6.  Going up to 2 vCPUs and 4GB RAM on each node would be sensible, but past that point we?d recommend increasing the number of nodes instead ? Clearwater is primarily architected to scale that way.
  7.  Our default throttling options should be well-tuned for most workloads ? you shouldn?t need to change them.

Hope that helps!

Best,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sunil Kumar
Sent: 29 March 2018 15:06
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Stress Testing Queries

Hi all,

I want to increase the # of calls per seconds. Please help me out. I have few doubt Please reply:

1) How many max calls per sec can be make using 1 vCPU and 2 GB RAM, 10 GB HDD, manual setup with stress test.

2) How can i increase the no. of calls per sec, Is there any specific parameter for that or Do I need to change in script (run_stress) or anywhere else please let me know.

3) I am using the stress node for making calls, I want to ask, On which nodes the calls or stress test is depends other than sprout and stress node. I have created the subscribers on vellum node.

4) # of calls is limited by stress node or sprout node or any other node?

5) What is the maximum limitation of --multiplier, i have checked up to 20 only?

6) Should I increase the CPUs, RAM and HDD on single node or Do I create new node for larger deployments (multiple nodes), which is better way?

7) I have not use any throttling options yet. Do I need to use them, if yes please let me know which for increasing the CPS, how can i use (just mentioned in shared_config?)

Please reply, i am waiting for it eagerly. Thanks in advance.

Regards,
suni

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180403/184c8da7/attachment.html>

From skgola1997 at gmail.com  Tue Apr  3 14:46:42 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Wed, 4 Apr 2018 00:16:42 +0530
Subject: [Project Clearwater] Stress Testing Queries
In-Reply-To: <BY2PR02MB2149822CA8A1DA775EE6097FF4A50@BY2PR02MB2149.namprd02.prod.outlook.com>
References: <CAHwYWpB_y_vwKpaRHgOPbo4_8GciF6uk=KF1N-wO+U9G0fa=mA@mail.gmail.com>
	<BY2PR02MB2149822CA8A1DA775EE6097FF4A50@BY2PR02MB2149.namprd02.prod.outlook.com>
Message-ID: <CAHwYWpB5JBnbQ+2BMV6OkFvpnsZuAAr5SuGA1OoW_X2avDmFwQ@mail.gmail.com>

Hi Rob,
Thanks for replying, So I have created the new node for sprout and vellum
(4GB RAM, 2CPUs) and added them to the existing deployment, but still there
is a same problem i face as earlier that no. of call per sec is not
increasing. When i am trying to increase them using multiplier it start
giving errors like 503, 401. time out. overloading etc.
SO according to you what will be the problem and how can i fix that.

As you are saying:

*? to increase the calls per second independently of registration rate,
you?d need to edit the script *

*What would i need to change, can u please tell me it would be great help.
I will appreciate that.*

Thanks in advance
Sunil

On Tue, Apr 3, 2018 at 10:13 PM, Robert Day <Robert.Day at metaswitch.com>
wrote:

> Hi Sunil,
>
>
>
> Answering your questions in order:
>
>
>
>    1. We don?t have fixed performance figures for Project Clearwater (as
>    opposed to our Clearwater Core product) ? we just provide the tools to let
>    the community do their own performance testing and analysis. This is partly
>    because hardware varies so much (CPU architecture, speed, etc.) that it?s
>    difficult to give precise figures.
>    2. When using the run_stress script, each emulated subscriber
>    generates 1.3 calls/second (50% incoming, 50% outgoing). This can be
>    increased with the ?multiplier? option (e.g. --multiplier 10 means 13
>    calls/second), but this also increases the registration rate ? to increase
>    the calls per second independently of registration rate, you?d need to edit
>    the script.
>    3. Call success depends on the stress node and all of Sprout, Dime and
>    Vellum. http://www.projectclearwater.org/technical/call-flows/ has
>    more information on how they all get involved in call flows.
>    4. It?s difficult to predict exactly where the bottlenecks would be,
>    but in general, the Sprout node does the most work (and so either it will
>    be the bottleneck or you?ll use more Sprout nodes than other nodes).
>    Different configurations will affect this a lot, though ? if using
>    homestead-prov rather than a HSS, the Cassandra database on Vellum will be
>    doing more work, and if using Rf billing, the Dime node will be doing more
>    work.
>    5. I don?t think we?ve tested a maximum value for --multiplier. My
>    guess would be that it starts to break down if calls/registrations are
>    happening faster than once per second per subscriber ? so 100 might be OK
>    but 2000 probably wouldn?t be.
>    6. Going up to 2 vCPUs and 4GB RAM on each node would be sensible, but
>    past that point we?d recommend increasing the number of nodes instead ?
>    Clearwater is primarily architected to scale that way.
>    7. Our default throttling options should be well-tuned for most
>    workloads ? you shouldn?t need to change them.
>
>
>
> Hope that helps!
>
>
>
> Best,
>
> Rob
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sunil Kumar
> *Sent:* 29 March 2018 15:06
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Stress Testing Queries
>
>
>
> Hi all,
>
>
>
> I want to increase the # of calls per seconds. Please help me out. I have
> few doubt Please reply:
>
>
>
> 1) *How many max calls per sec can be make* using 1 vCPU and 2 GB RAM, 10
> GB HDD, manual setup with stress test.
>
>
>
> 2)* How can i increase the no. of calls per sec*, Is there any specific
> parameter for that or Do I need to change in script (run_stress) or
> anywhere else please let me know.
>
>
>
> 3) I am using the stress node for making calls, I want to ask, *On which
> nodes the calls or stress test is depends other than sprout and stress node*.
> I have created the subscribers on vellum node.
>
>
>
> 4) *# of calls is limited by stress node or sprout node or any other
> node?*
>
>
>
> 5) *What is the maximum limitation of --multiplier*, i have checked up to
> 20 only?
>
>
>
> 6) *Should I increase the CPUs, RAM and HDD on single node or Do I create
> new node for larger deployments* (multiple nodes), which is better way?
>
>
>
> 7) I have not use any* throttling options *yet. Do I need to use them, if
> yes please let me know which for increasing the CPS, how can i use (just
> mentioned in shared_config?)
>
>
>
> Please reply, i am waiting for it eagerly. *Thanks in advance.*
>
>
>
> Regards,
>
> suni
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180404/b0514c43/attachment.html>

From wangwulin at huawei.com  Tue Apr  3 21:57:26 2018
From: wangwulin at huawei.com (Wangwulin (Linda))
Date: Wed, 4 Apr 2018 01:57:26 +0000
Subject: [Project Clearwater] [clearwater] Some calls failed with 480
 when do SIPp testing
In-Reply-To: <BY2PR02MB21492C7E1D2F45AEA4AA3ED5F4A50@BY2PR02MB2149.namprd02.prod.outlook.com>
References: <HK0PR03MB2755EBE5561B41362DC44420B9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>,
	<HK0PR03MB2755B712AF0925DFBE3F0070B9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>
	<HK0PR03MB27553BFBF07E1FA23D56D81BB9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>
	<3DE744999C06324789FFFC2A0230599701A01D09@dggemi502-mbs.china.huawei.com>
	<BY2PR02MB21492C7E1D2F45AEA4AA3ED5F4A50@BY2PR02MB2149.namprd02.prod.outlook.com>
Message-ID: <3DE744999C06324789FFFC2A0230599701A038A1@dggemi502-mbs.china.huawei.com>

Hi Rob,

Thanks for explaining ?multiplier? to me.
1. I have set reg_max_expires as 1800 in shared_config file on stress node and sprout node (Do I need to config the param on other nodes too?), but it did not increase the call speed as ?multiplier? did. Do I need to reboot the nodes or restart the services?  I am deploying a fresh Clearwater now, need to check that later.

2. Regarding to my previous result during 30 mins with 10000 subscribers:
Total calls: 32500
Successful calls: 32358 (99.5630769231%)
Failed calls: 142 (0.436923076923%)

It seems the totals calls 32500 is an accumulated value during the 30 mins, right?  But how could I get the result of calls which are online simultaneously during a period? Or how could I make all the successful calls online for 10 mins maybe without BYE. One parameter named ?call-length? in run_stress script, does it refer to the time duration the successful call can keep for?


Thanks,
Linda

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Robert Day
Sent: 2018?4?4? 0:20
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] [clearwater] Some calls failed with 480 when do SIPp testing

Hi Linda,

The ?multiplier? parameter increases the rate of activity for each subscriber ? if each subscriber normally makes 0.65 calls per hour and re-registers every 30 minutes, --multiplier 10 will mean they make 6.5 calls per hour and re-register every 3 minutes. This is effectively a way to generate more load with fewer subscribers (decreasing the amount of provisioning time needed).

If --multiplier 10 is making your calls work, it may be that you haven?t configured Clearwater?s registration expiry time correctly ? as http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html#deploying-a-stress-node says, ?You should also ensure that the reg_max_expires setting is set to 1800 rather than the default of 300?. If you haven?t done that, then registrations will time out too early, causing 480 Temporarily Unavailable errors when we try and make calls to subscribers with expired registrations. Increasing the multiplier means subscribers re-register more often, so happens to avoid this problem.

?ccf? is an IMS Charging Collection Function ? the network element that Ralf reports billing events to, as described in http://www.projectclearwater.org/rf-billing/. If you don?t have one, you can ignore that parameter.

Hope that helps!

Best,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Wangwulin (Linda)
Sent: 28 March 2018 11:03
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] [clearwater] Some calls failed with 480 when do SIPp testing

Hi,

When I set an additional param ?--multiplier 10? or other numbers (not the default 1), all the calls succeeded.

/usr/share/clearwater/bin/run_stress clearwater.opnfv 1000 10 --sipp-output --icscf-target 10.67.79.12:5052 --scscf-target 10.67.79.12:5054 --base-number 2010000000 --multiplier 10  --ccf 10.67.79.11

Total calls: 1083
Successful calls: 1083 (100.0%)
Failed calls: 0 (0.0%)
Unfinished calls: 0

Two questions:
1) What is ?multiplier? used for?
2) What does ?ccf? refer to? Actually the ip of ccf here is where the ralf service runs on my deployment. Is it necessary here?


Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com<mailto:wangwulin at hotmail.com>>
????: 2018?3?28? 14:40
???: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
??: [clearwater] Some calls failed with 480 when do SIPp testing


Hi clearwater team,



The successful call rate is quite low when I do SIPp testing via the command: /usr/share/clearwater/bin/run_stress clearwater.opnfv 100 30 --sipp-output --icscf-target sprout.clearwater.local:5052 --scscf-target sprout.clearwater.local:5054

Actually after 7 successful calls, the 480 error occurs.

Here is one capture when runing:

------------------------------ Scenario Screen -------- [1-9]: Change Screen --
  Call-rate(length)   Port   Total-time  Total-calls  Remote-host
0.0(5000 ms)/1.000s   5062    1234.12 s           22  10.67.79.12:5054(TCP)

  0 new calls during 1.004 s period      1 ms scheduler resolution
  0 calls (limit 1)                      Peak was 1 calls, after 55 s
  1 Running, 3 Paused, 3 Woken up
  0 dead call msg (discarded)            0 out-of-call msg (discarded)
  2 open sockets

                                 Messages  Retrans   Timeout   Unexpected-Msg
      INVITE ---------->         22        0         0
         100 <----------         22        0         0         0
         183 <----------         12        0         0         10
       PRACK ---------->         12        0
         200 <----------         12        0         0         0
      UPDATE ---------->         12        0
         201 <----------         12        0         0         0
         180 <----------  E-RTD1 12        0         0         0
         201 <----------         0         0         0         0
         200 <----------         12        0         0         0
         ACK ---------->         12        0
       Pause [   5000ms]         12                            0
         BYE ---------->         12        0         0
         200 <----------         12        0         0         0

------ [+|-|*|/]: Adjust rate ---- [q]: Soft exit ---- [p]: Pause traffic -----

Last Error: Aborting call on unexpected message for Call-Id '21-14814 at 10...


2018-03-28      06:33:54.190938 1522218834.190938: Aborting call on unexpected message for Call-Id '17-14814 at 10.67.79.24': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.24:29014;received=10.67.79.24;branch=z9hG4bK-14814-17-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 17-14814 at 10.67.79.24<mailto:17-14814 at 10.67.79.24>
From: <sip:2010000012 at clearwater.opnfv>;tag=14814SIPpTag0017
To: <sip:2010000015 at clearwater.opnfv>;tag=z9hG4bK-14814-17-0
CSeq: 1 INVITE
P-Charging-Vector: icid-value="14814SIPpTag0017";orig-ioi=clearwater.opnfv;term-ioi=clearwater.opnfv
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0

'.




1) I deoloyed clearwater via one testcase named "cloudify_ims" from opnfv/functest project, where 3 steps are run:

   * deploy a VNF orchestrator (Cloudify)

   * deploy a Clearwater vIMS (IP Multimedia Subsystem) VNF from this orchestrator based on a TOSCA blueprint defined in [1]
   * run suite of signaling tests on top of this VNF

[1]: https://github.com/Orange-OpenSource/opnfv-cloudify-clearwater/archive/master.zip



8 instances are created and I also created a new instance named "stress-node" according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html-
bash-4.4# openstack server list
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+
| ID                                   | Name                                                  | Status | Networks                                                                              | Image                | Flavor    |
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+
| df2d9f82-8aa2-4514-9f07-27974267b590 | stress-node                                           | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.24                 | ubuntu_14.04         | m1.small  |
| 48a41da3-c55d-410c-a8f3-ef26bc409aef | server_clearwater-opnfv_bono_host_llp4mt              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.23, 192.168.36.113 | ubuntu_14.04         | m1.small  |
| a992435c-a397-4622-b0d3-b8e515ebab51 | server_clearwater-opnfv_homer_host_vz97vi             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.19                 | ubuntu_14.04         | m1.small  |
| 283fef46-78e4-4d0a-9e91-ffeb14e7bfb9 | server_clearwater-opnfv_sprout_host_nnoxye            | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.12                 | ubuntu_14.04         | m1.small  |
| bfe5111d-ed7d-4c65-80c2-20144935ee8c | server_clearwater-opnfv_ellis_host_bo7auh             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.14, 192.168.36.111 | ubuntu_14.04         | m1.small  |
| b83e1727-b5a6-430e-8b5e-b3f0e6421675 | server_clearwater-opnfv_vellum_host_sh0ocy            | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.6                  | ubuntu_14.04         | m1.small  |
| 8b03d8fa-5873-489b-994b-5f1de24a85c0 | server_clearwater-opnfv_bind_host_b9f49w              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.16, 192.168.36.110 | ubuntu_14.04         | m1.small  |
| 0ca72f55-7e05-4477-ad2e-2661ad49f7ce | server_clearwater-opnfv_dime_host_xryxen              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.11                 | ubuntu_14.04         | m1.small  |
| 7d51e479-24b6-4db4-9b8f-3ef42be0b87e | server_clearwater-opnfv_proxy_host_rqr7bo             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.5                  | ubuntu_14.04         | m1.small  |
| 82672984-b967-49ff-8f00-09ffe3f7fc87 | cloudify_manager-6c79129a-8384-4069-81f7-a024738102cd | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.13, 192.168.36.105 | cloudify_manager_4.0 | m1.medium |
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+


2) I checked  the ralf log
root at dime-au6gte:/var/log/ralf# vim ralf_20180328T060000Z.txt
28-03-2018 06:25:57.063 UTC Debug dnscachedresolver.cpp:543: Create and execute DNS query transaction
28-03-2018 06:25:57.063 UTC Debug dnscachedresolver.cpp:556: Wait for query responses
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:704: Received DNS response for _diameter._tcp.clearwater.opnfv type SRV
28-03-2018 06:25:57.064 UTC Warning dnscachedresolver.cpp:846: Failed to retrieve record for _diameter._tcp.clearwater.opnfv: Domain name not found
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:946: Adding _diameter._tcp.clearwater.opnfv to cache expiry list with deletion time of 1522218957
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:704: Received DNS response for _diameter._sctp.clearwater.opnfv type SRV
28-03-2018 06:25:57.064 UTC Warning dnscachedresolver.cpp:846: Failed to retrieve record for _diameter._sctp.clearwater.opnfv: Domain name not found
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:946: Adding _diameter._sctp.clearwater.opnfv to cache expiry list with deletion time of 1522218957
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:560: Received all query responses
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:588: Pulling 0 records from cache for _diameter._tcp.clearwater.opnfv SRV
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:588: Pulling 0 records from cache for _diameter._sctp.clearwater.opnfv SRV
28-03-2018 06:25:57.064 UTC Debug diameterresolver.cpp:131: TCP SRV record _diameter._tcp.clearwater.opnfv returned 0 records
28-03-2018 06:25:57.064 UTC Debug diameterresolver.cpp:134: SCTP SRV record _diameter._sctp.clearwater.opnfv returned 0 records
28-03-2018 06:25:57.064 UTC Error diameterstack.cpp:864: No Diameter peers have been found
28-03-2018 06:25:57.068 UTC Verbose httpstack.cpp:345: Process request for URL /ping, args (null)
28-03-2018 06:25:57.068 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /ping, args (null)
28-03-2018 06:25:58.027 UTC Verbose httpstack.cpp:345: Process request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:128: Handling request, body:
{
    "peers": {
        "ccf": [
            ""
        ]
    },
    "event": {
        "Accounting-Record-Type": 1,
        "Event-Timestamp": 1522218357,
        "Service-Information": {
            "Subscription-Id": [
                {
                    "Subscription-Id-Type": 2,
                    "Subscription-Id-Data": "sip:2010000020 at clearwater.opnfv"
                }
            ],
            "IMS-Information": {
                "Event-Type": {
                    "SIP-Method": "REGISTER",
                    "Expires": 3600
                },
                "Role-Of-Node": 0,
                "Node-Functionality": 1,
                "User-Session-Id": "2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24>",
                "Called-Party-Address": "sip:2010000020 at clearwater.opnfv",
                "Associated-URI": [
                    "sip:2010000020 at clearwater.opnfv"
                ],
                "Time-Stamps": {
                    "SIP-Request-Timestamp": 1522218357,
                    "SIP-Request-Timestamp-Fraction": 960,
                    "SIP-Response-Timestamp": 1522218357,
                    "SIP-Response-Timestamp-Fraction": 967
                },
                "IMS-Charging-Identifier": "",
                "Cause-Code": -1,
                "From-Address": "<sip:2010000020 at clearwater.opnfv>;tag=8708SIPpTag0014990",
                "Route-Header-Received": "<sip:clearwater.opnfv;transport=TCP;lr>",
                "Route-Header-Transmitted": "<sip:icscf.sprout.clearwater.local:5052;transport=TCP;lr;orig>",
                "Instance-Id": "<urn:uuid:00000000-0000-0000-0000-000000000001>"
            }
        }
    }
}
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:244: Adding CCF
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:82: Handle the received message
28-03-2018 06:25:58.027 UTC Debug peer_message_sender.cpp:84: Sending message to  (number 0)
28-03-2018 06:25:58.027 UTC Debug rf.cpp:63: Building an Accounting-Request
28-03-2018 06:25:58.027 UTC Debug freeDiameter: No Session-Id AVP found in message 0x7f764c008d50
28-03-2018 06:25:58.027 UTC Verbose diameterstack.cpp:1470: Sending Diameter message of type 271 on transaction 0x7f764c008d00
28-03-2018 06:25:58.027 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.027 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
28-03-2018 06:25:58.027 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm clearwater.opnfv
28-03-2018 06:25:58.027 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
28-03-2018 06:25:58.027 UTC Debug freeDiameter: Calling callback registered when query was sent (0x43cb30, 0x7f764c008d00)
28-03-2018 06:25:58.027 UTC Verbose diameterstack.cpp:1130: Got Diameter response of type 271 - calling callback on transaction 0x7f764c008d00
28-03-2018 06:25:58.027 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
28-03-2018 06:25:58.027 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
28-03-2018 06:25:58.027 UTC Warning session_manager.cpp:414: Session for 2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24> received error from CDF
28-03-2018 06:25:58.032 UTC Verbose httpstack.cpp:345: Process request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:128: Handling request, body:
{
    "peers": {
        "ccf": [
            ""
        ]
    },
    "event": {
        "Accounting-Record-Type": 1,
        "Event-Timestamp": 1522218357,
        "Service-Information": {
            "Subscription-Id": [
                {
                    "Subscription-Id-Type": 2,
                    "Subscription-Id-Data": "sip:2010000021 at clearwater.opnfv"
                }
            ],
            "IMS-Information": {
                "Event-Type": {
                    "SIP-Method": "REGISTER",
                    "Expires": 3600
                },
                "Role-Of-Node": 0,
                "Node-Functionality": 1,
                "User-Session-Id": "2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24>",
                "Called-Party-Address": "sip:2010000021 at clearwater.opnfv",
                "Associated-URI": [
                    "sip:2010000021 at clearwater.opnfv"
                ],
                "Time-Stamps": {
                    "SIP-Request-Timestamp": 1522218357,
                    "SIP-Request-Timestamp-Fraction": 968,
                    "SIP-Response-Timestamp": 1522218357,
                    "SIP-Response-Timestamp-Fraction": 974
                },
                "IMS-Charging-Identifier": "",
                "Cause-Code": -1,
                "From-Address": "<sip:2010000021 at clearwater.opnfv>;tag=8708SIPpTag0014990",
                "Route-Header-Received": "<sip:clearwater.opnfv;transport=TCP;lr>",
                "Route-Header-Transmitted": "<sip:icscf.sprout.clearwater.local:5052;transport=TCP;lr;orig>",
                "Instance-Id": "<urn:uuid:00000000-0000-0000-0000-000000000001>"
            }
        }
    }
}
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:244: Adding CCF
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:82: Handle the received message
28-03-2018 06:25:58.032 UTC Debug peer_message_sender.cpp:84: Sending message to  (number 0)
28-03-2018 06:25:58.032 UTC Debug rf.cpp:63: Building an Accounting-Request
28-03-2018 06:25:58.032 UTC Debug freeDiameter: No Session-Id AVP found in message 0x7f764c001310
28-03-2018 06:25:58.032 UTC Verbose diameterstack.cpp:1470: Sending Diameter message of type 271 on transaction 0x7f764c006f30
28-03-2018 06:25:58.032 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.032 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
28-03-2018 06:25:58.032 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm clearwater.opnfv
28-03-2018 06:25:58.032 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
28-03-2018 06:25:58.032 UTC Debug freeDiameter: Calling callback registered when query was sent (0x43cb30, 0x7f764c006f30)
28-03-2018 06:25:58.032 UTC Verbose diameterstack.cpp:1130: Got Diameter response of type 271 - calling callback on transaction 0x7f764c006f30
28-03-2018 06:25:58.032 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
28-03-2018 06:25:58.032 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
28-03-2018 06:25:58.032 UTC Warning session_manager.cpp:414: Session for 2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24> received error from CDF

Have I missed something in shared_config?

root at dime-au6gte:/var/log/ralf# cat /etc/clearwater/shared_config
# Deployment definitions
home_domain=clearwater.opnfv
sprout_hostname=sprout.clearwater.local
chronos_hostname=10.67.79.16:7253
hs_hostname=hs.clearwater.local:8888
hs_provisioning_hostname=hs-prov.clearwater.local:8889
sprout_impi_store=vellum.clearwater.local
sprout_registration_store=vellum.clearwater.local
cassandra_hostname=vellum.clearwater.local
chronos_hostname=vellum.clearwater.local
ralf_session_store=vellum.clearwater.local
ralf_hostname=ralf.clearwater.local:10888
xdms_hostname=homer.clearwater.local:7888
signaling_dns_server=10.67.79.16
snmp_ip=10.67.79.11
reg_max_expires=1800
bgcf=0



# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


root at dime-au6gte:~# cat /etc/clearwater/local_config
local_ip=10.67.79.11
public_ip=
public_hostname=dime-au6gte.clearwater.local



homestead seems fine except one line "Failed TWO read for get_row. Try ONE":

root at dime-au6gte:/var/log/homestead# vim homestead_20180328T060000Z.txt
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:425: Attempt to parse vellum.clearwater.local as IP address
28-03-2018 06:30:58.045 UTC Verbose dnscachedresolver.cpp:486: Check cache for vellum.clearwater.local type 1
28-03-2018 06:30:58.045 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for vellum.clearwater.local A
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:231: Request for connection to IP: 10.67.79.6, port: 9160
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:244: Found existing connection 0x7fed8c0115a0 in pool
28-03-2018 06:30:58.045 UTC Debug cassandra_store.cpp:159: Generated Cassandra timestamp 1522218658045331
28-03-2018 06:30:58.045 UTC Debug cache.cpp:350: Issuing get for key sip:2010000021 at clearwater.opnfv
28-03-2018 06:30:58.045 UTC Debug cassandra_store.cpp:731: Failed TWO read for get_row. Try ONE
28-03-2018 06:30:58.045 UTC Debug cache.cpp:370: Retrieved XML column with TTL 0 and value <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>>
28-03-2018 06:30:58.045 UTC Debug cache.cpp:388: Retrieved is_registered column with value False and TTL 0
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1245: Got IMS subscription from cache
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1260: TTL for this database record is 0, IMS Subscription XML is not empty, registration state is UNREGISTERED, and the charging addresses are empty
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1510: Rejecting deregistration for user who was not registered
28-03-2018 06:30:58.045 UTC Verbose httpstack.cpp:93: Sending response 400 to request for URL /impu/sip%3A2010000021%40clearwater.opnfv/reg-data, args (null)
28-03-2018 06:30:58.046 UTC Debug cache.cpp:370: Retrieved XML column with TTL 0 and value <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>>
28-03-2018 06:30:58.046 UTC Debug cache.cpp:388: Retrieved is_registered column with value False and TTL 0
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1245: Got IMS subscription from cache
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1260: TTL for this database record is 0, IMS Subscription XML is not empty, registration state is UNREGISTERED, and the charging addresses are empty
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1286: Subscriber registering with new binding
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1457: Handling initial registration
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1654: Attempting to cache IMS subscription for public IDs
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1658: Got public IDs to cache against - doing it
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1663: Public ID sip:2010000021 at clearwater.opnfv
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:159: Generated Cassandra timestamp 1522218658046440
28-03-2018 06:30:58.046 UTC Debug a_record_resolver.cpp:80: ARecordResolver::resolve_iter for host vellum.clearwater.local, port 9160, family 2
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:425: Attempt to parse vellum.clearwater.local as IP address
28-03-2018 06:30:58.046 UTC Verbose dnscachedresolver.cpp:486: Check cache for vellum.clearwater.local type 1
28-03-2018 06:30:58.046 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for vellum.clearwater.local A
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:231: Request for connection to IP: 10.67.79.6, port: 9160
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:244: Found existing connection 0x7fed8c0115a0 in pool
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:612: Constructing cassandra put request with timestamp 1522218658046440 and per-column TTLs
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:629:   ims_subscription_xml => <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>> (TTL 0)
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:629:   is_registered =>  (TTL 0)
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:650: Executing put request operation
28-03-2018 06:30:58.047 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.047 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.047 UTC Debug handlers.cpp:1567: Sending 200 response (body was {"reqtype": "reg", "server_name": "sip:scscf.sprout.clearwater.local:5054;transport=TCP"})
28-03-2018 06:30:58.047 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /impu/sip%3A2010000021%40clearwater.opnfv/reg-data, args private_id=2010000021%40clearwater.opnfv



The error about "Could not get subscriber data from HSS" on Sprout node occurred sometimes.

root at sprout-2tl8g3:/var/log/sprout# vim sprout_20180328T060000Z.txt
28-03-2018 06:36:11.741 UTC Error httpclient.cpp:712: cURL failure with cURL error code 0 (see man 3 libcurl-errors) and HTTP error code 400
28-03-2018 06:36:11.741 UTC Error hssconnection.cpp:704: Could not get subscriber data from HSS
28-03-2018 06:36:25.875 UTC Status alarm.cpp:62: sprout issued 1004.1 alarm




root at dime-au6gte:/var/log/homestead# monit summary
Monit 5.18.1 uptime: 2h 27m
 Service Name                     Status                      Type
 node-dime-au6gte.clearwater....  Running                     System
 snmpd_process                    Running                     Process
 ralf_process                     Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 ralf_uptime                      Status ok                   Program
 poll_ralf                        Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


Any help/suggestion would be much appreciated.




Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180404/128309dc/attachment.html>

From david.kyselica at gmail.com  Wed Apr  4 03:56:24 2018
From: david.kyselica at gmail.com (=?UTF-8?Q?D=C3=A1vid_Kyselica?=)
Date: Wed, 4 Apr 2018 09:56:24 +0200
Subject: [Project Clearwater] Clearwater, ellis freezing
In-Reply-To: <BY2PR02MB2149E0B6776DB17C397F44CDF4AC0@BY2PR02MB2149.namprd02.prod.outlook.com>
References: <CAJpo2SrM2rViuhGy86MHv9sZ-QVXjS2J89KZzGbeh=UZ0e_1AQ@mail.gmail.com>
	<BY2PR02MB2149E0B6776DB17C397F44CDF4AC0@BY2PR02MB2149.namprd02.prod.outlook.com>
Message-ID: <CAJpo2SoabHN1RX6a_vACUk_McYB7z_tOM0N8e47Lsd-v3PWW+w@mail.gmail.com>

Hi Rob,
Thanks a lot for answer. I checked that log file, and there are many
hundreds of lines. What specifically should I look for ? In general, from
brief check of that file I found out that there are only java logs and no
errors.

Thanks again,
David

On Tue, Mar 27, 2018 at 4:46 PM, Robert Day <Robert.Day at metaswitch.com>
wrote:

> Hi D?vid,
>
>
>
> ?19-03-2018 08:42:49.839 UTC Error main.cpp:1016: Failed to initialize
> the Cassandra store with error$? suggests the error is in Cassandra. Is
> there anything in /var/log/cassandra/system.log?
>
>
>
> Best,
>
> Rob
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *D?vid Kyselica
> *Sent:* 19 March 2018 08:56
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Clearwater, ellis freezing
>
>
>
> Hi,
>
> I`m new to clearwater project. I installed all-in-one AMI into amazon ec2.
> After trying to access web interface, I was asked by web browser to reload
> the page by this message:Failed to update the server (see detailed
> diagnostics in developer console). Please refresh the page.
>
> I`m really confused what it can by caused by. I will be thankful for any
> type of tips. It`s a fresh installation with no important configuration
> changes.
>
>
>
> *log file of homestead: *
>
> 19-03-2018 08:42:49.718 UTC Status http_connection_pool.cpp:35: Connection
> pool will use calculated $
>
> 19-03-2018 08:42:49.837 UTC Status httpconnection.h:58: Configuring HTTP
> Connection
>
> 19-03-2018 08:42:49.837 UTC Status httpconnection.h:59:   Connection
> created for server ec2-52-91-24$
>
> 19-03-2018 08:42:49.837 UTC Status main.cpp:973: No HSS configured - using
> Homestead-prov
>
> 19-03-2018 08:42:49.837 UTC Status a_record_resolver.cpp:29: Created
> ARecordResolver
>
> 19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:266: Configuring
> store connection
>
> 19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:267:   Hostname:
> 127.0.0.1
>
> 19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:268:   Port:
> 9160
>
> 19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:296: Configuring
> store worker pool
>
> 19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:297:   Threads:   10
>
> 19-03-2018 08:42:49.837 UTC Status cassandra_store.cpp:298:   Max Queue: 0
>
> 19-03-2018 08:42:49.839 UTC Error main.cpp:1016: Failed to initialize the
> Cassandra store with error$
>
> 19-03-2018 08:42:49.839 UTC Status main.cpp:1017: Homestead is shutting
> down
>
>
>
> *log file of ellis:*
>
> 19-03-2018 08:31:30.662 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.64ms
>
> 19-03-2018 08:31:46.424 UTC INFO web.py:1447: 200 GET / (0.0.0.0) 616.39ms
>
> 19-03-2018 08:31:46.753 UTC INFO web.py:1447: 200 GET
> /css/bootstrap.min.css (0.0.0.0) 86.74ms
>
> 19-03-2018 08:31:46.793 UTC INFO web.py:1447: 200 GET
> /css/bootstrap-responsive.css (0.0.0.0) 39.08ms
>
> 19-03-2018 08:31:46.834 UTC INFO web.py:1447: 200 GET /css/style.css
> (0.0.0.0) 1.16ms
>
> 19-03-2018 08:31:46.835 UTC INFO web.py:1447: 200 GET
> /css/jquery.miniColors.css (0.0.0.0) 1.03ms
>
> 19-03-2018 08:31:46.836 UTC INFO web.py:1447: 200 GET
> /css/fileuploader.css (0.0.0.0) 0.94ms
>
> 19-03-2018 08:31:47.695 UTC INFO web.py:1447: 200 GET /js/jquery.js
> (0.0.0.0) 573.28ms
>
> 19-03-2018 08:31:47.740 UTC INFO web.py:1447: 200 GET
> /js/jquery.ba-bbq.min.js (0.0.0.0) 42.90ms
>
> 19-03-2018 08:31:48.104 UTC INFO web.py:1447: 200 GET /js/fileuploader.js
> (0.0.0.0) 364.63ms
>
> 19-03-2018 08:31:48.106 UTC INFO web.py:1447: 200 GET
> /js/jquery.miniColors.min.js (0.0.0.0) 1.58ms
>
> 19-03-2018 08:31:48.107 UTC INFO web.py:1447: 200 GET /js/jquery.cookie.js
> (0.0.0.0) 1.02ms
>
> 19-03-2018 08:31:48.149 UTC INFO web.py:1447: 200 GET
> /js/jquery.total-storage.min.js (0.0.0.0) 41.65ms
>
> 19-03-2018 08:31:48.314 UTC INFO web.py:1447: 200 GET /js/bootstrap.min.js
> (0.0.0.0) 2.34ms
>
> 19-03-2018 08:31:48.354 UTC INFO web.py:1447: 200 GET /js/common.js
> (0.0.0.0) 1.91ms
>
> 19-03-2018 08:31:48.561 UTC INFO web.py:1447: 200 GET /js/app.js (0.0.0.0)
> 168.24ms
>
> 19-03-2018 08:31:49.012 UTC WARNING web.py:1447: 404 GET
> /images/favicon.ico (0.0.0.0) 81.57ms
>
> 19-03-2018 08:31:50.286 UTC INFO web.py:1447: 200 GET /accounts/david%
> 40hmz.sk/numbers/?cb=2b0a26dea4K0 (0.0.0.0) 40.51ms
>
> 19-03-2018 08:32:20.995 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.21ms
>
> 19-03-2018 08:32:35.655 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 162.92ms
>
> 19-03-2018 08:33:21.845 UTC INFO web.py:1447: 200 GET /addressbook.html
> (0.0.0.0) 329.69ms
>
> 19-03-2018 08:33:22.052 UTC INFO web.py:1447: 200 GET /css/addressbook.css
> (0.0.0.0) 1.18ms
>
> 19-03-2018 08:33:22.709 UTC INFO web.py:1447: 200 GET /js/backbone.js
> (0.0.0.0) 455.22ms
>
> 19-03-2018 08:33:22.752 UTC INFO web.py:1447: 200 GET /js/underscore.js
> (0.0.0.0) 43.11ms
>
> 19-03-2018 08:33:22.874 UTC INFO web.py:1447: 200 GET /js/addressbook.js
> (0.0.0.0) 43.49ms
>
> 19-03-2018 08:33:23.121 UTC INFO web.py:1447: 200 GET
> /js/templates/addressbook-contacts.html (0.0.0.0) 2.48ms
>
> 19-03-2018 08:33:23.405 UTC INFO web.py:1447: 200 GET /gab (0.0.0.0) 2.09ms
>
> 19-03-2018 08:33:27.915 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.18ms
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180404/1b32bfba/attachment.html>

From skgola1997 at gmail.com  Wed Apr  4 04:46:21 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Wed, 4 Apr 2018 14:16:21 +0530
Subject: [Project Clearwater] Scale up Deployment
Message-ID: <CAHwYWpAonC0gW9cSWTg2BJVK1fezcAFRV__Me=qt56cR9XYM4A@mail.gmail.com>

 Hi all,
I have set up clearwater manually with single node. No I want to scale it
up with multiple nodes, regarding that i have few doubts:

i-> hostname (public_hostname)would be same as initial setup or it can be
anything (as eg. in initial setup hostname sprout is sprout, should for new
node name is sprout or it would be different as sprout-1 etc and in
public_hostname also would be same as sprout-1 or sprout)
Please guide.

ii-> how can i confirm that the new node has joined the existing
deployment, i checked using
  cw-check_cluster_state
 sudo cw-check_config_sync

but in logs there is no affect i see in sprout.. Thanks again

Best,
suni
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180404/2c64e598/attachment.html>

From skgola1997 at gmail.com  Fri Apr  6 02:15:21 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Fri, 6 Apr 2018 11:45:21 +0530
Subject: [Project Clearwater] Lost the IPs of clearwater node
Message-ID: <CAHwYWpDVEZP2PpzHYafkDrJi_445qjO8=mqXxmUuMxweV+ozkA@mail.gmail.com>

Hi all,
I have installed the clearwater using manual installation. I used briidge
adapter and ip is assigned by DHCP. Some how the machine is power off and
all the clearwater nodes are aborted (power off), in result some nodes have
lost their ip and some of them are interchange (IPs). Do i need to assign
IP manually same as earlier?
everything is lost that is not connected. What I need to do please let me
know.

Thanks,
sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180406/2a29b2cc/attachment.html>

From masood.tahir696 at gmail.com  Fri Apr  6 08:35:54 2018
From: masood.tahir696 at gmail.com (tahir Masood)
Date: Fri, 6 Apr 2018 17:35:54 +0500
Subject: [Project Clearwater] Lost the IPs of clearwater node
In-Reply-To: <CAHwYWpDVEZP2PpzHYafkDrJi_445qjO8=mqXxmUuMxweV+ozkA@mail.gmail.com>
References: <CAHwYWpDVEZP2PpzHYafkDrJi_445qjO8=mqXxmUuMxweV+ozkA@mail.gmail.com>
Message-ID: <CADabj4aBbJTZ=rq0qg1w6tHsb6=dy-S76arHqmiQqESoEuYp_A@mail.gmail.com>

Assign IP's again and update the hosts/DNS.

Regards,

Tahir

On Fri, Apr 6, 2018 at 11:15 AM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi all,
> I have installed the clearwater using manual installation. I used briidge
> adapter and ip is assigned by DHCP. Some how the machine is power off and
> all the clearwater nodes are aborted (power off), in result some nodes have
> lost their ip and some of them are interchange (IPs). Do i need to assign
> IP manually same as earlier?
> everything is lost that is not connected. What I need to do please let me
> know.
>
> Thanks,
> sunil
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180406/29550f4f/attachment.html>

From jake at dccllc.net  Fri Apr  6 10:01:25 2018
From: jake at dccllc.net (Jake Brown)
Date: Fri, 6 Apr 2018 14:01:25 +0000
Subject: [Project Clearwater] check registered state of user
Message-ID: <MWHPR18MB1151F3267DEF7E707CA11ABCA9BA0@MWHPR18MB1151.namprd18.prod.outlook.com>

I must have something wrong in the shared_config.  I would expect to see something in vellum in the homestead_cache/impu table that indicates registered in the is_registered column.  Is this not the correct place to check to see if a subscriber is registered?  Thank you for your guidance.

Jake
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180406/3e16a1a5/attachment.html>

From wangwulin at huawei.com  Sun Apr  8 00:16:44 2018
From: wangwulin at huawei.com (Wangwulin (Linda))
Date: Sun, 8 Apr 2018 04:16:44 +0000
Subject: [Project Clearwater] [clearwater] Some calls failed with 480
 when do SIPp testing
In-Reply-To: <BY2PR02MB21492C7E1D2F45AEA4AA3ED5F4A50@BY2PR02MB2149.namprd02.prod.outlook.com>
References: <HK0PR03MB2755EBE5561B41362DC44420B9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>,
	<HK0PR03MB2755B712AF0925DFBE3F0070B9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>
	<HK0PR03MB27553BFBF07E1FA23D56D81BB9A30@HK0PR03MB2755.apcprd03.prod.outlook.com>
	<3DE744999C06324789FFFC2A0230599701A01D09@dggemi502-mbs.china.huawei.com>
	<BY2PR02MB21492C7E1D2F45AEA4AA3ED5F4A50@BY2PR02MB2149.namprd02.prod.outlook.com>
Message-ID: <3DE744999C06324789FFFC2A0230599701A04800@dggemi502-mbs.china.huawei.com>

Hi Rob,

Thanks for explaining ?multiplier? to me.
1. I have set reg_max_expires as 1800 in shared_config file on stress node and sprout node (Do I need to config the param on other nodes too?), but it did not increase the call speed as ?multiplier? did. Do I need to reboot the nodes or restart the services?  I am deploying a fresh Clearwater now, need to check that later.

2. Regarding to my previous result during 30 mins with 10000 subscribers:
Total calls: 32500
Successful calls: 32358 (99.5630769231%)
Failed calls: 142 (0.436923076923%)

It seems the totals calls 32500 is an accumulated value during the 30 mins, right?  But how could I get the result of calls which are online simultaneously during a period? Or how could I make all the successful calls online for 10 mins maybe without BYE. One parameter named ?call-length? in run_stress script, does it refer to the time duration the successful call can keep for?


Thanks,
Linda

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Robert Day
Sent: 2018?4?4? 0:20
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] [clearwater] Some calls failed with 480 when do SIPp testing

Hi Linda,

The ?multiplier? parameter increases the rate of activity for each subscriber ? if each subscriber normally makes 0.65 calls per hour and re-registers every 30 minutes, --multiplier 10 will mean they make 6.5 calls per hour and re-register every 3 minutes. This is effectively a way to generate more load with fewer subscribers (decreasing the amount of provisioning time needed).

If --multiplier 10 is making your calls work, it may be that you haven?t configured Clearwater?s registration expiry time correctly ? as http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html#deploying-a-stress-node says, ?You should also ensure that the reg_max_expires setting is set to 1800 rather than the default of 300?. If you haven?t done that, then registrations will time out too early, causing 480 Temporarily Unavailable errors when we try and make calls to subscribers with expired registrations. Increasing the multiplier means subscribers re-register more often, so happens to avoid this problem.

?ccf? is an IMS Charging Collection Function ? the network element that Ralf reports billing events to, as described in http://www.projectclearwater.org/rf-billing/. If you don?t have one, you can ignore that parameter.

Hope that helps!

Best,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Wangwulin (Linda)
Sent: 28 March 2018 11:03
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] [clearwater] Some calls failed with 480 when do SIPp testing

Hi,

When I set an additional param ?--multiplier 10? or other numbers (not the default 1), all the calls succeeded.

/usr/share/clearwater/bin/run_stress clearwater.opnfv 1000 10 --sipp-output --icscf-target 10.67.79.12:5052 --scscf-target 10.67.79.12:5054 --base-number 2010000000 --multiplier 10  --ccf 10.67.79.11

Total calls: 1083
Successful calls: 1083 (100.0%)
Failed calls: 0 (0.0%)
Unfinished calls: 0

Two questions:
1) What is ?multiplier? used for?
2) What does ?ccf? refer to? Actually the ip of ccf here is where the ralf service runs on my deployment. Is it necessary here?


Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com<mailto:wangwulin at hotmail.com>>
????: 2018?3?28? 14:40
???: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
??: [clearwater] Some calls failed with 480 when do SIPp testing


Hi clearwater team,



The successful call rate is quite low when I do SIPp testing via the command: /usr/share/clearwater/bin/run_stress clearwater.opnfv 100 30 --sipp-output --icscf-target sprout.clearwater.local:5052 --scscf-target sprout.clearwater.local:5054

Actually after 7 successful calls, the 480 error occurs.

Here is one capture when runing:

------------------------------ Scenario Screen -------- [1-9]: Change Screen --
  Call-rate(length)   Port   Total-time  Total-calls  Remote-host
0.0(5000 ms)/1.000s   5062    1234.12 s           22  10.67.79.12:5054(TCP)

  0 new calls during 1.004 s period      1 ms scheduler resolution
  0 calls (limit 1)                      Peak was 1 calls, after 55 s
  1 Running, 3 Paused, 3 Woken up
  0 dead call msg (discarded)            0 out-of-call msg (discarded)
  2 open sockets

                                 Messages  Retrans   Timeout   Unexpected-Msg
      INVITE ---------->         22        0         0
         100 <----------         22        0         0         0
         183 <----------         12        0         0         10
       PRACK ---------->         12        0
         200 <----------         12        0         0         0
      UPDATE ---------->         12        0
         201 <----------         12        0         0         0
         180 <----------  E-RTD1 12        0         0         0
         201 <----------         0         0         0         0
         200 <----------         12        0         0         0
         ACK ---------->         12        0
       Pause [   5000ms]         12                            0
         BYE ---------->         12        0         0
         200 <----------         12        0         0         0

------ [+|-|*|/]: Adjust rate ---- [q]: Soft exit ---- [p]: Pause traffic -----

Last Error: Aborting call on unexpected message for Call-Id '21-14814 at 10...


2018-03-28      06:33:54.190938 1522218834.190938: Aborting call on unexpected message for Call-Id '17-14814 at 10.67.79.24': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.24:29014;received=10.67.79.24;branch=z9hG4bK-14814-17-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 17-14814 at 10.67.79.24<mailto:17-14814 at 10.67.79.24>
From: <sip:2010000012 at clearwater.opnfv>;tag=14814SIPpTag0017
To: <sip:2010000015 at clearwater.opnfv>;tag=z9hG4bK-14814-17-0
CSeq: 1 INVITE
P-Charging-Vector: icid-value="14814SIPpTag0017";orig-ioi=clearwater.opnfv;term-ioi=clearwater.opnfv
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0

'.




1) I deoloyed clearwater via one testcase named "cloudify_ims" from opnfv/functest project, where 3 steps are run:

   * deploy a VNF orchestrator (Cloudify)

   * deploy a Clearwater vIMS (IP Multimedia Subsystem) VNF from this orchestrator based on a TOSCA blueprint defined in [1]
   * run suite of signaling tests on top of this VNF

[1]: https://github.com/Orange-OpenSource/opnfv-cloudify-clearwater/archive/master.zip



8 instances are created and I also created a new instance named "stress-node" according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html-
bash-4.4# openstack server list
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+
| ID                                   | Name                                                  | Status | Networks                                                                              | Image                | Flavor    |
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+
| df2d9f82-8aa2-4514-9f07-27974267b590 | stress-node                                           | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.24                 | ubuntu_14.04         | m1.small  |
| 48a41da3-c55d-410c-a8f3-ef26bc409aef | server_clearwater-opnfv_bono_host_llp4mt              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.23, 192.168.36.113 | ubuntu_14.04         | m1.small  |
| a992435c-a397-4622-b0d3-b8e515ebab51 | server_clearwater-opnfv_homer_host_vz97vi             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.19                 | ubuntu_14.04         | m1.small  |
| 283fef46-78e4-4d0a-9e91-ffeb14e7bfb9 | server_clearwater-opnfv_sprout_host_nnoxye            | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.12                 | ubuntu_14.04         | m1.small  |
| bfe5111d-ed7d-4c65-80c2-20144935ee8c | server_clearwater-opnfv_ellis_host_bo7auh             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.14, 192.168.36.111 | ubuntu_14.04         | m1.small  |
| b83e1727-b5a6-430e-8b5e-b3f0e6421675 | server_clearwater-opnfv_vellum_host_sh0ocy            | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.6                  | ubuntu_14.04         | m1.small  |
| 8b03d8fa-5873-489b-994b-5f1de24a85c0 | server_clearwater-opnfv_bind_host_b9f49w              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.16, 192.168.36.110 | ubuntu_14.04         | m1.small  |
| 0ca72f55-7e05-4477-ad2e-2661ad49f7ce | server_clearwater-opnfv_dime_host_xryxen              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.11                 | ubuntu_14.04         | m1.small  |
| 7d51e479-24b6-4db4-9b8f-3ef42be0b87e | server_clearwater-opnfv_proxy_host_rqr7bo             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.5                  | ubuntu_14.04         | m1.small  |
| 82672984-b967-49ff-8f00-09ffe3f7fc87 | cloudify_manager-6c79129a-8384-4069-81f7-a024738102cd | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.13, 192.168.36.105 | cloudify_manager_4.0 | m1.medium |
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+


2) I checked  the ralf log
root at dime-au6gte:/var/log/ralf# vim ralf_20180328T060000Z.txt
28-03-2018 06:25:57.063 UTC Debug dnscachedresolver.cpp:543: Create and execute DNS query transaction
28-03-2018 06:25:57.063 UTC Debug dnscachedresolver.cpp:556: Wait for query responses
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:704: Received DNS response for _diameter._tcp.clearwater.opnfv type SRV
28-03-2018 06:25:57.064 UTC Warning dnscachedresolver.cpp:846: Failed to retrieve record for _diameter._tcp.clearwater.opnfv: Domain name not found
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:946: Adding _diameter._tcp.clearwater.opnfv to cache expiry list with deletion time of 1522218957
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:704: Received DNS response for _diameter._sctp.clearwater.opnfv type SRV
28-03-2018 06:25:57.064 UTC Warning dnscachedresolver.cpp:846: Failed to retrieve record for _diameter._sctp.clearwater.opnfv: Domain name not found
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:946: Adding _diameter._sctp.clearwater.opnfv to cache expiry list with deletion time of 1522218957
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:560: Received all query responses
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:588: Pulling 0 records from cache for _diameter._tcp.clearwater.opnfv SRV
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:588: Pulling 0 records from cache for _diameter._sctp.clearwater.opnfv SRV
28-03-2018 06:25:57.064 UTC Debug diameterresolver.cpp:131: TCP SRV record _diameter._tcp.clearwater.opnfv returned 0 records
28-03-2018 06:25:57.064 UTC Debug diameterresolver.cpp:134: SCTP SRV record _diameter._sctp.clearwater.opnfv returned 0 records
28-03-2018 06:25:57.064 UTC Error diameterstack.cpp:864: No Diameter peers have been found
28-03-2018 06:25:57.068 UTC Verbose httpstack.cpp:345: Process request for URL /ping, args (null)
28-03-2018 06:25:57.068 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /ping, args (null)
28-03-2018 06:25:58.027 UTC Verbose httpstack.cpp:345: Process request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:128: Handling request, body:
{
    "peers": {
        "ccf": [
            ""
        ]
    },
    "event": {
        "Accounting-Record-Type": 1,
        "Event-Timestamp": 1522218357,
        "Service-Information": {
            "Subscription-Id": [
                {
                    "Subscription-Id-Type": 2,
                    "Subscription-Id-Data": "sip:2010000020 at clearwater.opnfv"
                }
            ],
            "IMS-Information": {
                "Event-Type": {
                    "SIP-Method": "REGISTER",
                    "Expires": 3600
                },
                "Role-Of-Node": 0,
                "Node-Functionality": 1,
                "User-Session-Id": "2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24>",
                "Called-Party-Address": "sip:2010000020 at clearwater.opnfv",
                "Associated-URI": [
                    "sip:2010000020 at clearwater.opnfv"
                ],
                "Time-Stamps": {
                    "SIP-Request-Timestamp": 1522218357,
                    "SIP-Request-Timestamp-Fraction": 960,
                    "SIP-Response-Timestamp": 1522218357,
                    "SIP-Response-Timestamp-Fraction": 967
                },
                "IMS-Charging-Identifier": "",
                "Cause-Code": -1,
                "From-Address": "<sip:2010000020 at clearwater.opnfv>;tag=8708SIPpTag0014990",
                "Route-Header-Received": "<sip:clearwater.opnfv;transport=TCP;lr>",
                "Route-Header-Transmitted": "<sip:icscf.sprout.clearwater.local:5052;transport=TCP;lr;orig>",
                "Instance-Id": "<urn:uuid:00000000-0000-0000-0000-000000000001>"
            }
        }
    }
}
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:244: Adding CCF
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:82: Handle the received message
28-03-2018 06:25:58.027 UTC Debug peer_message_sender.cpp:84: Sending message to  (number 0)
28-03-2018 06:25:58.027 UTC Debug rf.cpp:63: Building an Accounting-Request
28-03-2018 06:25:58.027 UTC Debug freeDiameter: No Session-Id AVP found in message 0x7f764c008d50
28-03-2018 06:25:58.027 UTC Verbose diameterstack.cpp:1470: Sending Diameter message of type 271 on transaction 0x7f764c008d00
28-03-2018 06:25:58.027 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.027 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
28-03-2018 06:25:58.027 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm clearwater.opnfv
28-03-2018 06:25:58.027 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
28-03-2018 06:25:58.027 UTC Debug freeDiameter: Calling callback registered when query was sent (0x43cb30, 0x7f764c008d00)
28-03-2018 06:25:58.027 UTC Verbose diameterstack.cpp:1130: Got Diameter response of type 271 - calling callback on transaction 0x7f764c008d00
28-03-2018 06:25:58.027 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
28-03-2018 06:25:58.027 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
28-03-2018 06:25:58.027 UTC Warning session_manager.cpp:414: Session for 2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24> received error from CDF
28-03-2018 06:25:58.032 UTC Verbose httpstack.cpp:345: Process request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:128: Handling request, body:
{
    "peers": {
        "ccf": [
            ""
        ]
    },
    "event": {
        "Accounting-Record-Type": 1,
        "Event-Timestamp": 1522218357,
        "Service-Information": {
            "Subscription-Id": [
                {
                    "Subscription-Id-Type": 2,
                    "Subscription-Id-Data": "sip:2010000021 at clearwater.opnfv"
                }
            ],
            "IMS-Information": {
                "Event-Type": {
                    "SIP-Method": "REGISTER",
                    "Expires": 3600
                },
                "Role-Of-Node": 0,
                "Node-Functionality": 1,
                "User-Session-Id": "2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24>",
                "Called-Party-Address": "sip:2010000021 at clearwater.opnfv",
                "Associated-URI": [
                    "sip:2010000021 at clearwater.opnfv"
                ],
                "Time-Stamps": {
                    "SIP-Request-Timestamp": 1522218357,
                    "SIP-Request-Timestamp-Fraction": 968,
                    "SIP-Response-Timestamp": 1522218357,
                    "SIP-Response-Timestamp-Fraction": 974
                },
                "IMS-Charging-Identifier": "",
                "Cause-Code": -1,
                "From-Address": "<sip:2010000021 at clearwater.opnfv>;tag=8708SIPpTag0014990",
                "Route-Header-Received": "<sip:clearwater.opnfv;transport=TCP;lr>",
                "Route-Header-Transmitted": "<sip:icscf.sprout.clearwater.local:5052;transport=TCP;lr;orig>",
                "Instance-Id": "<urn:uuid:00000000-0000-0000-0000-000000000001>"
            }
        }
    }
}
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:244: Adding CCF
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:82: Handle the received message
28-03-2018 06:25:58.032 UTC Debug peer_message_sender.cpp:84: Sending message to  (number 0)
28-03-2018 06:25:58.032 UTC Debug rf.cpp:63: Building an Accounting-Request
28-03-2018 06:25:58.032 UTC Debug freeDiameter: No Session-Id AVP found in message 0x7f764c001310
28-03-2018 06:25:58.032 UTC Verbose diameterstack.cpp:1470: Sending Diameter message of type 271 on transaction 0x7f764c006f30
28-03-2018 06:25:58.032 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.032 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
28-03-2018 06:25:58.032 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm clearwater.opnfv
28-03-2018 06:25:58.032 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
28-03-2018 06:25:58.032 UTC Debug freeDiameter: Calling callback registered when query was sent (0x43cb30, 0x7f764c006f30)
28-03-2018 06:25:58.032 UTC Verbose diameterstack.cpp:1130: Got Diameter response of type 271 - calling callback on transaction 0x7f764c006f30
28-03-2018 06:25:58.032 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
28-03-2018 06:25:58.032 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
28-03-2018 06:25:58.032 UTC Warning session_manager.cpp:414: Session for 2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24> received error from CDF

Have I missed something in shared_config?

root at dime-au6gte:/var/log/ralf# cat /etc/clearwater/shared_config
# Deployment definitions
home_domain=clearwater.opnfv
sprout_hostname=sprout.clearwater.local
chronos_hostname=10.67.79.16:7253
hs_hostname=hs.clearwater.local:8888
hs_provisioning_hostname=hs-prov.clearwater.local:8889
sprout_impi_store=vellum.clearwater.local
sprout_registration_store=vellum.clearwater.local
cassandra_hostname=vellum.clearwater.local
chronos_hostname=vellum.clearwater.local
ralf_session_store=vellum.clearwater.local
ralf_hostname=ralf.clearwater.local:10888
xdms_hostname=homer.clearwater.local:7888
signaling_dns_server=10.67.79.16
snmp_ip=10.67.79.11
reg_max_expires=1800
bgcf=0



# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


root at dime-au6gte:~# cat /etc/clearwater/local_config
local_ip=10.67.79.11
public_ip=
public_hostname=dime-au6gte.clearwater.local



homestead seems fine except one line "Failed TWO read for get_row. Try ONE":

root at dime-au6gte:/var/log/homestead# vim homestead_20180328T060000Z.txt
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:425: Attempt to parse vellum.clearwater.local as IP address
28-03-2018 06:30:58.045 UTC Verbose dnscachedresolver.cpp:486: Check cache for vellum.clearwater.local type 1
28-03-2018 06:30:58.045 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for vellum.clearwater.local A
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:231: Request for connection to IP: 10.67.79.6, port: 9160
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:244: Found existing connection 0x7fed8c0115a0 in pool
28-03-2018 06:30:58.045 UTC Debug cassandra_store.cpp:159: Generated Cassandra timestamp 1522218658045331
28-03-2018 06:30:58.045 UTC Debug cache.cpp:350: Issuing get for key sip:2010000021 at clearwater.opnfv
28-03-2018 06:30:58.045 UTC Debug cassandra_store.cpp:731: Failed TWO read for get_row. Try ONE
28-03-2018 06:30:58.045 UTC Debug cache.cpp:370: Retrieved XML column with TTL 0 and value <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>>
28-03-2018 06:30:58.045 UTC Debug cache.cpp:388: Retrieved is_registered column with value False and TTL 0
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1245: Got IMS subscription from cache
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1260: TTL for this database record is 0, IMS Subscription XML is not empty, registration state is UNREGISTERED, and the charging addresses are empty
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1510: Rejecting deregistration for user who was not registered
28-03-2018 06:30:58.045 UTC Verbose httpstack.cpp:93: Sending response 400 to request for URL /impu/sip%3A2010000021%40clearwater.opnfv/reg-data, args (null)
28-03-2018 06:30:58.046 UTC Debug cache.cpp:370: Retrieved XML column with TTL 0 and value <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>>
28-03-2018 06:30:58.046 UTC Debug cache.cpp:388: Retrieved is_registered column with value False and TTL 0
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1245: Got IMS subscription from cache
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1260: TTL for this database record is 0, IMS Subscription XML is not empty, registration state is UNREGISTERED, and the charging addresses are empty
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1286: Subscriber registering with new binding
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1457: Handling initial registration
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1654: Attempting to cache IMS subscription for public IDs
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1658: Got public IDs to cache against - doing it
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1663: Public ID sip:2010000021 at clearwater.opnfv
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:159: Generated Cassandra timestamp 1522218658046440
28-03-2018 06:30:58.046 UTC Debug a_record_resolver.cpp:80: ARecordResolver::resolve_iter for host vellum.clearwater.local, port 9160, family 2
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:425: Attempt to parse vellum.clearwater.local as IP address
28-03-2018 06:30:58.046 UTC Verbose dnscachedresolver.cpp:486: Check cache for vellum.clearwater.local type 1
28-03-2018 06:30:58.046 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for vellum.clearwater.local A
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:231: Request for connection to IP: 10.67.79.6, port: 9160
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:244: Found existing connection 0x7fed8c0115a0 in pool
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:612: Constructing cassandra put request with timestamp 1522218658046440 and per-column TTLs
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:629:   ims_subscription_xml => <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>> (TTL 0)
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:629:   is_registered =>  (TTL 0)
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:650: Executing put request operation
28-03-2018 06:30:58.047 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.047 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.047 UTC Debug handlers.cpp:1567: Sending 200 response (body was {"reqtype": "reg", "server_name": "sip:scscf.sprout.clearwater.local:5054;transport=TCP"})
28-03-2018 06:30:58.047 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /impu/sip%3A2010000021%40clearwater.opnfv/reg-data, args private_id=2010000021%40clearwater.opnfv



The error about "Could not get subscriber data from HSS" on Sprout node occurred sometimes.

root at sprout-2tl8g3:/var/log/sprout# vim sprout_20180328T060000Z.txt
28-03-2018 06:36:11.741 UTC Error httpclient.cpp:712: cURL failure with cURL error code 0 (see man 3 libcurl-errors) and HTTP error code 400
28-03-2018 06:36:11.741 UTC Error hssconnection.cpp:704: Could not get subscriber data from HSS
28-03-2018 06:36:25.875 UTC Status alarm.cpp:62: sprout issued 1004.1 alarm




root at dime-au6gte:/var/log/homestead# monit summary
Monit 5.18.1 uptime: 2h 27m
 Service Name                     Status                      Type
 node-dime-au6gte.clearwater....  Running                     System
 snmpd_process                    Running                     Process
 ralf_process                     Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 ralf_uptime                      Status ok                   Program
 poll_ralf                        Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


Any help/suggestion would be much appreciated.




Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180408/1a86ce59/attachment.html>

From nagen_kr at yahoo.co.in  Sun Apr  8 04:49:55 2018
From: nagen_kr at yahoo.co.in (Nagendra Kumar)
Date: Sun, 8 Apr 2018 08:49:55 +0000 (UTC)
Subject: [Project Clearwater] Input required for High Availability use
 cases of clearwater
In-Reply-To: <372055626.178624.1522494458227@mail.yahoo.com>
References: <372055626.178624.1522494458227.ref@mail.yahoo.com>
	<372055626.178624.1522494458227@mail.yahoo.com>
Message-ID: <1255321429.332126.1523177395867@mail.yahoo.com>

Hi Team,?????????????? Any input on this ?
Thanks-Nagu
 

    On Saturday, 31 March, 2018, 4:37:38 PM IST, Nagendra Kumar <nagen_kr at yahoo.co.in> wrote:  
 
 Dear Team,
I have a high availability use case below, please provide your inputs.I have 6 nodes running for clearwater services on 6 different nodes and 1 for DNS(on Node7) as below:(I am using AWS AMI ubuntu 14 image and have manually installed as per instruction at Manual Install Instructions ? Project Clearwater 1.0 documentation)Desired configuration :
Node1 : EllisNode2 : BonoNode3 : SproutNode4 : HomerNode5 : DimeNode6 : Vellum

Now, we have a requirement of scale up and scale down.Scale Down:
1. if Node1 fails, Ellis should start working on Node2. That means Node2 hosts Ellis and Bono both running. 2. After that if Node2 fails, Ellis and Bono should shift to Node3. That means Node3 hosts Ellis, Bono and Sprout.3. And so on..... till Node5 fails and Node6 hosts Ellis, Bono, Sprout, Homer, Dime and Vellum.

Will it need any source code change or work with just configuration change? Please send me instruction for making changes.I want to try the following for the above use case:1. I install all software on each nodes. That means Ellis is installed on all nodes, sprout is installed on all nodes, etc.2. But, in the beginning, Ellis is started on Node1, Bono is started on Node2 and so on as shown above (desired configuration).3. When Node1? fails, then I start Ellis on Node 2(say manually, I will automate it later). That means Node2 has Ellis and Bono running.4. When Node2 fails, I start Ellis and Bono on Node2...And the like...5. In the end, I want Node6 to have all software running.
Scale UP: 1. After step 5 above, where Node6 are running all software.2. Now, Node 5 has started and came up, I want to stop Dime on Node6 and want to start on Node5. That means Node5 has Dime running and Node6 has Ellis, Bono, Sprout, Homer, and Vellum running.
3. Node Node 4 has come up and running. I want to stop Homer on Node 6 and start it on Node4. That means Node 4 has Homer running, Node5 has Dime running and Node 6 has Ellis, Bono, Sprout and Vellum running.4. And so on till I get the desired configuration running as shown above.

It should happen when call is running and call shouldn't drop.
Any help will be deeply appreciated.
Thanks-Nagu

  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180408/31543ded/attachment.html>

From nagen_kr at yahoo.co.in  Sun Apr  8 05:33:42 2018
From: nagen_kr at yahoo.co.in (Nagendra Kumar)
Date: Sun, 8 Apr 2018 09:33:42 +0000 (UTC)
Subject: [Project Clearwater] Lost the IPs of clearwater node
In-Reply-To: <CADabj4aBbJTZ=rq0qg1w6tHsb6=dy-S76arHqmiQqESoEuYp_A@mail.gmail.com>
References: <CAHwYWpDVEZP2PpzHYafkDrJi_445qjO8=mqXxmUuMxweV+ozkA@mail.gmail.com>
	<CADabj4aBbJTZ=rq0qg1w6tHsb6=dy-S76arHqmiQqESoEuYp_A@mail.gmail.com>
Message-ID: <44393775.342921.1523180022258@mail.yahoo.com>

Hi Sunil,????????????? I am not aware of your setup details, but its better to use static IPs for all the nodes as private IPs, so that when it comes up, it holds those IPs and you needn't make any changes in DNS. The public IP of nodes will change after reboot(because of DHCP), but it doesn't make any difference if you want to make a simple call except that public IP of bono is needed as you may have to access it from any remote node from SIP agent like Zoiper,Sipdroid, etc.
 So, configure all the nodes local_config as below for a simple call setup(6 nodes + 1 DNS node):
sudo vi /etc/clearwater/local_config
local_ip=10.0.0.173
public_ip=13.127.138.201
public_hostname=ip-10-0-0-173
etcd_cluster="10.0.0.83,10.0.0.196,10.0.0.249,10.0.0.225,10.0.0.195,10.0.0.173"

Above, local_ip is private IP (you can have static), public IP can be any thing. etcd_cluster should have private IPs(static IPs)
Also, if you are using bind9 DNS server then, you can configure db.domain (e.g. db.nags.hd) file as below(configure private IPs(static):
$TTL 5m ; Default TTL

; SOA, NS and A record for DNS server itself
@???????????????? 3600 IN SOA? ns admin ( 2014010800 ; Serial
????????????????????????????????????????? 3600?????? ; Refresh
????????????????????????????????????????? 3600?????? ; Retry
????????????????????????????????????????? 3600?????? ; Expire
????????????????????????????????????????? 300 )????? ; Minimum TTL
@???????????????? 3600 IN NS?? ns
ns??????????????? 3600 IN A??? 10.0.0.178 ; IPv4 address of BIND server
;ns??????????????? 3600 IN AAAA 1::1??? ; IPv6 address of BIND server

; bono
; ====
;
; Per-node records - not required to have both IPv4 and IPv6 records
bono-1???????????????? IN A???? 10.0.0.196
;bono-2???????????????? IN A???? 2.0.0.2
;bono-1???????????????? IN AAAA? 2::1
;bono-2???????????????? IN AAAA? 2::2
;
; Cluster A and AAAA records - UEs that don't support RFC 3263 will simply
; resolve the A or AAAA records and pick randomly from this set of addresses.
@????????????????????? IN A???? 10.0.0.196
;@????????????????????? IN A???? 2.0.0.2
;@????????????????????? IN AAAA? 2::1
;@????????????????????? IN AAAA? 2::2
;
; NAPTR and SRV records - these indicate a preference for TCP and then resolve
; to port 5060 on the per-node records defined above.
@????????????????????? IN NAPTR 1 1 "S" "SIP+D2T" "" _sip._tcp
@????????????????????? IN NAPTR 2 1 "S" "SIP+D2U" "" _sip._udp
_sip._tcp????????????? IN SRV?? 0 0 5060 bono-1
;_sip._tcp????????????? IN SRV?? 0 0 5060 bono-2
_sip._udp????????????? IN SRV?? 0 0 5060 bono-1
;_sip._udp????????????? IN SRV?? 0 0 5060 bono-2

; sprout
; ======
;
; Per-node records - not required to have both IPv4 and IPv6 records
sprout-1?????????????? IN A???? 10.0.0.249
;sprout-2?????????????? IN A???? 3.0.0.2
;sprout-1?????????????? IN AAAA? 3::1
;sprout-2?????????????? IN AAAA? 3::2
;
; Cluster A and AAAA records - P-CSCFs that don't support RFC 3263 will simply
; resolve the A or AAAA records and pick randomly from this set of addresses.
sprout???????????????? IN A???? 10.0.0.249
;sprout???????????????? IN A???? 3.0.0.2
;sprout???????????????? IN AAAA? 3::1
;sprout???????????????? IN AAAA? 3::2
;
; Cluster A and AAAA records - P-CSCFs that don't support RFC 3263 will simply
; resolve the A or AAAA records and pick randomly from this set of addresses.
scscf.sprout?????????? IN A???? 10.0.0.249
;scscf.sprout?????????? IN A???? 3.0.0.2
;scscf.sprout?????????? IN AAAA? 3::1
;scscf.sprout?????????? IN AAAA? 3::2
;
; NAPTR and SRV records - these indicate TCP support only and then resolve
; to port 5054 on the per-node records defined above.
sprout???????????????? IN NAPTR 1 1 "S" "SIP+D2T" "" _sip._tcp.sprout
_sip._tcp.sprout?????? IN SRV?? 0 0 5054 sprout-1
;_sip._tcp.sprout?????? IN SRV?? 0 0 5054 sprout-2
;
; NAPTR and SRV records for S-CSCF - these indicate TCP support only and
; then resolve to port 5054 on the per-node records defined above.
scscf.sprout?????????? IN NAPTR 1 1 "S" "SIP+D2T" "" _sip._tcp.scscf.sprout
_sip._tcp.scscf.sprout IN SRV?? 0 0 5054 sprout-1
;_sip._tcp.scscf.sprout IN SRV?? 0 0 5054 sprout-2
;
; Cluster A and AAAA records - P-CSCFs that don't support RFC 3263 will simply
; resolve the A or AAAA records and pick randomly from this set of addresses.
icscf.sprout?????????? IN A???? 10.0.0.249
;icscf.sprout?????????? IN A???? 3.0.0.2
;icscf.sprout?????????? IN AAAA? 3::1
;icscf.sprout?????????? IN AAAA? 3::2
;
; NAPTR and SRV records for I-CSCF - these indicate TCP support only and
; then resolve to port 5052 on the per-node records defined above.
icscf.sprout?????????? IN NAPTR 1 1 "S" "SIP+D2T" "" _sip._tcp.icscf.sprout
_sip._tcp.icscf.sprout IN SRV?? 0 0 5052 sprout-1
;_sip._tcp.icscf.sprout IN SRV?? 0 0 5052 sprout-2

; dime
; =========
;
; Per-node records - not required to have both IPv4 and IPv6 records
dime-1???????????????? IN A???? 10.0.0.195 
;dime-2???????????????? IN A???? 4.0.0.2
;dime-1???????????????? IN AAAA? 4::1
;dime-2???????????????? IN AAAA? 4::2
;
; Cluster A and AAAA records - sprout, bono and ellis pick randomly from these.
hs???????????????????? IN A???? 10.0.0.195
;hs???????????????????? IN A???? 4.0.0.2
;hs???????????????????? IN AAAA? 4::1
;hs???????????????????? IN AAAA? 4::2
ralf?????????????????? IN A???? 10.0.0.195
;ralf?????????????????? IN A???? 4.0.0.2
;ralf?????????????????? IN AAAA? 4::1
;ralf?????????????????? IN AAAA? 4::2
;
; (No need for NAPTR or SRV records as dime doesn't handle SIP traffic.)

; homer
; =====
;
; Per-node records - not required to have both IPv4 and IPv6 records
homer-1??????????????? IN A???? 10.0.0.225
;homer-2??????????????? IN A???? 5.0.0.2
;homer-1??????????????? IN AAAA? 5::1
;homer-2??????????????? IN AAAA? 5::2
;
; Cluster A and AAAA records - sprout picks randomly from these.
homer????????????????? IN A???? 10.0.0.225
;homer????????????????? IN A???? 5.0.0.2
;homer????????????????? IN AAAA? 5::1
;homer????????????????? IN AAAA? 5::2
;
; (No need for NAPTR or SRV records as homer doesn't handle SIP traffic.)

; vellum
; =====
;
; Per-node records - not required to have both IPv4 and IPv6 records
vellum-1?????????????? IN A???? 10.0.0.173
;vellum-2?????????????? IN A???? 6.0.0.2
;vellum-1?????????????? IN AAAA? 6::1
;vellum-2?????????????? IN AAAA? 6::2
;
; Cluster A and AAAA records - sprout, homer and dime pick randomly from these.
vellum???????????????? IN A???? 10.0.0.173
;vellum???????????????? IN A???? 6.0.0.2
;vellum???????????????? IN AAAA? 6::1
;vellum???????????????? IN AAAA? 6::2
;
; (No need for NAPTR or SRV records as vellum doesn't handle SIP traffic.)

; ellis
; =====
;
; ellis is not clustered, so there's only ever one node.
;
; Per-node record - not required to have both IPv4 and IPv6 records
ellis-1??????????????? IN A???? 10.0.0.83
;ellis-1??????????????? IN AAAA? 7::1
;
; "Cluster"/access A and AAAA record
ellis????????????????? IN A???? 10.0.0.83
;ellis????????????????? IN AAAA? 7::1


This will make your cluster reboot safe.
Thanks-Nagu
    On Friday, 6 April, 2018, 6:07:24 PM IST, tahir Masood <masood.tahir696 at gmail.com> wrote:  
 
 Assign IP's again and update the hosts/DNS.
Regards,
Tahir
On Fri, Apr 6, 2018 at 11:15 AM, Sunil Kumar <skgola1997 at gmail.com> wrote:

Hi all,I have installed the clearwater using manual installation. I used briidge adapter and ip is assigned by DHCP. Some how the machine is power off and all the clearwater nodes are aborted (power off), in result some nodes have lost their ip and some of them are interchange (IPs). Do i need to assign IP manually same as earlier??everything is lost that is not connected. What I need to do please let me know.
Thanks,sunil
______________________________ _________________
Clearwater mailing list
Clearwater at lists. projectclearwater.org
http://lists. projectclearwater.org/mailman/ listinfo/clearwater_lists. projectclearwater.org



_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180408/cb5a5976/attachment.html>

From nagen_kr at yahoo.co.in  Sun Apr  8 05:44:22 2018
From: nagen_kr at yahoo.co.in (Nagendra Kumar)
Date: Sun, 8 Apr 2018 09:44:22 +0000 (UTC)
Subject: [Project Clearwater] check registered state of user
In-Reply-To: <MWHPR18MB1151F3267DEF7E707CA11ABCA9BA0@MWHPR18MB1151.namprd18.prod.outlook.com>
References: <MWHPR18MB1151F3267DEF7E707CA11ABCA9BA0@MWHPR18MB1151.namprd18.prod.outlook.com>
Message-ID: <968533738.3679.1523180662493@mail.yahoo.com>

Hi Jake,?????????????? You can also check the registered user at ellis :
sudo mysql
use ellis; SELECT * FROM numbers ;

You can also check the data bases using the command: SHOW DATABASES;You can check selective users by command: SELECT * FROM numbers WHERE number = 'sip:6505550001 at nags.hd';
Thanks-Nagu



 

    On Friday, 6 April, 2018, 7:32:54 PM IST, Jake Brown <jake at dccllc.net> wrote:  
 
  <!--#yiv6766379439 _filtered #yiv6766379439 {font-family:"Cambria Math";panose-1:0 0 0 0 0 0 0 0 0 0;} _filtered #yiv6766379439 {font-family:Calibri;panose-1:2 15 5 2 2 2 4 3 2 4;}#yiv6766379439 #yiv6766379439 p.yiv6766379439MsoNormal, #yiv6766379439 li.yiv6766379439MsoNormal, #yiv6766379439 div.yiv6766379439MsoNormal {margin:0in;margin-bottom:.0001pt;font-size:11.0pt;font-family:"Calibri", sans-serif;}#yiv6766379439 a:link, #yiv6766379439 span.yiv6766379439MsoHyperlink {color:#0563C1;text-decoration:underline;}#yiv6766379439 a:visited, #yiv6766379439 span.yiv6766379439MsoHyperlinkFollowed {color:#954F72;text-decoration:underline;}#yiv6766379439 span.yiv6766379439EmailStyle17 {font-family:"Calibri", sans-serif;color:windowtext;}#yiv6766379439 .yiv6766379439MsoChpDefault {font-family:"Calibri", sans-serif;} _filtered #yiv6766379439 {margin:1.0in 1.0in 1.0in 1.0in;}#yiv6766379439 div.yiv6766379439WordSection1 {}-->
I must have something wrong in the shared_config.? I would expect to see something in vellum in the homestead_cache/impu table that indicates registered in the is_registered column.? Is this not the correct place to check to see if a subscriber is registered?? Thank you for your guidance.
 
 ?
 
Jake
 _______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180408/5fafcbe4/attachment.html>

From jake at dccllc.net  Sun Apr  8 09:23:07 2018
From: jake at dccllc.net (Jake Brown)
Date: Sun, 8 Apr 2018 13:23:07 +0000
Subject: [Project Clearwater] check registered state of user
In-Reply-To: <968533738.3679.1523180662493@mail.yahoo.com>
References: <MWHPR18MB1151F3267DEF7E707CA11ABCA9BA0@MWHPR18MB1151.namprd18.prod.outlook.com>,
	<968533738.3679.1523180662493@mail.yahoo.com>
Message-ID: <DM5PR18MB11456D3363E772524096E427A9B80@DM5PR18MB1145.namprd18.prod.outlook.com>

Nagu,

Would this mean that the only way to check registration status would be to use a Ellis node.  Currently we don't use one due to the lacking of browser support to modify the call forwarding features.

Thanks

Jake



Sent via the Samsung Galaxy S8+, an AT&T 4G LTE smartphone


-------- Original message --------
From: Nagendra Kumar <nagen_kr at yahoo.co.in>
Date: 4/8/18 4:44 AM (GMT-06:00)
To: clearwater at lists.projectclearwater.org, Jake Brown <jake at dccllc.net>
Subject: Re: [Project Clearwater] check registered state of user

Hi Jake,
               You can also check the registered user at ellis :

sudo mysql
use ellis;
SELECT * FROM numbers ;

You can also check the data bases using the command: SHOW DATABASES;
You can check selective users by command: SELECT * FROM numbers WHERE number = 'sip:6505550001 at nags.hd';

Thanks
-Nagu





On Friday, 6 April, 2018, 7:32:54 PM IST, Jake Brown <jake at dccllc.net> wrote:



I must have something wrong in the shared_config.  I would expect to see something in vellum in the homestead_cache/impu table that indicates registered in the is_registered column.  Is this not the correct place to check to see if a subscriber is registered?  Thank you for your guidance.



Jake

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180408/42d0363a/attachment.html>

From nagen_kr at yahoo.co.in  Sun Apr  8 09:52:30 2018
From: nagen_kr at yahoo.co.in (Nagendra Kumar)
Date: Sun, 8 Apr 2018 13:52:30 +0000 (UTC)
Subject: [Project Clearwater] check registered state of user
In-Reply-To: <DM5PR18MB11456D3363E772524096E427A9B80@DM5PR18MB1145.namprd18.prod.outlook.com>
References: <MWHPR18MB1151F3267DEF7E707CA11ABCA9BA0@MWHPR18MB1151.namprd18.prod.outlook.com>
	<968533738.3679.1523180662493@mail.yahoo.com>
	<DM5PR18MB11456D3363E772524096E427A9B80@DM5PR18MB1145.namprd18.prod.outlook.com>
Message-ID: <1820482130.384973.1523195550678@mail.yahoo.com>

Hi Jake,?????????????? I am not sure about any other ways of checking registration status.
@Experts ??
Thanks-Nagu
 

    On Sunday, 8 April, 2018, 6:53:10 PM IST, Jake Brown <jake at dccllc.net> wrote:  
 
 Nagu,
Would this mean that the only way to check registration status would be to use a Ellis node.? Currently we don't use one due to the lacking of browser support to modify the call forwarding features.
Thanks
Jake


Sent via the Samsung Galaxy S8+, an AT&T 4G LTE smartphone

-------- Original message --------From: Nagendra Kumar <nagen_kr at yahoo.co.in> Date: 4/8/18 4:44 AM (GMT-06:00) To: clearwater at lists.projectclearwater.org, Jake Brown <jake at dccllc.net> Subject: Re: [Project Clearwater] check registered state of user 
Hi Jake,?????????????? You can also check the registered user at ellis :
sudo mysql
use ellis;SELECT * FROM numbers ;

You can also check the data bases using the command: SHOW DATABASES;You can check selective users by command: SELECT * FROM numbers WHERE number = 'sip:6505550001 at nags.hd';
Thanks-Nagu





On Friday, 6 April, 2018, 7:32:54 PM IST, Jake Brown <jake at dccllc.net> wrote:

#yiv6041205200 #yiv6041205200 -- filtered {}#yiv6041205200 filtered {font-family:Calibri;}#yiv6041205200 p.yiv6041205200MsoNormal, #yiv6041205200 li.yiv6041205200MsoNormal, #yiv6041205200 div.yiv6041205200MsoNormal {margin:0in;margin-bottom:.0001pt;font-size:11.0pt;font-family:sans-serif;}#yiv6041205200 a:link, #yiv6041205200 span.yiv6041205200MsoHyperlink {color:#0563C1;text-decoration:underline;}#yiv6041205200 a:visited, #yiv6041205200 span.yiv6041205200MsoHyperlinkFollowed {color:#954F72;text-decoration:underline;}#yiv6041205200 span.yiv6041205200EmailStyle17 {font-family:sans-serif;color:windowtext;}#yiv6041205200 .yiv6041205200MsoChpDefault {font-family:sans-serif;}#yiv6041205200 filtered {margin:1.0in 1.0in 1.0in 1.0in;}#yiv6041205200 
I must have something wrong in the shared_config.? I would expect to see something in vellum in the homestead_cache/impu table that indicates registered in the is_registered column.? Is this not the correct place to check to see if a subscriber is registered?? Thank you for your guidance.

?

Jake
_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180408/c4a020d5/attachment.html>

From skgola1997 at gmail.com  Tue Apr 10 07:00:42 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Tue, 10 Apr 2018 16:30:42 +0530
Subject: [Project Clearwater] Lost the IPs of clearwater node
Message-ID: <CAHwYWpCrSbtV0DeLFwL_gOOcu0--gub70_LU-m3hU33augdeDw@mail.gmail.com>

Hi,
Now I am using NAT mode for networking, but i have some dooubts:
1) Should public_ip is the ip of the host on which virtualbox is installed?
2) Should all nodes having the same public_ip in NAT mode?

3) Instead of doing port forwarding for particular node, I have left blank
host ip field that means it is opens for world, or do I need to open for
particular node?

Thanks,
Sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180410/429539a6/attachment.html>

From masood.tahir696 at gmail.com  Tue Apr 10 10:04:25 2018
From: masood.tahir696 at gmail.com (tahir Masood)
Date: Tue, 10 Apr 2018 14:04:25 +0000
Subject: [Project Clearwater] Lost the IPs of clearwater node
In-Reply-To: <CAHwYWpCrSbtV0DeLFwL_gOOcu0--gub70_LU-m3hU33augdeDw@mail.gmail.com>
References: <CAHwYWpCrSbtV0DeLFwL_gOOcu0--gub70_LU-m3hU33augdeDw@mail.gmail.com>
Message-ID: <CADabj4YcbYS9+Nmc79QkaHSLtdAjeawxCthUtSvZ92Lv_A1r8Q@mail.gmail.com>

No the public ip is the one which is assigned on the interface of that VM.
No they will have different ip's public ip is only required on bono
You need to assign that public ip to bono if you want it to be publicly
accessible

On Tue, 10 Apr 2018, 4:02 PM Sunil Kumar, <skgola1997 at gmail.com> wrote:

> Hi,
> Now I am using NAT mode for networking, but i have some dooubts:
> 1) Should public_ip is the ip of the host on which virtualbox is installed?
> 2) Should all nodes having the same public_ip in NAT mode?
>
> 3) Instead of doing port forwarding for particular node, I have left blank
> host ip field that means it is opens for world, or do I need to open for
> particular node?
>
> Thanks,
> Sunil
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
>
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180410/25ff06ab/attachment.html>

From skgola1997 at gmail.com  Tue Apr 10 10:19:05 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Tue, 10 Apr 2018 19:49:05 +0530
Subject: [Project Clearwater] Lost the IPs of clearwater node
In-Reply-To: <CADabj4YcbYS9+Nmc79QkaHSLtdAjeawxCthUtSvZ92Lv_A1r8Q@mail.gmail.com>
References: <CAHwYWpCrSbtV0DeLFwL_gOOcu0--gub70_LU-m3hU33augdeDw@mail.gmail.com>
	<CADabj4YcbYS9+Nmc79QkaHSLtdAjeawxCthUtSvZ92Lv_A1r8Q@mail.gmail.com>
Message-ID: <CAHwYWpCzXz028+_XQgGRah5GLCu4pLnaHHSUiH0QjtCkGTOp4w@mail.gmail.com>

Hi,
thanks for replying,
How can i find the ip of interface of the vm?

one more thing, after poweroff ,the ip of vm is changed, so in which places
i need to update the changed IP. I have update in following places still it
did not works:

/etc/clearwater/local_config in all nodes
/etc/hosts
/etc/bind/db.ims.com

after that i stop the service like sudo service ellis etc.

where more do i need to update?

thanks,
sunil


On Tue, Apr 10, 2018 at 7:34 PM, tahir Masood <masood.tahir696 at gmail.com>
wrote:

> No the public ip is the one which is assigned on the interface of that VM.
> No they will have different ip's public ip is only required on bono
> You need to assign that public ip to bono if you want it to be publicly
> accessible
>
> On Tue, 10 Apr 2018, 4:02 PM Sunil Kumar, <skgola1997 at gmail.com> wrote:
>
>> Hi,
>> Now I am using NAT mode for networking, but i have some dooubts:
>> 1) Should public_ip is the ip of the host on which virtualbox is
>> installed?
>> 2) Should all nodes having the same public_ip in NAT mode?
>>
>> 3) Instead of doing port forwarding for particular node, I have left
>> blank host ip field that means it is opens for world, or do I need to open
>> for particular node?
>>
>> Thanks,
>> Sunil
>> _______________________________________________
>> Clearwater mailing list
>> Clearwater at lists.projectclearwater.org
>> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
>> projectclearwater.org
>>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180410/21005c8c/attachment.html>

From richard.whitehouse at projectclearwater.org  Tue Apr 10 11:32:03 2018
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Tue, 10 Apr 2018 15:32:03 +0000
Subject: [Project Clearwater] =?iso-8859-1?q?Release_note_for_Sprint_Zam?=
 =?iso-8859-1?q?=EEn?=
Message-ID: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>

Hi all,

The release for Project Clearwater sprint "Zam?n" has been cut. The code for this release is tagged as release-130 in GitHub.

This is a big release for us! Over the last year, we've put a huge amount of effort into increasing Project Clearwater's quality, as we know that reliability is extremely important to most users of IMS. We're proud of where we've got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first "stable" release.

We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It's time to move our core development to a new phase, focussed more on stability, and less on new features. That's going to mean that we won't expect to tag new releases as frequently. It's also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.


This release includes a number of bug fixes which improves Clearwater's reliability at performance and load.

To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.


Cheers,

Richard

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180410/0f8488f2/attachment.html>

From skgola1997 at gmail.com  Wed Apr 11 00:38:42 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Wed, 11 Apr 2018 10:08:42 +0530
Subject: [Project Clearwater] Regarding manual setup
Message-ID: <CAHwYWpBD5hFzenhRJEWqNnRNh_O8FwzTkfdXhovNH126jDSpAQ@mail.gmail.com>

Hi all,

Is there any one uses virtualbox for installing clearwater using NAT or NAT
network or any other network mode except bridge adapter.

I am using NAT and i got the the private IP like 10.0.2.15, my doubt is *what
is the public IP?*
How can i find the public IP, is this the IP of host machine on which i
have installed the virtualbox.
*Can all node having the same Public IP?*

using NAT all node is taking same private IP, but in NAT network all taking
IP in the range.

Thanks,
sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180411/026c540c/attachment.html>

From skgola1997 at gmail.com  Thu Apr 12 03:05:43 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Thu, 12 Apr 2018 12:35:43 +0530
Subject: [Project Clearwater] Manual installing
Message-ID: <CAHwYWpBEM0UqOCsjOhNon+h7wdHTyRCv8PFgrFKWZb9=TcsX0w@mail.gmail.com>

Hi all,
I am trying to install clearwater manually in virtualbox. I am using NAT
and host only network network mode there, and i make host only network's ip
as static and used in local_ip and dns also (NAT is used for connecting to
internet).
I have installed everything perfectly but facing problem while login to
ellis its not opening.
when i am trying to stress node it also not work giving error like:

2018-04-12      18:35:36.672571 1523538336.672571: Unknown remote host
'*sprout.imstest.com
<http://sprout.imstest.com>*' (Name or service not known, Inappropriate
ioctl for device).
Use 'sipp -h' for details.

*local_config:*
*local_ip=192.168.56.102*
*public_ip= 192.168.56.102 *
*public_hostname=dime-A*
*etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,192.168.56.1052,192.168.56.106,192.168.56.107"*



*bono's local_config:*

local_ip=192.168.56.105
public_ip= 10.224.61.82
public_hostname=bono-A
etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,192.168.56.1052,192.168.56.106,192.168.56.107"



i used public and private_ip same in local_config file i.e. both private, I
just want to run stress node. in* bono' local_config file i gave public_ip
as the ip of host itself because in NAT mode the vm is connecting to
internet using the ip of host itself thats why* (please guide if it is
wrong).


*monit summary:*


*[bono]ubuntu at bono-A:~$ sudo monit summary*
Monit 5.18.1 uptime: 2h 9m
 Service Name                     Status                      Type
 node-bono-A                      Running                     System
 restund_process                  Execution failed | Does...  Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 *etcd_process                     Execution failed | Does...  Process*
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 *poll_restund                     Wait parent                 Program*
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
* etcd_uptime                      Wait parent                 Program*
* poll_etcd_cluster                Wait parent                 Program*
* poll_etcd                        Wait parent                 Program*
* poll_bono                        Status failed               Program*


*[dime]ubuntu at dime-A:~$ sudo monit summary*
[sudo] password for ubuntu:
Monit 5.18.1 uptime: 2h 9m
 Service Name                     Status                      Type
 node-dime-A                      Running                     System
 snmpd_process                    Running                     Process
 ralf_process                     Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 ralf_uptime                      Status ok                   Program
 poll_ralf                        Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
* poll_etcd_cluster                Status failed               Program*
 poll_etcd                        Status ok                   Program
[dime]ubuntu at dime-A:~$




*[sprout]ubuntu at sprout-A:~$ sudo monit summary*
[sudo] password for ubuntu:
Monit 5.18.1 uptime: 2h 9m
 Service Name                     Status                      Type
 node-sprout-A                    Running                     System
 sprout_process                   Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 sprout_uptime                    Status ok                   Program
 poll_sprout_sip                  Status ok                   Program
 poll_sprout_http                 Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
* poll_etcd_cluster                Status failed               Program*
 poll_etcd                        Status ok                   Program


etc.


please guide any solution

thanks in avance

Regards
sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180412/138151ac/attachment.html>

From skgola1997 at gmail.com  Thu Apr 12 05:10:15 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Thu, 12 Apr 2018 14:40:15 +0530
Subject: [Project Clearwater] Manual installing
In-Reply-To: <CAHwYWpBEM0UqOCsjOhNon+h7wdHTyRCv8PFgrFKWZb9=TcsX0w@mail.gmail.com>
References: <CAHwYWpBEM0UqOCsjOhNon+h7wdHTyRCv8PFgrFKWZb9=TcsX0w@mail.gmail.com>
Message-ID: <CAHwYWpCrnYN2usWBCC8S6KvGnQadB63_CxOG3JLxEroZNXKy=A@mail.gmail.com>

Hi,

In bono it giving error like:

[bono]ubuntu at bono-A:~$* cw-config download shared_config*
Unable to contact the etcd cluster.

and there is no shared_config file, while in other nodes i am able to
download the shared file but i am not able to open it except dime.

what should i do?
Please provide some solution.


*[bono]ubuntu at bono-A:/var/log/bono$ cat bono_err.log*

11-04-2018 20:46:11.548 UTC Advanced stack dump (requires gdb):
sh: 1: /usr/bin/gdb: not found
gdb failed with return code 32512

12-04-2018 12:40:51.950 UTC Advanced stack dump (requires gdb):
sh: 1: /usr/bin/gdb: not found
gdb failed with return code 32512

12-04-2018 12:46:19.069 UTC Advanced stack dump (requires gdb):
sh: 1: /usr/bin/gdb: not found
gdb failed with return code 32512

12-04-2018 17:00:01.191 UTC Advanced stack dump (requires gdb):
sh: 1: /usr/bin/gdb: not found
gdb failed with return code 32512



*[bono]ubuntu at bono-A:/var/log/bono$ cat bono_current.log*
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)
12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
Not found (PJ_ENOTFOUND)


On Thu, Apr 12, 2018 at 12:35 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi all,
> I am trying to install clearwater manually in virtualbox. I am using NAT
> and host only network network mode there, and i make host only network's ip
> as static and used in local_ip and dns also (NAT is used for connecting to
> internet).
> I have installed everything perfectly but facing problem while login to
> ellis its not opening.
> when i am trying to stress node it also not work giving error like:
>
> 2018-04-12      18:35:36.672571 1523538336.672571: Unknown remote host '*sprout.imstest.com
> <http://sprout.imstest.com>*' (Name or service not known, Inappropriate
> ioctl for device).
> Use 'sipp -h' for details.
>
> *local_config:*
> *local_ip=192.168.56.102*
> *public_ip= 192.168.56.102 *
> *public_hostname=dime-A*
>
> *etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,192.168.56.1052,192.168.56.106,192.168.56.107"*
>
>
>
> *bono's local_config:*
>
> local_ip=192.168.56.105
> public_ip= 10.224.61.82
> public_hostname=bono-A
> etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,
> 192.168.56.1052,192.168.56.106,192.168.56.107"
>
>
>
> i used public and private_ip same in local_config file i.e. both private,
> I just want to run stress node. in* bono' local_config file i gave
> public_ip as the ip of host itself because in NAT mode the vm is connecting
> to internet using the ip of host itself thats why* (please guide if it is
> wrong).
>
>
> *monit summary:*
>
>
> *[bono]ubuntu at bono-A:~$ sudo monit summary*
> Monit 5.18.1 uptime: 2h 9m
>  Service Name                     Status                      Type
>  node-bono-A                      Running                     System
>  restund_process                  Execution failed | Does...  Process
>  ntp_process                      Running                     Process
>  clearwater_queue_manager_pro...  Running                     Process
>  *etcd_process                     Execution failed | Does...  Process*
>  clearwater_diags_monitor_pro...  Running                     Process
>  clearwater_config_manager_pr...  Running                     Process
>  clearwater_cluster_manager_p...  Running                     Process
>  bono_process                     Running                     Process
>  *poll_restund                     Wait parent                 Program*
>  monit_uptime                     Status ok                   Program
>  clearwater_queue_manager_uptime  Status ok                   Program
> * etcd_uptime                      Wait parent                 Program*
> * poll_etcd_cluster                Wait parent                 Program*
> * poll_etcd                        Wait parent                 Program*
> * poll_bono                        Status failed               Program*
>
>
> *[dime]ubuntu at dime-A:~$ sudo monit summary*
> [sudo] password for ubuntu:
> Monit 5.18.1 uptime: 2h 9m
>  Service Name                     Status                      Type
>  node-dime-A                      Running                     System
>  snmpd_process                    Running                     Process
>  ralf_process                     Running                     Process
>  ntp_process                      Running                     Process
>  nginx_process                    Running                     Process
>  homestead_process                Running                     Process
>  homestead-prov_process           Running                     Process
>  clearwater_queue_manager_pro...  Running                     Process
>  etcd_process                     Running                     Process
>  clearwater_diags_monitor_pro...  Running                     Process
>  clearwater_config_manager_pr...  Running                     Process
>  clearwater_cluster_manager_p...  Running                     Process
>  ralf_uptime                      Status ok                   Program
>  poll_ralf                        Status ok                   Program
>  nginx_ping                       Status ok                   Program
>  nginx_uptime                     Status ok                   Program
>  monit_uptime                     Status ok                   Program
>  homestead_uptime                 Status ok                   Program
>  poll_homestead                   Status ok                   Program
>  check_cx_health                  Status ok                   Program
>  poll_homestead-prov              Status ok                   Program
>  clearwater_queue_manager_uptime  Status ok                   Program
>  etcd_uptime                      Status ok                   Program
> * poll_etcd_cluster                Status failed               Program*
>  poll_etcd                        Status ok                   Program
> [dime]ubuntu at dime-A:~$
>
>
>
>
> *[sprout]ubuntu at sprout-A:~$ sudo monit summary*
> [sudo] password for ubuntu:
> Monit 5.18.1 uptime: 2h 9m
>  Service Name                     Status                      Type
>  node-sprout-A                    Running                     System
>  sprout_process                   Running                     Process
>  ntp_process                      Running                     Process
>  nginx_process                    Running                     Process
>  clearwater_queue_manager_pro...  Running                     Process
>  etcd_process                     Running                     Process
>  clearwater_diags_monitor_pro...  Running                     Process
>  clearwater_config_manager_pr...  Running                     Process
>  clearwater_cluster_manager_p...  Running                     Process
>  sprout_uptime                    Status ok                   Program
>  poll_sprout_sip                  Status ok                   Program
>  poll_sprout_http                 Status ok                   Program
>  nginx_ping                       Status ok                   Program
>  nginx_uptime                     Status ok                   Program
>  monit_uptime                     Status ok                   Program
>  clearwater_queue_manager_uptime  Status ok                   Program
>  etcd_uptime                      Status ok                   Program
> * poll_etcd_cluster                Status failed               Program*
>  poll_etcd                        Status ok                   Program
>
>
> etc.
>
>
> please guide any solution
>
> thanks in avance
>
> Regards
> sunil
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180412/ef3116a8/attachment.html>

From skgola1997 at gmail.com  Thu Apr 12 05:35:41 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Thu, 12 Apr 2018 15:05:41 +0530
Subject: [Project Clearwater] Manual installing
In-Reply-To: <CAHwYWpCrnYN2usWBCC8S6KvGnQadB63_CxOG3JLxEroZNXKy=A@mail.gmail.com>
References: <CAHwYWpBEM0UqOCsjOhNon+h7wdHTyRCv8PFgrFKWZb9=TcsX0w@mail.gmail.com>
	<CAHwYWpCrnYN2usWBCC8S6KvGnQadB63_CxOG3JLxEroZNXKy=A@mail.gmail.com>
Message-ID: <CAHwYWpCfK+vx1nCWv6RLcJRvnmvPhDX9N8w0R4oXEt_kG-EbqQ@mail.gmail.com>

Hi,
while installing there is no error and every node is install perfectly and
everything is fine but i don't what wrong is going on. can anyone please
help me, there is no shared_config file and dns.json file in bono node i
dont know why. How can i fix this problem.

*[bono]ubuntu at bono-A:/etc/clearwater$ cat /var/log/syslog*

Apr 12 22:24:10 bono-A issue-alarm: zmq_msg_recv: Invalid argument
Apr 12 22:24:22 bono-A clearwater-etcd: Failed to add the local node
(192.168.56.105) to the etcd cluster
Apr 12 22:24:28 bono-A queue-manager[1376]: dropped request: 'issue-alarm
queue-manager 9001.1'
Apr 12 22:24:31 bono-A queue-manager[1376]: dropped request: 'issue-alarm
queue-manager 9002.1'
Apr 12 22:24:37 bono-A ntpd_intres[2798]: host name not found:
0.ubuntu.pool.ntp.org
Apr 12 22:24:40 bono-A bono[7165]: 2005 - Description: Application started.
@@Cause: The application is starting. @@Effect: Normal. @@Action: None.
Apr 12 22:24:40 bono-A bono[7165]: 1013 - Description: DNS config file is
missing. @@Cause: The DNS config file /etc/clearwater/dns.json is not
present. @@Effect: The DNS config file will be ignored, and all DNS queries
will be directed at the DNS server rather than using any local overrides.
@@Action: (1). Replace the missing DNS config file if desired.(2). Upload
the corrected config with
/usr/share/clearwater/clearwater-config-manager/scripts/upload_dns_json (if
no config file is present, no DNS overrides will be applied)
Apr 12 22:24:40 bono-A bono[7165]: 1013 - Description: DNS config file is
missing. @@Cause: The DNS config file /etc/clearwater/dns.json is not
present. @@Effect: The DNS config file will be ignored, and all DNS queries
will be directed at the DNS server rather than using any local overrides.
@@Action: (1). Replace the missing DNS config file if desired.(2). Upload
the corrected config with
/usr/share/clearwater/clearwater-config-manager/scripts/upload_dns_json (if
no config file is present, no DNS overrides will be applied)
Apr 12 22:24:40 bono-A bono[7165]: 1015 - Description: The SAS config file
is missing. @@Cause: The SAS config file /etc/clearwater/sas.json is
missing. @@Effect: The SAS config has not been updated.  The last valid
configuration will continue to be used @@Action: The SAS configuration
should be defined in /etc/clearwater/sas.json. Populate this file according
to the documentation.
Apr 12 22:24:40 bono-A bono[7165]: 2013 - Description: The application did
not start a connection to Ralf because Ralf is not enabled. @@Cause: Ralf
was not configured in the /etc/clearwater/config file. @@Effect: Billing
service will not be available. @@Action: Correct the /etc/clearwater/config
file if the billing feature is desired.
Apr 12 22:24:40 bono-A bono[7165]: 2060 - Description: The fallback iFCs
configuration file is not present. @@Cause: The S-CSCF supports fallback
iFCs, but the configuration file for them does not exist. @@Effect: The
S-CSCF will not be able to correctly apply any fallback iFCs. @@Action: The
fallback iFCs should be defined in /etc/clearwater/fallback_ifcs.xml.
Create this file according to the documentation. If you are expecting
clearwater-config-manager to be managing this file, check that it is
running and that there are no ENT logs relating to it or clearwater-etcd.
Apr 12 22:24:40 bono-A bono[7165]: 2067 - Description: The RPH file is not
present. @@Cause: The S-CSCF supports message prioritization based on the
Resource-Priority header, but the configuration file for this does not
exist. @@Effect: The S-CSCF will not be able to prioritize messages based
on a Resource-Priority header. @@Action: The RPH configuration should be
defined in /etc/clearwater/rph.json. Create this file according to the
documentation. If you are expecting clearwater-config-manager to be
managing this file, check that it is running and that there are no ENT logs
relating to it or clearwater-etcd.
Apr 12 22:24:41 bono-A bono[7165]: <analytics>
2018-04-12T16:54:41.141+00:00 Call-Disconnected: CALL_ID=poll-sip-15447
REASON=408
Apr 12 22:24:53 bono-A dnsmasq[10628]: exiting on receipt of SIGTERM
Apr 12 22:24:55 bono-A dnsmasq[7276]: started, version 2.68 cachesize 150
Apr 12 22:24:55 bono-A dnsmasq[7276]: compile time options: IPv6 GNU-getopt
DBus i18n IDN DHCP DHCPv6 no-Lua TFTP conntrack ipset auth
Apr 12 22:24:55 bono-A dnsmasq[7276]: reading /etc/dnsmasq.resolv.conf
Apr 12 22:24:55 bono-A dnsmasq[7276]: using nameserver 10.224.61.20#53
Apr 12 22:24:55 bono-A dnsmasq[7276]: using nameserver 10.224.61.82#53
Apr 12 22:24:55 bono-A dnsmasq[7276]: read /etc/hosts - 7 addresses
Apr 12 22:24:55 bono-A queue-manager[1376]: dropped request: 'issue-alarm
queue-manager 9002.1'
Apr 12 22:24:57 bono-A queue-manager[1376]: dropped request: 'issue-alarm
queue-manager 9001.1'
Apr 12 22:24:57 bono-A ntpd_intres[2798]: host name not found:
1.ubuntu.pool.ntp.org
Apr 12 22:24:58 bono-A queue-manager[1376]: dropped request: 'issue-alarm
queue-manager 9001.1'
Apr 12 22:25:01 bono-A queue-manager[1376]: dropped request: 'issue-alarm
queue-manager 9002.1'
Apr 12 22:25:01 bono-A CRON[7347]: (root) CMD (/usr/lib/sysstat/sadc 1 1
/var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Apr 12 22:25:01 bono-A CRON[7348]: (root) CMD (command -v debian-sa1 >
/dev/null && debian-sa1 1 1)
Apr 12 22:25:01 bono-A CRON[7349]: (root) CMD (/usr/sbin/iotop -b -o -t -n
1 -k >> /var/log/iotop.log 2>&1)
Apr 12 22:25:17 bono-A ntpd_intres[2798]: host name not found:
2.ubuntu.pool.ntp.org
Apr 12 22:25:28 bono-A queue-manager[1376]: dropped request: 'issue-alarm
queue-manager 9001.1'
Apr 12 22:25:31 bono-A queue-manager[1376]: dropped request: 'issue-alarm
queue-manager 9002.1'
Apr 12 22:25:37 bono-A ntpd_intres[2798]: host name not found:
3.ubuntu.pool.ntp.org
Apr 12 22:25:57 bono-A issue-alarm: zmq_msg_recv: Invalid argument
Apr 12 22:25:57 bono-A issue-alarm: zmq_msg_recv: Invalid argument
Apr 12 22:25:57 bono-A issue-alarm: zmq_msg_recv: Invalid argument
Apr 12 22:25:57 bono-A ntpd_intres[2798]: host name not found:
ntp.ubuntu.com
Apr 12 22:25:58 bono-A queue-manager[1376]: dropped request: 'issue-alarm
queue-manager 9001.1'
Apr 12 22:26:00 bono-A queue-manager[1376]: dropped request: 'issue-alarm
queue-manager 9002.1'
Apr 12 22:26:00 bono-A bono[7165]: 2021 - Description: The application is
ending -- Shutting down. @@Cause: The application has been terminated by
monit or has exited. @@Effect: Application services are no longer
available. @@Action: (1). This occurs normally when the application is
stopped. (2). If the application failed to respond to monit queries in a
timely manner, monit restarts the application.  This can occur if the
application is busy or unresponsive.
Apr 12 22:26:01 bono-A CRON[7468]: (root) CMD (/usr/sbin/iotop -b -o -t -n
1 -k >> /var/log/iotop.log 2>&1)
Apr 12 22:26:01 bono-A CRON[7469]: (root) CMD (/usr/lib/sysstat/sadc 1 1
/var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Apr 12 22:26:01 bono-A queue-manager[1376]: dropped request: 'issue-alarm
queue-manager 9002.1'
Apr 12 22:26:02 bono-A queue-manager[1376]: dropped request: 'issue-alarm
queue-manager 9001.1'
Apr 12 22:26:26 bono-A clearwater-etcd: Failed to add the local node
(192.168.56.105) to the etcd cluster




*[bono]ubuntu at bono-A:/etc/clearwater$ cat /var/log/monit.log*

Connection to 192.168.56.105 5058 port [tcp/*] succeeded!
stdout was:
SIP/2.0 408 Request Timeout
Via: SIP/2.0/TCP
192.168.56.105;rport=44514;received=192.168.56.105;branch=z9hG4bK-17478
Call-ID: poll-sip-17478
From: "poll-sip" <sip:poll-sip at 192.168.56.105>;tag=17478
To: <sip:poll-sip at 192.168.56.105>;tag=z9hG4bK-17478
CSeq: 17478 OPTIONS
Content-Length:  0
[IST Apr 12 22:59:52] error    : 'restund_process' process is not running
[IST Apr 12 22:59:52] info     : 'restund_process' trying to restart
[IST Apr 12 22:59:52] info     : 'restund_process' restart:
/etc/init.d/restund
[IST Apr 12 23:00:23] error    : 'restund_process' failed to restart (exit
status 0) -- /etc/init.d/restund: error loading configuration:
/etc/clearwater/restund.conf: No such file or directory

[IST Apr 12 23:00:23] error    : 'etcd_process' process is not running
[IST Apr 12 23:00:23] info     : 'etcd_process' trying to restart
[IST Apr 12 23:00:23] info     : 'etcd_process' restart: /bin/bash
[IST Apr 12 23:00:54] error    : 'etcd_process' failed to restart (exit
status 2) -- /bin/bash: zmq_msg_recv: Resource temporarily unavailable
client: etcd cluster is unavailable or misconfigured; error #0: client:
etcd member http://192.168.56.106:4000 has no leader
; error #1: client: etcd member http://192.168.56.103:4000 has no leader
[IST Apr 12 23:00:54] error    : 'poll_bono'
'/usr/share/clearwater/bin/poll_bono.sh' failed with exit status (1) -- SIP
poll failed to 192.168.56.105:5058 with Call-ID poll-sip-17550 at
2018-04-12 17:29:42.988483149+00:00
stderr was:
Connection to 192.168.56.105 5058 port [tcp/*] succeeded!
stdout was:
SIP/2.0 408 Request Timeout
Via: SIP/2.0/TCP
192.168.56.105;rport=44628;received=192.168.56.105;branch=z9hG4bK-17550
Call-ID: poll-sip-17550
From: "poll-sip" <sip:poll-sip at 192.168.56.105>;tag=17550
To: <sip:poll-sip at 192.168.56.105>;tag=z9hG4bK-17550
CSeq: 17550 OPTIONS
Content-Length:  0
[IST Apr 12 23:01:04] error    : 'restund_process' process is not running
[IST Apr 12 23:01:04] info     : 'restund_process' trying to restart
[IST Apr 12 23:01:04] info     : 'restund_process' restart:
/etc/init.d/restund


On Thu, Apr 12, 2018 at 2:40 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi,
>
> In bono it giving error like:
>
> [bono]ubuntu at bono-A:~$* cw-config download shared_config*
> Unable to contact the etcd cluster.
>
> and there is no shared_config file, while in other nodes i am able to
> download the shared file but i am not able to open it except dime.
>
> what should i do?
> Please provide some solution.
>
>
> *[bono]ubuntu at bono-A:/var/log/bono$ cat bono_err.log*
>
> 11-04-2018 20:46:11.548 UTC Advanced stack dump (requires gdb):
> sh: 1: /usr/bin/gdb: not found
> gdb failed with return code 32512
>
> 12-04-2018 12:40:51.950 UTC Advanced stack dump (requires gdb):
> sh: 1: /usr/bin/gdb: not found
> gdb failed with return code 32512
>
> 12-04-2018 12:46:19.069 UTC Advanced stack dump (requires gdb):
> sh: 1: /usr/bin/gdb: not found
> gdb failed with return code 32512
>
> 12-04-2018 17:00:01.191 UTC Advanced stack dump (requires gdb):
> sh: 1: /usr/bin/gdb: not found
> gdb failed with return code 32512
>
>
>
> *[bono]ubuntu at bono-A:/var/log/bono$ cat bono_current.log*
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
> 12-04-2018 16:46:25.967 UTC [7fe1ca7fc700] Error
> sip_connection_pool.cpp:190: Failed to resolve icscf. to an IP address -
> Not found (PJ_ENOTFOUND)
>
>
> On Thu, Apr 12, 2018 at 12:35 PM, Sunil Kumar <skgola1997 at gmail.com>
> wrote:
>
>> Hi all,
>> I am trying to install clearwater manually in virtualbox. I am using NAT
>> and host only network network mode there, and i make host only network's ip
>> as static and used in local_ip and dns also (NAT is used for connecting to
>> internet).
>> I have installed everything perfectly but facing problem while login to
>> ellis its not opening.
>> when i am trying to stress node it also not work giving error like:
>>
>> 2018-04-12      18:35:36.672571 1523538336.672571: Unknown remote host '*sprout.imstest.com
>> <http://sprout.imstest.com>*' (Name or service not known, Inappropriate
>> ioctl for device).
>> Use 'sipp -h' for details.
>>
>> *local_config:*
>> *local_ip=192.168.56.102*
>> *public_ip= 192.168.56.102 *
>> *public_hostname=dime-A*
>>
>> *etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,192.168.56.1052,192.168.56.106,192.168.56.107"*
>>
>>
>>
>> *bono's local_config:*
>>
>> local_ip=192.168.56.105
>> public_ip= 10.224.61.82
>> public_hostname=bono-A
>> etcd_cluster="192.168.56.102,192.168.56.103,192.168.56.104,1
>> 92.168.56.1052,192.168.56.106,192.168.56.107"
>>
>>
>>
>> i used public and private_ip same in local_config file i.e. both private,
>> I just want to run stress node. in* bono' local_config file i gave
>> public_ip as the ip of host itself because in NAT mode the vm is connecting
>> to internet using the ip of host itself thats why* (please guide if it
>> is wrong).
>>
>>
>> *monit summary:*
>>
>>
>> *[bono]ubuntu at bono-A:~$ sudo monit summary*
>> Monit 5.18.1 uptime: 2h 9m
>>  Service Name                     Status                      Type
>>  node-bono-A                      Running                     System
>>  restund_process                  Execution failed | Does...  Process
>>  ntp_process                      Running                     Process
>>  clearwater_queue_manager_pro...  Running                     Process
>>  *etcd_process                     Execution failed | Does...  Process*
>>  clearwater_diags_monitor_pro...  Running                     Process
>>  clearwater_config_manager_pr...  Running                     Process
>>  clearwater_cluster_manager_p...  Running                     Process
>>  bono_process                     Running                     Process
>>  *poll_restund                     Wait parent                 Program*
>>  monit_uptime                     Status ok                   Program
>>  clearwater_queue_manager_uptime  Status ok                   Program
>> * etcd_uptime                      Wait parent                 Program*
>> * poll_etcd_cluster                Wait parent                 Program*
>> * poll_etcd                        Wait parent                 Program*
>> * poll_bono                        Status failed               Program*
>>
>>
>> *[dime]ubuntu at dime-A:~$ sudo monit summary*
>> [sudo] password for ubuntu:
>> Monit 5.18.1 uptime: 2h 9m
>>  Service Name                     Status                      Type
>>  node-dime-A                      Running                     System
>>  snmpd_process                    Running                     Process
>>  ralf_process                     Running                     Process
>>  ntp_process                      Running                     Process
>>  nginx_process                    Running                     Process
>>  homestead_process                Running                     Process
>>  homestead-prov_process           Running                     Process
>>  clearwater_queue_manager_pro...  Running                     Process
>>  etcd_process                     Running                     Process
>>  clearwater_diags_monitor_pro...  Running                     Process
>>  clearwater_config_manager_pr...  Running                     Process
>>  clearwater_cluster_manager_p...  Running                     Process
>>  ralf_uptime                      Status ok                   Program
>>  poll_ralf                        Status ok                   Program
>>  nginx_ping                       Status ok                   Program
>>  nginx_uptime                     Status ok                   Program
>>  monit_uptime                     Status ok                   Program
>>  homestead_uptime                 Status ok                   Program
>>  poll_homestead                   Status ok                   Program
>>  check_cx_health                  Status ok                   Program
>>  poll_homestead-prov              Status ok                   Program
>>  clearwater_queue_manager_uptime  Status ok                   Program
>>  etcd_uptime                      Status ok                   Program
>> * poll_etcd_cluster                Status failed               Program*
>>  poll_etcd                        Status ok                   Program
>> [dime]ubuntu at dime-A:~$
>>
>>
>>
>>
>> *[sprout]ubuntu at sprout-A:~$ sudo monit summary*
>> [sudo] password for ubuntu:
>> Monit 5.18.1 uptime: 2h 9m
>>  Service Name                     Status                      Type
>>  node-sprout-A                    Running                     System
>>  sprout_process                   Running                     Process
>>  ntp_process                      Running                     Process
>>  nginx_process                    Running                     Process
>>  clearwater_queue_manager_pro...  Running                     Process
>>  etcd_process                     Running                     Process
>>  clearwater_diags_monitor_pro...  Running                     Process
>>  clearwater_config_manager_pr...  Running                     Process
>>  clearwater_cluster_manager_p...  Running                     Process
>>  sprout_uptime                    Status ok                   Program
>>  poll_sprout_sip                  Status ok                   Program
>>  poll_sprout_http                 Status ok                   Program
>>  nginx_ping                       Status ok                   Program
>>  nginx_uptime                     Status ok                   Program
>>  monit_uptime                     Status ok                   Program
>>  clearwater_queue_manager_uptime  Status ok                   Program
>>  etcd_uptime                      Status ok                   Program
>> * poll_etcd_cluster                Status failed               Program*
>>  poll_etcd                        Status ok                   Program
>>
>>
>> etc.
>>
>>
>> please guide any solution
>>
>> thanks in avance
>>
>> Regards
>> sunil
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180412/aa02d6d3/attachment.html>

From spencer.builds.networks at gmail.com  Thu Apr 12 20:05:54 2018
From: spencer.builds.networks at gmail.com (Spencer Sevilla)
Date: Thu, 12 Apr 2018 17:05:54 -0700
Subject: [Project Clearwater] setting up clearwater as ims for lte network
Message-ID: <CALRF2+FcBtMm0f_20BP_a9wWC-H9P9t=swnPQ+o9OCfHMq3yFw@mail.gmail.com>

Hi Everyone,

I'm trying to setup a super basic Clearwater deployment (just using the
All-in-One OVF image on VirtualBox) and use it to support Voice and SMS in
a small LTE testbed network (powered by the OpenAirInterface EPC).

I've got an Android connected to our LTE testbed, and have been able to
setup Clearwater and use it to make test calls with Zoiper. Now, I'm trying
to figure out what I have to do to get it to support voice/text from the
Android itself (not just the app). We currently have a "default" APN that
supports LTE bearer establishment and I've tried adding an ims-specific APN
for Clearwater (trying a bunch of different values) but it doesn't seem
like the Android is generating any traffic or trying to talk to Clearwater.

>From documentation, I get the sense that Clearwater can support this, but I
haven't had any success in finding information on how to set things up. I
have a bunch of questions that might help me get on the right track, or if
anyone has some good pointers on how to do this I'd be super thankful!

1) Per some LTE resources, I created an APN setting with the following
values:
type: ims
name: example.com
proxy: 192.168.140.1 (this is the address of the Clearwater server)
port: 8060
username: 650555XXXX at example.com
password: ******
server: 192.168.140.1

Does this seem right? For what it's worth, the APN automatically populates
some other values (MCC and MNC) from my SIM card and I can't change them.
Does this mean that really I need to change Clearwater's entries so that
the numbers line up?

2) Right now I've been using the standard "example.com" value for
Clearwater's configuration - this has been fine for SIP and Zoiper, but
I've gotten the sense from some LTE resources that I'll have to change it
to one of the FQDNs that looks like "ims.
apn.epc.mnc111.mcc222.3gppnetwork.org" or something similar. If so, will I
also have to own/register DNS entries to get resolution to work? This whole
system is a small, self-contained local network so I'd prefer to avoid
having to integrate in if necessary.

3) This is much more of an LTE specific question, but as long as the
entries are lined up, do I have to point Clearwater at my network's HSS
(and vice-versa)? Right now they're separate programs but running on the
same machine, so it's certainly doable, but I imagine it'd be way simpler
if I could just setup my user configuration correctly (via APN) and just
have it connect :-)

Thanks!!!
Spencer Sevilla
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180412/35a21eea/attachment.html>

From skgola1997 at gmail.com  Fri Apr 13 05:39:59 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Fri, 13 Apr 2018 15:09:59 +0530
Subject: [Project Clearwater] regarding freeing the cache of nodes
Message-ID: <CAHwYWpCYuScbpdJanD_9Ooxd7gnB5Q1kjyCHJ-=T47JL4Uyt3w@mail.gmail.com>

Hi all,
Is there a any way to free the memory of clearwater nodes (vellum, dime etc)
can u give me the path.

thanks,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180413/db727e5b/attachment.html>

From nagen_kr at yahoo.co.in  Sun Apr 15 05:06:16 2018
From: nagen_kr at yahoo.co.in (Nagendra Kumar)
Date: Sun, 15 Apr 2018 09:06:16 +0000 (UTC)
Subject: [Project Clearwater] Lost the IPs of clearwater node
In-Reply-To: <CAHwYWpCrSbtV0DeLFwL_gOOcu0--gub70_LU-m3hU33augdeDw@mail.gmail.com>
References: <CAHwYWpCrSbtV0DeLFwL_gOOcu0--gub70_LU-m3hU33augdeDw@mail.gmail.com>
Message-ID: <1225110707.305024.1523783176613@mail.yahoo.com>

Hi Sunil,??????????????? You can configure one of your VM interface as "Host-only Adaptor" option and configure static IP address as below(example in Oracle VM VIrtualBox Manager):
vi /etc/network/interfaces# interfaces(5) file used by ifup(8) and ifdown(8)
auto lo
iface lo inet loopback

auto eth0
iface eth0 inet static
?? address 192.168.56.100
?? netmask 255.255.255.0
?? network 192.168.56.1
?? gateway 192.168.56.1
?? dns-nameservers 8.8.8.8

When you reboot the machine, it will always have this IP address, use this IP address as private address on your nodes.
Thanks-Nagu
 

    On Tuesday, 10 April, 2018, 4:30:45 PM IST, Sunil Kumar <skgola1997 at gmail.com> wrote:  
 
 Hi,Now I am using NAT mode for networking, but i have some dooubts:1) Should public_ip is the ip of the host on which virtualbox is installed?2) Should all nodes having the same public_ip in NAT mode?
3) Instead of doing port forwarding for particular node, I have left blank host ip field that means it is opens for world, or do I need to open for particular node?
Thanks,Sunil  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180415/a073a9dd/attachment.html>

From nagen_kr at yahoo.co.in  Sun Apr 15 05:50:53 2018
From: nagen_kr at yahoo.co.in (Nagendra Kumar)
Date: Sun, 15 Apr 2018 09:50:53 +0000 (UTC)
Subject: [Project Clearwater] Manual installation of Clearwater at one node
References: <97967631.315186.1523785853801.ref@mail.yahoo.com>
Message-ID: <97967631.315186.1523785853801@mail.yahoo.com>

Hi Team,????????????????? If I want to install all components from source code on one machine and in future on few machines(not necessarily one component on one node, there may be few components on one node, etc), how can I compile and install it on one node.
In the source repository(https://github.com/Metaswitch) , I can only find sprout(and bono) and ellis, I don't find homer, dime and vellum source code. I find Homestead and Ralf, which is combined to form Dime, but not dime as separate.
Can you please send/refer me steps as per latest architecture (having Homer, Dime and Vellum as separate component.)
Thanks-Nagu



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180415/bb9dbe10/attachment.html>

From masood.tahir696 at gmail.com  Mon Apr 16 11:52:30 2018
From: masood.tahir696 at gmail.com (tahir Masood)
Date: Mon, 16 Apr 2018 15:52:30 +0000
Subject: [Project Clearwater] Deploying Clearwater using Heat Template
In-Reply-To: <CAKHfQt7+g8thaq7+7os6jZWfD+kZX7o0VE3K-ctF-6VnBQfhig@mail.gmail.com>
References: <CAKHfQt7+g8thaq7+7os6jZWfD+kZX7o0VE3K-ctF-6VnBQfhig@mail.gmail.com>
Message-ID: <CADabj4YOaJYfU782rVFhU3h6=k_9iq=DLMt37CfNnBHK=S7d2Q@mail.gmail.com>

Download all the yaml files to your openstack controller
Create the key using the command provided in the read me of git hub repo
and deploy the stack.
Make sure sure you are in the folder where all other yaml files are present
Clearwater.yaml is the main file used to trigger the stack creation all
other will be called by this file automatically

Regards
Tahir

On Mon, 16 Apr 2018, 8:47 PM joehary ar, <joehary at gmail.com> wrote:

> Hi,
>
> I would like to seek support and guidance on how I can deploy clearwater
> using heat template based on the yaml file
> https://github.com/Metaswitch/clearwater-heat
>
> May I know anybody successful try this template?
>
> 1. Do i need to download all the file in git onto my openstack folder?
> 2. Just create heat stack with only one yaml file --> clearwater.yaml ?
>
> Please assist me further.
>
> Thank you.
>
> Regards,
> Joe
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
>
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180416/54cfd7ed/attachment.html>

From skgola1997 at gmail.com  Tue Apr 17 11:25:16 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Tue, 17 Apr 2018 20:55:16 +0530
Subject: [Project Clearwater] initial registration failed in stress testing
Message-ID: <CAHwYWpALncp55UF3zo9r=RWFzj1Ea6ZsCV1G9DNPO-C5kPAeNw@mail.gmail.com>

Hi all,
I have installed clearwater manually and configured the dns also. I am able
to ping among the node using host name but while running stress testing it
giving errors like:

* unknown remote host 'sprout.ims.com <http://sprout.ims.com>'  (Name or
service not known, Inappropriate ioctl for device).*


*[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress ims.com
<http://ims.com> 100 1*
*Starting initial registration, will take 1 seconds*
*Initial registration failed - see
/var/log/clearwater-sip-stress/9096_initial_reg_errors.log for details of
the errors*

*[]ubuntu at stress:~$ cat
/var/log/clearwater-sip-stress/8862_initial_reg_errors.log*
*sipp: The following events occured:*
*2018-04-18      04:45:58.362472 1524006958.362472: Unknown remote host
'sprout.ims.com <http://sprout.ims.com>' (Name or service not known,
Inappropriate ioctl for device).*
*Use 'sipp -h' for details.*


and i am also not able to ping from stress node to sprout node or any other
node using hostname but using ip address i am able to ping (I am using
bridge adapter)

Please reply

Thanks in advance
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180417/75fdb2fd/attachment.html>

From case at gci.com  Wed Apr 18 16:58:49 2018
From: case at gci.com (Roger Case)
Date: Wed, 18 Apr 2018 20:58:49 +0000
Subject: [Project Clearwater] 
 =?utf-8?q?Release_note_for_Sprint_Zam=C3=AE?= =?utf-8?q?n?=
In-Reply-To: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
Message-ID: <A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>

Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?

Am I missing something here?  This worked before.  Thanks!

Roger

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.

This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.

We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.


This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.

To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.


Cheers,

Richard

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180418/e3935509/attachment.html>

From skgola1997 at gmail.com  Thu Apr 19 00:54:41 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Thu, 19 Apr 2018 10:24:41 +0530
Subject: [Project Clearwater] Fwd: etcd is not working
In-Reply-To: <CAHwYWpB4RevpumRW6GDck-GZ+ZARmOvj94zqHO=XHFckuotD4Q@mail.gmail.com>
References: <CAHwYWpD_RHexchrr5neDDZQpxSQAh=ArNYGWDOCUYeY0azyGEg@mail.gmail.com>
	<CAHwYWpB4RevpumRW6GDck-GZ+ZARmOvj94zqHO=XHFckuotD4Q@mail.gmail.com>
Message-ID: <CAHwYWpAsDTgkZ-EQ5WKD8sWkU2UZ6++TKX5kUfpVjDXHt4phtw@mail.gmail.com>

Hi,
I have installed clearwater manually using single node. I also scale up
using 2 more node for sprout 1 for vellum and dime. After some time my ip
is lost and i reset the ip in local_config file restart the services and the*
scale up node is no more (they are deleted)*


*Please reply, i want to fix it. I am waiting for your reply,, thanks in
advance*

*i have doubt that the scale up node is not remove properly. I just deleted
them*

I am getting the following errors in every node:



*[vellum]ubuntu at vellum:~$ cw-config download shared_config*
Unable to contact the etcd cluster.




*[bono]ubuntu at bono:~$ sudo monit summary*
[sudo] password for ubuntu:
Monit 5.18.1 uptime: 2h 38m
 Service Name                     Status                      Type
 node-bono                        Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Execution failed | Does...  Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
* etcd_uptime                      Wait parent                 Program*
* poll_etcd_cluster                Wait parent                 Program*
* poll_etcd                        Wait parent                 Program*
 poll_bono                        Status ok                   Program







 *[vellum]ubuntu at vellum:~$ sudo cw-check_config_sync*
[sudo] password for ubuntu:
Traceback (most recent call last):
  File "/usr/share/clearwater/clearwater-config-manager/scripts/check_config_sync.py",
line 29, in <module>
    result = client.get("/" + etcd_key + "/" + site + "/configuration/" +
plugin.key())
  File "/usr/share/clearwater/clearwater-config-manager/env/local/
lib/python2.7/site-packages/etcd/client.py", line 679, in get
    return self.read(key)
  File "/usr/share/clearwater/clearwater-config-manager/env/local/
lib/python2.7/site-packages/etcd/client.py", line 536, in read
    timeout=timeout)
  File "/usr/share/clearwater/clearwater-config-manager/env/local/
lib/python2.7/site-packages/etcd/client.py", line 834, in wrapper
    cause=e
etcd.EtcdConnectionFailed: Connection to etcd failed due to
MaxRetryError("HTTPConnectionPool(host='10.224.61.45', port=4000): Max
retries exceeded with url: /v2/keys/clearwater/site1/configuration/dns_json
(Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at
0x7f479c1d3910>: Failed to establish a new connection: [Errno 111]
Connection refused',))",)






*[vellum]ubuntu at vellum:~$ clearwater-etcdctl cluster-health*
cluster may be unhealthy: failed to list members
Error:  client: etcd cluster is unavailable or misconfigured; error #0:
dial tcp 10.224.61.45:4000: getsockopt: connection refused

error #0: dial tcp 10.224.61.45:4000: getsockopt: connection refused



*[vellum]ubuntu at vellum:~$ clearwater-etcdctl member list*
Error:  client: etcd cluster is unavailable or misconfigured; error #0:
dial tcp 10.224.61.45:4000: getsockopt: connection refused

error #0: dial tcp 10.224.61.45:4000: getsockopt: connection refused








*[bono]ubuntu at bono:/var/log/clearwater-etcd$ cat clearwater-etcd-initd.log*
context deadline exceeded
2018-04-19 06:25:52.190441844 etcdctl returned 1
2018-04-19 06:25:52.203217708 Joining existing cluster...
2018-04-19 06:26:21.211523618 Configured ETCDCTL_PEERS: 10.224.61.112:4000,
10.224.61.8:4000,10.224.61.109:4000,10.224.61.19:4000,10.224.61.47:4000,
10.224.61.24:4000,10.224.61.22:4000,10.224.61.115:4000,10.224.61.27:4000,
10.224.61.45:4000,
2018-04-19 06:26:21.213445229 Check cluster is healthy
2018-04-19 06:26:21.217376667 Running etcdctl cluster-health
2018-04-19 06:26:22.331785696 Found leaked etcd 2414 (correct is ) -
killing 2414
2018-04-19 06:26:22.335431738 etcdctl returned 137
2018-04-19 06:26:22.337320589 Restarting etcd clearwater-etcd
2018-04-19 06:26:22.343212899 Configured ETCDCTL_PEERS: 10.224.61.112:4000,
10.224.61.8:4000,10.224.61.109:4000,10.224.61.19:4000,10.224.61.47:4000,
10.224.61.24:4000,10.224.61.22:4000,10.224.61.115:4000,10.224.61.27:4000,
10.224.61.45:4000,
2018-04-19 06:26:22.343830818 Check for previous failed startup attempt
2018-04-19 06:26:22.344751074 Running etcdctl member list
context deadline exceeded
2018-04-19 06:26:32.357759475 etcdctl returned 1
2018-04-19 06:26:32.371886200 Joining existing cluster...
2018-04-19 06:26:37.382282245 Configured ETCDCTL_PEERS: 10.224.61.112:4000,
10.224.61.8:4000,10.224.61.109:4000,10.224.61.19:4000,10.224.61.47:4000,
10.224.61.24:4000,10.224.61.22:4000,10.224.61.115:4000,10.224.61.27:4000,
10.224.61.45:4000,
2018-04-19 06:26:37.383949088 Check cluster is healthy
2018-04-19 06:26:37.386734717 Running etcdctl cluster-health
cluster may be unhealthy: failed to list members
Error:  client: etcd cluster is unavailable or misconfigured; error #0:
client: endpoint http://10.224.61.109:4000 exceeded header timeout
; error #1: dial tcp 10.224.61.19:4000: getsockopt: connection refused
; error #2: dial tcp 10.224.61.47:4000: getsockopt: connection refused
; error #3: http: no Host in request URL
; error #4: dial tcp 10.224.61.24:4000: getsockopt: connection refused
; error #5: dial tcp 10.224.61.45:4000: getsockopt: connection refused
; error #6: dial tcp 10.224.61.8:4000: getsockopt: connection refused
; error #7: client: endpoint http://10.224.61.115:4000 exceeded header
timeout
; error #8: client: endpoint http://10.224.61.112:4000 exceeded header
timeout
; error #9: dial tcp 10.224.61.27:4000: getsockopt: connection refused
; error #10: dial tcp 10.224.61.22:4000: getsockopt: connection refused

error #0: client: endpoint http://10.224.61.109:4000 exceeded header timeout
error #1: dial tcp 10.224.61.19:4000: getsockopt: connection refused
error #2: dial tcp 10.224.61.47:4000: getsockopt: connection refused
error #3: http: no Host in request URL
error #4: dial tcp 10.224.61.24:4000: getsockopt: connection refused
error #5: dial tcp 10.224.61.45:4000: getsockopt: connection refused
error #6: dial tcp 10.224.61.8:4000: getsockopt: connection refused
error #7: client: endpoint http://10.224.61.115:4000 exceeded header timeout
error #8: client: endpoint http://10.224.61.112:4000 exceeded header timeout
error #9: dial tcp 10.224.61.27:4000: getsockopt: connection refused
error #10: dial tcp 10.224.61.22:4000: getsockopt: connection refused

2018-04-19 06:26:43.409826012 etcdctl returned 4
2018-04-19 06:26:43.412523362 Not joining an unhealthy cluster
2018-04-19 06:27:03.126255654 Restarting etcd clearwater-etcd
2018-04-19 06:27:03.133102368 Configured ETCDCTL_PEERS: 10.224.61.112:4000,
10.224.61.8:4000,10.224.61.109:4000,10.224.61.19:4000,10.224.61.47:4000,
10.224.61.24:4000,10.224.61.22:4000,10.224.61.115:4000,10.224.61.27:4000,
10.224.61.45:4000,
2018-04-19 06:27:03.133904517 Check for previous failed startup attempt
2018-04-19 06:27:03.135105646 Running etcdctl member list
context deadline exceeded
2018-04-19 06:27:13.148679161 etcdctl returned 1
2018-04-19 06:27:13.162058102 Joining existing cluster...
2018-04-19 06:27:34.170718991 Configured ETCDCTL_PEERS: 10.224.61.112:4000,
10.224.61.8:4000,10.224.61.109:4000,10.224.61.19:4000,10.224.61.47:4000,
10.224.61.24:4000,10.224.61.22:4000,10.224.61.115:4000,10.224.61.27:4000,
10.224.61.45:4000,
2018-04-19 06:27:34.172721201 Check cluster is healthy
2018-04-19 06:27:34.176441659 Running etcdctl cluster-health
cluster may be unhealthy: failed to list members
Error:  client: etcd cluster is unavailable or misconfigured; error #0:
client: endpoint http://10.224.61.109:4000 exceeded header timeout
; error #1: dial tcp 10.224.61.19:4000: getsockopt: connection refused
; error #2: client: endpoint http://10.224.61.112:4000 exceeded header
timeout
; error #3: dial tcp 10.224.61.47:4000: getsockopt: connection refused
; error #4: dial tcp 10.224.61.45:4000: getsockopt: connection refused
; error #5: dial tcp 10.224.61.8:4000: getsockopt: connection refused
; error #6: dial tcp 10.224.61.27:4000: getsockopt: connection refused
; error #7: client: endpoint http://10.224.61.115:4000 exceeded header
timeout
; error #8: dial tcp 10.224.61.22:4000: getsockopt: connection refused
; error #9: http: no Host in request URL
; error #10: dial tcp 10.224.61.24:4000: getsockopt: connection refused

error #0: client: endpoint http://10.224.61.109:4000 exceeded header timeout
error #1: dial tcp 10.224.61.19:4000: getsockopt: connection refused
error #2: client: endpoint http://10.224.61.112:4000 exceeded header timeout
error #3: dial tcp 10.224.61.47:4000: getsockopt: connection refused
error #4: dial tcp 10.224.61.45:4000: getsockopt: connection refused
error #5: dial tcp 10.224.61.8:4000: getsockopt: connection refused
error #6: dial tcp 10.224.61.27:4000: getsockopt: connection refused
error #7: client: endpoint http://10.224.61.115:4000 exceeded header timeout
error #8: dial tcp 10.224.61.22:4000: getsockopt: connection refused
error #9: http: no Host in request URL
error #10: dial tcp 10.224.61.24:4000: getsockopt: connection refused

thanks,
sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180419/aa4b36bc/attachment.html>

From skgola1997 at gmail.com  Thu Apr 19 01:33:36 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Thu, 19 Apr 2018 11:03:36 +0530
Subject: [Project Clearwater] Unable to contact the etcd cluster
Message-ID: <CAHwYWpBv7iy6bZcNWX1ERDKK2_-4=jWX0AuvZb=FXgdmE==LeQ@mail.gmail.com>

Hi,
Any body can help me on this. after ip lost, i update the ip in
local_config and dns and restart the service. extra vm is deleted lik  i
had 3 sprout node so 2 are deleted.

[vellum]ubuntu at vellum:~$ cw-config upload shared_config
Unable to contact the etcd cluster.

thanks
sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180419/d8b24b4b/attachment.html>

From skgola1997 at gmail.com  Thu Apr 19 02:15:56 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Thu, 19 Apr 2018 11:45:56 +0530
Subject: [Project Clearwater] Unable to contact the etcd cluster
In-Reply-To: <CAHwYWpBv7iy6bZcNWX1ERDKK2_-4=jWX0AuvZb=FXgdmE==LeQ@mail.gmail.com>
References: <CAHwYWpBv7iy6bZcNWX1ERDKK2_-4=jWX0AuvZb=FXgdmE==LeQ@mail.gmail.com>
Message-ID: <CAHwYWpAnGZFSBvirAXH1MRaba5ZOfLtvYuHsO7J3WTO7NpSR4Q@mail.gmail.com>

Hi,
the node with ip 10.224.61.109, 10.224.61.112 etc is no more there, I have
deleted the node directly. It looks like they are still in the etcd
cluster. Can you please tell me how to remove them

[IST Apr 19 19:32:45] error    : 'etcd_process' process is not running
[IST Apr 19 19:32:45] info     : 'etcd_process' trying to restart
[IST Apr 19 19:32:45] info     : 'etcd_process' restart: /bin/bash
[IST Apr 19 19:33:15] error    : 'etcd_process' failed to restart (exit
status -1) -- /bin/bash: Program timed out -- zmq_msg_recv: Resource
temporarily unavailable
cat: /var/run/clearwater-etcd/clearwater-etcd.pid: No such file or directory
cat: /var/run/clearwater-etcd/clearwater-etcd.pid: No such file or directory
context deadline excee
[IST Apr 19 19:33:25] error    : 'etcd_process' process is not running
[IST Apr 19 19:33:25] info     : 'etcd_process' trying to restart
[IST Apr 19 19:33:25] info     : 'etcd_process' restart: /bin/bash
[IST Apr 19 19:33:55] error    : 'etcd_process' failed to restart (exit
status -1) -- /bin/bash: Program timed out -- zmq_msg_recv: Resource
temporarily unavailable
client: etcd cluster is unavailable or misconfigured; error #0: *dial tcp
10.224.61.109:4000 <http://10.224.61.109:4000>*: getsockopt: no route to
host
; error #1: dial tcp 10.224.61.47:4000: getsockopt: co
[IST Apr 19 19:34:05] error    : 'etcd_process' process is not running
[IST Apr 19 19:34:05] info     : 'etcd_process' trying to restart
[IST Apr 19 19:34:05] info     : 'etcd_process' restart: /bin/bash
[IST Apr 19 19:34:36] error    : 'etcd_process' failed to restart (exit
status 2) -- /bin/bash: zmq_msg_recv: Resource temporarily unavailable
context deadline exceeded


On Thu, Apr 19, 2018 at 11:03 AM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi,
> Any body can help me on this. after ip lost, i update the ip in
> local_config and dns and restart the service. extra vm is deleted lik  i
> had 3 sprout node so 2 are deleted.
>
> [vellum]ubuntu at vellum:~$ cw-config upload shared_config
> Unable to contact the etcd cluster.
>
> thanks
> sunil
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180419/5e377f1c/attachment.html>

From Adam.Lindley at metaswitch.com  Thu Apr 19 05:12:29 2018
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Thu, 19 Apr 2018 09:12:29 +0000
Subject: [Project Clearwater] Unable to contact the etcd cluster
In-Reply-To: <CAHwYWpAnGZFSBvirAXH1MRaba5ZOfLtvYuHsO7J3WTO7NpSR4Q@mail.gmail.com>
References: <CAHwYWpBv7iy6bZcNWX1ERDKK2_-4=jWX0AuvZb=FXgdmE==LeQ@mail.gmail.com>
	<CAHwYWpAnGZFSBvirAXH1MRaba5ZOfLtvYuHsO7J3WTO7NpSR4Q@mail.gmail.com>
Message-ID: <CY1PR02MB204447191A1D35EAFFE123ACE2B50@CY1PR02MB2044.namprd02.prod.outlook.com>

Hi Sunil,

I?m afraid the steps you?ve taken are not supported in Project Clearwater deployments. Both changing the ?local_ip? of a node, and removing nodes just by deleting the VMs.

On the first point, you need to be able to give your VMs permanent static IP addresses.
On the second, by deleting the VMs in your cluster, your underlying etcd cluster has lost quorum. I would suggest http://clearwater.readthedocs.io/en/stable/Handling_Multiple_Failed_Nodes.htm as a starting point for recovering information from it. However, as your single remaining node will likely also have problems due to the local IP changing, you may simply want to redeploy from scratch.

More in general, you seem to have hit a substantial number of issues in deploying Project Clearwater, which is both not what we want, and not what the experience of many other users seems to be. I would suggest taking a wider look over our provided documentation, and making sure your environment matches our expectations, and that you?re clear on our processes. This should make your next deployment a lot smoother.

Cheers, and good luck,
Adam

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sunil Kumar
Sent: 19 April 2018 07:16
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Unable to contact the etcd cluster

Hi,
the node with ip 10.224.61.109, 10.224.61.112 etc is no more there, I have deleted the node directly. It looks like they are still in the etcd cluster. Can you please tell me how to remove them

[IST Apr 19 19:32:45] error    : 'etcd_process' process is not running
[IST Apr 19 19:32:45] info     : 'etcd_process' trying to restart
[IST Apr 19 19:32:45] info     : 'etcd_process' restart: /bin/bash
[IST Apr 19 19:33:15] error    : 'etcd_process' failed to restart (exit status -1) -- /bin/bash: Program timed out -- zmq_msg_recv: Resource temporarily unavailable
cat: /var/run/clearwater-etcd/clearwater-etcd.pid: No such file or directory
cat: /var/run/clearwater-etcd/clearwater-etcd.pid: No such file or directory
context deadline excee
[IST Apr 19 19:33:25] error    : 'etcd_process' process is not running
[IST Apr 19 19:33:25] info     : 'etcd_process' trying to restart
[IST Apr 19 19:33:25] info     : 'etcd_process' restart: /bin/bash
[IST Apr 19 19:33:55] error    : 'etcd_process' failed to restart (exit status -1) -- /bin/bash: Program timed out -- zmq_msg_recv: Resource temporarily unavailable
client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 10.224.61.109:4000<http://10.224.61.109:4000>: getsockopt: no route to host
; error #1: dial tcp 10.224.61.47:4000<http://10.224.61.47:4000>: getsockopt: co
[IST Apr 19 19:34:05] error    : 'etcd_process' process is not running
[IST Apr 19 19:34:05] info     : 'etcd_process' trying to restart
[IST Apr 19 19:34:05] info     : 'etcd_process' restart: /bin/bash
[IST Apr 19 19:34:36] error    : 'etcd_process' failed to restart (exit status 2) -- /bin/bash: zmq_msg_recv: Resource temporarily unavailable
context deadline exceeded


On Thu, Apr 19, 2018 at 11:03 AM, Sunil Kumar <skgola1997 at gmail.com<mailto:skgola1997 at gmail.com>> wrote:
Hi,
Any body can help me on this. after ip lost, i update the ip in local_config and dns and restart the service. extra vm is deleted lik  i had 3 sprout node so 2 are deleted.

[vellum]ubuntu at vellum:~$ cw-config upload shared_config
Unable to contact the etcd cluster.

thanks
sunil

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180419/79a7951b/attachment.html>

From skgola1997 at gmail.com  Thu Apr 19 05:53:03 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Thu, 19 Apr 2018 15:23:03 +0530
Subject: [Project Clearwater] Unable to contact the etcd cluster
In-Reply-To: <CY1PR02MB204447191A1D35EAFFE123ACE2B50@CY1PR02MB2044.namprd02.prod.outlook.com>
References: <CAHwYWpBv7iy6bZcNWX1ERDKK2_-4=jWX0AuvZb=FXgdmE==LeQ@mail.gmail.com>
	<CAHwYWpAnGZFSBvirAXH1MRaba5ZOfLtvYuHsO7J3WTO7NpSR4Q@mail.gmail.com>
	<CY1PR02MB204447191A1D35EAFFE123ACE2B50@CY1PR02MB2044.namprd02.prod.outlook.com>
Message-ID: <CAHwYWpBaKQRYRCkKCuhbRKTLtVBHS=bTg24dNdVOERj74f-iAQ@mail.gmail.com>

Hi Adam,

Thanks a lot for replying.  I am using virtualbox for installing the VMs,
earlier I was using bridge adapter so it takes the network IP from dhcp, I
assign them as public and local_ip same.

As you mention for static IP I tried with with *NAT + Host-only Network (*NAT
as primary interface eth0*), but all node having same IP in eth0 in NAT as
10.0.2.15 (is it fine to have all node same IP because I am not using that
IP)* and i assign host-only ip as static as 192.168.56.110 etc.

1) can I use both local_ip and public_ip same as host only ip
(192.168.56.110 etc) or public_ip would be the ip of host machine on which
virtualbox is installed as in NAT VM use Host IP as public IP to contact
outer world.

2) Is public_ip necessary as I only want stress testing to run in same
network, I don't want to install the no. on client like zoiper and all.

3) Is port forwarding isnecessary in  *NAT + Host-only Network, because * nodes
are able to communicate each other and in host only network so I don't
think port forwarding is necessary.

4) I just want to run stress testing for handling 1 lack call/sec. so how
many sprout, vellum node is needed for this much calls.

Thanks,
Sunil


On Thu, Apr 19, 2018 at 2:42 PM, Adam Lindley <Adam.Lindley at metaswitch.com>
wrote:

> Hi Sunil,
>
>
>
> I?m afraid the steps you?ve taken are not supported in Project Clearwater
> deployments. Both changing the ?local_ip? of a node, and removing nodes
> just by deleting the VMs.
>
>
>
> On the first point, you need to be able to give your VMs permanent static
> IP addresses.
>
> On the second, by deleting the VMs in your cluster, your underlying etcd
> cluster has lost quorum. I would suggest http://clearwater.readthedocs.
> io/en/stable/Handling_Multiple_Failed_Nodes.htm as a starting point for
> recovering information from it. However, as your single remaining node will
> likely also have problems due to the local IP changing, you may simply want
> to redeploy from scratch.
>
>
>
> More in general, you seem to have hit a substantial number of issues in
> deploying Project Clearwater, which is both not what we want, and not what
> the experience of many other users seems to be. I would suggest taking a
> wider look over our provided documentation, and making sure your
> environment matches our expectations, and that you?re clear on our
> processes. This should make your next deployment a lot smoother.
>
>
>
> Cheers, and good luck,
>
> Adam
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sunil Kumar
> *Sent:* 19 April 2018 07:16
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Unable to contact the etcd cluster
>
>
>
> Hi,
>
> the node with ip 10.224.61.109, 10.224.61.112 etc is no more there, I have
> deleted the node directly. It looks like they are still in the etcd
> cluster. Can you please tell me how to remove them
>
>
>
> [IST Apr 19 19:32:45] error    : 'etcd_process' process is not running
>
> [IST Apr 19 19:32:45] info     : 'etcd_process' trying to restart
>
> [IST Apr 19 19:32:45] info     : 'etcd_process' restart: /bin/bash
>
> [IST Apr 19 19:33:15] error    : 'etcd_process' failed to restart (exit
> status -1) -- /bin/bash: Program timed out -- zmq_msg_recv: Resource
> temporarily unavailable
>
> cat: /var/run/clearwater-etcd/clearwater-etcd.pid: No such file or
> directory
>
> cat: /var/run/clearwater-etcd/clearwater-etcd.pid: No such file or
> directory
>
> context deadline excee
>
> [IST Apr 19 19:33:25] error    : 'etcd_process' process is not running
>
> [IST Apr 19 19:33:25] info     : 'etcd_process' trying to restart
>
> [IST Apr 19 19:33:25] info     : 'etcd_process' restart: /bin/bash
>
> [IST Apr 19 19:33:55] error    : 'etcd_process' failed to restart (exit
> status -1) -- /bin/bash: Program timed out -- zmq_msg_recv: Resource
> temporarily unavailable
>
> client: etcd cluster is unavailable or misconfigured; error #0: *dial tcp
> 10.224.61.109:4000 <http://10.224.61.109:4000>*: getsockopt: no route to
> host
>
> ; error #1: dial tcp 10.224.61.47:4000: getsockopt: co
>
> [IST Apr 19 19:34:05] error    : 'etcd_process' process is not running
>
> [IST Apr 19 19:34:05] info     : 'etcd_process' trying to restart
>
> [IST Apr 19 19:34:05] info     : 'etcd_process' restart: /bin/bash
>
> [IST Apr 19 19:34:36] error    : 'etcd_process' failed to restart (exit
> status 2) -- /bin/bash: zmq_msg_recv: Resource temporarily unavailable
>
> context deadline exceeded
>
>
>
>
>
> On Thu, Apr 19, 2018 at 11:03 AM, Sunil Kumar <skgola1997 at gmail.com>
> wrote:
>
> Hi,
>
> Any body can help me on this. after ip lost, i update the ip in
> local_config and dns and restart the service. extra vm is deleted lik  i
> had 3 sprout node so 2 are deleted.
>
>
>
> [vellum]ubuntu at vellum:~$ cw-config upload shared_config
>
> Unable to contact the etcd cluster.
>
>
>
> thanks
>
> sunil
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/
> clearwater_lists.projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180419/740ef71a/attachment.html>

From Mark.Perryman at metaswitch.com  Thu Apr 19 06:00:44 2018
From: Mark.Perryman at metaswitch.com (Mark Perryman)
Date: Thu, 19 Apr 2018 10:00:44 +0000
Subject: [Project Clearwater] setting up clearwater as ims for lte
 network
Message-ID: <CY4PR02MB25180C0FB4CD0554B9D359568FB50@CY4PR02MB2518.namprd02.prod.outlook.com>

There are a couple of issues that might be a problem here.

Firstly, although Clearwater is used as the IMS core in VoLTE networks, the Bono node doesn't support AKA authentication and so probably won't work as a VoLTE P-CSCF.

Secondly, the phone you are using might be whitelisted to only support VoLTE on certain PLMNs.  You should check that the phone is making DNS queries and trying to send SIP.  Until the phone is attempting to SIP REGISTER with Clearwater, then there is nothing Clearwater can do.

1) Looks plausible, and yes, you'll need to change Clearwater configuration.

2) Yes, you'll either need to own that DNS, or configure OpenAirInterface EPC to send the P-CSCF IP address to the UE.

3) It's not required to have Clearwater and your EPC talking to the same HSS

Good luck!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180419/50cea475/attachment.html>

From skgola1997 at gmail.com  Thu Apr 19 06:16:19 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Thu, 19 Apr 2018 15:46:19 +0530
Subject: [Project Clearwater] Unable to contact the etcd cluster
In-Reply-To: <CAHwYWpBaKQRYRCkKCuhbRKTLtVBHS=bTg24dNdVOERj74f-iAQ@mail.gmail.com>
References: <CAHwYWpBv7iy6bZcNWX1ERDKK2_-4=jWX0AuvZb=FXgdmE==LeQ@mail.gmail.com>
	<CAHwYWpAnGZFSBvirAXH1MRaba5ZOfLtvYuHsO7J3WTO7NpSR4Q@mail.gmail.com>
	<CY1PR02MB204447191A1D35EAFFE123ACE2B50@CY1PR02MB2044.namprd02.prod.outlook.com>
	<CAHwYWpBaKQRYRCkKCuhbRKTLtVBHS=bTg24dNdVOERj74f-iAQ@mail.gmail.com>
Message-ID: <CAHwYWpBPe5Vzpf1M4vArzV1auZqZKA7HwDZ+veXhq8JSFP=x2A@mail.gmail.com>

the lost node is not the master node but IP of master nodes is changed and
I update them.

thanks

On Thu, Apr 19, 2018 at 3:23 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi Adam,
>
> Thanks a lot for replying.  I am using virtualbox for installing the VMs,
> earlier I was using bridge adapter so it takes the network IP from dhcp, I
> assign them as public and local_ip same.
>
> As you mention for static IP I tried with with *NAT + Host-only Network (*NAT
> as primary interface eth0*), but all node having same IP in eth0 in NAT
> as 10.0.2.15 (is it fine to have all node same IP because I am not using
> that IP)* and i assign host-only ip as static as 192.168.56.110 etc.
>
> 1) can I use both local_ip and public_ip same as host only ip
> (192.168.56.110 etc) or public_ip would be the ip of host machine on which
> virtualbox is installed as in NAT VM use Host IP as public IP to contact
> outer world.
>
> 2) Is public_ip necessary as I only want stress testing to run in same
> network, I don't want to install the no. on client like zoiper and all.
>
> 3) Is port forwarding isnecessary in  *NAT + Host-only Network, because * nodes
> are able to communicate each other and in host only network so I don't
> think port forwarding is necessary.
>
> 4) I just want to run stress testing for handling 1 lack call/sec. so how
> many sprout, vellum node is needed for this much calls.
>
> Thanks,
> Sunil
>
>
> On Thu, Apr 19, 2018 at 2:42 PM, Adam Lindley <Adam.Lindley at metaswitch.com
> > wrote:
>
>> Hi Sunil,
>>
>>
>>
>> I?m afraid the steps you?ve taken are not supported in Project Clearwater
>> deployments. Both changing the ?local_ip? of a node, and removing nodes
>> just by deleting the VMs.
>>
>>
>>
>> On the first point, you need to be able to give your VMs permanent static
>> IP addresses.
>>
>> On the second, by deleting the VMs in your cluster, your underlying etcd
>> cluster has lost quorum. I would suggest http://clearwater.readthedocs.
>> io/en/stable/Handling_Multiple_Failed_Nodes.htm as a starting point for
>> recovering information from it. However, as your single remaining node will
>> likely also have problems due to the local IP changing, you may simply want
>> to redeploy from scratch.
>>
>>
>>
>> More in general, you seem to have hit a substantial number of issues in
>> deploying Project Clearwater, which is both not what we want, and not what
>> the experience of many other users seems to be. I would suggest taking a
>> wider look over our provided documentation, and making sure your
>> environment matches our expectations, and that you?re clear on our
>> processes. This should make your next deployment a lot smoother.
>>
>>
>>
>> Cheers, and good luck,
>>
>> Adam
>>
>>
>>
>> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
>> *On Behalf Of *Sunil Kumar
>> *Sent:* 19 April 2018 07:16
>> *To:* clearwater at lists.projectclearwater.org
>> *Subject:* Re: [Project Clearwater] Unable to contact the etcd cluster
>>
>>
>>
>> Hi,
>>
>> the node with ip 10.224.61.109, 10.224.61.112 etc is no more there, I
>> have deleted the node directly. It looks like they are still in the etcd
>> cluster. Can you please tell me how to remove them
>>
>>
>>
>> [IST Apr 19 19:32:45] error    : 'etcd_process' process is not running
>>
>> [IST Apr 19 19:32:45] info     : 'etcd_process' trying to restart
>>
>> [IST Apr 19 19:32:45] info     : 'etcd_process' restart: /bin/bash
>>
>> [IST Apr 19 19:33:15] error    : 'etcd_process' failed to restart (exit
>> status -1) -- /bin/bash: Program timed out -- zmq_msg_recv: Resource
>> temporarily unavailable
>>
>> cat: /var/run/clearwater-etcd/clearwater-etcd.pid: No such file or
>> directory
>>
>> cat: /var/run/clearwater-etcd/clearwater-etcd.pid: No such file or
>> directory
>>
>> context deadline excee
>>
>> [IST Apr 19 19:33:25] error    : 'etcd_process' process is not running
>>
>> [IST Apr 19 19:33:25] info     : 'etcd_process' trying to restart
>>
>> [IST Apr 19 19:33:25] info     : 'etcd_process' restart: /bin/bash
>>
>> [IST Apr 19 19:33:55] error    : 'etcd_process' failed to restart (exit
>> status -1) -- /bin/bash: Program timed out -- zmq_msg_recv: Resource
>> temporarily unavailable
>>
>> client: etcd cluster is unavailable or misconfigured; error #0: *dial
>> tcp 10.224.61.109:4000 <http://10.224.61.109:4000>*: getsockopt: no
>> route to host
>>
>> ; error #1: dial tcp 10.224.61.47:4000: getsockopt: co
>>
>> [IST Apr 19 19:34:05] error    : 'etcd_process' process is not running
>>
>> [IST Apr 19 19:34:05] info     : 'etcd_process' trying to restart
>>
>> [IST Apr 19 19:34:05] info     : 'etcd_process' restart: /bin/bash
>>
>> [IST Apr 19 19:34:36] error    : 'etcd_process' failed to restart (exit
>> status 2) -- /bin/bash: zmq_msg_recv: Resource temporarily unavailable
>>
>> context deadline exceeded
>>
>>
>>
>>
>>
>> On Thu, Apr 19, 2018 at 11:03 AM, Sunil Kumar <skgola1997 at gmail.com>
>> wrote:
>>
>> Hi,
>>
>> Any body can help me on this. after ip lost, i update the ip in
>> local_config and dns and restart the service. extra vm is deleted lik  i
>> had 3 sprout node so 2 are deleted.
>>
>>
>>
>> [vellum]ubuntu at vellum:~$ cw-config upload shared_config
>>
>> Unable to contact the etcd cluster.
>>
>>
>>
>> thanks
>>
>> sunil
>>
>>
>>
>> _______________________________________________
>> Clearwater mailing list
>> Clearwater at lists.projectclearwater.org
>> http://lists.projectclearwater.org/mailman/listinfo/clearwat
>> er_lists.projectclearwater.org
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180419/e593f2b6/attachment.html>

From William.Yates at metaswitch.com  Thu Apr 19 06:58:06 2018
From: William.Yates at metaswitch.com (William Yates)
Date: Thu, 19 Apr 2018 10:58:06 +0000
Subject: [Project Clearwater] 
 =?utf-8?q?Release_note_for_Sprint_Zam=C3=AE?= =?utf-8?q?n?=
In-Reply-To: <A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
Message-ID: <BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>

Hi Roger,

Please could you tell me what version of vSphere you are using?

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 18 April 2018 21:59
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?

Am I missing something here?  This worked before.  Thanks!

Roger

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.

This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.

We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.


This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.

To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.


Cheers,

Richard

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180419/3130cb30/attachment.html>

From skgola1997 at gmail.com  Thu Apr 19 07:56:56 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Thu, 19 Apr 2018 17:26:56 +0530
Subject: [Project Clearwater] Unable to contact the etcd cluster
In-Reply-To: <CAHwYWpBPe5Vzpf1M4vArzV1auZqZKA7HwDZ+veXhq8JSFP=x2A@mail.gmail.com>
References: <CAHwYWpBv7iy6bZcNWX1ERDKK2_-4=jWX0AuvZb=FXgdmE==LeQ@mail.gmail.com>
	<CAHwYWpAnGZFSBvirAXH1MRaba5ZOfLtvYuHsO7J3WTO7NpSR4Q@mail.gmail.com>
	<CY1PR02MB204447191A1D35EAFFE123ACE2B50@CY1PR02MB2044.namprd02.prod.outlook.com>
	<CAHwYWpBaKQRYRCkKCuhbRKTLtVBHS=bTg24dNdVOERj74f-iAQ@mail.gmail.com>
	<CAHwYWpBPe5Vzpf1M4vArzV1auZqZKA7HwDZ+veXhq8JSFP=x2A@mail.gmail.com>
Message-ID: <CAHwYWpBUSXTKO_UzJNXOUhh3=FtrAhOKzag78ZreBcDB-M+z=Q@mail.gmail.com>

Thanks for replying Adam,

all node start working but *clearwater_cluster_manager_process *does not
exist

[bono]ubuntu at bono:~$ sudo monit summary
[sudo] password for ubuntu:
Monit 5.18.1 uptime: 22h 7m
 Service Name                     Status                      Type
 node-bono                        Running                     System
 restund_process                  Running                     Process
 ntp_process                      Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 *clearwater_cluster_manager_p...  Execution failed | Does...  Process*
 bono_process                     Running                     Process
 poll_restund                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program
 poll_bono                        Status ok                   Program


how can I run it it is showing execution failed | does not exit on every
node except vellum.

In parallel I am also installing the clearwater from scratch using static
IP (Using NAT + Host-only network). Please guide some solution regarding
that also if possible.

thansk,
sunil

On Thu, Apr 19, 2018 at 3:46 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> the lost node is not the master node but IP of master nodes is changed and
> I update them.
>
> thanks
>
> On Thu, Apr 19, 2018 at 3:23 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:
>
>> Hi Adam,
>>
>> Thanks a lot for replying.  I am using virtualbox for installing the VMs,
>> earlier I was using bridge adapter so it takes the network IP from dhcp, I
>> assign them as public and local_ip same.
>>
>> As you mention for static IP I tried with with *NAT + Host-only Network
>> (*NAT as primary interface eth0*), but all node having same IP in eth0
>> in NAT as 10.0.2.15 (is it fine to have all node same IP because I am not
>> using that IP)* and i assign host-only ip as static as 192.168.56.110
>> etc.
>>
>> 1) can I use both local_ip and public_ip same as host only ip
>> (192.168.56.110 etc) or public_ip would be the ip of host machine on which
>> virtualbox is installed as in NAT VM use Host IP as public IP to contact
>> outer world.
>>
>> 2) Is public_ip necessary as I only want stress testing to run in same
>> network, I don't want to install the no. on client like zoiper and all.
>>
>> 3) Is port forwarding isnecessary in  *NAT + Host-only Network, because * nodes
>> are able to communicate each other and in host only network so I don't
>> think port forwarding is necessary.
>>
>> 4) I just want to run stress testing for handling 1 lack call/sec. so how
>> many sprout, vellum node is needed for this much calls.
>>
>> Thanks,
>> Sunil
>>
>>
>> On Thu, Apr 19, 2018 at 2:42 PM, Adam Lindley <
>> Adam.Lindley at metaswitch.com> wrote:
>>
>>> Hi Sunil,
>>>
>>>
>>>
>>> I?m afraid the steps you?ve taken are not supported in Project
>>> Clearwater deployments. Both changing the ?local_ip? of a node, and
>>> removing nodes just by deleting the VMs.
>>>
>>>
>>>
>>> On the first point, you need to be able to give your VMs permanent
>>> static IP addresses.
>>>
>>> On the second, by deleting the VMs in your cluster, your underlying etcd
>>> cluster has lost quorum. I would suggest http://clearwater.readthedocs.
>>> io/en/stable/Handling_Multiple_Failed_Nodes.htm as a starting point for
>>> recovering information from it. However, as your single remaining node will
>>> likely also have problems due to the local IP changing, you may simply want
>>> to redeploy from scratch.
>>>
>>>
>>>
>>> More in general, you seem to have hit a substantial number of issues in
>>> deploying Project Clearwater, which is both not what we want, and not what
>>> the experience of many other users seems to be. I would suggest taking a
>>> wider look over our provided documentation, and making sure your
>>> environment matches our expectations, and that you?re clear on our
>>> processes. This should make your next deployment a lot smoother.
>>>
>>>
>>>
>>> Cheers, and good luck,
>>>
>>> Adam
>>>
>>>
>>>
>>> *From:* Clearwater [mailto:clearwater-bounces at lis
>>> ts.projectclearwater.org] *On Behalf Of *Sunil Kumar
>>> *Sent:* 19 April 2018 07:16
>>> *To:* clearwater at lists.projectclearwater.org
>>> *Subject:* Re: [Project Clearwater] Unable to contact the etcd cluster
>>>
>>>
>>>
>>> Hi,
>>>
>>> the node with ip 10.224.61.109, 10.224.61.112 etc is no more there, I
>>> have deleted the node directly. It looks like they are still in the etcd
>>> cluster. Can you please tell me how to remove them
>>>
>>>
>>>
>>> [IST Apr 19 19:32:45] error    : 'etcd_process' process is not running
>>>
>>> [IST Apr 19 19:32:45] info     : 'etcd_process' trying to restart
>>>
>>> [IST Apr 19 19:32:45] info     : 'etcd_process' restart: /bin/bash
>>>
>>> [IST Apr 19 19:33:15] error    : 'etcd_process' failed to restart (exit
>>> status -1) -- /bin/bash: Program timed out -- zmq_msg_recv: Resource
>>> temporarily unavailable
>>>
>>> cat: /var/run/clearwater-etcd/clearwater-etcd.pid: No such file or
>>> directory
>>>
>>> cat: /var/run/clearwater-etcd/clearwater-etcd.pid: No such file or
>>> directory
>>>
>>> context deadline excee
>>>
>>> [IST Apr 19 19:33:25] error    : 'etcd_process' process is not running
>>>
>>> [IST Apr 19 19:33:25] info     : 'etcd_process' trying to restart
>>>
>>> [IST Apr 19 19:33:25] info     : 'etcd_process' restart: /bin/bash
>>>
>>> [IST Apr 19 19:33:55] error    : 'etcd_process' failed to restart (exit
>>> status -1) -- /bin/bash: Program timed out -- zmq_msg_recv: Resource
>>> temporarily unavailable
>>>
>>> client: etcd cluster is unavailable or misconfigured; error #0: *dial
>>> tcp 10.224.61.109:4000 <http://10.224.61.109:4000>*: getsockopt: no
>>> route to host
>>>
>>> ; error #1: dial tcp 10.224.61.47:4000: getsockopt: co
>>>
>>> [IST Apr 19 19:34:05] error    : 'etcd_process' process is not running
>>>
>>> [IST Apr 19 19:34:05] info     : 'etcd_process' trying to restart
>>>
>>> [IST Apr 19 19:34:05] info     : 'etcd_process' restart: /bin/bash
>>>
>>> [IST Apr 19 19:34:36] error    : 'etcd_process' failed to restart (exit
>>> status 2) -- /bin/bash: zmq_msg_recv: Resource temporarily unavailable
>>>
>>> context deadline exceeded
>>>
>>>
>>>
>>>
>>>
>>> On Thu, Apr 19, 2018 at 11:03 AM, Sunil Kumar <skgola1997 at gmail.com>
>>> wrote:
>>>
>>> Hi,
>>>
>>> Any body can help me on this. after ip lost, i update the ip in
>>> local_config and dns and restart the service. extra vm is deleted lik  i
>>> had 3 sprout node so 2 are deleted.
>>>
>>>
>>>
>>> [vellum]ubuntu at vellum:~$ cw-config upload shared_config
>>>
>>> Unable to contact the etcd cluster.
>>>
>>>
>>>
>>> thanks
>>>
>>> sunil
>>>
>>>
>>>
>>> _______________________________________________
>>> Clearwater mailing list
>>> Clearwater at lists.projectclearwater.org
>>> http://lists.projectclearwater.org/mailman/listinfo/clearwat
>>> er_lists.projectclearwater.org
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180419/bdcb0c4a/attachment.html>

From skgola1997 at gmail.com  Thu Apr 19 10:22:39 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Thu, 19 Apr 2018 19:52:39 +0530
Subject: [Project Clearwater] stress testing - initial registration is failed
Message-ID: <CAHwYWpDguaN6qFQhCtMYJZU-O_WY25QsdT5kxB9XdhgaiUuSSQ@mail.gmail.com>

Hi,

Any one can please help me to debug the following issue:
I have installed the cleawater manually use NAT + Host-only network and
make host only ip as static and use them as local_ip while public_ip is any
dummy ip like 1.1.1.2. etc (as I don't want to access from outside), I just
want stress testing in same network environment.

Please help me finding the problem.  Thanks a lot in advance:

*[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress
ims.clearwater.com <http://ims.clearwater.com> 50 2*
[sudo] password for ubuntu:
Starting initial registration, will take 0 seconds
Initial registration failed - see
/var/log/clearwater-sip-stress/8770_initial_reg_errors.log for details of
the errors
*[]ubuntu at stress:~$ cat
/var/log/clearwater-sip-stress/8770_initial_reg_errors.log*
sipp: The following events occured:
2018-04-20      03:45:50.803049 1524176150.803049: Call-Id: 1-8777 at 127.0.1.1,
receive timeout on message Clearwater registration:1 without label to jump
to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.810338 1524176150.810338: Call-Id: 2-8777 at 127.0.1.1,
receive timeout on message Clearwater registration:1 without label to jump
to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.826867 1524176150.826867: Call-Id: 3-8777 at 127.0.1.1,
receive timeout on message Clearwater registration:1 without label to jump
to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.838481 1524176150.838481: Call-Id: 4-8777 at 127.0.1.1,
receive timeout on message Clearwater registration:1 without label to jump
to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.846833 1524176150.846833: Call-Id: 5-8777 at 127.0.1.1,
receive timeout on message Clearwater registration:1 without label to jump
to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.863077 1524176150.863077: Call-Id: 6-8777 at 127.0.1.1,
receive timeout on message Clearwater registration:1 without label to jump
to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.878650 1524176150.878650: Call-Id: 7-8777 at 127.0.1.1,
receive timeout on message Clearwater registration:1 without label to jump
to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.887176 1524176150.887176: Call-Id: 8-8777 at 127.0.1.1,
receive timeout on message Clearwater registration:1 without label to jump
to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.902412 1524176150.902412: Call-Id: 9-8777 at 127.0.1.1,
receive timeout on message Clearwater registration:1 without label to jump
to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.914283 1524176150.914283: Call-Id:
10-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
without label to jump to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.919105 1524176150.919105: Call-Id:
11-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
without label to jump to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.938373 1524176150.938373: Call-Id:
12-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
without label to jump to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.946694 1524176150.946694: Call-Id:
13-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
without label to jump to (ontimeout attribute): aborting call.
2018-04-20      03:45:50.954909 1524176150.954909: Call-Id:
14-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
without label to jump to (ontimeout attribute): aborting call.


Thanks,
sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180419/85225784/attachment.html>

From skgola1997 at gmail.com  Thu Apr 19 12:45:33 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Thu, 19 Apr 2018 22:15:33 +0530
Subject: [Project Clearwater] stress testing - initial registration is
 failed
In-Reply-To: <CAHwYWpDguaN6qFQhCtMYJZU-O_WY25QsdT5kxB9XdhgaiUuSSQ@mail.gmail.com>
References: <CAHwYWpDguaN6qFQhCtMYJZU-O_WY25QsdT5kxB9XdhgaiUuSSQ@mail.gmail.com>
Message-ID: <CAHwYWpAtxc8DMngDz153gMrxD+k=zMJQ7SSX+qjvQNY86=-FvA@mail.gmail.com>

I am also not able to ping sprout or any other node from stress node using
their hostname like sprout.ims.clearwater.com but using IP address I am
able to ping everynode from stress node like ping 192.168.56.112 etc.

I checked the sprout log there is no SIP log there, So calls are not
reaching till sprout node.

Please can you help me finding the issues with sprout node. Do I need to
mention IP of sprout node somewhere.

Using hostname like sprout.ims.clearwater.com i am able to ping among the
node only, but from stress node i am not able to ping. Stress node is also
in the same network IP of stress node is 192.168.56.116.

Please reply I am waiting for your response.

Thanks,
sunil

On Thu, Apr 19, 2018 at 7:52 PM, Sunil Kumar <skgola1997 at gmail.com> wrote:

> Hi,
>
> Any one can please help me to debug the following issue:
> I have installed the cleawater manually use NAT + Host-only network and
> make host only ip as static and use them as local_ip while public_ip is any
> dummy ip like 1.1.1.2. etc (as I don't want to access from outside), I just
> want stress testing in same network environment.
>
> Please help me finding the problem.  Thanks a lot in advance:
>
> *[]ubuntu at stress:~$ sudo /usr/share/clearwater/bin/run_stress
> ims.clearwater.com <http://ims.clearwater.com> 50 2*
> [sudo] password for ubuntu:
> Starting initial registration, will take 0 seconds
> Initial registration failed - see /var/log/clearwater-sip-
> stress/8770_initial_reg_errors.log for details of the errors
> *[]ubuntu at stress:~$ cat
> /var/log/clearwater-sip-stress/8770_initial_reg_errors.log*
> sipp: The following events occured:
> 2018-04-20      03:45:50.803049 1524176150.803049: Call-Id:
> 1-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.810338 1524176150.810338: Call-Id:
> 2-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.826867 1524176150.826867: Call-Id:
> 3-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.838481 1524176150.838481: Call-Id:
> 4-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.846833 1524176150.846833: Call-Id:
> 5-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.863077 1524176150.863077: Call-Id:
> 6-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.878650 1524176150.878650: Call-Id:
> 7-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.887176 1524176150.887176: Call-Id:
> 8-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.902412 1524176150.902412: Call-Id:
> 9-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.914283 1524176150.914283: Call-Id:
> 10-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.919105 1524176150.919105: Call-Id:
> 11-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.938373 1524176150.938373: Call-Id:
> 12-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.946694 1524176150.946694: Call-Id:
> 13-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
> 2018-04-20      03:45:50.954909 1524176150.954909: Call-Id:
> 14-8777 at 127.0.1.1, receive timeout on message Clearwater registration:1
> without label to jump to (ontimeout attribute): aborting call.
>
>
> Thanks,
> sunil
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180419/15ca8c4c/attachment.html>

From case at gci.com  Thu Apr 19 15:14:46 2018
From: case at gci.com (Roger Case)
Date: Thu, 19 Apr 2018 19:14:46 +0000
Subject: [Project Clearwater] 
 =?utf-8?q?Release_note_for_Sprint_Zam=C3=AE?= =?utf-8?q?n?=
In-Reply-To: <BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
	<BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>
Message-ID: <A7E1158244C52C428F0B61BDC9CF760C016D9674D4@ex-mbx-prd02.gci.com>

Hi Will,

vSphere 6.0, vCenter Server 6.5.

Thanks!

Roger

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of William Yates
Sent: Thursday, April 19, 2018 2:58 AM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
Hi Roger,

Please could you tell me what version of vSphere you are using?

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 18 April 2018 21:59
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?

Am I missing something here?  This worked before.  Thanks!

Roger

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.

This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.

We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.


This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.

To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.


Cheers,

Richard

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180419/2b8cf051/attachment.html>

From William.Yates at metaswitch.com  Fri Apr 20 05:47:47 2018
From: William.Yates at metaswitch.com (William Yates)
Date: Fri, 20 Apr 2018 09:47:47 +0000
Subject: [Project Clearwater] 
 =?utf-8?q?Release_note_for_Sprint_Zam=C3=AE?= =?utf-8?q?n?=
In-Reply-To: <A7E1158244C52C428F0B61BDC9CF760C016D9674D4@ex-mbx-prd02.gci.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
	<BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D9674D4@ex-mbx-prd02.gci.com>
Message-ID: <BLUPR0201MB149036A21130C85A1A66829E95B40@BLUPR0201MB1490.namprd02.prod.outlook.com>

Hi Roger,

When this worked before, was it on a previous version of vSphere?

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 19 April 2018 20:15
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Hi Will,

vSphere 6.0, vCenter Server 6.5.

Thanks!

Roger

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of William Yates
Sent: Thursday, April 19, 2018 2:58 AM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
Hi Roger,

Please could you tell me what version of vSphere you are using?

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 18 April 2018 21:59
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?

Am I missing something here?  This worked before.  Thanks!

Roger

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.

This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.

We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.


This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.

To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.


Cheers,

Richard

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180420/1e0ecf2e/attachment.html>

From Mark.Perryman at metaswitch.com  Fri Apr 20 06:03:45 2018
From: Mark.Perryman at metaswitch.com (Mark Perryman)
Date: Fri, 20 Apr 2018 10:03:45 +0000
Subject: [Project Clearwater] Manual installation of Clearwater at one
 node
Message-ID: <CY4PR02MB25189F98D528308265E30A758FB40@CY4PR02MB2518.namprd02.prod.outlook.com>

To build an all in one node, see https://github.com/Metaswitch/clearwater-vm-images/tree/master/ubuntu-ovf.

The instructions for source building are linked to from https://clearwater.readthedocs.io/en/stable/Installation_Instructions.html#detailed-instructions.  Note that 'crest' is the base for both Homer and Homestead Prov.

Dime is just a meta package and is in infrastructure: https://github.com/Metaswitch/clearwater-infrastructure/blob/master/debian/control#L98-L109

Hope that helps.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180420/b3445a97/attachment.html>

From case at gci.com  Fri Apr 20 12:25:09 2018
From: case at gci.com (Roger Case)
Date: Fri, 20 Apr 2018 16:25:09 +0000
Subject: [Project Clearwater] 
 =?windows-1252?q?Release_note_for_Sprint_Za?=
 =?windows-1252?q?m=EEn?=
In-Reply-To: <BLUPR0201MB149036A21130C85A1A66829E95B40@BLUPR0201MB1490.namprd02.prod.outlook.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
	<BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D9674D4@ex-mbx-prd02.gci.com>,
	<BLUPR0201MB149036A21130C85A1A66829E95B40@BLUPR0201MB1490.namprd02.prod.outlook.com>
Message-ID: <9ED8EE39-46CC-4466-8A61-168201C9DDBC@gci.com>

Hi Will,

No, also 6.0.
Previous AIO, however.
Other Metaswitch .OVA still work....

Sent from my GCiPhone

On Apr 20, 2018, at 01:49, William Yates <William.Yates at metaswitch.com<mailto:William.Yates at metaswitch.com>> wrote:

[External Email]

Hi Roger,

When this worked before, was it on a previous version of vSphere?

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 19 April 2018 20:15
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Hi Will,

vSphere 6.0, vCenter Server 6.5.

Thanks!

Roger

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of William Yates
Sent: Thursday, April 19, 2018 2:58 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
Hi Roger,

Please could you tell me what version of vSphere you are using?

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 18 April 2018 21:59
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?

Am I missing something here?  This worked before.  Thanks!

Roger

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org<http://projectclearwater.org>)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.

This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.

We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.


This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.

To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.


Cheers,

Richard

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180420/de5b18c1/attachment.html>

From case at gci.com  Fri Apr 20 15:57:40 2018
From: case at gci.com (Roger Case)
Date: Fri, 20 Apr 2018 19:57:40 +0000
Subject: [Project Clearwater] 
 =?utf-8?q?Release_note_for_Sprint_Zam=C3=AE?= =?utf-8?q?n?=
In-Reply-To: <1B813795-9351-41F9-9831-B07F54D3AC30@vmware.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
	<BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D9674D4@ex-mbx-prd02.gci.com>,
	<BLUPR0201MB149036A21130C85A1A66829E95B40@BLUPR0201MB1490.namprd02.prod.outlook.com>,
	<9ED8EE39-46CC-4466-8A61-168201C9DDBC@gci.com>
	<1B813795-9351-41F9-9831-B07F54D3AC30@vmware.com>
Message-ID: <A7E1158244C52C428F0B61BDC9CF760C016D98E595@ex-mbx-prd02.gci.com>

Hi Frank,


1)      Where would I get the checksum to compare with the one I derived from the downloaded file?

2)      Unzip OK.

3)      Checksums of .ovf and .vmdk match those in the .mf file

4)      Same error when selecting all three unzipped files to deploy ovf.

5)      Removing the .mf file and attempting to deploy the template using just the .ovf and .vmdk results in different errors:


[cid:image001.png at 01D3D89E.C6EEAF80]

Thanks,

Roger

From: Frank Escaros-Buechsel [mailto:fescarosbuechs at vmware.com]
Sent: Friday, April 20, 2018 9:12 AM
To: clearwater at lists.projectclearwater.org; Roger Case
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
1)Verify the checksum to ensure you don?t have a corrupt download
2)unzip the ova into a folder, you will get an ovf, vmdk and mf file
3)run sha1 sum on all files separately, compare output against the values in mf file, if one deviates update the value in mf to correct one
4)do not zip again, deploy through web client and select ovf and all vmdk files

This should let you deploy.

Another way would be to follow step 1 and 2 and simply delete the mf file, as this will skip the checksum check (not recommended for environments which need to proof authenticity of uploaded files though)

Hope that helps.

Frank


Sent from VMware Boxer
On 20 April 2018 at 19:27:24 GMT+3, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:
Hi Will,

No, also 6.0.
Previous AIO, however.
Other Metaswitch .OVA still work....

Sent from my GCiPhone

On Apr 20, 2018, at 01:49, William Yates <William.Yates at metaswitch.com<mailto:William.Yates at metaswitch.com>> wrote:
[External Email]

Hi Roger,



When this worked before, was it on a previous version of vSphere?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 19 April 2018 20:15
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Hi Will,



vSphere 6.0, vCenter Server 6.5.



Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of William Yates
Sent: Thursday, April 19, 2018 2:58 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi Roger,



Please could you tell me what version of vSphere you are using?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 18 April 2018 21:59
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?



Am I missing something here?  This worked before.  Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=2qwlj9Q3_bQcSmxUEMuhYsYgR9cLAYq8Yow_4Awb6sg&e=>)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.



This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.



We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.





This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.



To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__docs.projectclearwater.org_en_stable_Upgrading-5Fa-5FClearwater-5Fdeployment.html&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=qcTPSTk5rbrrb9tKuc7vPepjpenEiPPEJ2lTtsmp9Es&e=>.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova<https://urldefense.proofpoint.com/v2/url?u=http-3A__vm-2Dimages.cw-2Dngv.com_cw-2Daio.ova&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=PUMku_aSKgNoiXkqZ8ant31BoZ-iIM3wJCW24TKelsU&e=>) has been updated for this release.





Cheers,



Richard


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.projectclearwater.org_mailman_listinfo_clearwater-5Flists.projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=Z5v_f1O7HlUlYiseOcH3vO5bpWsuXKfrYT-IaaL4saE&e=>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180420/b4286c45/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 8624 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180420/b4286c45/attachment.png>

From fescarosbuechs at vmware.com  Fri Apr 20 13:11:59 2018
From: fescarosbuechs at vmware.com (Frank Escaros-Buechsel)
Date: Fri, 20 Apr 2018 17:11:59 +0000
Subject: [Project Clearwater] 
 =?utf-8?q?Release_note_for_Sprint_Zam=C3=AE?= =?utf-8?q?n?=
In-Reply-To: <9ED8EE39-46CC-4466-8A61-168201C9DDBC@gci.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
	<BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D9674D4@ex-mbx-prd02.gci.com>,
	<BLUPR0201MB149036A21130C85A1A66829E95B40@BLUPR0201MB1490.namprd02.prod.outlook.com>,
	<9ED8EE39-46CC-4466-8A61-168201C9DDBC@gci.com>
Message-ID: <1B813795-9351-41F9-9831-B07F54D3AC30@vmware.com>

1)Verify the checksum to ensure you don?t have a corrupt download
2)unzip the ova into a folder, you will get an ovf, vmdk and mf file
3)run sha1 sum on all files separately, compare output against the values in mf file, if one deviates update the value in mf to correct one
4)do not zip again, deploy through web client and select ovf and all vmdk files

This should let you deploy.

Another way would be to follow step 1 and 2 and simply delete the mf file, as this will skip the checksum check (not recommended for environments which need to proof authenticity of uploaded files though)

Hope that helps.

Frank


Sent from VMware Boxer

On 20 April 2018 at 19:27:24 GMT+3, Roger Case <case at gci.com> wrote:
Hi Will,

No, also 6.0.
Previous AIO, however.
Other Metaswitch .OVA still work....

Sent from my GCiPhone

On Apr 20, 2018, at 01:49, William Yates <William.Yates at metaswitch.com<mailto:William.Yates at metaswitch.com>> wrote:

[External Email]


Hi Roger,



When this worked before, was it on a previous version of vSphere?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 19 April 2018 20:15
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Hi Will,



vSphere 6.0, vCenter Server 6.5.



Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of William Yates
Sent: Thursday, April 19, 2018 2:58 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi Roger,



Please could you tell me what version of vSphere you are using?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 18 April 2018 21:59
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?



Am I missing something here?  This worked before.  Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=2qwlj9Q3_bQcSmxUEMuhYsYgR9cLAYq8Yow_4Awb6sg&e=>)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.



This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.



We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.





This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.



To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__docs.projectclearwater.org_en_stable_Upgrading-5Fa-5FClearwater-5Fdeployment.html&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=qcTPSTk5rbrrb9tKuc7vPepjpenEiPPEJ2lTtsmp9Es&e=>.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova<https://urldefense.proofpoint.com/v2/url?u=http-3A__vm-2Dimages.cw-2Dngv.com_cw-2Daio.ova&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=PUMku_aSKgNoiXkqZ8ant31BoZ-iIM3wJCW24TKelsU&e=>) has been updated for this release.





Cheers,



Richard



_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.projectclearwater.org_mailman_listinfo_clearwater-5Flists.projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=Z5v_f1O7HlUlYiseOcH3vO5bpWsuXKfrYT-IaaL4saE&e=>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180420/c0000ed7/attachment.html>

From fescarosbuechs at vmware.com  Fri Apr 20 16:02:43 2018
From: fescarosbuechs at vmware.com (Frank Escaros-Buechsel)
Date: Fri, 20 Apr 2018 20:02:43 +0000
Subject: [Project Clearwater] 
 =?utf-8?q?Release_note_for_Sprint_Zam=C3=AE?= =?utf-8?q?n?=
In-Reply-To: <A7E1158244C52C428F0B61BDC9CF760C016D98E595@ex-mbx-prd02.gci.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
	<BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D9674D4@ex-mbx-prd02.gci.com>,
	<BLUPR0201MB149036A21130C85A1A66829E95B40@BLUPR0201MB1490.namprd02.prod.outlook.com>,
	<9ED8EE39-46CC-4466-8A61-168201C9DDBC@gci.com>
	<1B813795-9351-41F9-9831-B07F54D3AC30@vmware.com>,
	<A7E1158244C52C428F0B61BDC9CF760C016D98E595@ex-mbx-prd02.gci.com>
Message-ID: <54A349FD-766F-4DFE-B775-56C65C396E83@vmware.com>

Hi Roger,
Looks like a malformed ofv file for the vsphere build you are using, I can try to reproduce in my lab over the weekend.

Frank Escaros-Buechsel
Consulting Architect NFV
Parnell House, Barrack Square, Ballincollig, Co. Cork
P31 PF68, IRL
Twitter: @fbuechsel

On 20 Apr 2018, at 22:58, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:

Hi Frank,


1)      Where would I get the checksum to compare with the one I derived from the downloaded file?

2)      Unzip OK.

3)      Checksums of .ovf and .vmdk match those in the .mf file

4)      Same error when selecting all three unzipped files to deploy ovf.

5)      Removing the .mf file and attempting to deploy the template using just the .ovf and .vmdk results in different errors:


<image001.png>

Thanks,

Roger

From: Frank Escaros-Buechsel [mailto:fescarosbuechs at vmware.com]
Sent: Friday, April 20, 2018 9:12 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Roger Case
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
1)Verify the checksum to ensure you don?t have a corrupt download
2)unzip the ova into a folder, you will get an ovf, vmdk and mf file
3)run sha1 sum on all files separately, compare output against the values in mf file, if one deviates update the value in mf to correct one
4)do not zip again, deploy through web client and select ovf and all vmdk files

This should let you deploy.

Another way would be to follow step 1 and 2 and simply delete the mf file, as this will skip the checksum check (not recommended for environments which need to proof authenticity of uploaded files though)

Hope that helps.

Frank


Sent from VMware Boxer
On 20 April 2018 at 19:27:24 GMT+3, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:
Hi Will,

No, also 6.0.
Previous AIO, however.
Other Metaswitch .OVA still work....

Sent from my GCiPhone

On Apr 20, 2018, at 01:49, William Yates <William.Yates at metaswitch.com<mailto:William.Yates at metaswitch.com>> wrote:
[External Email]

Hi Roger,



When this worked before, was it on a previous version of vSphere?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 19 April 2018 20:15
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Hi Will,



vSphere 6.0, vCenter Server 6.5.



Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of William Yates
Sent: Thursday, April 19, 2018 2:58 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi Roger,



Please could you tell me what version of vSphere you are using?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 18 April 2018 21:59
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?



Am I missing something here?  This worked before.  Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=2qwlj9Q3_bQcSmxUEMuhYsYgR9cLAYq8Yow_4Awb6sg&e=>)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.



This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.



We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.





This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.



To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__docs.projectclearwater.org_en_stable_Upgrading-5Fa-5FClearwater-5Fdeployment.html&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=qcTPSTk5rbrrb9tKuc7vPepjpenEiPPEJ2lTtsmp9Es&e=>.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova<https://urldefense.proofpoint.com/v2/url?u=http-3A__vm-2Dimages.cw-2Dngv.com_cw-2Daio.ova&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=PUMku_aSKgNoiXkqZ8ant31BoZ-iIM3wJCW24TKelsU&e=>) has been updated for this release.





Cheers,



Richard


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.projectclearwater.org_mailman_listinfo_clearwater-5Flists.projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=Z5v_f1O7HlUlYiseOcH3vO5bpWsuXKfrYT-IaaL4saE&e=>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180420/1258c97c/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 8624 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180420/1258c97c/attachment.png>

From fescarosbuechs at vmware.com  Mon Apr 23 19:08:37 2018
From: fescarosbuechs at vmware.com (Frank Escaros-Buechsel)
Date: Mon, 23 Apr 2018 23:08:37 +0000
Subject: [Project Clearwater] 
 =?utf-8?q?Release_note_for_Sprint_Zam=C3=AE?= =?utf-8?q?n?=
In-Reply-To: <54A349FD-766F-4DFE-B775-56C65C396E83@vmware.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
	<BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D9674D4@ex-mbx-prd02.gci.com>,
	<BLUPR0201MB149036A21130C85A1A66829E95B40@BLUPR0201MB1490.namprd02.prod.outlook.com>,
	<9ED8EE39-46CC-4466-8A61-168201C9DDBC@gci.com>
	<1B813795-9351-41F9-9831-B07F54D3AC30@vmware.com>,
	<A7E1158244C52C428F0B61BDC9CF760C016D98E595@ex-mbx-prd02.gci.com>
	<54A349FD-766F-4DFE-B775-56C65C396E83@vmware.com>
Message-ID: <BN6PR05MB3074301874BF4C5590B190A2B9890@BN6PR05MB3074.namprd05.prod.outlook.com>

Hi Roger,
After reading the documentation I do not think that a vCenter deploy is extensively tested as only ESXi deploy is mentioned in the clearwater wiki.

I get exactly the same error message as you do when I try to deploy through vCenter, yet I am able to successfully deploy the OVA directly to ESXi.

@Clearwater team:
I modified the OVF file after extraction and removed the offending fields from Roger?s screenshot which also allows me then to deploy successfully through vCenter.
I have copied the new contents of the OVF file to https://pastebin.com/raw/NStkhajn for your review.
When trying to boot the VM it is complaining about not being able to connect to SATA0 controller.
What is the actual intended disk configuration for the AIO image based on vSphere? Which virtual storage controller is the target and how many vdisks should the resulting VM have, as currently the OVA through vCenter as well as through ESXi deploys with a single disk.

Ellis and Bono do seem to come up successfully despite the error I am seeing, which makes me think it?s purely cosmetic but with knowing the correct disk controller type for vSphere which is intended by you I can make further modifications to the OVF to avoid the cosmetic error as well:

[cw-aio]ubuntu at cw-aio:~$ netstat -ano | grep 5060
tcp        0      0 10.27.32.111:5060       0.0.0.0:*               LISTEN      off (0.00/0/0)
udp        0      0 10.27.32.111:5060       0.0.0.0:*                           off (0.00/0/0)

Kind Regards,
Frank Escaros-Buechsel
Consulting Architect NFV
fescarosbuechs at vmware.com<mailto:fescarosbuechs at vmware.com>
Parnell  House, Barrack Square, Ballincollig, Co. Cork, P31 PF68, IRL
Twitter: @fbuechsel

Advanced notice of absence:


[VMwareLogo]

From: Frank Escaros-Buechsel
Sent: Friday 20 April 2018 22:03
To: Roger Case <case at gci.com>
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Hi Roger,
Looks like a malformed ofv file for the vsphere build you are using, I can try to reproduce in my lab over the weekend.
Frank Escaros-Buechsel
Consulting Architect NFV
Parnell House, Barrack Square, Ballincollig, Co. Cork
P31 PF68, IRL
Twitter: @fbuechsel

On 20 Apr 2018, at 22:58, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:
Hi Frank,


  1.  Where would I get the checksum to compare with the one I derived from the downloaded file?
  2.  Unzip OK.
  3.  Checksums of .ovf and .vmdk match those in the .mf file
  4.  Same error when selecting all three unzipped files to deploy ovf.
  5.  Removing the .mf file and attempting to deploy the template using just the .ovf and .vmdk results in different errors:


<image001.png>

Thanks,

Roger

From: Frank Escaros-Buechsel [mailto:fescarosbuechs at vmware.com]
Sent: Friday, April 20, 2018 9:12 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Roger Case
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
1)Verify the checksum to ensure you don?t have a corrupt download
2)unzip the ova into a folder, you will get an ovf, vmdk and mf file
3)run sha1 sum on all files separately, compare output against the values in mf file, if one deviates update the value in mf to correct one
4)do not zip again, deploy through web client and select ovf and all vmdk files

This should let you deploy.

Another way would be to follow step 1 and 2 and simply delete the mf file, as this will skip the checksum check (not recommended for environments which need to proof authenticity of uploaded files though)

Hope that helps.

Frank


Sent from VMware Boxer
On 20 April 2018 at 19:27:24 GMT+3, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:
Hi Will,

No, also 6.0.
Previous AIO, however.
Other Metaswitch .OVA still work....

Sent from my GCiPhone

On Apr 20, 2018, at 01:49, William Yates <William.Yates at metaswitch.com<mailto:William.Yates at metaswitch.com>> wrote:
[External Email]

Hi Roger,



When this worked before, was it on a previous version of vSphere?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 19 April 2018 20:15
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Hi Will,



vSphere 6.0, vCenter Server 6.5.



Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of William Yates
Sent: Thursday, April 19, 2018 2:58 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi Roger,



Please could you tell me what version of vSphere you are using?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 18 April 2018 21:59
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?



Am I missing something here?  This worked before.  Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=2qwlj9Q3_bQcSmxUEMuhYsYgR9cLAYq8Yow_4Awb6sg&e=>)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.



This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.



We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.





This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.



To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__docs.projectclearwater.org_en_stable_Upgrading-5Fa-5FClearwater-5Fdeployment.html&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=qcTPSTk5rbrrb9tKuc7vPepjpenEiPPEJ2lTtsmp9Es&e=>.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova<https://urldefense.proofpoint.com/v2/url?u=http-3A__vm-2Dimages.cw-2Dngv.com_cw-2Daio.ova&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=PUMku_aSKgNoiXkqZ8ant31BoZ-iIM3wJCW24TKelsU&e=>) has been updated for this release.





Cheers,



Richard


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.projectclearwater.org_mailman_listinfo_clearwater-5Flists.projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=Z5v_f1O7HlUlYiseOcH3vO5bpWsuXKfrYT-IaaL4saE&e=>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180423/5099e524/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.gif
Type: image/gif
Size: 4572 bytes
Desc: image001.gif
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180423/5099e524/attachment.gif>

From arunlalsingh23 at gmail.com  Tue Apr 24 04:58:03 2018
From: arunlalsingh23 at gmail.com (Arun Lal)
Date: Tue, 24 Apr 2018 14:28:03 +0530
Subject: [Project Clearwater] Regarding Increasing #calls using stress
 testing
Message-ID: <CAL1mNUF2AdfjhOZ38Mt0Jr2wZWnp1+mF91RzEFgwiHoRtPa78g@mail.gmail.com>

 Hi,

Nice work by clearwater team, I really appreciate your work.

The document of clearwater is self explanatory, most of the task I did by
reading that, but I have stuck at some points:

> I have 3 sprout, 2 vellum, 2 dime, 1 bono, 1 homer, 1 ellis nodes. I want
to make 15000 calls/sec or more. I can increase more nodes if possible.

> I am not able to make more calls, I tried

[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress rags.mf 30000 1
--multiplier=20

but it gives around 3000 calls.

> what exact command should I use to make more calls per sec. All the node
are working fine
I have check that zoiper and other sip client working fine.

> *Can you please share exact command so that I can increase no. of calls
per sec.*

Or how many more nodes are required, I will add.



Thanks,
Arun Lal Singh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180424/a66ad680/attachment.html>

From William.Yates at metaswitch.com  Tue Apr 24 05:39:38 2018
From: William.Yates at metaswitch.com (William Yates)
Date: Tue, 24 Apr 2018 09:39:38 +0000
Subject: [Project Clearwater] 
 =?utf-8?q?Release_note_for_Sprint_Zam=C3=AE?= =?utf-8?q?n?=
In-Reply-To: <BN6PR05MB3074301874BF4C5590B190A2B9890@BN6PR05MB3074.namprd05.prod.outlook.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
	<BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D9674D4@ex-mbx-prd02.gci.com>,
	<BLUPR0201MB149036A21130C85A1A66829E95B40@BLUPR0201MB1490.namprd02.prod.outlook.com>,
	<9ED8EE39-46CC-4466-8A61-168201C9DDBC@gci.com>
	<1B813795-9351-41F9-9831-B07F54D3AC30@vmware.com>,
	<A7E1158244C52C428F0B61BDC9CF760C016D98E595@ex-mbx-prd02.gci.com>
	<54A349FD-766F-4DFE-B775-56C65C396E83@vmware.com>
	<BN6PR05MB3074301874BF4C5590B190A2B9890@BN6PR05MB3074.namprd05.prod.outlook.com>
Message-ID: <BLUPR0201MB1490EDCA3422D3B75EEE4D0E95880@BLUPR0201MB1490.namprd02.prod.outlook.com>

Thanks Frank, we'll get back on the SATA0 issue.

Roger, would you be happy to raise a github issue for this, and we'll consider fixing it in a future stable release ?

Also, the manual install offers another workaround, described here: https://clearwater.readthedocs.io/en/stable/All_in_one_Images.html#manual-build

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Frank Escaros-Buechsel
Sent: 24 April 2018 00:09
To: Roger Case <case at gci.com>
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Hi Roger,
After reading the documentation I do not think that a vCenter deploy is extensively tested as only ESXi deploy is mentioned in the clearwater wiki.

I get exactly the same error message as you do when I try to deploy through vCenter, yet I am able to successfully deploy the OVA directly to ESXi.

@Clearwater team:
I modified the OVF file after extraction and removed the offending fields from Roger?s screenshot which also allows me then to deploy successfully through vCenter.
I have copied the new contents of the OVF file to https://pastebin.com/raw/NStkhajn for your review.
When trying to boot the VM it is complaining about not being able to connect to SATA0 controller.
What is the actual intended disk configuration for the AIO image based on vSphere? Which virtual storage controller is the target and how many vdisks should the resulting VM have, as currently the OVA through vCenter as well as through ESXi deploys with a single disk.

Ellis and Bono do seem to come up successfully despite the error I am seeing, which makes me think it?s purely cosmetic but with knowing the correct disk controller type for vSphere which is intended by you I can make further modifications to the OVF to avoid the cosmetic error as well:

[cw-aio]ubuntu at cw-aio:~$ netstat -ano | grep 5060
tcp        0      0 10.27.32.111:5060       0.0.0.0:*               LISTEN      off (0.00/0/0)
udp        0      0 10.27.32.111:5060       0.0.0.0:*                           off (0.00/0/0)

Kind Regards,
Frank Escaros-Buechsel
Consulting Architect NFV
fescarosbuechs at vmware.com<mailto:fescarosbuechs at vmware.com>
Parnell  House, Barrack Square, Ballincollig, Co. Cork, P31 PF68, IRL
Twitter: @fbuechsel

Advanced notice of absence:


[VMwareLogo]

From: Frank Escaros-Buechsel
Sent: Friday 20 April 2018 22:03
To: Roger Case <case at gci.com>
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Hi Roger,
Looks like a malformed ofv file for the vsphere build you are using, I can try to reproduce in my lab over the weekend.
Frank Escaros-Buechsel
Consulting Architect NFV
Parnell House, Barrack Square, Ballincollig, Co. Cork
P31 PF68, IRL
Twitter: @fbuechsel

On 20 Apr 2018, at 22:58, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:
Hi Frank,


  1.  Where would I get the checksum to compare with the one I derived from the downloaded file?
  2.  Unzip OK.
  3.  Checksums of .ovf and .vmdk match those in the .mf file
  4.  Same error when selecting all three unzipped files to deploy ovf.
  5.  Removing the .mf file and attempting to deploy the template using just the .ovf and .vmdk results in different errors:


<image001.png>

Thanks,

Roger

From: Frank Escaros-Buechsel [mailto:fescarosbuechs at vmware.com]
Sent: Friday, April 20, 2018 9:12 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Roger Case
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
1)Verify the checksum to ensure you don?t have a corrupt download
2)unzip the ova into a folder, you will get an ovf, vmdk and mf file
3)run sha1 sum on all files separately, compare output against the values in mf file, if one deviates update the value in mf to correct one
4)do not zip again, deploy through web client and select ovf and all vmdk files

This should let you deploy.

Another way would be to follow step 1 and 2 and simply delete the mf file, as this will skip the checksum check (not recommended for environments which need to proof authenticity of uploaded files though)

Hope that helps.

Frank


Sent from VMware Boxer
On 20 April 2018 at 19:27:24 GMT+3, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:
Hi Will,

No, also 6.0.
Previous AIO, however.
Other Metaswitch .OVA still work....

Sent from my GCiPhone

On Apr 20, 2018, at 01:49, William Yates <William.Yates at metaswitch.com<mailto:William.Yates at metaswitch.com>> wrote:
[External Email]

Hi Roger,



When this worked before, was it on a previous version of vSphere?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 19 April 2018 20:15
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Hi Will,



vSphere 6.0, vCenter Server 6.5.



Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of William Yates
Sent: Thursday, April 19, 2018 2:58 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi Roger,



Please could you tell me what version of vSphere you are using?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 18 April 2018 21:59
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?



Am I missing something here?  This worked before.  Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=2qwlj9Q3_bQcSmxUEMuhYsYgR9cLAYq8Yow_4Awb6sg&e=>)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.



This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.



We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.





This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.



To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__docs.projectclearwater.org_en_stable_Upgrading-5Fa-5FClearwater-5Fdeployment.html&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=qcTPSTk5rbrrb9tKuc7vPepjpenEiPPEJ2lTtsmp9Es&e=>.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova<https://urldefense.proofpoint.com/v2/url?u=http-3A__vm-2Dimages.cw-2Dngv.com_cw-2Daio.ova&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=PUMku_aSKgNoiXkqZ8ant31BoZ-iIM3wJCW24TKelsU&e=>) has been updated for this release.





Cheers,



Richard


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.projectclearwater.org_mailman_listinfo_clearwater-5Flists.projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=Z5v_f1O7HlUlYiseOcH3vO5bpWsuXKfrYT-IaaL4saE&e=>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180424/5476b4f4/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.gif
Type: image/gif
Size: 4572 bytes
Desc: image001.gif
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180424/5476b4f4/attachment.gif>

From Ying.Huang at metaswitch.com  Tue Apr 24 09:25:06 2018
From: Ying.Huang at metaswitch.com (Ying Huang)
Date: Tue, 24 Apr 2018 13:25:06 +0000
Subject: [Project Clearwater] Input required for High Availability use
 cases of clearwater
In-Reply-To: <1255321429.332126.1523177395867@mail.yahoo.com>
References: <372055626.178624.1522494458227.ref@mail.yahoo.com>
	<372055626.178624.1522494458227@mail.yahoo.com>
	<1255321429.332126.1523177395867@mail.yahoo.com>
Message-ID: <BN6PR02MB2802E6CF15BE50453FC47663F8880@BN6PR02MB2802.namprd02.prod.outlook.com>

Hi Nagu,

You probably have missed our answer, quoted below -


Hi Nagendra,

The high-availability approach you describe looks different from the one Clearwater is designed to support. Rather than having a Bono node become a combined Bono/Ellis node if the Ellis node fails, we have an approach where:

  *   You have pools of nodes of each type ? e.g. you might have 3 or 4 Sprout nodes, depending on load requirements
  *   Other components use DNS to balance load across those 4 Sprout nodes
  *   If one of the 4 Sprout nodes fails, it gets blacklisted ? and the other 3 Sprout nodes can handle the traffic meant for it, as the relevant state is stored in redundant data stores on Vellum

In this model, scaling in is as simple as removing a node from DNS and deleting it, and scaling out is as simple as adding a node and then adding it to DNS.

http://www.projectclearwater.org/technical/call-flows/ shows the call flows when Sprout, Dime etc. fail in more detail.

Best,
Rob


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Nagendra Kumar
Sent: 08 April 2018 09:50
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Input required for High Availability use cases of clearwater

Hi Team,
               Any input on this ?

Thanks
-Nagu


On Saturday, 31 March, 2018, 4:37:38 PM IST, Nagendra Kumar <nagen_kr at yahoo.co.in<mailto:nagen_kr at yahoo.co.in>> wrote:


Dear Team,

I have a high availability use case below, please provide your inputs.
I have 6 nodes running for clearwater services on 6 different nodes and 1 for DNS(on Node7) as below:
(I am using AWS AMI ubuntu 14 image and have manually installed as per instruction at Manual Install Instructions ? Project Clearwater 1.0 documentation<http://clearwater.readthedocs.io/en/stable/Manual_Install.html>)
Desired configuration :
Node1 : Ellis
Node2 : Bono
Node3 : Sprout
Node4 : Homer
Node5 : Dime
Node6 : Vellum


Now, we have a requirement of scale up and scale down.
Scale Down:
1. if Node1 fails, Ellis should start working on Node2. That means Node2 hosts Ellis and Bono both running.
2. After that if Node2 fails, Ellis and Bono should shift to Node3. That means Node3 hosts Ellis, Bono and Sprout.
3. And so on..... till Node5 fails and Node6 hosts Ellis, Bono, Sprout, Homer, Dime and Vellum.

Will it need any source code change or work with just configuration change? Please send me instruction for making changes.
I want to try the following for the above use case:
1. I install all software on each nodes. That means Ellis is installed on all nodes, sprout is installed on all nodes, etc.
2. But, in the beginning, Ellis is started on Node1, Bono is started on Node2 and so on as shown above (desired configuration).
3. When Node1  fails, then I start Ellis on Node 2(say manually, I will automate it later). That means Node2 has Ellis and Bono running.
4. When Node2 fails, I start Ellis and Bono on Node2...And the like...
5. In the end, I want Node6 to have all software running.

Scale UP:
1. After step 5 above, where Node6 are running all software.
2. Now, Node 5 has started and came up, I want to stop Dime on Node6 and want to start on Node5. That means Node5 has Dime running and Node6 has Ellis, Bono, Sprout, Homer, and Vellum running.
3. Node Node 4 has come up and running. I want to stop Homer on Node 6 and start it on Node4. That means Node 4 has Homer running, Node5 has Dime running and Node 6 has Ellis, Bono, Sprout and Vellum running.
4. And so on till I get the desired configuration running as shown above.
It should happen when call is running and call shouldn't drop.

Any help will be deeply appreciated.

Thanks
-Nagu

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180424/1c333219/attachment.html>

From spencer.builds.networks at gmail.com  Tue Apr 24 09:55:37 2018
From: spencer.builds.networks at gmail.com (Spencer Sevilla)
Date: Tue, 24 Apr 2018 06:55:37 -0700
Subject: [Project Clearwater] setting up clearwater as ims for lte
 network
In-Reply-To: <CY4PR02MB25180C0FB4CD0554B9D359568FB50@CY4PR02MB2518.namprd02.prod.outlook.com>
References: <CY4PR02MB25180C0FB4CD0554B9D359568FB50@CY4PR02MB2518.namprd02.prod.outlook.com>
Message-ID: <CALRF2+HSR-bYgTHBwa+QZnL1irLRh-0-fED8PE40gwsugk3eag@mail.gmail.com>

Just saw this message, thanks for the reply, Mark! Really appreciate your
taking the time to help me with these pointers. I will get working on all
this and report back soon :-)

On Thu, Apr 19, 2018 at 3:00 AM, Mark Perryman <Mark.Perryman at metaswitch.com
> wrote:

> There are a couple of issues that might be a problem here.
>
>
>
> Firstly, although Clearwater is used as the IMS core in VoLTE networks,
> the Bono node doesn?t support AKA authentication and so probably won?t work
> as a VoLTE P-CSCF.
>
>
>
> Secondly, the phone you are using might be whitelisted to only support
> VoLTE on certain PLMNs.  You should check that the phone is making DNS
> queries and trying to send SIP.  Until the phone is attempting to SIP
> REGISTER with Clearwater, then there is nothing Clearwater can do.
>
>
>
> 1) Looks plausible, and yes, you'll need to change Clearwater
> configuration.
>
>
>
> 2) Yes, you'll either need to own that DNS, or configure OpenAirInterface
> EPC to send the P-CSCF IP address to the UE.
>
>
>
> 3) It's not required to have Clearwater and your EPC talking to the same
> HSS
>
>
>
> Good luck!
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180424/3c9f086f/attachment.html>

From arunlalsingh23 at gmail.com  Tue Apr 24 12:37:25 2018
From: arunlalsingh23 at gmail.com (Arun Lal)
Date: Tue, 24 Apr 2018 22:07:25 +0530
Subject: [Project Clearwater] Regarding Increasing #calls using stress
 testing
In-Reply-To: <CAL1mNUF2AdfjhOZ38Mt0Jr2wZWnp1+mF91RzEFgwiHoRtPa78g@mail.gmail.com>
References: <CAL1mNUF2AdfjhOZ38Mt0Jr2wZWnp1+mF91RzEFgwiHoRtPa78g@mail.gmail.com>
Message-ID: <CAL1mNUFU_MaLbPEiEbwgBLsJEOn4z=n=SiCm-_-hMBANS=w47A@mail.gmail.com>

Hi guys,

it give following result:

*[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress rags.mf 40000 1
--multiplier=12*
Starting initial registration, will take 500 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:01:43
Start: 2018-04-25 05:28:52.781011
End: 2018-04-25 05:30:57.535772

Total calls: 5200
Successful calls:* 5168 (99.3846153846%*)
Failed calls: 32 (0.615384615385%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 10387.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 3 (0.0576923076923%)
# of calls with 2000+ms from INVITE to 180 Ringing: 5167 (99.3653846154%)
Failed: call success rate 99.3846153846% is lower than target 100.0%!

Total re-REGISTERs: 16000
Successful re-REGISTERs: 15559 (97.24375%)
Failed re-REGISTERS: 441 (2.75625%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 5149.0ms
Failed: re-registration success rate 97.24375% is lower than target 100.0%!

Log files at /var/log/clearwater-sip-stress/2679_*



*some time it gives like:*


*[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress iind.intel.com
<http://iind.intel.com> 50000 1 --multiplier=8*
Starting initial registration, will take 625 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
    call_success_rate = 100 * float(row['SuccessfulCall(C)']) /
float(row['TotalCallCreated'])
ZeroDivisionError: float division by zero


I want to make 1 million calls per second (is it possible?) because I have
tried by varying the #subscribers and multiplier and other parameter but it
is not increasing.
Firsy I want it is able to make 20K calls per sec (or 50K or 1lack per
sec). Every node is working fine, I don't know why I am not able to scale
the performance or # calls .

If anything need to change in script run_stress, please let me know where
and what.

your reply will highly appreciate.

Regards,
Arun Lal

On Tue, Apr 24, 2018 at 2:28 PM, Arun Lal <arunlalsingh23 at gmail.com> wrote:

> Hi,
>
> Nice work by clearwater team, I really appreciate your work.
>
> The document of clearwater is self explanatory, most of the task I did by
> reading that, but I have stuck at some points:
>
> > I have 3 sprout, 2 vellum, 2 dime, 1 bono, 1 homer, 1 ellis nodes. I
> want to make 15000 calls/sec or more. I can increase more nodes if possible.
>
> > I am not able to make more calls, I tried
>
> []ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress rags.mf 30000 1
> --multiplier=20
>
> but it gives around 3000 calls.
>
> > what exact command should I use to make more calls per sec. All the node
> are working fine
> I have check that zoiper and other sip client working fine.
>
> > *Can you please share exact command so that I can increase no. of calls
> per sec.*
>
> Or how many more nodes are required, I will add.
>
>
>
> Thanks,
> Arun Lal Singh
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180424/a63c1942/attachment.html>

From Alex.Hockey at metaswitch.com  Tue Apr 24 12:54:32 2018
From: Alex.Hockey at metaswitch.com (Alex Hockey)
Date: Tue, 24 Apr 2018 16:54:32 +0000
Subject: [Project Clearwater] check registered state of user
In-Reply-To: <MWHPR02MB281319D96986648177351FB3F8880@MWHPR02MB2813.namprd02.prod.outlook.com>
References: <MWHPR18MB1151F3267DEF7E707CA11ABCA9BA0@MWHPR18MB1151.namprd18.prod.outlook.com>
	<968533738.3679.1523180662493@mail.yahoo.com>
	<DM5PR18MB11456D3363E772524096E427A9B80@DM5PR18MB1145.namprd18.prod.outlook.com>
	<1820482130.384973.1523195550678@mail.yahoo.com>
	<MWHPR02MB281319D96986648177351FB3F8880@MWHPR02MB2813.namprd02.prod.outlook.com>
Message-ID: <SN1PR0201MB1456975CC4EC5994F85BA3D897880@SN1PR0201MB1456.namprd02.prod.outlook.com>

Thanks for your question Jake, and thanks Nagu for helping out! Apologies for the delay in getting back to you.

There are a few different datastores that clearwater uses:

  *   Ellis stores which numbers are available for use, and which ones have been given out to a user, in its MySQL database.
  *   When a number is given out to a user, it is configured in homestead-prov, which stores it in the Cassandra database.
  *   When a phone registers, this gets stored in the memcached database used by homestead and sprout.


It?s this last bullet that you care about. The easiest way to get the information is to query homestead?s REST API for the subscriber?s registration data. You can do this by using cURL to do a GET to the resource defined here: https://github.com/Metaswitch/homestead/blob/release-130/docs/homestead_api.md#impu---persistent-registration-state. I?ve not tried running this command, but something like this should do the trick:

    curl http://<IP >:8888/impu/<IMPU>/reg-data

<IP> is the IP address of your dime node. Note that 127.0.0.1/localhost will not work (even if you are on the dime node).
<IMPU> is the SIP URI of the subscriber you are interested in. You have to URL escape this. Eg. instead of sip:1234567890 at example.com you would use sip%3A123456789%40example.com

Hope this helps,
Alex.

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Nagendra Kumar
Sent: 08 April 2018 14:53
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; DCC LLC - Jake Brown
Subject: Re: [Project Clearwater] check registered state of user

Hi Jake,
I am not sure about any other ways of checking registration status.

@Experts ??

Thanks
-Nagu


On Sunday, 8 April, 2018, 6:53:10 PM IST, Jake Brown <jake at dccllc.net<mailto:jake at dccllc.net>> wrote:


Nagu,

Would this mean that the only way to check registration status would be to use a Ellis node. Currently we don't use one due to the lacking of browser support to modify the call forwarding features.

Thanks

Jake



Sent via the Samsung Galaxy S8+, an AT&T 4G LTE smartphone


-------- Original message --------
From: Nagendra Kumar <nagen_kr at yahoo.co.in<mailto:nagen_kr at yahoo.co.in>>
Date: 4/8/18 4:44 AM (GMT-06:00)
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>, Jake Brown <jake at dccllc.net<mailto:jake at dccllc.net>>
Subject: Re: [Project Clearwater] check registered state of user

Hi Jake,
You can also check the registered user at ellis :

sudo mysql
use ellis;
SELECT * FROM numbers ;

You can also check the data bases using the command: SHOW DATABASES;
You can check selective users by command: SELECT * FROM numbers WHERE number = 'sip:6505550001 at nags.hd';

Thanks
-Nagu



On Friday, 6 April, 2018, 7:32:54 PM IST, Jake Brown <jake at dccllc.net<mailto:jake at dccllc.net>> wrote:



I must have something wrong in the shared_config. I would expect to see something in vellum in the homestead_cache/impu table that indicates registered in the is_registered column. Is this not the correct place to check to see if a subscriber is registered? Thank you for your guidance.



Jake
_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180424/fbe9d2fe/attachment.html>

From case at gci.com  Tue Apr 24 15:29:08 2018
From: case at gci.com (Roger Case)
Date: Tue, 24 Apr 2018 19:29:08 +0000
Subject: [Project Clearwater] 
 =?utf-8?q?Release_note_for_Sprint_Zam=C3=AE?= =?utf-8?q?n?=
In-Reply-To: <BN6PR05MB3074301874BF4C5590B190A2B9890@BN6PR05MB3074.namprd05.prod.outlook.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
	<BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D9674D4@ex-mbx-prd02.gci.com>,
	<BLUPR0201MB149036A21130C85A1A66829E95B40@BLUPR0201MB1490.namprd02.prod.outlook.com>,
	<9ED8EE39-46CC-4466-8A61-168201C9DDBC@gci.com>
	<1B813795-9351-41F9-9831-B07F54D3AC30@vmware.com>,
	<A7E1158244C52C428F0B61BDC9CF760C016D98E595@ex-mbx-prd02.gci.com>
	<54A349FD-766F-4DFE-B775-56C65C396E83@vmware.com>
	<BN6PR05MB3074301874BF4C5590B190A2B9890@BN6PR05MB3074.namprd05.prod.outlook.com>
Message-ID: <A7E1158244C52C428F0B61BDC9CF760C016D996BD8@ex-mbx-prd02.gci.com>

Hi Frank,

Thank you very much for testing this.  Come to think of it I probably was not using vCenter Server when I initially brought up the All-In-One.  It never occurred to me that that the base processes would be any different using vCenter, and I have deployed many other OVFs since deploying the server.  Very interesting indeed.  Really appreciate your effort!

Best regards,

Roger

From: Frank Escaros-Buechsel [mailto:fescarosbuechs at vmware.com]
Sent: Monday, April 23, 2018 3:09 PM
To: Roger Case
Cc: clearwater at lists.projectclearwater.org
Subject: RE: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
Hi Roger,
After reading the documentation I do not think that a vCenter deploy is extensively tested as only ESXi deploy is mentioned in the clearwater wiki.

I get exactly the same error message as you do when I try to deploy through vCenter, yet I am able to successfully deploy the OVA directly to ESXi.

@Clearwater team:
I modified the OVF file after extraction and removed the offending fields from Roger?s screenshot which also allows me then to deploy successfully through vCenter.
I have copied the new contents of the OVF file to https://pastebin.com/raw/NStkhajn for your review.
When trying to boot the VM it is complaining about not being able to connect to SATA0 controller.
What is the actual intended disk configuration for the AIO image based on vSphere? Which virtual storage controller is the target and how many vdisks should the resulting VM have, as currently the OVA through vCenter as well as through ESXi deploys with a single disk.

Ellis and Bono do seem to come up successfully despite the error I am seeing, which makes me think it?s purely cosmetic but with knowing the correct disk controller type for vSphere which is intended by you I can make further modifications to the OVF to avoid the cosmetic error as well:

[cw-aio]ubuntu at cw-aio:~$ netstat -ano | grep 5060
tcp        0      0 10.27.32.111:5060       0.0.0.0:*               LISTEN      off (0.00/0/0)
udp        0      0 10.27.32.111:5060       0.0.0.0:*                           off (0.00/0/0)

Kind Regards,
Frank Escaros-Buechsel
Consulting Architect NFV
fescarosbuechs at vmware.com<mailto:fescarosbuechs at vmware.com>
Parnell  House, Barrack Square, Ballincollig, Co. Cork, P31 PF68, IRL
Twitter: @fbuechsel

Advanced notice of absence:


[VMwareLogo]

From: Frank Escaros-Buechsel
Sent: Friday 20 April 2018 22:03
To: Roger Case <case at gci.com>
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Hi Roger,
Looks like a malformed ofv file for the vsphere build you are using, I can try to reproduce in my lab over the weekend.
Frank Escaros-Buechsel
Consulting Architect NFV
Parnell House, Barrack Square, Ballincollig, Co. Cork
P31 PF68, IRL
Twitter: @fbuechsel

On 20 Apr 2018, at 22:58, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:
Hi Frank,


  1.  Where would I get the checksum to compare with the one I derived from the downloaded file?
  2.  Unzip OK.
  3.  Checksums of .ovf and .vmdk match those in the .mf file
  4.  Same error when selecting all three unzipped files to deploy ovf.
  5.  Removing the .mf file and attempting to deploy the template using just the .ovf and .vmdk results in different errors:


<image001.png>

Thanks,

Roger

From: Frank Escaros-Buechsel [mailto:fescarosbuechs at vmware.com]
Sent: Friday, April 20, 2018 9:12 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Roger Case
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
1)Verify the checksum to ensure you don?t have a corrupt download
2)unzip the ova into a folder, you will get an ovf, vmdk and mf file
3)run sha1 sum on all files separately, compare output against the values in mf file, if one deviates update the value in mf to correct one
4)do not zip again, deploy through web client and select ovf and all vmdk files

This should let you deploy.

Another way would be to follow step 1 and 2 and simply delete the mf file, as this will skip the checksum check (not recommended for environments which need to proof authenticity of uploaded files though)

Hope that helps.

Frank


Sent from VMware Boxer
On 20 April 2018 at 19:27:24 GMT+3, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:
Hi Will,

No, also 6.0.
Previous AIO, however.
Other Metaswitch .OVA still work....

Sent from my GCiPhone

On Apr 20, 2018, at 01:49, William Yates <William.Yates at metaswitch.com<mailto:William.Yates at metaswitch.com>> wrote:
[External Email]

Hi Roger,



When this worked before, was it on a previous version of vSphere?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 19 April 2018 20:15
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Hi Will,



vSphere 6.0, vCenter Server 6.5.



Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of William Yates
Sent: Thursday, April 19, 2018 2:58 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi Roger,



Please could you tell me what version of vSphere you are using?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 18 April 2018 21:59
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?



Am I missing something here?  This worked before.  Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=2qwlj9Q3_bQcSmxUEMuhYsYgR9cLAYq8Yow_4Awb6sg&e=>)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.



This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.



We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.





This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.



To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__docs.projectclearwater.org_en_stable_Upgrading-5Fa-5FClearwater-5Fdeployment.html&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=qcTPSTk5rbrrb9tKuc7vPepjpenEiPPEJ2lTtsmp9Es&e=>.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova<https://urldefense.proofpoint.com/v2/url?u=http-3A__vm-2Dimages.cw-2Dngv.com_cw-2Daio.ova&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=PUMku_aSKgNoiXkqZ8ant31BoZ-iIM3wJCW24TKelsU&e=>) has been updated for this release.





Cheers,



Richard


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.projectclearwater.org_mailman_listinfo_clearwater-5Flists.projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=Z5v_f1O7HlUlYiseOcH3vO5bpWsuXKfrYT-IaaL4saE&e=>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180424/d98874be/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.gif
Type: image/gif
Size: 4572 bytes
Desc: image001.gif
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180424/d98874be/attachment.gif>

From skgola1997 at gmail.com  Wed Apr 25 00:44:15 2018
From: skgola1997 at gmail.com (Sunil Kumar)
Date: Wed, 25 Apr 2018 10:14:15 +0530
Subject: [Project Clearwater] SIP/2.0 403 Forbidden
Message-ID: <CAHwYWpDLiLm5cW1asUSsA6dM9B_U-pOVQChnSJF2sXNqdrqZJw@mail.gmail.com>

Hi all,
Is any one have any idea why I am getting this error in stress testing.
Initial registration is also failed:

*[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress imsunil.org
<http://imsunil.org> 50 2 --icscf-target 192.168.56.112:5052
<http://192.168.56.112:5052> --scscf-target 192.168.56.112:5054
<http://192.168.56.112:5054>*
Starting initial registration, will take 0 seconds
Initial registration failed - see
/var/log/clearwater-sip-stress/26377_initial_reg_errors.log for details of
the errors


*$cat /var/log/clearwater-sip-stress/26377_initial_reg_errors.log*
2018-04-25      17:26:11.498523 1524657371.498523: Aborting call on
unexpected message for Call-Id '48-25046 at 192.168.56.116': while expecting
'401' (index 1), received 'SIP/2.0 403 Forbidden
Via: SIP/2.0/TCP 192.168.56.116:35760
;received=192.168.56.116;branch=z9hG4bK-25046-48-0
Call-ID: 48-25046 at 192.168.56.116
From: <sip:2010000036 at imsunil.org>;tag=25046SIPpTag0048
To: <sip:2010000036 at imsunil.org
>;tag=z9hG4bKPjtI4ClsjDofR5gP4xyDBk9C3kW49Sbbch
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f48"
Content-Length:  0

*Part of sprout log:*

*cat sprout_current.txt*
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP
192.168.56.112;rport=41096;received=192.168.56.112;branch=z9hG4bK-319664
Call-ID: poll-sip-319664
From: "poll-sip" <sip:poll-sip at 192.168.56.112>;tag=319664
To: <sip:poll-sip at 192.168.56.112>;tag=z9hG4bK-319664
CSeq: 319664 OPTIONS
Content-Length:  0


--end msg--
25-04-2018 12:39:28.573 UTC [7f41f6fd5700] Debug
common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
25-04-2018 12:39:28.573 UTC [7f41f6fd5700] Debug pjsip: tdta0x7f421c01
Destroying txdata Response msg 200/OPTIONS/cseq=319664 (tdta0x7f421c017980)
25-04-2018 12:39:28.573 UTC [7f41f6fd5700] Debug thread_dispatcher.cpp:273:
Worker thread completed processing message 0x7f40fc051208
25-04-2018 12:39:28.573 UTC [7f41f6fd5700] Debug thread_dispatcher.cpp:287:
Request latency = 269us
25-04-2018 12:39:28.573 UTC [7f41f6fd5700] Debug
event_statistic_accumulator.cpp:32: Accumulate 269 for 0x21f53d8
25-04-2018 12:39:28.573 UTC [7f41f6fd5700] Debug
event_statistic_accumulator.cpp:32: Accumulate 269 for 0x21f5450
25-04-2018 12:39:28.573 UTC [7f41f6fd5700] Debug load_monitor.cpp:341: Not
recalculating rate as we haven't processed 20 requests yet (only 15).
25-04-2018 12:39:28.573 UTC [7f41f6fd5700] Debug utils.cpp:878: Removed
IOHook 0x7f41f6fd4df0 to stack. There are now 0 hooks
25-04-2018 12:39:28.573 UTC [7f41f6fd5700] Debug thread_dispatcher.cpp:161:
Attempting to process queue element
25-04-2018 12:39:28.573 UTC [7f4102ded700] Verbose httpstack.cpp:308:
Process request for URL /ping, args (null)
25-04-2018 12:39:28.573 UTC [7f4102ded700] Verbose httpstack.cpp:68:
Sending response 200 to request for URL /ping, args (null)
25-04-2018 12:39:30.574 UTC [7f41035ee700] Verbose pjsip: tcps0x7f40fc02
TCP connection closed
25-04-2018 12:39:30.574 UTC [7f41035ee700] Debug connection_tracker.cpp:67:
Connection 0x7f40fc028928 has been destroyed
25-04-2018 12:39:30.574 UTC [7f41035ee700] Verbose pjsip: tcps0x7f40fc02
TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
25-04-2018 12:39:30.585 UTC [7f41035ee700] Verbose pjsip:    tcplis:5053
TCP listener 192.168.56.112:5053: got incoming TCP connection from
192.168.56.112:38808, sock=747
25-04-2018 12:39:30.585 UTC [7f41035ee700] Verbose pjsip: tcps0x7f40fc02
tcp->base.local_name: 192.168.56.112
25-04-2018 12:39:30.585 UTC [7f41035ee700] Verbose pjsip: tcps0x7f40fc02
TCP server transport created
25-04-2018 12:39:30.585 UTC [7f41035ee700] Debug pjsip: sip_endpoint.c
Processing incoming message: Request msg OPTIONS/cseq=319666
(rdata0x7f40fc028c60)
25-04-2018 12:39:30.585 UTC [7f41035ee700] Verbose
common_sip_processing.cpp:87: RX 361 bytes Request msg OPTIONS/cseq=319666
(rdata0x7f40fc028c60) from TCP 192.168.56.112:38808:
--start msg--

OPTIONS sip:poll-sip at 192.168.56.112:5053 SIP/2.0
Via: SIP/2.0/TCP 192.168.56.112;rport;branch=z9hG4bK-319666
Max-Forwards: 2
To: <sip:poll-sip at 192.168.56.112:5053>
From: poll-sip <sip:poll-sip at 192.168.56.112>;tag=319666
Call-ID: poll-sip-319666
CSeq: 319666 OPTIONS
Contact: <sip:192.168.56.112>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
25-04-2018 12:39:30.586 UTC [7f41035ee700] Debug uri_classifier.cpp:139:
home domain: false, local_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
25-04-2018 12:39:30.586 UTC [7f41035ee700] Debug uri_classifier.cpp:173:
Classified URI sip:poll-sip at 192.168.56.112:5053 as 3
25-04-2018 12:39:30.586 UTC [7f41035ee700] Debug
common_sip_processing.cpp:180: Skipping SAS logging for OPTIONS request
25-04-2018 12:39:30.586 UTC [7f41035ee700] Debug thread_dispatcher.cpp:568:
Received message 0x7f40fc028c60
25-04-2018 12:39:30.586 UTC [7f41035ee700] Debug thread_dispatcher.cpp:585:
Admitted request 0x7f40fc028c60
25-04-2018 12:39:30.586 UTC [7f41035ee700] Debug thread_dispatcher.cpp:620:
Incoming message 0x7f40fc028c60 cloned to 0x7f40fc051208
25-04-2018 12:39:30.586 UTC [7f41035ee700] Debug thread_dispatcher.cpp:639:
Queuing cloned received message 0x7f40fc051208 for worker threads with
priority 15
25-04-2018 12:39:30.586 UTC [7f41035ee700] Debug
event_statistic_accumulator.cpp:32: Accumulate 0 for 0x21f9338
25-04-2018 12:39:30.586 UTC [7f41035ee700] Debug
event_statistic_accumulator.cpp:32: Accumulate 0 for 0x21f93e0
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug utils.cpp:872: Added
IOHook 0x7f41f7fd6df0 to stack. There are now 1 hooks
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug thread_dispatcher.cpp:181:
Worker thread dequeue message 0x7f40fc051208
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug thread_dispatcher.cpp:186:
Request latency so far = 140us
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug pjsip: sip_endpoint.c
Distributing rdata to modules: Request msg OPTIONS/cseq=319666
(rdata0x7f40fc051208)
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug uri_classifier.cpp:139:
home domain: false, local_to_node: true, is_gruu: false,
enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug uri_classifier.cpp:173:
Classified URI sip:poll-sip at 192.168.56.112:5053 as 3
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug pjsip:       endpoint
Response msg 200/OPTIONS/cseq=319666 (tdta0x322f910) created
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Verbose
common_sip_processing.cpp:103: TX 290 bytes Response msg
200/OPTIONS/cseq=319666 (tdta0x322f910) to TCP 192.168.56.112:38808:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP
192.168.56.112;rport=38808;received=192.168.56.112;branch=z9hG4bK-319666
Call-ID: poll-sip-319666
From: "poll-sip" <sip:poll-sip at 192.168.56.112>;tag=319666
To: <sip:poll-sip at 192.168.56.112>;tag=z9hG4bK-319666
CSeq: 319666 OPTIONS
Content-Length:  0


--end msg--
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug
common_sip_processing.cpp:275: Skipping SAS logging for OPTIONS response
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug pjsip:  tdta0x322f910
Destroying txdata Response msg 200/OPTIONS/cseq=319666 (tdta0x322f910)
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug thread_dispatcher.cpp:273:
Worker thread completed processing message 0x7f40fc051208
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug thread_dispatcher.cpp:287:
Request latency = 432us
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug
event_statistic_accumulator.cpp:32: Accumulate 432 for 0x21f53a8
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug
event_statistic_accumulator.cpp:32: Accumulate 432 for 0x21f5450
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug load_monitor.cpp:341: Not
recalculating rate as we haven't processed 20 requests yet (only 16).
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug utils.cpp:878: Removed
IOHook 0x7f41f7fd6df0 to stack. There are now 0 hooks
25-04-2018 12:39:30.586 UTC [7f41f7fd7700] Debug thread_dispatcher.cpp:161:
Attempting to process queue element


thanks,
sunil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180425/34b3319e/attachment.html>

From sathiyan.sivathas at projectclearwater.org  Wed Apr 25 11:38:23 2018
From: sathiyan.sivathas at projectclearwater.org (Sathiyan (projectclearwater.org))
Date: Wed, 25 Apr 2018 15:38:23 +0000
Subject: [Project Clearwater] [clearwater] Some calls failed with 480
 when do SIPp testing
Message-ID: <MWHPR02MB2814DB42EA7838708FAF4AA8868F0@MWHPR02MB2814.namprd02.prod.outlook.com>

Hi Linda,


  1.  You will need to restart the sprout process for it to pick up the change to reg_max_expires. You can do this using the command ?sudo service sprout restart?. In future when changing the shared_config file, I?d recommend that you use the cw-config plugin as documented here<http://clearwater.readthedocs.io/en/latest/Modifying_Clearwater_settings.html?highlight=cw_config#modifying-settings-in-etc-clearwater-shared-config>. This automatically restarts services for you and shares changes with other nodes in your deployment.

Increasing reg_max_expires doesn?t increase the call speed, it increases the maximum registration expiry to 30 minutes. Rob recommended that you do this so that your calls would still succeed without the multiplier set to 10. With this change the 480 errors you saw should be fixed. If you want to increase the call rate you need to use the multiplier option.


  1.  You?re correct that the 32500 is from the whole 30 minutes. You could use the --sipp-output option to show the SIPp output screen but otherwise the run_stress script doesn?t allow you to see how many calls are currently online. If you want more detailed statistics, I suggest you look at our SNMP statistics<http://clearwater.readthedocs.io/en/latest/Clearwater_SNMP_Statistics.html>.

Yes, you can set the --call-length option to 600 to make all calls last 10 minutes.
Hope that helps,

Sathiyan

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Wangwulin (Linda)
Sent: 08 April 2018 05:17
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] [clearwater] Some calls failed with 480 when do SIPp testing

Hi Rob,

Thanks for explaining ?multiplier? to me.
1. I have set reg_max_expires as 1800 in shared_config file on stress node and sprout node (Do I need to config the param on other nodes too?), but it did not increase the call speed as ?multiplier? did. Do I need to reboot the nodes or restart the services?  I am deploying a fresh Clearwater now, need to check that later.

2. Regarding to my previous result during 30 mins with 10000 subscribers:
Total calls: 32500
Successful calls: 32358 (99.5630769231%)
Failed calls: 142 (0.436923076923%)

It seems the totals calls 32500 is an accumulated value during the 30 mins, right?  But how could I get the result of calls which are online simultaneously during a period? Or how could I make all the successful calls online for 10 mins maybe without BYE. One parameter named ?call-length? in run_stress script, does it refer to the time duration the successful call can keep for?


Thanks,
Linda

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Robert Day
Sent: 2018?4?4? 0:20
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] [clearwater] Some calls failed with 480 when do SIPp testing

Hi Linda,

The ?multiplier? parameter increases the rate of activity for each subscriber ? if each subscriber normally makes 0.65 calls per hour and re-registers every 30 minutes, --multiplier 10 will mean they make 6.5 calls per hour and re-register every 3 minutes. This is effectively a way to generate more load with fewer subscribers (decreasing the amount of provisioning time needed).

If --multiplier 10 is making your calls work, it may be that you haven?t configured Clearwater?s registration expiry time correctly ? as http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html#deploying-a-stress-node says, ?You should also ensure that the reg_max_expires setting is set to 1800 rather than the default of 300?. If you haven?t done that, then registrations will time out too early, causing 480 Temporarily Unavailable errors when we try and make calls to subscribers with expired registrations. Increasing the multiplier means subscribers re-register more often, so happens to avoid this problem.

?ccf? is an IMS Charging Collection Function ? the network element that Ralf reports billing events to, as described in http://www.projectclearwater.org/rf-billing/. If you don?t have one, you can ignore that parameter.

Hope that helps!

Best,
Rob

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Wangwulin (Linda)
Sent: 28 March 2018 11:03
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] [clearwater] Some calls failed with 480 when do SIPp testing

Hi,

When I set an additional param ?--multiplier 10? or other numbers (not the default 1), all the calls succeeded.

/usr/share/clearwater/bin/run_stress clearwater.opnfv 1000 10 --sipp-output --icscf-target 10.67.79.12:5052 --scscf-target 10.67.79.12:5054 --base-number 2010000000 --multiplier 10  --ccf 10.67.79.11

Total calls: 1083
Successful calls: 1083 (100.0%)
Failed calls: 0 (0.0%)
Unfinished calls: 0

Two questions:
1) What is ?multiplier? used for?
2) What does ?ccf? refer to? Actually the ip of ccf here is where the ralf service runs on my deployment. Is it necessary here?


Thanks,
Linda
________________________________
???: wang wulin <wangwulin at hotmail.com<mailto:wangwulin at hotmail.com>>
????: 2018?3?28? 14:40
???: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
??: [clearwater] Some calls failed with 480 when do SIPp testing


Hi clearwater team,



The successful call rate is quite low when I do SIPp testing via the command: /usr/share/clearwater/bin/run_stress clearwater.opnfv 100 30 --sipp-output --icscf-target sprout.clearwater.local:5052 --scscf-target sprout.clearwater.local:5054

Actually after 7 successful calls, the 480 error occurs.

Here is one capture when runing:

------------------------------ Scenario Screen -------- [1-9]: Change Screen --
  Call-rate(length)   Port   Total-time  Total-calls  Remote-host
0.0(5000 ms)/1.000s   5062    1234.12 s           22  10.67.79.12:5054(TCP)

  0 new calls during 1.004 s period      1 ms scheduler resolution
  0 calls (limit 1)                      Peak was 1 calls, after 55 s
  1 Running, 3 Paused, 3 Woken up
  0 dead call msg (discarded)            0 out-of-call msg (discarded)
  2 open sockets

                                 Messages  Retrans   Timeout   Unexpected-Msg
      INVITE ---------->         22        0         0
         100 <----------         22        0         0         0
         183 <----------         12        0         0         10
       PRACK ---------->         12        0
         200 <----------         12        0         0         0
      UPDATE ---------->         12        0
         201 <----------         12        0         0         0
         180 <----------  E-RTD1 12        0         0         0
         201 <----------         0         0         0         0
         200 <----------         12        0         0         0
         ACK ---------->         12        0
       Pause [   5000ms]         12                            0
         BYE ---------->         12        0         0
         200 <----------         12        0         0         0

------ [+|-|*|/]: Adjust rate ---- [q]: Soft exit ---- [p]: Pause traffic -----

Last Error: Aborting call on unexpected message for Call-Id '21-14814 at 10...


2018-03-28      06:33:54.190938 1522218834.190938: Aborting call on unexpected message for Call-Id '17-14814 at 10.67.79.24': while expecting '183' (index 2), received 'SIP/2.0 480 Temporarily Unavailable
Via: SIP/2.0/TCP 10.67.79.24:29014;received=10.67.79.24;branch=z9hG4bK-14814-17-0
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-term>
Record-Route: <sip:scscf.sprout.clearwater.local:5054;transport=TCP;lr;billing-role=charge-orig>
Call-ID: 17-14814 at 10.67.79.24<mailto:17-14814 at 10.67.79.24>
From: <sip:2010000012 at clearwater.opnfv>;tag=14814SIPpTag0017
To: <sip:2010000015 at clearwater.opnfv>;tag=z9hG4bK-14814-17-0
CSeq: 1 INVITE
P-Charging-Vector: icid-value="14814SIPpTag0017";orig-ioi=clearwater.opnfv;term-ioi=clearwater.opnfv
P-Charging-Function-Addresses: ccf=0.0.0.0
Content-Length:  0

'.




1) I deoloyed clearwater via one testcase named "cloudify_ims" from opnfv/functest project, where 3 steps are run:

   * deploy a VNF orchestrator (Cloudify)

   * deploy a Clearwater vIMS (IP Multimedia Subsystem) VNF from this orchestrator based on a TOSCA blueprint defined in [1]
   * run suite of signaling tests on top of this VNF

[1]: https://github.com/Orange-OpenSource/opnfv-cloudify-clearwater/archive/master.zip



8 instances are created and I also created a new instance named "stress-node" according to this guidance: http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html-
bash-4.4# openstack server list
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+
| ID                                   | Name                                                  | Status | Networks                                                                              | Image                | Flavor    |
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+
| df2d9f82-8aa2-4514-9f07-27974267b590 | stress-node                                           | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.24                 | ubuntu_14.04         | m1.small  |
| 48a41da3-c55d-410c-a8f3-ef26bc409aef | server_clearwater-opnfv_bono_host_llp4mt              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.23, 192.168.36.113 | ubuntu_14.04         | m1.small  |
| a992435c-a397-4622-b0d3-b8e515ebab51 | server_clearwater-opnfv_homer_host_vz97vi             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.19                 | ubuntu_14.04         | m1.small  |
| 283fef46-78e4-4d0a-9e91-ffeb14e7bfb9 | server_clearwater-opnfv_sprout_host_nnoxye            | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.12                 | ubuntu_14.04         | m1.small  |
| bfe5111d-ed7d-4c65-80c2-20144935ee8c | server_clearwater-opnfv_ellis_host_bo7auh             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.14, 192.168.36.111 | ubuntu_14.04         | m1.small  |
| b83e1727-b5a6-430e-8b5e-b3f0e6421675 | server_clearwater-opnfv_vellum_host_sh0ocy            | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.6                  | ubuntu_14.04         | m1.small  |
| 8b03d8fa-5873-489b-994b-5f1de24a85c0 | server_clearwater-opnfv_bind_host_b9f49w              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.16, 192.168.36.110 | ubuntu_14.04         | m1.small  |
| 0ca72f55-7e05-4477-ad2e-2661ad49f7ce | server_clearwater-opnfv_dime_host_xryxen              | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.11                 | ubuntu_14.04         | m1.small  |
| 7d51e479-24b6-4db4-9b8f-3ef42be0b87e | server_clearwater-opnfv_proxy_host_rqr7bo             | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.5                  | ubuntu_14.04         | m1.small  |
| 82672984-b967-49ff-8f00-09ffe3f7fc87 | cloudify_manager-6c79129a-8384-4069-81f7-a024738102cd | ACTIVE | cloudify_ims_network-6c79129a-8384-4069-81f7-a024738102cd=10.67.79.13, 192.168.36.105 | cloudify_manager_4.0 | m1.medium |
+--------------------------------------+-------------------------------------------------------+--------+---------------------------------------------------------------------------------------+----------------------+-----------+


2) I checked  the ralf log
root at dime-au6gte:/var/log/ralf# vim ralf_20180328T060000Z.txt
28-03-2018 06:25:57.063 UTC Debug dnscachedresolver.cpp:543: Create and execute DNS query transaction
28-03-2018 06:25:57.063 UTC Debug dnscachedresolver.cpp:556: Wait for query responses
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:704: Received DNS response for _diameter._tcp.clearwater.opnfv type SRV
28-03-2018 06:25:57.064 UTC Warning dnscachedresolver.cpp:846: Failed to retrieve record for _diameter._tcp.clearwater.opnfv: Domain name not found
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:946: Adding _diameter._tcp.clearwater.opnfv to cache expiry list with deletion time of 1522218957
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:704: Received DNS response for _diameter._sctp.clearwater.opnfv type SRV
28-03-2018 06:25:57.064 UTC Warning dnscachedresolver.cpp:846: Failed to retrieve record for _diameter._sctp.clearwater.opnfv: Domain name not found
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:946: Adding _diameter._sctp.clearwater.opnfv to cache expiry list with deletion time of 1522218957
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:560: Received all query responses
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:588: Pulling 0 records from cache for _diameter._tcp.clearwater.opnfv SRV
28-03-2018 06:25:57.064 UTC Debug dnscachedresolver.cpp:588: Pulling 0 records from cache for _diameter._sctp.clearwater.opnfv SRV
28-03-2018 06:25:57.064 UTC Debug diameterresolver.cpp:131: TCP SRV record _diameter._tcp.clearwater.opnfv returned 0 records
28-03-2018 06:25:57.064 UTC Debug diameterresolver.cpp:134: SCTP SRV record _diameter._sctp.clearwater.opnfv returned 0 records
28-03-2018 06:25:57.064 UTC Error diameterstack.cpp:864: No Diameter peers have been found
28-03-2018 06:25:57.068 UTC Verbose httpstack.cpp:345: Process request for URL /ping, args (null)
28-03-2018 06:25:57.068 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /ping, args (null)
28-03-2018 06:25:58.027 UTC Verbose httpstack.cpp:345: Process request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:128: Handling request, body:
{
    "peers": {
        "ccf": [
            ""
        ]
    },
    "event": {
        "Accounting-Record-Type": 1,
        "Event-Timestamp": 1522218357,
        "Service-Information": {
            "Subscription-Id": [
                {
                    "Subscription-Id-Type": 2,
                    "Subscription-Id-Data": "sip:2010000020 at clearwater.opnfv"
                }
            ],
            "IMS-Information": {
                "Event-Type": {
                    "SIP-Method": "REGISTER",
                    "Expires": 3600
                },
                "Role-Of-Node": 0,
                "Node-Functionality": 1,
                "User-Session-Id": "2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24>",
                "Called-Party-Address": "sip:2010000020 at clearwater.opnfv",
                "Associated-URI": [
                    "sip:2010000020 at clearwater.opnfv"
                ],
                "Time-Stamps": {
                    "SIP-Request-Timestamp": 1522218357,
                    "SIP-Request-Timestamp-Fraction": 960,
                    "SIP-Response-Timestamp": 1522218357,
                    "SIP-Response-Timestamp-Fraction": 967
                },
                "IMS-Charging-Identifier": "",
                "Cause-Code": -1,
                "From-Address": "<sip:2010000020 at clearwater.opnfv>;tag=8708SIPpTag0014990",
                "Route-Header-Received": "<sip:clearwater.opnfv;transport=TCP;lr>",
                "Route-Header-Transmitted": "<sip:icscf.sprout.clearwater.local:5052;transport=TCP;lr;orig>",
                "Instance-Id": "<urn:uuid:00000000-0000-0000-0000-000000000001>"
            }
        }
    }
}
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:244: Adding CCF
28-03-2018 06:25:58.027 UTC Debug handlers.cpp:82: Handle the received message
28-03-2018 06:25:58.027 UTC Debug peer_message_sender.cpp:84: Sending message to  (number 0)
28-03-2018 06:25:58.027 UTC Debug rf.cpp:63: Building an Accounting-Request
28-03-2018 06:25:58.027 UTC Debug freeDiameter: No Session-Id AVP found in message 0x7f764c008d50
28-03-2018 06:25:58.027 UTC Verbose diameterstack.cpp:1470: Sending Diameter message of type 271 on transaction 0x7f764c008d00
28-03-2018 06:25:58.027 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.027 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
28-03-2018 06:25:58.027 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm clearwater.opnfv
28-03-2018 06:25:58.027 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
28-03-2018 06:25:58.027 UTC Debug freeDiameter: Calling callback registered when query was sent (0x43cb30, 0x7f764c008d00)
28-03-2018 06:25:58.027 UTC Verbose diameterstack.cpp:1130: Got Diameter response of type 271 - calling callback on transaction 0x7f764c008d00
28-03-2018 06:25:58.027 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
28-03-2018 06:25:58.027 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
28-03-2018 06:25:58.027 UTC Warning session_manager.cpp:414: Session for 2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24> received error from CDF
28-03-2018 06:25:58.032 UTC Verbose httpstack.cpp:345: Process request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:128: Handling request, body:
{
    "peers": {
        "ccf": [
            ""
        ]
    },
    "event": {
        "Accounting-Record-Type": 1,
        "Event-Timestamp": 1522218357,
        "Service-Information": {
            "Subscription-Id": [
                {
                    "Subscription-Id-Type": 2,
                    "Subscription-Id-Data": "sip:2010000021 at clearwater.opnfv"
                }
            ],
            "IMS-Information": {
                "Event-Type": {
                    "SIP-Method": "REGISTER",
                    "Expires": 3600
                },
                "Role-Of-Node": 0,
                "Node-Functionality": 1,
                "User-Session-Id": "2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24>",
                "Called-Party-Address": "sip:2010000021 at clearwater.opnfv",
                "Associated-URI": [
                    "sip:2010000021 at clearwater.opnfv"
                ],
                "Time-Stamps": {
                    "SIP-Request-Timestamp": 1522218357,
                    "SIP-Request-Timestamp-Fraction": 968,
                    "SIP-Response-Timestamp": 1522218357,
                    "SIP-Response-Timestamp-Fraction": 974
                },
                "IMS-Charging-Identifier": "",
                "Cause-Code": -1,
                "From-Address": "<sip:2010000021 at clearwater.opnfv>;tag=8708SIPpTag0014990",
                "Route-Header-Received": "<sip:clearwater.opnfv;transport=TCP;lr>",
                "Route-Header-Transmitted": "<sip:icscf.sprout.clearwater.local:5052;transport=TCP;lr;orig>",
                "Instance-Id": "<urn:uuid:00000000-0000-0000-0000-000000000001>"
            }
        }
    }
}
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:244: Adding CCF
28-03-2018 06:25:58.032 UTC Debug handlers.cpp:82: Handle the received message
28-03-2018 06:25:58.032 UTC Debug peer_message_sender.cpp:84: Sending message to  (number 0)
28-03-2018 06:25:58.032 UTC Debug rf.cpp:63: Building an Accounting-Request
28-03-2018 06:25:58.032 UTC Debug freeDiameter: No Session-Id AVP found in message 0x7f764c001310
28-03-2018 06:25:58.032 UTC Verbose diameterstack.cpp:1470: Sending Diameter message of type 271 on transaction 0x7f764c006f30
28-03-2018 06:25:58.032 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /call-id/2010000020%2F%2F%2F14990-8708%4010.67.79.24, args (null)
28-03-2018 06:25:58.032 UTC Debug diameterstack.cpp:397: Routing out callback from freeDiameter
28-03-2018 06:25:58.032 UTC Error diameterstack.cpp:293: Routing error: 'No remaining suitable candidate to route the message to' for message with Command-Code 271, Destination-Host  and Destination-Realm clearwater.opnfv
28-03-2018 06:25:58.032 UTC Debug freeDiameter: Iterating on rules of COMMAND: '(generic error format)'.
28-03-2018 06:25:58.032 UTC Debug freeDiameter: Calling callback registered when query was sent (0x43cb30, 0x7f764c006f30)
28-03-2018 06:25:58.032 UTC Verbose diameterstack.cpp:1130: Got Diameter response of type 271 - calling callback on transaction 0x7f764c006f30
28-03-2018 06:25:58.032 UTC Warning peer_message_sender.cpp:125: Failed to send ACR to  (number 0)
28-03-2018 06:25:58.032 UTC Error peer_message_sender.cpp:145: Failed to connect to all CCFs, message not sent
28-03-2018 06:25:58.032 UTC Warning session_manager.cpp:414: Session for 2010000020///14990-8708 at 10.67.79.24<mailto:2010000020///14990-8708 at 10.67.79.24> received error from CDF

Have I missed something in shared_config?

root at dime-au6gte:/var/log/ralf# cat /etc/clearwater/shared_config
# Deployment definitions
home_domain=clearwater.opnfv
sprout_hostname=sprout.clearwater.local
chronos_hostname=10.67.79.16:7253
hs_hostname=hs.clearwater.local:8888
hs_provisioning_hostname=hs-prov.clearwater.local:8889
sprout_impi_store=vellum.clearwater.local
sprout_registration_store=vellum.clearwater.local
cassandra_hostname=vellum.clearwater.local
chronos_hostname=vellum.clearwater.local
ralf_session_store=vellum.clearwater.local
ralf_hostname=ralf.clearwater.local:10888
xdms_hostname=homer.clearwater.local:7888
signaling_dns_server=10.67.79.16
snmp_ip=10.67.79.11
reg_max_expires=1800
bgcf=0



# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org<mailto:email_recovery_sender=clearwater at example.org>

# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


root at dime-au6gte:~# cat /etc/clearwater/local_config
local_ip=10.67.79.11
public_ip=
public_hostname=dime-au6gte.clearwater.local



homestead seems fine except one line "Failed TWO read for get_row. Try ONE":

root at dime-au6gte:/var/log/homestead# vim homestead_20180328T060000Z.txt
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:425: Attempt to parse vellum.clearwater.local as IP address
28-03-2018 06:30:58.045 UTC Verbose dnscachedresolver.cpp:486: Check cache for vellum.clearwater.local type 1
28-03-2018 06:30:58.045 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for vellum.clearwater.local A
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:231: Request for connection to IP: 10.67.79.6, port: 9160
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:244: Found existing connection 0x7fed8c0115a0 in pool
28-03-2018 06:30:58.045 UTC Debug cassandra_store.cpp:159: Generated Cassandra timestamp 1522218658045331
28-03-2018 06:30:58.045 UTC Debug cache.cpp:350: Issuing get for key sip:2010000021 at clearwater.opnfv
28-03-2018 06:30:58.045 UTC Debug cassandra_store.cpp:731: Failed TWO read for get_row. Try ONE
28-03-2018 06:30:58.045 UTC Debug cache.cpp:370: Retrieved XML column with TTL 0 and value <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>>
28-03-2018 06:30:58.045 UTC Debug cache.cpp:388: Retrieved is_registered column with value False and TTL 0
28-03-2018 06:30:58.045 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.045 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1245: Got IMS subscription from cache
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1260: TTL for this database record is 0, IMS Subscription XML is not empty, registration state is UNREGISTERED, and the charging addresses are empty
28-03-2018 06:30:58.045 UTC Debug handlers.cpp:1510: Rejecting deregistration for user who was not registered
28-03-2018 06:30:58.045 UTC Verbose httpstack.cpp:93: Sending response 400 to request for URL /impu/sip%3A2010000021%40clearwater.opnfv/reg-data, args (null)
28-03-2018 06:30:58.046 UTC Debug cache.cpp:370: Retrieved XML column with TTL 0 and value <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>>
28-03-2018 06:30:58.046 UTC Debug cache.cpp:388: Retrieved is_registered column with value False and TTL 0
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1245: Got IMS subscription from cache
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1260: TTL for this database record is 0, IMS Subscription XML is not empty, registration state is UNREGISTERED, and the charging addresses are empty
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1286: Subscriber registering with new binding
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1457: Handling initial registration
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1654: Attempting to cache IMS subscription for public IDs
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1658: Got public IDs to cache against - doing it
28-03-2018 06:30:58.046 UTC Debug handlers.cpp:1663: Public ID sip:2010000021 at clearwater.opnfv
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:159: Generated Cassandra timestamp 1522218658046440
28-03-2018 06:30:58.046 UTC Debug a_record_resolver.cpp:80: ARecordResolver::resolve_iter for host vellum.clearwater.local, port 9160, family 2
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:425: Attempt to parse vellum.clearwater.local as IP address
28-03-2018 06:30:58.046 UTC Verbose dnscachedresolver.cpp:486: Check cache for vellum.clearwater.local type 1
28-03-2018 06:30:58.046 UTC Debug dnscachedresolver.cpp:588: Pulling 1 records from cache for vellum.clearwater.local A
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:366: Found 1 A/AAAA records, creating iterator
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:819: 10.67.79.6:9160 transport 6 has state: WHITE
28-03-2018 06:30:58.046 UTC Debug baseresolver.cpp:1004: Added a whitelisted server, now have 1 of 1
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:231: Request for connection to IP: 10.67.79.6, port: 9160
28-03-2018 06:30:58.046 UTC Debug connection_pool.h:244: Found existing connection 0x7fed8c0115a0 in pool
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:612: Constructing cassandra put request with timestamp 1522218658046440 and per-column TTLs
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:629:   ims_subscription_xml => <?xml version='1.0' encoding='UTF-8'?><IMSSubscription xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="CxDataType.xsd"><PrivateID>2010000021 at clearwater.opnfv</PrivateID><ServiceProfile><InitialFilterCriteria><TriggerPoint><ConditionTypeCNF>0</ConditionTypeCNF><SPT><ConditionNegated>0</ConditionNegated><Group>0</Group><Method>INVITE</Method><Extension<mailto:2010000021 at clearwater.opnfv%3c/PrivateID%3e%3cServiceProfile%3e%3cInitialFilterCriteria%3e%3cTriggerPoint%3e%3cConditionTypeCNF%3e0%3c/ConditionTypeCNF%3e%3cSPT%3e%3cConditionNegated%3e0%3c/ConditionNegated%3e%3cGroup%3e0%3c/Group%3e%3cMethod%3eINVITE%3c/Method%3e%3cExtension> /></SPT></TriggerPoint><ApplicationServer><ServerName>sip:mmtel.clearwater.opnfv</ServerName><DefaultHandling>0</DefaultHandling></ApplicationServer></InitialFilterCriteria><PublicIdentity><BarringIndication>1</BarringIndication><Identity>sip:2010000021 at clearwater.opnfv</Identity></PublicIdentity></ServiceProfile></IMSSubscription<sip:mmtel.clearwater.opnfv%3c/ServerName%3e%3cDefaultHandling%3e0%3c/DefaultHandling%3e%3c/ApplicationServer%3e%3c/InitialFilterCriteria%3e%3cPublicIdentity%3e%3cBarringIndication%3e1%3c/BarringIndication%3e%3cIdentity%3esip:2010000021 at clearwater.opnfv%3c/Identity%3e%3c/PublicIdentity%3e%3c/ServiceProfile%3e%3c/IMSSubscription>> (TTL 0)
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:629:   is_registered =>  (TTL 0)
28-03-2018 06:30:58.046 UTC Debug cassandra_store.cpp:650: Executing put request operation
28-03-2018 06:30:58.047 UTC Debug baseresolver.cpp:830: Successful response from  10.67.79.6:9160 transport 6
28-03-2018 06:30:58.047 UTC Debug connection_pool.h:267: Release connection to IP: 10.67.79.6, port: 9160 to pool
28-03-2018 06:30:58.047 UTC Debug handlers.cpp:1567: Sending 200 response (body was {"reqtype": "reg", "server_name": "sip:scscf.sprout.clearwater.local:5054;transport=TCP"})
28-03-2018 06:30:58.047 UTC Verbose httpstack.cpp:93: Sending response 200 to request for URL /impu/sip%3A2010000021%40clearwater.opnfv/reg-data, args private_id=2010000021%40clearwater.opnfv



The error about "Could not get subscriber data from HSS" on Sprout node occurred sometimes.

root at sprout-2tl8g3:/var/log/sprout# vim sprout_20180328T060000Z.txt
28-03-2018 06:36:11.741 UTC Error httpclient.cpp:712: cURL failure with cURL error code 0 (see man 3 libcurl-errors) and HTTP error code 400
28-03-2018 06:36:11.741 UTC Error hssconnection.cpp:704: Could not get subscriber data from HSS
28-03-2018 06:36:25.875 UTC Status alarm.cpp:62: sprout issued 1004.1 alarm




root at dime-au6gte:/var/log/homestead# monit summary
Monit 5.18.1 uptime: 2h 27m
 Service Name                     Status                      Type
 node-dime-au6gte.clearwater....  Running                     System
 snmpd_process                    Running                     Process
 ralf_process                     Running                     Process
 ntp_process                      Running                     Process
 nginx_process                    Running                     Process
 homestead_process                Running                     Process
 homestead-prov_process           Running                     Process
 clearwater_queue_manager_pro...  Running                     Process
 etcd_process                     Running                     Process
 clearwater_diags_monitor_pro...  Running                     Process
 clearwater_config_manager_pr...  Running                     Process
 clearwater_cluster_manager_p...  Running                     Process
 ralf_uptime                      Status ok                   Program
 poll_ralf                        Status ok                   Program
 nginx_ping                       Status ok                   Program
 nginx_uptime                     Status ok                   Program
 monit_uptime                     Status ok                   Program
 homestead_uptime                 Status ok                   Program
 poll_homestead                   Status ok                   Program
 check_cx_health                  Status ok                   Program
 poll_homestead-prov              Status ok                   Program
 clearwater_queue_manager_uptime  Status ok                   Program
 etcd_uptime                      Status ok                   Program
 poll_etcd_cluster                Status ok                   Program
 poll_etcd                        Status ok                   Program


Any help/suggestion would be much appreciated.




Thanks,

Linda
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180425/966cbbac/attachment.html>

From William.Yates at metaswitch.com  Thu Apr 26 07:39:50 2018
From: William.Yates at metaswitch.com (William Yates)
Date: Thu, 26 Apr 2018 11:39:50 +0000
Subject: [Project Clearwater] Regarding Increasing #calls using stress
 testing
In-Reply-To: <CAL1mNUFU_MaLbPEiEbwgBLsJEOn4z=n=SiCm-_-hMBANS=w47A@mail.gmail.com>
References: <CAL1mNUF2AdfjhOZ38Mt0Jr2wZWnp1+mF91RzEFgwiHoRtPa78g@mail.gmail.com>
	<CAL1mNUFU_MaLbPEiEbwgBLsJEOn4z=n=SiCm-_-hMBANS=w47A@mail.gmail.com>
Message-ID: <BLUPR0201MB1490666DD0395BC86F03A549958E0@BLUPR0201MB1490.namprd02.prod.outlook.com>

Hi Arun,

This subject has been raised recently, with some comprehensive answers from Rob Day here - please have a read:
http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/2018-April/003866.html

More nodes seems like the right approach, but it's difficult to predict what you would need to achieve your aims.

I hope this helps, but get back if not.

Regarding the ZeroDivisionError, there is already an issue to cover this: https://github.com/Metaswitch/project-clearwater-issues/issues/30

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Arun Lal
Sent: 24 April 2018 17:37
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Regarding Increasing #calls using stress testing

Hi guys,

it give following result:

[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress rags.mf 40000 1 --multiplier=12
Starting initial registration, will take 500 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:01:43
Start: 2018-04-25 05:28:52.781011
End: 2018-04-25 05:30:57.535772

Total calls: 5200
Successful calls: 5168 (99.3846153846%)
Failed calls: 32 (0.615384615385%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 10387.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 3 (0.0576923076923%)
# of calls with 2000+ms from INVITE to 180 Ringing: 5167 (99.3653846154%)
Failed: call success rate 99.3846153846% is lower than target 100.0%!

Total re-REGISTERs: 16000
Successful re-REGISTERs: 15559 (97.24375%)
Failed re-REGISTERS: 441 (2.75625%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 5149.0ms
Failed: re-registration success rate 97.24375% is lower than target 100.0%!

Log files at /var/log/clearwater-sip-stress/2679_*



some time it gives like:


[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress iind.intel.com<http://iind.intel.com> 50000 1 --multiplier=8
Starting initial registration, will take 625 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
    call_success_rate = 100 * float(row['SuccessfulCall(C)']) / float(row['TotalCallCreated'])
ZeroDivisionError: float division by zero


I want to make 1 million calls per second (is it possible?) because I have tried by varying the #subscribers and multiplier and other parameter but it is not increasing.
Firsy I want it is able to make 20K calls per sec (or 50K or 1lack per sec). Every node is working fine, I don't know why I am not able to scale the performance or # calls .

If anything need to change in script run_stress, please let me know where and what.

your reply will highly appreciate.

Regards,
Arun Lal

On Tue, Apr 24, 2018 at 2:28 PM, Arun Lal <arunlalsingh23 at gmail.com<mailto:arunlalsingh23 at gmail.com>> wrote:
Hi,

Nice work by clearwater team, I really appreciate your work.

The document of clearwater is self explanatory, most of the task I did by reading that, but I have stuck at some points:

> I have 3 sprout, 2 vellum, 2 dime, 1 bono, 1 homer, 1 ellis nodes. I want to make 15000 calls/sec or more. I can increase more nodes if possible.

> I am not able to make more calls, I tried

[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress rags.mf 30000 1 --multiplier=20

but it gives around 3000 calls.

> what exact command should I use to make more calls per sec. All the node are working fine
I have check that zoiper and other sip client working fine.

> Can you please share exact command so that I can increase no. of calls per sec.

Or how many more nodes are required, I will add.



Thanks,
Arun Lal Singh



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180426/b7a7922d/attachment.html>

From Thomas.Watson at metaswitch.com  Thu Apr 26 11:32:18 2018
From: Thomas.Watson at metaswitch.com (Thomas Watson)
Date: Thu, 26 Apr 2018 15:32:18 +0000
Subject: [Project Clearwater] Sprout sending OPTIONS to sprout and not AS
In-Reply-To: <MWHPR02MB2813AD536424251A032E868CF8880@MWHPR02MB2813.namprd02.prod.outlook.com>
References: <HE1PR08MB28101C9B25EC9F0DCB61F6CFD7A20@HE1PR08MB2810.eurprd08.prod.outlook.com>
	<MWHPR02MB2813AD536424251A032E868CF8880@MWHPR02MB2813.namprd02.prod.outlook.com>
Message-ID: <BY2PR02MB12846F6E3CF034C37F429E4B888E0@BY2PR02MB1284.namprd02.prod.outlook.com>

Hi Daniel,

Apologies for the delay in getting back to you.

It seems likely that the problem you are experiencing as a result of a missing or incorrect P-Asserted-Identity header on the OPTIONS request. This header is used to identify which IFCs to invoke, so if it isn't set correctly, the IFCs that would direct your request to the AS won't have any effect. Without any IFCs, the best sprout can do is forward the request on.

If you're sending the request via a P-CSCF, the P-CSCF should add this header to the request, but if you're sending the message directly to sprout, you'll have to add it yourself. If you're looking to poll the health of the AS, you may be better off sending the request directly to the AS.

In order to help any further, I'll need a bit more information. First of all, are you sending the request directly to the sprout, or via a P-CSCF? Also, could you supply sprout debug logs containing the full text of the OPTIONS request, and, if you're sending the request via bono, the corresponding debug logs from that process too.

Thanks,

Tom

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Daniel Marques
Sent: 29 March 2018 14:50
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Sprout sending OPTIONS to sprout and not AS


Hi



I am setting up one clearwater environment using docker.



I have successfully deployed the ims containers and two AS with IFCs.



The REGISTER requests work with success and reach the AS.



The OPTIONS requests don't work ( i have copied the IFCs from an older installation of clearwater, so I believe that the format of the IFCs is OK).



I took a pcap in sprout, and what I observe is that sprout receives the OPTIONS and executes a NAPTR dns query to the user domain. The resulting dns query points the request to sprout again.



I believe that instead of sprout attempting to execute the above dns query, should check the IFCs and send the OPTIONS towards the AS.



Can anyone help me with this?



I send here a part of the logs.


29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=1 (rdata0x7f21b4068500)
29-03-2018 13:45:19.173 UTC [7f21b8819700] Verbose common_sip_processing.cpp:87: RX 1686 bytes Request msg OPTIONS/cseq=1 (rdata0x7f21b4068500) from TCP 172.50.0.9:38087:
--start msg--

OPTIONS sip:+917722000104 at ims.mnc874.mcc405.3gppnetwork.org SIP/2.0
Record-Route: <sip:172.50.0.9:5058;transport=TCP;lr>
...
Route: <sip:sprout:5054;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f>
Content-Length:  0
...

--end msg--
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug uri_classifier.cpp:172: Classified URI as 5
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug pjutils.cpp:1771: Logging SAS Call-ID marker, Call-ID mHkJ0rybfT
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug thread_dispatcher.cpp:554: Recieved message 0x7f21b4068500 on worker thread
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug thread_dispatcher.cpp:571: Admitted request 0x7f21b4068500 on worker thread
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug thread_dispatcher.cpp:606: Incoming message 0x7f21b4068500 cloned to 0x7f21b406ba18
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug thread_dispatcher.cpp:625: Queuing cloned received message 0x7f21b406ba18 for worker threads with priority 15
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x21b2ad8
29-03-2018 13:45:19.173 UTC [7f21b8819700] Debug event_statistic_accumulator.cpp:32: Accumulate 0 for 0x21b2b20
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug utils.cpp:872: Added IOHook 0x7f25927cbe30 to stack. There are now 1 hooks
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug thread_dispatcher.cpp:178: Worker thread dequeue message 0x7f21b406ba18
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug thread_dispatcher.cpp:183: Request latency so far = 278us
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=1 (rdata0x7f21b406ba18)
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug uri_classifier.cpp:172: Classified URI as 5
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug basicproxy.cpp:62: Process OPTIONS request
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug sproutletproxy.cpp:631: Sproutlet Proxy transaction (0x7f219c021810) created. There are now 1 instances
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug basicproxy.cpp:1318: Report SAS start marker - trail (f)
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug pjutils.cpp:719: Cloned Request msg OPTIONS/cseq=1 (rdata0x7f21b406ba18) to tdta0x7f219c021c60
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug pjsip: tsx0x7f219c025 Transaction created for Request msg OPTIONS/cseq=1 (rdata0x7f21b406ba18)
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug pjsip: tsx0x7f219c025 Incoming Request msg OPTIONS/cseq=1 (rdata0x7f21b406ba18) in state Null
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug pjsip: tsx0x7f219c025 State changed from Null to Trying, event=RX_MSG
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug basicproxy.cpp:183: tsx0x7f219c025098 - tu_on_tsx_state UAS, TSX_STATE RX_MSG state=Trying
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug pjsip:       endpoint Response msg 408/OPTIONS/cseq=1 (tdta0x7f219c025800) created
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:sprout:5054;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug sproutletproxy.cpp:237: No Sproutlet found using service name or host
29-03-2018 13:45:19.173 UTC [7f25927cc700] Debug sproutletproxy.cpp:243: Find default service for port 5054
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:429: Creating URI for service registrar
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:455: Constructed URI sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=registrar
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:1276: Remove top Route header Route: <sip:sprout:5054;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f>
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=registrar
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:302: Found services param - registrar
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:172: Classified URI as 5
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:429: Creating URI for service subscription
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:302: Found services param - registrar
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:455: Constructed URI sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=subscription
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:1276: Remove top Route header Route: <sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=registrar>
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=subscription
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:302: Found services param - subscription
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:429: Creating URI for service scscf-proxy
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:302: Found services param - subscription
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:455: Constructed URI sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=scscf-proxy
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:1276: Remove top Route header Route: <sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=subscription>
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=scscf-proxy
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:302: Found services param - scscf-proxy
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug scscfsproutlet.cpp:424: S-CSCF Transaction (0x7f219c0278f0) created

29-03-2018 13:45:19.174 UTC [7f25927cc700] Verbose sproutletproxy.cpp:1384: Created Sproutlet scscf-proxy-0x7f219c0278f0 for Request msg OPTIONS/cseq=1 (tdta0x7f219c021c60)
29-03-2018 13:45:19.174 UTC [7f25927cc700] Verbose sproutletproxy.cpp:2487: Routing Request msg OPTIONS/cseq=1 (tdta0x7f219c021c60) (1727 bytes) to downstream sproutlet scscf-proxy:
--start msg--
OPTIONS sip:+917722000104 at ims.mnc874.mcc405.3gppnetwork.org SIP/2.0
Route: <sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=scscf-proxy>
Record-Route: <sip:172.50.0.9:5058;transport=TCP;lr>

...
--end msg--
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('EXTERNAL'->'scscf'/'scscf-proxy')
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('EXTERNAL'->'scscf'/'scscf-proxy')
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2517: Internal network function boundary: no
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug pjutils.cpp:736: Cloned tdta0x7f219c021c60 to tdta0x7f219c028430
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:1450: Remove top Route header Route: <sip:sprout;transport=tcp;lr;orig;username=+917722000103%40ims.mnc874.mcc405.3gppnetwork.org;nonce=7061e85c400e635f;service=scscf-proxy>
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2115: Adding message 0x7f219c028a40 => txdata 0x7f219c0284d8 mapping
29-03-2018 13:45:19.174 UTC [7f25927cc700] Verbose sproutletproxy.cpp:1946: scscf-proxy-0x7f219c0278f0 pass initial request Request msg OPTIONS/cseq=1 (tdta0x7f219c028430) to Sproutlet
29-03-2018 13:45:19.174 UTC [7f25927cc700] Info scscfsproutlet.cpp:471: S-CSCF received initial request
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:172: Classified URI as 3
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug scscfsproutlet.cpp:945: Route header references this system
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug scscfsproutlet.cpp:998: No ODI token, or invalid ODI token, on request, and no P-Charging-Vector header (so can't log ICID for correlation)
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug scscfsproutlet.cpp:1004: Got our Route header, session case orig, OD=None
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug pjutils.cpp:294: Served user from P-Asserted-Identity header
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:172: Classified URI as 5
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:172: Classified URI as 5
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug scscfsproutlet.cpp:1348: URI is not locally hosted
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug acr.cpp:1797: Create RalfACR for node type S-CSCF with role Originating
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug acr.cpp:24: Created ACR (0x7f219c02b3a0)
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug acr.cpp:170: Created S-CSCF Ralf ACR
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug acr.cpp:29: Destroyed ACR (0x7f219c02b3a0)
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug uri_classifier.cpp:172: Classified URI as 5
29-03-2018 13:45:19.174 UTC [7f25927cc700] Info scscfsproutlet.cpp:656: Route request without applying services
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:1621: Sproutlet send_request 0x7f219c028a40
29-03-2018 13:45:19.174 UTC [7f25927cc700] Verbose sproutletproxy.cpp:1662: scscf-proxy-0x7f219c0278f0 sending Request msg OPTIONS/cseq=1 (tdta0x7f219c028430) on fork 0
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2130: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2170: Processing request 0x7f219c0284d8, fork = 0
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2334: scscf-proxy-0x7f219c0278f0 transmitting request on fork 0
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2349: scscf-proxy-0x7f219c0278f0 store reference to non-ACK request Request msg OPTIONS/cseq=1 (tdta0x7f219c028430) on fork 0
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:2122: Removing message 0x7f219c028a40 => txdata 0x7f219c0284d8 mapping
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:165: Find target Sproutlet for request
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:199: Found next routable URI: sip:+917722000104 at ims.mnc874.mcc405.3gppnetwork.org
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:342: Possible service name ims will be used if mnc874.mcc405.3gppnetwork.org is a local hostname
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:365: Found user part - +917722000104
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sproutletproxy.cpp:1007: No local sproutlet matches request
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug pjsip: tsx0x7f219c02b Transaction created for Request msg OPTIONS/cseq=1 (tdta0x7f219c028430)
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug basicproxy.cpp:1669: Added trail identifier 15 to UAC transaction
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug pjutils.cpp:510: Next hop node is encoded in Request-URI
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug sipresolver.cpp:84: SIPResolver::resolve for name ims.mnc874.mcc405.3gppnetwork.org, port 0, transport -1, family 2
29-03-2018 13:45:19.174 UTC [7f25927cc700] Debug utils.cpp:446: Attempt to parse ims.mnc874.mcc405.3gppnetwork.org as IP address
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug sipresolver.cpp:147: Do NAPTR look-up for ims.mnc874.mcc405.3gppnetwork.org
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug ttlcache.h:200: Time now is 1522331119, expiry time of entry at head of expiry list is 1522330274
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug ttlcache.h:93: Entry not in cache, so create new entry
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug baseresolver.cpp:252: NAPTR cache factory called for ims.mnc874.mcc405.3gppnetwork.org
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug baseresolver.cpp:264: Sending DNS NAPTR query for ims.mnc874.mcc405.3gppnetwork.org
29-03-2018 13:45:19.175 UTC [7f25927cc700] Verbose dnscachedresolver.cpp:468: Check cache for ims.mnc874.mcc405.3gppnetwork.org type 35
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug dnscachedresolver.cpp:474: No entry found in cache
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug dnscachedresolver.cpp:477: Create cache entry pending query
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug dnscachedresolver.cpp:525: Create and execute DNS query transaction
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug dnscachedresolver.cpp:538: Wait for query responses
29-03-2018 13:45:19.175 UTC [7f25927cc700] Debug thread_dispatcher.cpp:117: Pausing stopwatch due to DNS query




Thanks



Daniel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180426/af1fd2e0/attachment.html>

From arunlalsingh23 at gmail.com  Fri Apr 27 01:23:57 2018
From: arunlalsingh23 at gmail.com (Arun Lal)
Date: Fri, 27 Apr 2018 10:53:57 +0530
Subject: [Project Clearwater] Regarding Increasing #calls using stress
 testing
In-Reply-To: <BLUPR0201MB1490666DD0395BC86F03A549958E0@BLUPR0201MB1490.namprd02.prod.outlook.com>
References: <CAL1mNUF2AdfjhOZ38Mt0Jr2wZWnp1+mF91RzEFgwiHoRtPa78g@mail.gmail.com>
	<CAL1mNUFU_MaLbPEiEbwgBLsJEOn4z=n=SiCm-_-hMBANS=w47A@mail.gmail.com>
	<BLUPR0201MB1490666DD0395BC86F03A549958E0@BLUPR0201MB1490.namprd02.prod.outlook.com>
Message-ID: <CAL1mNUGz+jnqWRdO83MqsnTArobipb333bS9UDO6Xv=Y6bVLiA@mail.gmail.com>

Hi William,
I have a look the Rob's answer and verify that in my setup, according to
Rob:

->When using the run_stress script, each emulated subscriber generates
1.3 calls/second (50% incoming, 50% outgoing). This can be increased
with the ?multiplier? option (e.g. --multiplier 10 means 13
calls/second)

But here I use  * --multiplier=1000, so according to rob it would be 1300
call/sec. But it is around 16 call/sec (below in sipp-output format). So my
question was, How can I increase the Call/sec may be 10000 call/sec or more
(is it possible using stress testing clearwater?)*

*[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress rags.mf 100 1
--multiplier=1000*
Starting initial registration, will take 1 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:01:07
Start: 2018-04-27 18:10:06.778960
End: 2018-04-27 18:11:14.140203

Total calls: 1083
Successful calls: 1082 (99.9076638966%)
Failed calls: 1 (0.0923361034164%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 124.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 5 (0.461680517082%)
# of calls with 50-100ms from INVITE to 180 Ringing: 409 (37.7654662973%)
# of calls with 100-200ms from INVITE to 180 Ringing: 582 (53.7396121884%)
# of calls with 200-500ms from INVITE to 180 Ringing: 81 (7.47922437673%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 6 (0.554016620499%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 99.9076638966% is lower than target 100.0%!

Total re-REGISTERs: 3333
Successful re-REGISTERs: 3333 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 49.0ms


----------------------------- Statistics Screen ------- [1-9]: Change
Screen --
  Start Time             | 2018-04-27   18:11:59.203632 1524832919.203632
  Last Reset Time        | 2018-04-27   18:13:06.570393 1524832986.570393
  Current Time           | 2018-04-27   18:13:06.570934 1524832986.570934
-------------------------+---------------------------+--------------------------
  Counter Name           | Periodic value            | Cumulative value
-------------------------+---------------------------+--------------------------
  Elapsed Time           | 00:00:00:000000           | 00:01:07:367000
 * Call Rate              |    0.000 cps              |   16.076 cps*
-------------------------+---------------------------+--------------------------
  Incoming call created  |        0                  |        0
  OutGoing call created  |        0                  |     1083
  Total Call created     |                          |     1083
  Current Call           |        0                  |
-------------------------+---------------------------+--------------------------
  Successful call        |        0                  |     1083
  Failed call            |        0                  |        0
-------------------------+---------------------------+--------------------------
  Response Time 1        | 00:00:00:000000           | 00:00:00:119000
  Call Length            | 00:00:00:000000           | 00:00:07:413000
------------------------------ Test Terminated
--------------------------------



Another thing from Rob's answer:

->*to increase the calls per second independently of registration rate,
you?d need to edit the script.*

*where and what I need to change?*

I tried to change in run_stress

BHCA = 1.3 i increase it to 4.3

OUTGOING_BHCA = BHCA/2.0 make it BHCA/1.0
RE_REG_PER_HOUR = 2.0 make it to 1.0

Regards,
Arun Lal


On Thu, Apr 26, 2018 at 5:09 PM, William Yates <William.Yates at metaswitch.com
> wrote:

> Hi Arun,
>
>
>
> This subject has been raised recently, with some comprehensive answers
> from Rob Day here - please have a read:
>
> http://lists.projectclearwater.org/pipermail/clearwater_lists.
> projectclearwater.org/2018-April/003866.html
>
>
>
> More nodes seems like the right approach, but it's difficult to predict
> what you would need to achieve your aims.
>
>
>
> I hope this helps, but get back if not.
>
>
>
> Regarding the ZeroDivisionError, there is already an issue to cover this:
> https://github.com/Metaswitch/project-clearwater-issues/issues/30
>
>
>
> Cheers,
>
> Will
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Arun Lal
> *Sent:* 24 April 2018 17:37
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Regarding Increasing #calls using
> stress testing
>
>
>
> Hi guys,
>
>
>
> it give following result:
>
>
>
> *[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress rags.mf 40000 1
> --multiplier=12*
>
> Starting initial registration, will take 500 seconds
>
> Initial registration succeeded
>
> Starting test
>
> Test complete
>
>
>
> Elapsed time: 00:01:43
>
> Start: 2018-04-25 05:28:52.781011
>
> End: 2018-04-25 05:30:57.535772
>
>
>
> Total calls: 5200
>
> Successful calls:* 5168 (99.3846153846%*)
>
> Failed calls: 32 (0.615384615385%)
>
> Unfinished calls: 0
>
>
>
> Retransmissions: 0
>
>
>
> Average time from INVITE to 180 Ringing: 10387.0ms
>
> # of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
>
> # of calls with 1000-2000ms from INVITE to 180 Ringing: 3
> (0.0576923076923%)
>
> # of calls with 2000+ms from INVITE to 180 Ringing: 5167 (99.3653846154%)
>
> Failed: call success rate 99.3846153846% is lower than target 100.0%!
>
>
>
> Total re-REGISTERs: 16000
>
> Successful re-REGISTERs: 15559 (97.24375%)
>
> Failed re-REGISTERS: 441 (2.75625%)
>
>
>
> REGISTER retransmissions: 0
>
>
>
> Average time from REGISTER to 200 OK: 5149.0ms
>
> Failed: re-registration success rate 97.24375% is lower than target 100.0%!
>
>
>
> Log files at /var/log/clearwater-sip-stress/2679_*
>
>
>
>
>
>
>
> *some time it gives like:*
>
>
>
>
>
> *[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress iind.intel.com
> <http://iind.intel.com> 50000 1 --multiplier=8*
>
> Starting initial registration, will take 625 seconds
>
> Initial registration succeeded
>
> Starting test
>
> Test complete
>
> Traceback (most recent call last):
>
>   File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
>
>     call_success_rate = 100 * float(row['SuccessfulCall(C)']) /
> float(row['TotalCallCreated'])
>
> ZeroDivisionError: float division by zero
>
>
>
>
>
> *I want to make 1 million calls per second (is it possible?) because I
> have tried by varying the #subscribers and multiplier and other parameter
> but it is not increasing.*
>
> *Firsy I want it is able to make 20K calls per sec (or 50K or 1lack per
> sec). Every node is working fine, I don't know why I am not able to scale
> the performance or # calls .*
>
>
>
> *If anything need to change in script run_stress, please let me know where
> and what.*
>
>
>
> *your reply will highly appreciate.*
>
>
>
> *Regards,*
>
> *Arun Lal*
>
>
>
> On Tue, Apr 24, 2018 at 2:28 PM, Arun Lal <arunlalsingh23 at gmail.com>
> wrote:
>
> Hi,
>
>
>
> Nice work by clearwater team, I really appreciate your work.
>
>
>
> The document of clearwater is self explanatory, most of the task I did by
> reading that, but I have stuck at some points:
>
>
>
> > I have 3 sprout, 2 vellum, 2 dime, 1 bono, 1 homer, 1 ellis nodes. I
> want to make 15000 calls/sec or more. I can increase more nodes if possible.
>
>
>
> > I am not able to make more calls, I tried
>
>
>
> []ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress rags.mf 30000 1
> --multiplier=20
>
>
>
> but it gives around 3000 calls.
>
>
>
> > what exact command should I use to make more calls per sec. All the node
> are working fine
>
> I have check that zoiper and other sip client working fine.
>
>
>
> > *Can you please share exact command so that I can increase no. of calls
> per sec.*
>
>
>
> Or how many more nodes are required, I will add.
>
>
>
>
>
>
>
> Thanks,
>
> Arun Lal Singh
>
>
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180427/d179a9ab/attachment.html>

From William.Yates at metaswitch.com  Fri Apr 27 10:26:30 2018
From: William.Yates at metaswitch.com (William Yates)
Date: Fri, 27 Apr 2018 14:26:30 +0000
Subject: [Project Clearwater] Regarding Increasing #calls using stress
 testing
In-Reply-To: <CAL1mNUGz+jnqWRdO83MqsnTArobipb333bS9UDO6Xv=Y6bVLiA@mail.gmail.com>
References: <CAL1mNUF2AdfjhOZ38Mt0Jr2wZWnp1+mF91RzEFgwiHoRtPa78g@mail.gmail.com>
	<CAL1mNUFU_MaLbPEiEbwgBLsJEOn4z=n=SiCm-_-hMBANS=w47A@mail.gmail.com>
	<BLUPR0201MB1490666DD0395BC86F03A549958E0@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<CAL1mNUGz+jnqWRdO83MqsnTArobipb333bS9UDO6Xv=Y6bVLiA@mail.gmail.com>
Message-ID: <BLUPR0201MB1490C16A534AFFC1D2933782958D0@BLUPR0201MB1490.namprd02.prod.outlook.com>

Hi Arun,

To decouple the registration rate from the multiplier, you would alter this area as you see fit:
-----
# Calculate calls/registers per second based on the input
RPS = RE_REG_PER_HOUR * args.subscriber_count * args.multiplier / 3600
CPS = OUTGOING_BHCA * args.subscriber_count * args.multiplier / 3600
-----

Looking at Rob's mail, it should read calls/hour, not calls/second, as mentioned here:
https://clearwater.readthedocs.io/en/latest/Clearwater_stress_testing.html#running-stress-ims-core-only

Note that the multiplier is only going to get you so far - as Rob pointed out, it will probably fail over around one reg/call per second per subscriber.

You'll need to add more subscribers to generate more load, and potentially nodes to handle it.

Good luck!

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Arun Lal
Sent: 27 April 2018 06:24
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Regarding Increasing #calls using stress testing

Hi William,
I have a look the Rob's answer and verify that in my setup, according to Rob:


->When using the run_stress script, each emulated subscriber generates 1.3 calls/second (50% incoming, 50% outgoing). This can be increased with the ?multiplier? option (e.g. --multiplier 10 means 13 calls/second)
But here I use   --multiplier=1000, so according to rob it would be 1300 call/sec. But it is around 16 call/sec (below in sipp-output format). So my question was, How can I increase the Call/sec may be 10000 call/sec or more (is it possible using stress testing clearwater?)

[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress rags.mf 100 1 --multiplier=1000
Starting initial registration, will take 1 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:01:07
Start: 2018-04-27 18:10:06.778960
End: 2018-04-27 18:11:14.140203

Total calls: 1083
Successful calls: 1082 (99.9076638966%)
Failed calls: 1 (0.0923361034164%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 124.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 5 (0.461680517082%)
# of calls with 50-100ms from INVITE to 180 Ringing: 409 (37.7654662973%)
# of calls with 100-200ms from INVITE to 180 Ringing: 582 (53.7396121884%)
# of calls with 200-500ms from INVITE to 180 Ringing: 81 (7.47922437673%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 6 (0.554016620499%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2000+ms from INVITE to 180 Ringing: 0 (0.0%)
Failed: call success rate 99.9076638966% is lower than target 100.0%!

Total re-REGISTERs: 3333
Successful re-REGISTERs: 3333 (100.0%)
Failed re-REGISTERS: 0 (0.0%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 49.0ms


----------------------------- Statistics Screen ------- [1-9]: Change Screen --
  Start Time             | 2018-04-27   18:11:59.203632 1524832919.203632
  Last Reset Time        | 2018-04-27   18:13:06.570393 1524832986.570393
  Current Time           | 2018-04-27   18:13:06.570934 1524832986.570934
-------------------------+---------------------------+--------------------------
  Counter Name           | Periodic value            | Cumulative value
-------------------------+---------------------------+--------------------------
  Elapsed Time           | 00:00:00:000000           | 00:01:07:367000
  Call Rate              |    0.000 cps              |   16.076 cps
-------------------------+---------------------------+--------------------------
  Incoming call created  |        0                  |        0
  OutGoing call created  |        0                  |     1083
  Total Call created     |                          |     1083
  Current Call           |        0                  |
-------------------------+---------------------------+--------------------------
  Successful call        |        0                  |     1083
  Failed call            |        0                  |        0
-------------------------+---------------------------+--------------------------
  Response Time 1        | 00:00:00:000000           | 00:00:00:119000
  Call Length            | 00:00:00:000000           | 00:00:07:413000
------------------------------ Test Terminated --------------------------------



Another thing from Rob's answer:

->to increase the calls per second independently of registration rate, you?d need to edit the script.

where and what I need to change?

I tried to change in run_stress

BHCA = 1.3 i increase it to 4.3

OUTGOING_BHCA = BHCA/2.0 make it BHCA/1.0
RE_REG_PER_HOUR = 2.0 make it to 1.0

Regards,
Arun Lal


On Thu, Apr 26, 2018 at 5:09 PM, William Yates <William.Yates at metaswitch.com<mailto:William.Yates at metaswitch.com>> wrote:
Hi Arun,

This subject has been raised recently, with some comprehensive answers from Rob Day here - please have a read:
http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/2018-April/003866.html

More nodes seems like the right approach, but it's difficult to predict what you would need to achieve your aims.

I hope this helps, but get back if not.

Regarding the ZeroDivisionError, there is already an issue to cover this: https://github.com/Metaswitch/project-clearwater-issues/issues/30

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Arun Lal
Sent: 24 April 2018 17:37
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Regarding Increasing #calls using stress testing

Hi guys,

it give following result:

[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress rags.mf 40000 1 --multiplier=12
Starting initial registration, will take 500 seconds
Initial registration succeeded
Starting test
Test complete

Elapsed time: 00:01:43
Start: 2018-04-25 05:28:52.781011
End: 2018-04-25 05:30:57.535772

Total calls: 5200
Successful calls: 5168 (99.3846153846%)
Failed calls: 32 (0.615384615385%)
Unfinished calls: 0

Retransmissions: 0

Average time from INVITE to 180 Ringing: 10387.0ms
# of calls with 0-2ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 2-10ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 10-20ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 20-50ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 50-100ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 100-200ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 200-500ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 500-1000ms from INVITE to 180 Ringing: 0 (0.0%)
# of calls with 1000-2000ms from INVITE to 180 Ringing: 3 (0.0576923076923%)
# of calls with 2000+ms from INVITE to 180 Ringing: 5167 (99.3653846154%)
Failed: call success rate 99.3846153846% is lower than target 100.0%!

Total re-REGISTERs: 16000
Successful re-REGISTERs: 15559 (97.24375%)
Failed re-REGISTERS: 441 (2.75625%)

REGISTER retransmissions: 0

Average time from REGISTER to 200 OK: 5149.0ms
Failed: re-registration success rate 97.24375% is lower than target 100.0%!

Log files at /var/log/clearwater-sip-stress/2679_*



some time it gives like:


[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress iind.intel.com<http://iind.intel.com> 50000 1 --multiplier=8
Starting initial registration, will take 625 seconds
Initial registration succeeded
Starting test
Test complete
Traceback (most recent call last):
  File "/usr/share/clearwater/bin/run_stress", line 346, in <module>
    call_success_rate = 100 * float(row['SuccessfulCall(C)']) / float(row['TotalCallCreated'])
ZeroDivisionError: float division by zero


I want to make 1 million calls per second (is it possible?) because I have tried by varying the #subscribers and multiplier and other parameter but it is not increasing.
Firsy I want it is able to make 20K calls per sec (or 50K or 1lack per sec). Every node is working fine, I don't know why I am not able to scale the performance or # calls .

If anything need to change in script run_stress, please let me know where and what.

your reply will highly appreciate.

Regards,
Arun Lal

On Tue, Apr 24, 2018 at 2:28 PM, Arun Lal <arunlalsingh23 at gmail.com<mailto:arunlalsingh23 at gmail.com>> wrote:
Hi,

Nice work by clearwater team, I really appreciate your work.

The document of clearwater is self explanatory, most of the task I did by reading that, but I have stuck at some points:

> I have 3 sprout, 2 vellum, 2 dime, 1 bono, 1 homer, 1 ellis nodes. I want to make 15000 calls/sec or more. I can increase more nodes if possible.

> I am not able to make more calls, I tried

[]ubuntu at stress:~$ /usr/share/clearwater/bin/run_stress rags.mf 30000 1 --multiplier=20

but it gives around 3000 calls.

> what exact command should I use to make more calls per sec. All the node are working fine
I have check that zoiper and other sip client working fine.

> Can you please share exact command so that I can increase no. of calls per sec.

Or how many more nodes are required, I will add.



Thanks,
Arun Lal Singh




_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180427/b7d11994/attachment.html>

