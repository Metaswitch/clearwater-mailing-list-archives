From Eleanor.Merry at metaswitch.com  Thu Oct  1 05:53:42 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Thu, 1 Oct 2015 09:53:42 +0000
Subject: [Clearwater] Deploying Clearwater with Haet
In-Reply-To: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
References: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
Message-ID: <BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Shay,

It doesn't look like /etc/clearwater/shared_config has successfully propagated to the Homestead node.

In order to set up the diameter configuration file (used by Homestead) and the local_settings file (used by Homestead-prov) correctly, Homestead/Homestead-prov need information from the /etc/clearwater/shared_config file (e.g. the hs_hostname). If the shared_config file is missing; the install will succeed, but Homestead/Homestead-prov won't be functional until they've got their configuration files set up.

Looking at the template, we install the clearwater-management package after we install the Homestead/Homestead-prov packages, but don't then do anything to apply the shared configuration to the node. I've raised an issue (https://github.com/Metaswitch/clearwater-heat/issues/15) to fix this; in the meantime can you please run "sudo /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config" on all your nodes?

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 29 September 2015 16:10
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] Deploying Clearwater with Haet

Hello,
I am trying deploying Clearwater on my OpenStack using the Heat templates as defined in here https://github.com/Metaswitch/clearwater-heat
I am running into issues with Homestead where I get (see below):

1.       Why LOCAL_IP=MUST BE CONFIGURED is not configured?

2.       Regarding the Diameter stack I saw an open issue https://github.com/Metaswitch/homestead/issues/73 but when deleting the certificates and starting the Clearwater-infrastructure it fails again on the same problem.

Thanks,
Shay



4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:39:55 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:39:55 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:39:55 homestead-0 monit:     exec code in run_globals
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:39:55 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:39:55 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:39:55 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:39:55 homestead-0 monit:    ...fail!
Sep 29 06:40:01 homestead-0 CRON[15030]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:40:22 homestead-0 monit: CMD /etc/init.d/homestead-prov start
Sep 29 06:40:26 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:40:26 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:40:26 homestead-0 monit:     exec code in run_globals
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:40:26 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:40:26 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:40:26 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:41:01 homestead-0 CRON[15050]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:41:03 homestead-0 monit: CMD /etc/init.d/homestead-prov restart
Sep 29 06:41:03 homestead-0 monit:  * Restarting homestead-prov homestead-prov
Sep 29 06:41:03 homestead-0 homestead[15074]: 4005 - Description: Homestead started. @@Cause: The Homestead application is starting. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1001 - Description: Diameter stack is starting. @@Cause: Diameter stack is beginning initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1002 - Description: Diameter stack initialization completed. @@Cause: Diameter stack has completed initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:41:06 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:41:06 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:41:06 homestead-0 monit:     exec code in run_globals
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151001/184c0e15/attachment.html>

From Eleanor.Merry at metaswitch.com  Thu Oct  1 06:28:17 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Thu, 1 Oct 2015 10:28:17 +0000
Subject: [Clearwater] Is CW-AIO losing registered SIP URIs?
In-Reply-To: <CP1PR80MB214A25200573C8CAA47D4BBCE420@CP1PR80MB214.lamprd80.prod.outlook.com>
References: <CP1PR80MB214A25200573C8CAA47D4BBCE420@CP1PR80MB214.lamprd80.prod.outlook.com>
Message-ID: <BN3PR02MB125563822E6987D0A83B1FC19B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Juliano,

Do the users still exist on Homestead - can you check in the Cassandra database? Do the Homestead logs (in /var/log/homestead) give any more details when attempting to get the AV for a subscriber? Do the Sprout logs (in /var/log/sprout) indicate why the call is being rejected? You may need to turn on debug logging for this ? to do so create/edit the file /etc/clearwater/user_settings, add log_level=5 and then restart Sprout/Homestead (service <sprout/homestead> stop ? they?re automatically restarted by monit).

Also, I?ve noticed that you?re running a fairly old version of Clearwater ? we strongly recommend upgrading regularly (we release every two weeks). In this case, I recommend creating a new CW-AIO box as we?ve changed what Ubuntu version we support (it?s now 14.04).

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juliano Medeiros Coimbra
Sent: 25 September 2015 17:42
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] Is CW-AIO losing registered SIP URIs?


Hello,



I am using a CW-AIO image in the following version:



$ dpkg -l sprout homer homestead

Desired=Unknown/Install/Remove/Purge/Hold

| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend

|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)

||/ Name                       Version                    Description

+++-==========================-==========================-====================================================================

ii  homer                      1.0-150313.142251          homer, the Cassandra powered XDMS

ii  homestead                  1.0-150313.140357          homestead, the HSS Cache/Gateway

ii  sprout                     1.0-150313.150041          sprout, the SIP Router



I've created a few users using Ellis (of course with no "demo account" option) and also using Homer and Homestead APIs, but after some time (sorry, I don't know how much time, but it seems to be days) those users got the following state:


$ curl -X GET http://localhost:8889/impi/6505550716%40example.com/av
{"status": 404, "message": "Not Found", "reason": "unknown", "detail": {}, "error": true}

Therefore they also doesn't get registered through Sprout. The other scenario is a user that is found through this command above, but Sprout starts signaling "SIP/2.0 403 Forbidden".

Am I missing a configuration parameter? Or just messing around with the APIs?



I am sorry if this is a trivial question, but I am quite newbie with Project Clearwater. [?]



Thanks


Juliano Medeiros Coimbra, Architect
T: +55.19.3112-1200 ext. 1454
DaitanGroup | www.daitangroup.com<http://www.daitangroup.com/> | Highly Reliable Outsourcing. Value Added Services Worldwide.
Privileged and confidential. If this message has been received in error, please notify sender and delete it immediately.
Conte?do confidencial. Se esta mensagem foi recebida por engano, favor avisar o remetente e apag?-la.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151001/97bba5b7/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 488 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151001/97bba5b7/attachment.png>

From Eleanor.Merry at metaswitch.com  Thu Oct  1 06:42:49 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Thu, 1 Oct 2015 10:42:49 +0000
Subject: [Clearwater] Manual installation does not pass any live test
In-Reply-To: <CAOVZBjCKd2-SaWZMiarswjXvm+QZyU9e4G3BHWPAFmt7yqqVBA@mail.gmail.com>
References: <CAOVZBjCKd2-SaWZMiarswjXvm+QZyU9e4G3BHWPAFmt7yqqVBA@mail.gmail.com>
Message-ID: <BN3PR02MB12555A46BE16D7AE218A1D719B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Austin,

Can you please send me the full Sprout and Homestead logs for this (as attachments). Can you also turn on debug logging - to do so create/edit the file /etc/clearwater/user_settings, add log_level=5 and then restart Sprout/Homestead (service <sprout/homestead> stop ? they?re automatically restarted by monit).

Thanks,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Austin Marston
Sent: 30 September 2015 09:04
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] Manual installation does not pass any live test

Hi list,

I just manually deployed over a VMware platform five VMs respectively with bono, homer,sprout,homestead, and ellis, with no HSS lookups nor I-CSCF function.
I tried to test it with clearwater-live-test but every tests fail with the following error:
RuntimeError thrown: - Expected 200, got 401

I understood from old messages from the list that "401s are expected. The first time a client registers with Clearwater it will not provide authentication credentials. Sprout rejects the REGISTER message with a 401 response, containing an authentication challenge. The client then sends another REGISTER with credentials."
But still, I cannot get what I'm missing here even with level 5 logs.
Indeed, Bono seems to fail with both 403 and 401 errors.

In the remaining of this email I put parts of the logs of each VM to illustrate my problem.

Sprout logs:
29-09-2015 16:31:33.994 UTC Error hssconnection.cpp:147: Failed to get Authentication Vector for 2010076236 at my.local.domain<mailto:2010076236 at my.local.domain>
29-09-2015 16:31:34.005 UTC Error httpconnection.cpp:743: cURL failure with cURL error code 0 (see man 3 libcurl-errors) and HTTP error code 404
29-09-2015 16:31:34.005 UTC Error hssconnection.cpp:147: Failed to get Authentication Vector for 2010018950 at my.local.domain<mailto:2010018950 at my.local.domain>
[...]
29-09-2015 16:31:34.145 UTC Error hssconnection.cpp:147: Failed to get Authentication Vector for 2010024050 at my.local.domain<mailto:2010024050 at my.local.domain>
29-09-2015 16:31:39.414 UTC Status load_monitor.cpp:237: Maximum incoming request rate/second increased to 1675.491699 (based on a smoothed mean latency of 2810 and 0 upstream overload responses)
29-09-2015 16:31:41.596 UTC Error memcachedstore.cpp:719: Failed to write data for av\\6505550473<tel:6505550473>@my.local.domain\0826351d69f0f550 to 1 replicas
29-09-2015 16:31:41.596 UTC Error avstore.cpp:84: Failed to write Authentication Vector for private_id 6505550473<tel:6505550473>@my.local.domain
29-09-2015 16:31:41.830 UTC Error memcachedstore.cpp:514: Failed to read data for av\\6505550473<tel:6505550473>@my.local.domain\0826351d69f0f550 from 1 replicas
29-09-2015 16:31:41.830 UTC Warning authentication.cpp:242: Received an authentication request for 6505550473<tel:6505550473>@my.local.domain with nonce 0826351d69f0f550, but no matching AV found


Homer logs:
29-09-2015 16:31:33.566 UTC DEBUG base.py:283: Writing response body: {}
29-09-2015 16:31:33.566 UTC INFO base.py:269: Sending 200 response to localhost for PUT http://http_homer/org.etsi.ngn.simservs/users/sip%3A6505550980%40my.local.domain/simservs.xml
29-09-2015 16:31:39.797 UTC INFO base.py:256: Received request from localhost - PUT http://http_homer/org.etsi.ngn.simservs/users/sip%3A6505550758%40my.local.domain/simservs.xml
29-09-2015 16:31:39.798 UTC INFO xsd.py:76: Performing XSD validation
29-09-2015 16:31:39.801 UTC DEBUG base.py:283: Writing response body: {}
29-09-2015 16:31:39.802 UTC INFO base.py:269: Sending 200 response to localhost for PUT http://http_homer/org.etsi.ngn.simservs/users/sip%3A6505550758%40my.local.domain/simservs.xml
29-09-2015 16:31:43.436 UTC INFO base.py:256: Received request from localhost - PUT http://http_homer/org.etsi.ngn.simservs/users/sip%3A6505550048%40my.local.domain/simservs.xml
29-09-2015 16:31:43.436 UTC INFO xsd.py:76: Performing XSD validation
29-09-2015 16:31:43.439 UTC DEBUG base.py:283: Writing response body: {}


Homestead-prov logs:
29-09-2015 16:31:33.538 UTC INFO base.py:269: Sending 200 response to localhost for PUT http://http_homestead_prov/irs/1004b727-c478-4123-98f6-9b63802c6a8c/service_profiles/19e5a738-f3ec-4664-bd2b-958ab5a8310a/filter_criteria
29-09-2015 16:31:34.183 UTC INFO base.py:256: Received request from localhost - PUT http://http_homestead_prov/private/6505550473%40my.local.domain
29-09-2015 16:31:34.184 UTC DEBUG models.py:303: Create private ID6505550473<tel:6505550473>@my.local.domain
29-09-2015 16:31:34.185 UTC DEBUG cache.py:85: Put private ID '6505550473<tel:6505550473>@my.local.domain' into cache with AV: {'digest': {'ha1': u'33accced67a669ca7b3d7e96640ef7f4', 'realm': u'my.local.domain', 'qop': 'auth'}}
29-09-2015 16:31:34.187 UTC INFO base.py:269: Sending 200 response to localhost for PUT http://http_homestead_prov/private/6505550473%40my.local.domain


Homestead logs:
29-09-2015 16:31:33.999 UTC Debug cassandra_store.cpp:731: Failed ONE read for get_columns. Try QUORUM
29-09-2015 16:31:34.000 UTC Debug cassandra_store.cpp:432: Cassandra request failed: rc=2, Row 2010018950 at my.local.domain<mailto:2010018950 at my.local.domain> not present in column_family impi
29-09-2015 16:31:34.000 UTC Debug handlers.cpp:196: Cache query failed - reject request
29-09-2015 16:31:34.000 UTC Debug handlers.cpp:201: No cached av found for private ID 2010018950 at my.local.domain<mailto:2010018950 at my.local.domain>, public ID sip:2010018950 at my.local.domain - reject
29-09-2015 16:31:34.000 UTC Verbose httpstack.cpp:69: Sending response 404 to request for URL /impi/2010018950%40my.local.domain/av, args impu=sip%3A2010018950%40my.local.domain
29-09-2015 16:31:34.000 UTC Verbose httpstack.cpp:286: Process request for URL /impi/2010070984%40my.local.domain/av, args impu=sip%3A2010070984%40my.local.domain
29-09-2015 16:31:34.000 UTC Debug handlers.cpp:147: Parsed HTTP request: private ID 2010070984 at my.local.domain<mailto:2010070984 at my.local.domain>, public ID sip:2010070984 at my.local.domain, scheme Unknown, authorization
29-09-2015 16:31:34.000 UTC Debug handlers.cpp:167: Querying cache for authentication vector for 2010070984 at my.local.domain/sip:2010070984 at my.local.domain<mailto:2010070984 at my.local.domain/sip:2010070984 at my.local.domain>
29-09-2015 16:31:34.000 UTC Debug cassandra_store.cpp:279: Getting thread-local Client
29-09-2015 16:31:34.000 UTC Debug cache.cpp:673: Looking for authentication vector for 2010070984 at my.local.domain<mailto:2010070984 at my.local.domain>
29-09-2015 16:31:34.000 UTC Debug cache.cpp:686: Checking public ID sip:2010070984 at my.local.domain
29-09-2015 16:31:34.000 UTC Debug cache.cpp:696: Issuing cache query
29-09-2015 16:31:34.000 UTC Debug cassandra_store.cpp:731: Failed ONE read for get_columns. Try QUORUM
29-09-2015 16:31:34.001 UTC Verbose httpstack.cpp:286: Process request for URL /impi/2010052484%40my.local.domain/av, args impu=sip%3A2010052484%40my.local.domain
29-09-2015 16:31:34.001 UTC Debug handlers.cpp:147: Parsed HTTP request: private ID 2010052484 at my.local.domain<mailto:2010052484 at my.local.domain>, public ID sip:2010052484 at my.local.domain, scheme Unknown, authorization


Bono logs:
29-09-2015 16:31:33.996 UTC Call-Disconnected: CALL_ID=2010076236///401485-9633 at 172.16.1.29<mailto:401485-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.008 UTC Call-Disconnected: CALL_ID=2010018950///419639-9633 at 172.16.1.29<mailto:419639-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.008 UTC Call-Disconnected: CALL_ID=2010070984///456772-9633 at 172.16.1.29<mailto:456772-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.008 UTC Call-Disconnected: CALL_ID=2010052484///428115-9633 at 172.16.1.29<mailto:428115-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.013 UTC Call-Disconnected: CALL_ID=2010023276///389993-9633 at 172.16.1.29<mailto:389993-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.013 UTC Call-Disconnected: CALL_ID=2010004256///394841-9633 at 172.16.1.29<mailto:394841-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.014 UTC Call-Disconnected: CALL_ID=2010076066///463823-9633 at 172.16.1.29<mailto:463823-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.014 UTC Call-Disconnected: CALL_ID=2010040994///436539-9633 at 172.16.1.29<mailto:436539-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.020 UTC Call-Disconnected: CALL_ID=2010047262///392603-9633 at 172.16.1.29<mailto:392603-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.021 UTC Call-Disconnected: CALL_ID=2010070408///400941-9633 at 172.16.1.29<mailto:400941-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.024 UTC Call-Disconnected: CALL_ID=2010001186///425962-9633 at 172.16.1.29<mailto:425962-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.030 UTC Call-Disconnected: CALL_ID=2010080220///434892-9633 at 172.16.1.29<mailto:434892-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.037 UTC Call-Disconnected: CALL_ID=2010026340///453694-9633 at 172.16.1.29<mailto:453694-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.049 UTC Call-Disconnected: CALL_ID=2010059662///396784-9633 at 172.16.1.29<mailto:396784-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.051 UTC Call-Disconnected: CALL_ID=2010007964///407878-9633 at 172.16.1.29<mailto:407878-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.059 UTC Call-Disconnected: CALL_ID=2010001412///417601-9633 at 172.16.1.29<mailto:417601-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.084 UTC Call-Disconnected: CALL_ID=2010048846///437109-9633 at 172.16.1.29<mailto:437109-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.085 UTC Call-Disconnected: CALL_ID=2010031554///390144-9633 at 172.16.1.29<mailto:390144-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.092 UTC Call-Disconnected: CALL_ID=2010083556///425110-9633 at 172.16.1.29<mailto:425110-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.092 UTC Call-Disconnected: CALL_ID=2010025210///446867-9633 at 172.16.1.29<mailto:446867-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.110 UTC Call-Disconnected: CALL_ID=2010060732///434889-9633 at 172.16.1.29<mailto:434889-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.112 UTC Call-Disconnected: CALL_ID=2010020690///399131-9633 at 172.16.1.29<mailto:399131-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.120 UTC Call-Disconnected: CALL_ID=2010065600///423893-9633 at 172.16.1.29<mailto:423893-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.123 UTC Call-Disconnected: CALL_ID=2010058752///460196-9633 at 172.16.1.29<mailto:460196-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.129 UTC Call-Disconnected: CALL_ID=2010070682///453479-9633 at 172.16.1.29<mailto:453479-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.129 UTC Call-Disconnected: CALL_ID=2010046438///394971-9633 at 172.16.1.29<mailto:394971-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.142 UTC Call-Disconnected: CALL_ID=2010049994///458825-9633 at 172.16.1.29<mailto:458825-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.144 UTC Call-Disconnected: CALL_ID=2010070470///420696-9633 at 172.16.1.29<mailto:420696-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.146 UTC Call-Disconnected: CALL_ID=2010024050///437499-9633 at 172.16.1.29<mailto:437499-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:41.598 UTC Call-Disconnected: CALL_ID=17745eb3f9e50ecb8155e83aea81edae REASON=401
29-09-2015 16:31:41.835 UTC Call-Disconnected: CALL_ID=17745eb3f9e50ecb8155e83aea81edae REASON=401
29-09-2015 16:31:42.136 UTC Call-Disconnected: CALL_ID=17745eb3f9e50ecb8155e83aea81edae REASON=401
29-09-2015 16:31:42.437 UTC Call-Disconnected: CALL_ID=17745eb3f9e50ecb8155e83aea81edae REASON=401
29-09-2015 16:31:45.312 UTC Call-Disconnected: CALL_ID=88a59417552efe9582128c1c9d134862 REASON=401
29-09-2015 16:31:45.320 UTC Call-Disconnected: CALL_ID=88a59417552efe9582128c1c9d134862 REASON=401
29-09-2015 16:31:45.327 UTC Call-Disconnected: CALL_ID=88a59417552efe9582128c1c9d134862 REASON=401
29-09-2015 16:31:45.334 UTC Call-Disconnected: CALL_ID=88a59417552efe9582128c1c9d134862 REASON=401
29-09-2015 16:31:51.173 UTC Call-Disconnected: CALL_ID=a716eed59a862336ae71d2a329b77400 REASON=401
29-09-2015 16:31:51.465 UTC Call-Disconnected: CALL_ID=a716eed59a862336ae71d2a329b77400 REASON=401
29-09-2015 16:31:51.766 UTC Call-Disconnected: CALL_ID=a716eed59a862336ae71d2a329b77400 REASON=401


Ellis logs:
29-09-2015 16:31:32.727 UTC DEBUG utils.py:94: Still expecting 0 callbacks
29-09-2015 16:31:34.324 UTC DEBUG numbers.py:121: Number allocation API call (PSTN = false)
29-09-2015 16:31:34.650 UTC DEBUG numbers.py:134: Allocating a non-PSTN number
29-09-2015 16:31:34.653 UTC DEBUG numbers.py:148: Fetched e7d0bf0c-e00a-41c8-8bc7-5377aa76dc97
29-09-2015 16:31:35.426 UTC DEBUG numbers.py:160: Updated the owner
29-09-2015 16:31:35.427 UTC DEBUG numbers.py:130: SIP URI sip:6505550758<tel:6505550758>@my.local.domain
29-09-2015 16:31:39.686 UTC DEBUG numbers.py:159: Populating other servers...
29-09-2015 16:31:39.686 UTC DEBUG numbers.py:170: About to create private ID at Homestead
29-09-2015 16:31:39.686 UTC WARNING homestead.py:279: Passing SIP password in the clear over http
29-09-2015 16:31:39.698 UTC WARNING homestead.py:279: Passing SIP password in the clear over http
29-09-2015 16:31:39.704 UTC WARNING homestead.py:279: Passing SIP password in the clear over http
29-09-2015 16:31:39.720 UTC DEBUG numbers.py:175: Created private ID at Homestead
29-09-2015 16:31:39.721 UTC WARNING homestead.py:279: Passing SIP password in the clear over http
29-09-2015 16:31:39.794 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.015350818634033203,buffer=<_io.BytesIO object at 0x7ff1a10d6f50>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7ff1a1070d50>,effective_url='http://hs.my.local.domain:8889/private/6505550758%40my.local.domain/associated_implicit_registration_sets/1ed1c2eb-745d-4d53-8be2-68116882268e',headers={'Date': 'Tue, 29 Sep 2015 16:31:39 GMT', 'Content-Length': '0', 'Content-Type': 'text/html; charset=UTF-8', 'Connection': 'close', 'Server': 'nginx/1.4.6 (Ubuntu)'},error=None)
29-09-2015 16:31:39.794 UTC DEBUG utils.py:94: Still expecting 2 callbacks
29-09-2015 16:31:39.803 UTC DEBUG utils.py:94: Still expecting 1 callbacks
29-09-2015 16:31:39.823 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.030350208282470703,buffer=<_io.BytesIO object at 0x7ff1a10d6d10>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7ff1a10705d0>,effective_url='http://hs.my.local.domain:8889/irs/1ed1c2eb-745d-4d53-8be2-68116882268e/service_profiles/113003da-f6f3-4efa-8612-bcb4f5be908b/filter_criteria',headers={'Date': 'Tue, 29 Sep 2015 16:31:39 GMT', 'Content-Length': '0', 'Content-Type': 'text/html; charset=UTF-8', 'Connection': 'close', 'Server': 'nginx/1.4.6 (Ubuntu)'},error=None)
29-09-2015 16:31:39.823 UTC DEBUG utils.py:92: All requests successful.

Many thanks for your help !

PS: Let me know if the logs are not clear enough or if I should put them in attachment.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151001/af5e9488/attachment.html>

From Eleanor.Merry at metaswitch.com  Thu Oct  1 06:48:29 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Thu, 1 Oct 2015 10:48:29 +0000
Subject: [Clearwater] bono on two interfaces for public and private
 network
In-Reply-To: <01c501d0fb5a$b3019c40$1904d4c0$@gmail.com>
References: <01c501d0fb5a$b3019c40$1904d4c0$@gmail.com>
Message-ID: <BN3PR02MB1255F44A6F71198D48564F009B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi, 

You can bind Bono to two interfaces for management and signalling (e.g. ssh access on the management network, all SIP processing on the signalling network). You can see details of how to set this up at: http://clearwater.readthedocs.org/en/stable/Multiple_Network_Support/index.html. 

We don't support binding to two signalling networks yet - we're tracking this issue at https://github.com/Metaswitch/sprout/issues/364 (but it's not something we're actively working on right now). 

Ellie

-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Yuri Isaakyan
Sent: 30 September 2015 09:34
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] bono on two interfaces for public and private network

Hi all,

Is it possible to bind bono to two network interfaces for public and private network?

BR,
Yuri


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



From fmalka at modulo.co.il  Thu Oct  1 07:30:43 2015
From: fmalka at modulo.co.il (Franck Malka)
Date: Thu, 1 Oct 2015 14:30:43 +0300
Subject: [Clearwater] Configuration of iFCs rules
Message-ID: <8b677254bf5f329ee7f3c991ca911a7e@mail.gmail.com>

Hi,



I could not find in the doc pages a full guide for the configuration of iFC
rules.

I need to configure a rule for 3rd party registrar and for MMTel
Conferencing.

I there some information available around this topic somewhere?



-Franck.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151001/231ed09d/attachment.html>

From Eleanor.Merry at metaswitch.com  Thu Oct  1 08:07:38 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Thu, 1 Oct 2015 12:07:38 +0000
Subject: [Clearwater] Configuration of iFCs rules
In-Reply-To: <8b677254bf5f329ee7f3c991ca911a7e@mail.gmail.com>
References: <8b677254bf5f329ee7f3c991ca911a7e@mail.gmail.com>
Message-ID: <BN3PR02MB1255054A38E92AD1D0E6A74D9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Franck,

We?ve got some examples of iFC configuration in http://clearwater.readthedocs.org/en/stable/Configuring_an_Application_Server/index.html#direct-configuration-via-curl and https://github.com/Metaswitch/memento/blob/dev/docs/memento_overview.md#configuration/.

For a full description of how to configure iFCs, I recommend that you check out TS 29.228 (release 10, Annex C and F).

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Franck Malka
Sent: 01 October 2015 12:31
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] Configuration of iFCs rules

Hi,

I could not find in the doc pages a full guide for the configuration of iFC rules.
I need to configure a rule for 3rd party registrar and for MMTel Conferencing.
I there some information available around this topic somewhere?

-Franck.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151001/4c90e071/attachment.html>

From fmalka at modulo.co.il  Thu Oct  1 08:10:34 2015
From: fmalka at modulo.co.il (Franck Malka)
Date: Thu, 1 Oct 2015 15:10:34 +0300
Subject: [Clearwater] Configuration of iFCs rules
In-Reply-To: <BN3PR02MB1255054A38E92AD1D0E6A74D9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
References: <8b677254bf5f329ee7f3c991ca911a7e@mail.gmail.com>
	<BN3PR02MB1255054A38E92AD1D0E6A74D9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <CAKX=ev2Gsow9yMDDhQiDfVPXBExNuZ7z9hSdDHjnybycUV_S+g@mail.gmail.com>

Hi,

i need to include the original register request in the 3rd register
request, the doc doesn't cover that.

i will look at the spec.

Franck
On Oct 1, 2015 3:07 PM, "Eleanor Merry" <Eleanor.Merry at metaswitch.com>
wrote:

> Hi Franck,
>
>
>
> We?ve got some examples of iFC configuration in
> http://clearwater.readthedocs.org/en/stable/Configuring_an_Application_Server/index.html#direct-configuration-via-curl
> and
> https://github.com/Metaswitch/memento/blob/dev/docs/memento_overview.md#configuration/.
>
>
>
>
> For a full description of how to configure iFCs, I recommend that you
> check out TS 29.228 (release 10, Annex C and F).
>
>
>
> Ellie
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Franck Malka
> *Sent:* 01 October 2015 12:31
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Clearwater] Configuration of iFCs rules
>
>
>
> Hi,
>
>
>
> I could not find in the doc pages a full guide for the configuration of
> iFC rules.
>
> I need to configure a rule for 3rd party registrar and for MMTel
> Conferencing.
>
> I there some information available around this topic somewhere?
>
>
>
> -Franck.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151001/03f8062f/attachment.html>

From shayn at gigaspaces.com  Thu Oct  1 12:22:01 2015
From: shayn at gigaspaces.com (Shay Naeh)
Date: Thu, 1 Oct 2015 16:22:01 +0000
Subject: [Clearwater] Deploying Clearwater with Haet
In-Reply-To: <BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
References: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
	<BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <AMXPR04MB069A6C79056AFC6762139BDC74C0@AMXPR04MB069.eurprd04.prod.outlook.com>

Thanks Eleanor,

I run the /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config

But still I get the following errors when running 'sudo monit status'

Process 'homestead_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59

Program 'poll_homestead'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Thu, 01 Oct 2015 16:10:49

Process 'homestead-prov_process'
  status                            Execution failed
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59



Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 12:54 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

It doesn't look like /etc/clearwater/shared_config has successfully propagated to the Homestead node.

In order to set up the diameter configuration file (used by Homestead) and the local_settings file (used by Homestead-prov) correctly, Homestead/Homestead-prov need information from the /etc/clearwater/shared_config file (e.g. the hs_hostname). If the shared_config file is missing; the install will succeed, but Homestead/Homestead-prov won't be functional until they've got their configuration files set up.

Looking at the template, we install the clearwater-management package after we install the Homestead/Homestead-prov packages, but don't then do anything to apply the shared configuration to the node. I've raised an issue (https://github.com/Metaswitch/clearwater-heat/issues/15) to fix this; in the meantime can you please run "sudo /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config" on all your nodes?

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 29 September 2015 16:10
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Deploying Clearwater with Haet

Hello,
I am trying deploying Clearwater on my OpenStack using the Heat templates as defined in here https://github.com/Metaswitch/clearwater-heat
I am running into issues with Homestead where I get (see below):

1.       Why LOCAL_IP=MUST BE CONFIGURED is not configured?

2.       Regarding the Diameter stack I saw an open issue https://github.com/Metaswitch/homestead/issues/73 but when deleting the certificates and starting the Clearwater-infrastructure it fails again on the same problem.

Thanks,
Shay



4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:39:55 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:39:55 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:39:55 homestead-0 monit:     exec code in run_globals
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:39:55 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:39:55 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:39:55 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:39:55 homestead-0 monit:    ...fail!
Sep 29 06:40:01 homestead-0 CRON[15030]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:40:22 homestead-0 monit: CMD /etc/init.d/homestead-prov start
Sep 29 06:40:26 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:40:26 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:40:26 homestead-0 monit:     exec code in run_globals
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:40:26 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:40:26 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:40:26 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:41:01 homestead-0 CRON[15050]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:41:03 homestead-0 monit: CMD /etc/init.d/homestead-prov restart
Sep 29 06:41:03 homestead-0 monit:  * Restarting homestead-prov homestead-prov
Sep 29 06:41:03 homestead-0 homestead[15074]: 4005 - Description: Homestead started. @@Cause: The Homestead application is starting. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1001 - Description: Diameter stack is starting. @@Cause: Diameter stack is beginning initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1002 - Description: Diameter stack initialization completed. @@Cause: Diameter stack has completed initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:41:06 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:41:06 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:41:06 homestead-0 monit:     exec code in run_globals
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151001/fe43963b/attachment.html>

From Eleanor.Merry at metaswitch.com  Thu Oct  1 12:45:50 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Thu, 1 Oct 2015 16:45:50 +0000
Subject: [Clearwater] Deploying Clearwater with Haet
In-Reply-To: <AMXPR04MB069A6C79056AFC6762139BDC74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
References: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
	<BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069A6C79056AFC6762139BDC74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
Message-ID: <BN3PR02MB125571D8C63B36001E324CD99B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>

Does the /etc/clearwater/shared_config file exist on your Homestead node? Does it exist on the Ellis node?

If the shared_config file does exist, can you run 'sudo service clearwater-infrastructure restart'?

Ellie


From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:22
To: Eleanor Merry; clearwater at lists.projectclearwater.org
Subject: RE: Deploying Clearwater with Haet

Thanks Eleanor,

I run the /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config

But still I get the following errors when running 'sudo monit status'

Process 'homestead_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59

Program 'poll_homestead'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Thu, 01 Oct 2015 16:10:49

Process 'homestead-prov_process'
  status                            Execution failed
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59



Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 12:54 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

It doesn't look like /etc/clearwater/shared_config has successfully propagated to the Homestead node.

In order to set up the diameter configuration file (used by Homestead) and the local_settings file (used by Homestead-prov) correctly, Homestead/Homestead-prov need information from the /etc/clearwater/shared_config file (e.g. the hs_hostname). If the shared_config file is missing; the install will succeed, but Homestead/Homestead-prov won't be functional until they've got their configuration files set up.

Looking at the template, we install the clearwater-management package after we install the Homestead/Homestead-prov packages, but don't then do anything to apply the shared configuration to the node. I've raised an issue (https://github.com/Metaswitch/clearwater-heat/issues/15) to fix this; in the meantime can you please run "sudo /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config" on all your nodes?

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 29 September 2015 16:10
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Deploying Clearwater with Haet

Hello,
I am trying deploying Clearwater on my OpenStack using the Heat templates as defined in here https://github.com/Metaswitch/clearwater-heat
I am running into issues with Homestead where I get (see below):

1.       Why LOCAL_IP=MUST BE CONFIGURED is not configured?

2.       Regarding the Diameter stack I saw an open issue https://github.com/Metaswitch/homestead/issues/73 but when deleting the certificates and starting the Clearwater-infrastructure it fails again on the same problem.

Thanks,
Shay



4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:39:55 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:39:55 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:39:55 homestead-0 monit:     exec code in run_globals
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:39:55 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:39:55 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:39:55 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:39:55 homestead-0 monit:    ...fail!
Sep 29 06:40:01 homestead-0 CRON[15030]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:40:22 homestead-0 monit: CMD /etc/init.d/homestead-prov start
Sep 29 06:40:26 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:40:26 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:40:26 homestead-0 monit:     exec code in run_globals
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:40:26 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:40:26 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:40:26 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:41:01 homestead-0 CRON[15050]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:41:03 homestead-0 monit: CMD /etc/init.d/homestead-prov restart
Sep 29 06:41:03 homestead-0 monit:  * Restarting homestead-prov homestead-prov
Sep 29 06:41:03 homestead-0 homestead[15074]: 4005 - Description: Homestead started. @@Cause: The Homestead application is starting. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1001 - Description: Diameter stack is starting. @@Cause: Diameter stack is beginning initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1002 - Description: Diameter stack initialization completed. @@Cause: Diameter stack has completed initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:41:06 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:41:06 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:41:06 homestead-0 monit:     exec code in run_globals
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151001/1bced805/attachment.html>

From Eleanor.Merry at metaswitch.com  Thu Oct  1 12:59:29 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Thu, 1 Oct 2015 16:59:29 +0000
Subject: [Clearwater] Deploying Clearwater with Haet
In-Reply-To: <AMXPR04MB069FD082C705B75402A4DA2C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
References: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
	<BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069A6C79056AFC6762139BDC74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB125571D8C63B36001E324CD99B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069FD082C705B75402A4DA2C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
Message-ID: <BN3PR02MB1255BC0AB2D26E2CBF1ADC849B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi,

Is your etcd cluster healthy? What's the result of running sudo clearwater-etcdctl cluster-health? If these look OK then can you also please try running 'sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config' on the Ellis node, then the apply_shared_config script again on Homestead.

If this still doesn't work, you can copy over the shared_config file on Ellis to Homestead, run 'sudo service clearwater infrastructure' and this should unblock you. I'd be interested to dig further into why the apply_shared_config isn't working though.

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:48
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

It doesn't exist on Homestead but exists on Ellis.
I did clearwater-infrastructure restart but it didn't help.
Can we conduct a short webex, do you want me to send you webex details?
Thanks,
Shay
From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:46 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Does the /etc/clearwater/shared_config file exist on your Homestead node? Does it exist on the Ellis node?

If the shared_config file does exist, can you run 'sudo service clearwater-infrastructure restart'?

Ellie


From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:22
To: Eleanor Merry; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Thanks Eleanor,

I run the /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config

But still I get the following errors when running 'sudo monit status'

Process 'homestead_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59

Program 'poll_homestead'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Thu, 01 Oct 2015 16:10:49

Process 'homestead-prov_process'
  status                            Execution failed
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59



Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 12:54 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

It doesn't look like /etc/clearwater/shared_config has successfully propagated to the Homestead node.

In order to set up the diameter configuration file (used by Homestead) and the local_settings file (used by Homestead-prov) correctly, Homestead/Homestead-prov need information from the /etc/clearwater/shared_config file (e.g. the hs_hostname). If the shared_config file is missing; the install will succeed, but Homestead/Homestead-prov won't be functional until they've got their configuration files set up.

Looking at the template, we install the clearwater-management package after we install the Homestead/Homestead-prov packages, but don't then do anything to apply the shared configuration to the node. I've raised an issue (https://github.com/Metaswitch/clearwater-heat/issues/15) to fix this; in the meantime can you please run "sudo /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config" on all your nodes?

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 29 September 2015 16:10
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Deploying Clearwater with Haet

Hello,
I am trying deploying Clearwater on my OpenStack using the Heat templates as defined in here https://github.com/Metaswitch/clearwater-heat
I am running into issues with Homestead where I get (see below):

1.       Why LOCAL_IP=MUST BE CONFIGURED is not configured?

2.       Regarding the Diameter stack I saw an open issue https://github.com/Metaswitch/homestead/issues/73 but when deleting the certificates and starting the Clearwater-infrastructure it fails again on the same problem.

Thanks,
Shay



4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:39:55 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:39:55 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:39:55 homestead-0 monit:     exec code in run_globals
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:39:55 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:39:55 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:39:55 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:39:55 homestead-0 monit:    ...fail!
Sep 29 06:40:01 homestead-0 CRON[15030]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:40:22 homestead-0 monit: CMD /etc/init.d/homestead-prov start
Sep 29 06:40:26 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:40:26 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:40:26 homestead-0 monit:     exec code in run_globals
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:40:26 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:40:26 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:40:26 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:41:01 homestead-0 CRON[15050]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:41:03 homestead-0 monit: CMD /etc/init.d/homestead-prov restart
Sep 29 06:41:03 homestead-0 monit:  * Restarting homestead-prov homestead-prov
Sep 29 06:41:03 homestead-0 homestead[15074]: 4005 - Description: Homestead started. @@Cause: The Homestead application is starting. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1001 - Description: Diameter stack is starting. @@Cause: Diameter stack is beginning initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1002 - Description: Diameter stack initialization completed. @@Cause: Diameter stack has completed initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:41:06 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:41:06 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:41:06 homestead-0 monit:     exec code in run_globals
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151001/71be076b/attachment.html>

From shayn at gigaspaces.com  Thu Oct  1 14:57:21 2015
From: shayn at gigaspaces.com (Shay Naeh)
Date: Thu, 1 Oct 2015 18:57:21 +0000
Subject: [Clearwater] FW: Deploying Clearwater with Haet
References: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
	<BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069A6C79056AFC6762139BDC74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB125571D8C63B36001E324CD99B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069FD082C705B75402A4DA2C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB1255BC0AB2D26E2CBF1ADC849B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <AMXPR04MB069C562DDF2E1B656EE7AD7C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>

Hi ellie,

1.       I had to copy the shared_config from ellis to homestead and now it is ok

2.       I had the same problem on homer but after copying the shared_config and restarting Clearwater-infrastructure it tells me when doing 'sudo monit status' thar homer_process failed

3.       When I point with the browser to Ellis IP address I get the welcome message of Nginx but no login screen as I am used to get. What application should run on top of Nginx?

Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:59 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi,

Is your etcd cluster healthy? What's the result of running sudo clearwater-etcdctl cluster-health? If these look OK then can you also please try running 'sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config' on the Ellis node, then the apply_shared_config script again on Homestead.

If this still doesn't work, you can copy over the shared_config file on Ellis to Homestead, run 'sudo service clearwater infrastructure' and this should unblock you. I'd be interested to dig further into why the apply_shared_config isn't working though.

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:48
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

It doesn't exist on Homestead but exists on Ellis.
I did clearwater-infrastructure restart but it didn't help.
Can we conduct a short webex, do you want me to send you webex details?
Thanks,
Shay
From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:46 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Does the /etc/clearwater/shared_config file exist on your Homestead node? Does it exist on the Ellis node?

If the shared_config file does exist, can you run 'sudo service clearwater-infrastructure restart'?

Ellie


From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:22
To: Eleanor Merry; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Thanks Eleanor,

I run the /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config

But still I get the following errors when running 'sudo monit status'

Process 'homestead_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59

Program 'poll_homestead'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Thu, 01 Oct 2015 16:10:49

Process 'homestead-prov_process'
  status                            Execution failed
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59



Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 12:54 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

It doesn't look like /etc/clearwater/shared_config has successfully propagated to the Homestead node.

In order to set up the diameter configuration file (used by Homestead) and the local_settings file (used by Homestead-prov) correctly, Homestead/Homestead-prov need information from the /etc/clearwater/shared_config file (e.g. the hs_hostname). If the shared_config file is missing; the install will succeed, but Homestead/Homestead-prov won't be functional until they've got their configuration files set up.

Looking at the template, we install the clearwater-management package after we install the Homestead/Homestead-prov packages, but don't then do anything to apply the shared configuration to the node. I've raised an issue (https://github.com/Metaswitch/clearwater-heat/issues/15) to fix this; in the meantime can you please run "sudo /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config" on all your nodes?

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 29 September 2015 16:10
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Deploying Clearwater with Haet

Hello,
I am trying deploying Clearwater on my OpenStack using the Heat templates as defined in here https://github.com/Metaswitch/clearwater-heat
I am running into issues with Homestead where I get (see below):

1.       Why LOCAL_IP=MUST BE CONFIGURED is not configured?

2.       Regarding the Diameter stack I saw an open issue https://github.com/Metaswitch/homestead/issues/73 but when deleting the certificates and starting the Clearwater-infrastructure it fails again on the same problem.

Thanks,
Shay



4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:39:55 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:39:55 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:39:55 homestead-0 monit:     exec code in run_globals
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:39:55 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:39:55 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:39:55 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:39:55 homestead-0 monit:    ...fail!
Sep 29 06:40:01 homestead-0 CRON[15030]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:40:22 homestead-0 monit: CMD /etc/init.d/homestead-prov start
Sep 29 06:40:26 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:40:26 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:40:26 homestead-0 monit:     exec code in run_globals
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:40:26 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:40:26 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:40:26 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:41:01 homestead-0 CRON[15050]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:41:03 homestead-0 monit: CMD /etc/init.d/homestead-prov restart
Sep 29 06:41:03 homestead-0 monit:  * Restarting homestead-prov homestead-prov
Sep 29 06:41:03 homestead-0 homestead[15074]: 4005 - Description: Homestead started. @@Cause: The Homestead application is starting. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1001 - Description: Diameter stack is starting. @@Cause: Diameter stack is beginning initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1002 - Description: Diameter stack initialization completed. @@Cause: Diameter stack has completed initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:41:06 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:41:06 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:41:06 homestead-0 monit:     exec code in run_globals
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151001/7a985443/attachment.html>

From Robert.Day at metaswitch.com  Thu Oct  1 15:40:37 2015
From: Robert.Day at metaswitch.com (Robert Day)
Date: Thu, 1 Oct 2015 19:40:37 +0000
Subject: [Clearwater] Deploying Clearwater with Haet
In-Reply-To: <AMXPR04MB069C562DDF2E1B656EE7AD7C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
References: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
	<BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069A6C79056AFC6762139BDC74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB125571D8C63B36001E324CD99B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069FD082C705B75402A4DA2C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB1255BC0AB2D26E2CBF1ADC849B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069C562DDF2E1B656EE7AD7C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
Message-ID: <CY1PR0201MB0924E05F204B2DB63A4E8A66F44C0@CY1PR0201MB0924.namprd02.prod.outlook.com>

Hi Shay,

Good news that your Homestead node is now OK!

For your Homer node, do the log files in /var/log/homer contain any errors that help explain why it isn't starting? What happens if you run "sudo service homer restart" manually - does that print any errors?

On the Ellis node, the /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config script should create the Nginx config to redirect it to Ellis. Running 'sudo service clearwater-infrastructure' should have run that script, but what happens if you run /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config manually? Does it create a /etc/nginx/sites-available/ellis file? If not, can you send us the contents of /etc/clearwater/config, /etc/clearwater/shared_config and /etc/clearwater/local_config?

Thanks,
Rob

--
Rob Day
Software Engineer, Project Clearwater

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 01 October 2015 19:57
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] FW: Deploying Clearwater with Haet

Hi ellie,

1.       I had to copy the shared_config from ellis to homestead and now it is ok

2.       I had the same problem on homer but after copying the shared_config and restarting Clearwater-infrastructure it tells me when doing 'sudo monit status' thar homer_process failed

3.       When I point with the browser to Ellis IP address I get the welcome message of Nginx but no login screen as I am used to get. What application should run on top of Nginx?

Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:59 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi,

Is your etcd cluster healthy? What's the result of running sudo clearwater-etcdctl cluster-health? If these look OK then can you also please try running 'sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config' on the Ellis node, then the apply_shared_config script again on Homestead.

If this still doesn't work, you can copy over the shared_config file on Ellis to Homestead, run 'sudo service clearwater infrastructure' and this should unblock you. I'd be interested to dig further into why the apply_shared_config isn't working though.

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:48
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

It doesn't exist on Homestead but exists on Ellis.
I did clearwater-infrastructure restart but it didn't help.
Can we conduct a short webex, do you want me to send you webex details?
Thanks,
Shay
From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:46 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Does the /etc/clearwater/shared_config file exist on your Homestead node? Does it exist on the Ellis node?

If the shared_config file does exist, can you run 'sudo service clearwater-infrastructure restart'?

Ellie


From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:22
To: Eleanor Merry; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Thanks Eleanor,

I run the /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config

But still I get the following errors when running 'sudo monit status'

Process 'homestead_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59

Program 'poll_homestead'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Thu, 01 Oct 2015 16:10:49

Process 'homestead-prov_process'
  status                            Execution failed
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59



Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 12:54 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

It doesn't look like /etc/clearwater/shared_config has successfully propagated to the Homestead node.

In order to set up the diameter configuration file (used by Homestead) and the local_settings file (used by Homestead-prov) correctly, Homestead/Homestead-prov need information from the /etc/clearwater/shared_config file (e.g. the hs_hostname). If the shared_config file is missing; the install will succeed, but Homestead/Homestead-prov won't be functional until they've got their configuration files set up.

Looking at the template, we install the clearwater-management package after we install the Homestead/Homestead-prov packages, but don't then do anything to apply the shared configuration to the node. I've raised an issue (https://github.com/Metaswitch/clearwater-heat/issues/15) to fix this; in the meantime can you please run "sudo /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config" on all your nodes?

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 29 September 2015 16:10
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Deploying Clearwater with Haet

Hello,
I am trying deploying Clearwater on my OpenStack using the Heat templates as defined in here https://github.com/Metaswitch/clearwater-heat
I am running into issues with Homestead where I get (see below):

1.       Why LOCAL_IP=MUST BE CONFIGURED is not configured?

2.       Regarding the Diameter stack I saw an open issue https://github.com/Metaswitch/homestead/issues/73 but when deleting the certificates and starting the Clearwater-infrastructure it fails again on the same problem.

Thanks,
Shay



4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:39:55 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:39:55 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:39:55 homestead-0 monit:     exec code in run_globals
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:39:55 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:39:55 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:39:55 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:39:55 homestead-0 monit:    ...fail!
Sep 29 06:40:01 homestead-0 CRON[15030]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:40:22 homestead-0 monit: CMD /etc/init.d/homestead-prov start
Sep 29 06:40:26 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:40:26 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:40:26 homestead-0 monit:     exec code in run_globals
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:40:26 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:40:26 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:40:26 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:41:01 homestead-0 CRON[15050]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:41:03 homestead-0 monit: CMD /etc/init.d/homestead-prov restart
Sep 29 06:41:03 homestead-0 monit:  * Restarting homestead-prov homestead-prov
Sep 29 06:41:03 homestead-0 homestead[15074]: 4005 - Description: Homestead started. @@Cause: The Homestead application is starting. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1001 - Description: Diameter stack is starting. @@Cause: Diameter stack is beginning initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1002 - Description: Diameter stack initialization completed. @@Cause: Diameter stack has completed initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:41:06 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:41:06 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:41:06 homestead-0 monit:     exec code in run_globals
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151001/96e3863d/attachment.html>

From vze10n7ur at verizon.net  Thu Oct  1 23:05:13 2015
From: vze10n7ur at verizon.net (vze10n7ur at verizon.net)
Date: Thu, 01 Oct 2015 22:05:13 -0500 (CDT)
Subject: [Clearwater] bono on two interfaces for public and private
 network
Message-ID: <22131571.790321.1443755113232.JavaMail.root@vznit170060.mailsrvcs.net>

 
 Does multiple network support actually work?  I tried a few weeks ago on OpenStack and I could not get it to play along.  I wanted my VMs' eth0 to be management, their eth1 (via a namespace) to be inter-VM traffic, and some sort of floating IP arrangement for the nodes that had to expose themselves to UEs and browsers (so bono, ellis and homer).  I wanted everything nameable in DNS with specific domains for each (e.g. the floating IP for bono would resolve to, say, bono01.public.example.com whereas it's mgmt interface would be bono01.mgmt.example.com).


I read and re-read the documentation but I could not find the right combination of parameters in the configuration files to get all the nodes to talk as they should.  It feels like my case would be not too outlandish?


--keller

 
On 10/01/15, Eleanor Merry<Eleanor.Merry at metaswitch.com> wrote:
 
Hi, 

You can bind Bono to two interfaces for management and signalling (e.g. ssh access on the management network, all SIP processing on the signalling network). You can see details of how to set this up at: http://clearwater.readthedocs.org/en/stable/Multiple_Network_Support/index.html. 
[cut]



From Eleanor.Merry at metaswitch.com  Fri Oct  2 06:02:47 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 2 Oct 2015 10:02:47 +0000
Subject: [Clearwater] Manual installation does not pass any live test
In-Reply-To: <CAOVZBjBmYtnTSC1koWaEKWe1s8amgbCWVh1CaATEXCru4FwJGQ@mail.gmail.com>
References: <CAOVZBjCKd2-SaWZMiarswjXvm+QZyU9e4G3BHWPAFmt7yqqVBA@mail.gmail.com>
	<BN3PR02MB12555A46BE16D7AE218A1D719B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<CAOVZBjCLc6nrfOO5_B7UBwfH=at911+7=DGXrAt3p7HWOML-ug@mail.gmail.com>
	<BN3PR02MB1255D0D909681CA87A652FFB9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<CAOVZBjCRfk3x3YCCwkcp8B9wkzUNfnqNQBcumigqQZVmY8eodA@mail.gmail.com>
	<BN3PR02MB12550AC814AEFC577E6EFB249B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<CAOVZBjBmYtnTSC1koWaEKWe1s8amgbCWVh1CaATEXCru4FwJGQ@mail.gmail.com>
Message-ID: <BN3PR02MB12550701E04310D6530C38ED9B4B0@BN3PR02MB1255.namprd02.prod.outlook.com>

The cluster_settings file is created from the values in /etc/clearwater/local_config. Can you check that the values in your local_config file are correct for the Sprout node, and change the cluster settings file to use Sprout?s local IP, not Bono?s.

Also, what?s the result of running ?sudo monit summary? on your Sprout node?

Ellie

From: Austin Marston [mailto:marstonaustin at gmail.com]
Sent: 01 October 2015 16:23
To: Eleanor Merry
Subject: Re: [Clearwater] Manual installation does not pass any live test

I do not use cluster for the moment and I did not configured clearwater-cluster-manager.
I followed the most simple deployment following this page http://clearwater.readthedocs.org/en/stable/Manual_Install/index.html without followinf the Larger Scale Deployment part.

The /etc/clearwater/cluster_settings file was created automatically and with the 172.16.1.20 address like you found, which is my Bono host.


Austin

2015-10-01 17:02 GMT+02:00 Eleanor Merry <Eleanor.Merry at metaswitch.com<mailto:Eleanor.Merry at metaswitch.com>>:
Hi Austin,

You?re seeing repeated 401s in the tests because Sprout can?t write the authentication vectors to memcached.

On an initial register, Sprout gets an authentication vector from Homestead, stores it in memcached, then responds with a 401 challenge to the client. The client then should send it a new REGISTER with an authorization header. Sprout will then test the supplied authorisation credentials against the authentication vector it?s just stored, and if it succeeds respond with a 200.

In this case, Sprout doesn?t have a stored authentication vector (as the write to memcached failed), so it repeats the first step.

Looking at the logs in detail, we see:
01-10-2015 12:36:06.700 UTC Debug memcachedstore.cpp:916: ADD/CAS returned rc = 5 (WRITE FAILURE)
(19914928) CONNECTION FAILURE(Connection refused),  host: 172.16.1.20:11211<http://172.16.1.20:11211> -> libmemcached/connect.cc:131

This is attempting to connect to host 172.16.1.20 ? this doesn?t match the Sprout IP address you?ve put below. Can you check that the value in /etc/clearwater/cluster_settings are correct for your current system (you can see more details at http://clearwater.readthedocs.org/en/latest/Old_Manual_Install/index.html#larger-scale-deployments, Clustering Sprout).

Also, are you using our automatic clustering tool (clearwater-cluster-manager)?

Ellie

From: Austin Marston [mailto:marstonaustin at gmail.com<mailto:marstonaustin at gmail.com>]
Sent: 01 October 2015 15:13
To: Eleanor Merry
Subject: Re: [Clearwater] Manual installation does not pass any live test

??Yes I run the create_number.py script.
Indeed I can create users manually if I want to, on Ellis' interface.
I created one 2 users as my first tests before trying the live tests.

Actually, the access files are really light, but the sprout/homestead_date files are huge so here they are cut.

Thanks,
Austin

2015-10-01 15:18 GMT+02:00 Eleanor Merry <Eleanor.Merry at metaswitch.com<mailto:Eleanor.Merry at metaswitch.com>>:
The file is a bit too large. Can you just send the homestead_<date> and sprout_<date> logs (so not the access logs)?

Also, the clearwater-live-tests create their own subscribers ? you shouldn?t create any yourself (although you do need to run the create_numbers.py script on Ellis so that it has a pool of number it can provision subscribers from during the live tests ? see https://github.com/Metaswitch/ellis/blob/dev/docs/create-numbers.md for more details).

Ellie

From: Austin Marston [mailto:marstonaustin at gmail.com<mailto:marstonaustin at gmail.com>]
Sent: 01 October 2015 14:10
To: Eleanor Merry
Subject: Re: [Clearwater] Manual installation does not pass any live test

?
Hi Ellis,

You'll find as attachments in google drive the Homestead and Sprout logs corresponding to a new try of the live tests.
They are the most recent log files (a total of 4 files) in both Sprout and Homestead VMs.
-The test was run between 12:36 and 12:42 on 10/01/2015.
-172.16.1.28 is my test client
-172.16.1.21 is Sprout.
-172.16.1.23 is Homestead.
-176.16.1.29 is the DNS server.

The log level 5 was already up in Homestead and not in Sprout.

Two more notes :
-> I just found that while running all the tests, at the very end of the execution, I see this output on my test client:
Failed to delete leaked number, check Ellis logs
-> I also have issue running the sipp testing. Even thought I created the users following all steps of the "Bulk-provisioning numbers" page and run the scripts I get some 403 and 401 errors in Bono and nothing is created in the Ellis database.


Thank you so much for your help!

Austin

2015-10-01 12:42 GMT+02:00 Eleanor Merry <Eleanor.Merry at metaswitch.com<mailto:Eleanor.Merry at metaswitch.com>>:
Hi Austin,

Can you please send me the full Sprout and Homestead logs for this (as attachments). Can you also turn on debug logging - to do so create/edit the file /etc/clearwater/user_settings, add log_level=5 and then restart Sprout/Homestead (service <sprout/homestead> stop ? they?re automatically restarted by monit).

Thanks,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Austin Marston
Sent: 30 September 2015 09:04
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Manual installation does not pass any live test

Hi list,

I just manually deployed over a VMware platform five VMs respectively with bono, homer,sprout,homestead, and ellis, with no HSS lookups nor I-CSCF function.
I tried to test it with clearwater-live-test but every tests fail with the following error:
RuntimeError thrown: - Expected 200, got 401

I understood from old messages from the list that "401s are expected. The first time a client registers with Clearwater it will not provide authentication credentials. Sprout rejects the REGISTER message with a 401 response, containing an authentication challenge. The client then sends another REGISTER with credentials."
But still, I cannot get what I'm missing here even with level 5 logs.
Indeed, Bono seems to fail with both 403 and 401 errors.

In the remaining of this email I put parts of the logs of each VM to illustrate my problem.

Sprout logs:
29-09-2015 16:31:33.994 UTC Error hssconnection.cpp:147: Failed to get Authentication Vector for 2010076236 at my.local.domain<mailto:2010076236 at my.local.domain>
29-09-2015 16:31:34.005 UTC Error httpconnection.cpp:743: cURL failure with cURL error code 0 (see man 3 libcurl-errors) and HTTP error code 404
29-09-2015 16:31:34.005 UTC Error hssconnection.cpp:147: Failed to get Authentication Vector for 2010018950 at my.local.domain<mailto:2010018950 at my.local.domain>
[...]
29-09-2015 16:31:34.145 UTC Error hssconnection.cpp:147: Failed to get Authentication Vector for 2010024050 at my.local.domain<mailto:2010024050 at my.local.domain>
29-09-2015 16:31:39.414 UTC Status load_monitor.cpp:237: Maximum incoming request rate/second increased to 1675.491699 (based on a smoothed mean latency of 2810 and 0 upstream overload responses)
29-09-2015 16:31:41.596 UTC Error memcachedstore.cpp:719: Failed to write data for av\\6505550473<tel:6505550473>@my.local.domain\0826351d69f0f550 to 1 replicas
29-09-2015 16:31:41.596 UTC Error avstore.cpp:84: Failed to write Authentication Vector for private_id 6505550473<tel:6505550473>@my.local.domain
29-09-2015 16:31:41.830 UTC Error memcachedstore.cpp:514: Failed to read data for av\\6505550473<tel:6505550473>@my.local.domain\0826351d69f0f550 from 1 replicas
29-09-2015 16:31:41.830 UTC Warning authentication.cpp:242: Received an authentication request for 6505550473<tel:6505550473>@my.local.domain with nonce 0826351d69f0f550, but no matching AV found


Homer logs:
29-09-2015 16:31:33.566 UTC DEBUG base.py:283: Writing response body: {}
29-09-2015 16:31:33.566 UTC INFO base.py:269: Sending 200 response to localhost for PUT http://http_homer/org.etsi.ngn.simservs/users/sip%3A6505550980%40my.local.domain/simservs.xml
29-09-2015 16:31:39.797 UTC INFO base.py:256: Received request from localhost - PUT http://http_homer/org.etsi.ngn.simservs/users/sip%3A6505550758%40my.local.domain/simservs.xml
29-09-2015 16:31:39.798 UTC INFO xsd.py:76: Performing XSD validation
29-09-2015 16:31:39.801 UTC DEBUG base.py:283: Writing response body: {}
29-09-2015 16:31:39.802 UTC INFO base.py:269: Sending 200 response to localhost for PUT http://http_homer/org.etsi.ngn.simservs/users/sip%3A6505550758%40my.local.domain/simservs.xml
29-09-2015 16:31:43.436 UTC INFO base.py:256: Received request from localhost - PUT http://http_homer/org.etsi.ngn.simservs/users/sip%3A6505550048%40my.local.domain/simservs.xml
29-09-2015 16:31:43.436 UTC INFO xsd.py:76: Performing XSD validation
29-09-2015 16:31:43.439 UTC DEBUG base.py:283: Writing response body: {}


Homestead-prov logs:
29-09-2015 16:31:33.538 UTC INFO base.py:269: Sending 200 response to localhost for PUT http://http_homestead_prov/irs/1004b727-c478-4123-98f6-9b63802c6a8c/service_profiles/19e5a738-f3ec-4664-bd2b-958ab5a8310a/filter_criteria
29-09-2015 16:31:34.183 UTC INFO base.py:256: Received request from localhost - PUT http://http_homestead_prov/private/6505550473%40my.local.domain
29-09-2015 16:31:34.184 UTC DEBUG models.py:303: Create private ID6505550473<tel:6505550473>@my.local.domain
29-09-2015 16:31:34.185 UTC DEBUG cache.py:85: Put private ID '6505550473<tel:6505550473>@my.local.domain' into cache with AV: {'digest': {'ha1': u'33accced67a669ca7b3d7e96640ef7f4', 'realm': u'my.local.domain', 'qop': 'auth'}}
29-09-2015 16:31:34.187 UTC INFO base.py:269: Sending 200 response to localhost for PUT http://http_homestead_prov/private/6505550473%40my.local.domain


Homestead logs:
29-09-2015 16:31:33.999 UTC Debug cassandra_store.cpp:731: Failed ONE read for get_columns. Try QUORUM
29-09-2015 16:31:34.000 UTC Debug cassandra_store.cpp:432: Cassandra request failed: rc=2, Row 2010018950 at my.local.domain<mailto:2010018950 at my.local.domain> not present in column_family impi
29-09-2015 16:31:34.000 UTC Debug handlers.cpp:196: Cache query failed - reject request
29-09-2015 16:31:34.000 UTC Debug handlers.cpp:201: No cached av found for private ID 2010018950 at my.local.domain<mailto:2010018950 at my.local.domain>, public ID sip:2010018950 at my.local.domain - reject
29-09-2015 16:31:34.000 UTC Verbose httpstack.cpp:69: Sending response 404 to request for URL /impi/2010018950%40my.local.domain/av, args impu=sip%3A2010018950%40my.local.domain
29-09-2015 16:31:34.000 UTC Verbose httpstack.cpp:286: Process request for URL /impi/2010070984%40my.local.domain/av, args impu=sip%3A2010070984%40my.local.domain
29-09-2015 16:31:34.000 UTC Debug handlers.cpp:147: Parsed HTTP request: private ID 2010070984 at my.local.domain<mailto:2010070984 at my.local.domain>, public ID sip:2010070984 at my.local.domain, scheme Unknown, authorization
29-09-2015 16:31:34.000 UTC Debug handlers.cpp:167: Querying cache for authentication vector for 2010070984 at my.local.domain/sip:2010070984 at my.local.domain<mailto:2010070984 at my.local.domain/sip:2010070984 at my.local.domain>
29-09-2015 16:31:34.000 UTC Debug cassandra_store.cpp:279: Getting thread-local Client
29-09-2015 16:31:34.000 UTC Debug cache.cpp:673: Looking for authentication vector for 2010070984 at my.local.domain<mailto:2010070984 at my.local.domain>
29-09-2015 16:31:34.000 UTC Debug cache.cpp:686: Checking public ID sip:2010070984 at my.local.domain
29-09-2015 16:31:34.000 UTC Debug cache.cpp:696: Issuing cache query
29-09-2015 16:31:34.000 UTC Debug cassandra_store.cpp:731: Failed ONE read for get_columns. Try QUORUM
29-09-2015 16:31:34.001 UTC Verbose httpstack.cpp:286: Process request for URL /impi/2010052484%40my.local.domain/av, args impu=sip%3A2010052484%40my.local.domain
29-09-2015 16:31:34.001 UTC Debug handlers.cpp:147: Parsed HTTP request: private ID 2010052484 at my.local.domain<mailto:2010052484 at my.local.domain>, public ID sip:2010052484 at my.local.domain, scheme Unknown, authorization


Bono logs:
29-09-2015 16:31:33.996 UTC Call-Disconnected: CALL_ID=2010076236///401485-9633 at 172.16.1.29<mailto:401485-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.008 UTC Call-Disconnected: CALL_ID=2010018950///419639-9633 at 172.16.1.29<mailto:419639-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.008 UTC Call-Disconnected: CALL_ID=2010070984///456772-9633 at 172.16.1.29<mailto:456772-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.008 UTC Call-Disconnected: CALL_ID=2010052484///428115-9633 at 172.16.1.29<mailto:428115-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.013 UTC Call-Disconnected: CALL_ID=2010023276///389993-9633 at 172.16.1.29<mailto:389993-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.013 UTC Call-Disconnected: CALL_ID=2010004256///394841-9633 at 172.16.1.29<mailto:394841-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.014 UTC Call-Disconnected: CALL_ID=2010076066///463823-9633 at 172.16.1.29<mailto:463823-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.014 UTC Call-Disconnected: CALL_ID=2010040994///436539-9633 at 172.16.1.29<mailto:436539-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.020 UTC Call-Disconnected: CALL_ID=2010047262///392603-9633 at 172.16.1.29<mailto:392603-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.021 UTC Call-Disconnected: CALL_ID=2010070408///400941-9633 at 172.16.1.29<mailto:400941-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.024 UTC Call-Disconnected: CALL_ID=2010001186///425962-9633 at 172.16.1.29<mailto:425962-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.030 UTC Call-Disconnected: CALL_ID=2010080220///434892-9633 at 172.16.1.29<mailto:434892-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.037 UTC Call-Disconnected: CALL_ID=2010026340///453694-9633 at 172.16.1.29<mailto:453694-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.049 UTC Call-Disconnected: CALL_ID=2010059662///396784-9633 at 172.16.1.29<mailto:396784-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.051 UTC Call-Disconnected: CALL_ID=2010007964///407878-9633 at 172.16.1.29<mailto:407878-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.059 UTC Call-Disconnected: CALL_ID=2010001412///417601-9633 at 172.16.1.29<mailto:417601-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.084 UTC Call-Disconnected: CALL_ID=2010048846///437109-9633 at 172.16.1.29<mailto:437109-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.085 UTC Call-Disconnected: CALL_ID=2010031554///390144-9633 at 172.16.1.29<mailto:390144-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.092 UTC Call-Disconnected: CALL_ID=2010083556///425110-9633 at 172.16.1.29<mailto:425110-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.092 UTC Call-Disconnected: CALL_ID=2010025210///446867-9633 at 172.16.1.29<mailto:446867-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.110 UTC Call-Disconnected: CALL_ID=2010060732///434889-9633 at 172.16.1.29<mailto:434889-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.112 UTC Call-Disconnected: CALL_ID=2010020690///399131-9633 at 172.16.1.29<mailto:399131-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.120 UTC Call-Disconnected: CALL_ID=2010065600///423893-9633 at 172.16.1.29<mailto:423893-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.123 UTC Call-Disconnected: CALL_ID=2010058752///460196-9633 at 172.16.1.29<mailto:460196-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.129 UTC Call-Disconnected: CALL_ID=2010070682///453479-9633 at 172.16.1.29<mailto:453479-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.129 UTC Call-Disconnected: CALL_ID=2010046438///394971-9633 at 172.16.1.29<mailto:394971-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.142 UTC Call-Disconnected: CALL_ID=2010049994///458825-9633 at 172.16.1.29<mailto:458825-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.144 UTC Call-Disconnected: CALL_ID=2010070470///420696-9633 at 172.16.1.29<mailto:420696-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:34.146 UTC Call-Disconnected: CALL_ID=2010024050///437499-9633 at 172.16.1.29<mailto:437499-9633 at 172.16.1.29> REASON=403
29-09-2015 16:31:41.598 UTC Call-Disconnected: CALL_ID=17745eb3f9e50ecb8155e83aea81edae REASON=401
29-09-2015 16:31:41.835 UTC Call-Disconnected: CALL_ID=17745eb3f9e50ecb8155e83aea81edae REASON=401
29-09-2015 16:31:42.136 UTC Call-Disconnected: CALL_ID=17745eb3f9e50ecb8155e83aea81edae REASON=401
29-09-2015 16:31:42.437 UTC Call-Disconnected: CALL_ID=17745eb3f9e50ecb8155e83aea81edae REASON=401
29-09-2015 16:31:45.312 UTC Call-Disconnected: CALL_ID=88a59417552efe9582128c1c9d134862 REASON=401
29-09-2015 16:31:45.320 UTC Call-Disconnected: CALL_ID=88a59417552efe9582128c1c9d134862 REASON=401
29-09-2015 16:31:45.327 UTC Call-Disconnected: CALL_ID=88a59417552efe9582128c1c9d134862 REASON=401
29-09-2015 16:31:45.334 UTC Call-Disconnected: CALL_ID=88a59417552efe9582128c1c9d134862 REASON=401
29-09-2015 16:31:51.173 UTC Call-Disconnected: CALL_ID=a716eed59a862336ae71d2a329b77400 REASON=401
29-09-2015 16:31:51.465 UTC Call-Disconnected: CALL_ID=a716eed59a862336ae71d2a329b77400 REASON=401
29-09-2015 16:31:51.766 UTC Call-Disconnected: CALL_ID=a716eed59a862336ae71d2a329b77400 REASON=401


Ellis logs:
29-09-2015 16:31:32.727 UTC DEBUG utils.py:94: Still expecting 0 callbacks
29-09-2015 16:31:34.324 UTC DEBUG numbers.py:121: Number allocation API call (PSTN = false)
29-09-2015 16:31:34.650 UTC DEBUG numbers.py:134: Allocating a non-PSTN number
29-09-2015 16:31:34.653 UTC DEBUG numbers.py:148: Fetched e7d0bf0c-e00a-41c8-8bc7-5377aa76dc97
29-09-2015 16:31:35.426 UTC DEBUG numbers.py:160: Updated the owner
29-09-2015 16:31:35.427 UTC DEBUG numbers.py:130: SIP URI sip:6505550758<tel:6505550758>@my.local.domain
29-09-2015 16:31:39.686 UTC DEBUG numbers.py:159: Populating other servers...
29-09-2015 16:31:39.686 UTC DEBUG numbers.py:170: About to create private ID at Homestead
29-09-2015 16:31:39.686 UTC WARNING homestead.py:279: Passing SIP password in the clear over http
29-09-2015 16:31:39.698 UTC WARNING homestead.py:279: Passing SIP password in the clear over http
29-09-2015 16:31:39.704 UTC WARNING homestead.py:279: Passing SIP password in the clear over http
29-09-2015 16:31:39.720 UTC DEBUG numbers.py:175: Created private ID at Homestead
29-09-2015 16:31:39.721 UTC WARNING homestead.py:279: Passing SIP password in the clear over http
29-09-2015 16:31:39.794 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.015350818634033203,buffer=<_io.BytesIO object at 0x7ff1a10d6f50>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7ff1a1070d50>,effective_url='http://hs.my.local.domain:8889/private/6505550758%40my.local.domain/associated_implicit_registration_sets/1ed1c2eb-745d-4d53-8be2-68116882268e',headers={'Date': 'Tue, 29 Sep 2015 16:31:39 GMT', 'Content-Length': '0', 'Content-Type': 'text/html; charset=UTF-8', 'Connection': 'close', 'Server': 'nginx/1.4.6 (Ubuntu)'},error=None)
29-09-2015 16:31:39.794 UTC DEBUG utils.py:94: Still expecting 2 callbacks
29-09-2015 16:31:39.803 UTC DEBUG utils.py:94: Still expecting 1 callbacks
29-09-2015 16:31:39.823 UTC DEBUG utils.py:84: OK HTTP response. HTTPResponse(code=200,request_time=0.030350208282470703,buffer=<_io.BytesIO object at 0x7ff1a10d6d10>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest object at 0x7ff1a10705d0>,effective_url='http://hs.my.local.domain:8889/irs/1ed1c2eb-745d-4d53-8be2-68116882268e/service_profiles/113003da-f6f3-4efa-8612-bcb4f5be908b/filter_criteria',headers={'Date': 'Tue, 29 Sep 2015 16:31:39 GMT', 'Content-Length': '0', 'Content-Type': 'text/html; charset=UTF-8', 'Connection': 'close', 'Server': 'nginx/1.4.6 (Ubuntu)'},error=None)
29-09-2015 16:31:39.823 UTC DEBUG utils.py:92: All requests successful.

Many thanks for your help !

PS: Let me know if the logs are not clear enough or if I should put them in attachment.



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151002/5dd8765f/attachment.html>

From marstonaustin at gmail.com  Fri Oct  2 08:20:57 2015
From: marstonaustin at gmail.com (Austin Marston)
Date: Fri, 2 Oct 2015 14:20:57 +0200
Subject: [Clearwater] Manual installation does not pass any live test
In-Reply-To: <CAOVZBjCOPHmiv8pLrVfOnH2=2YypDc+WCQEH26qD0WxjVLMoeQ@mail.gmail.com>
References: <CAOVZBjCKd2-SaWZMiarswjXvm+QZyU9e4G3BHWPAFmt7yqqVBA@mail.gmail.com>
	<BN3PR02MB12555A46BE16D7AE218A1D719B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<CAOVZBjCLc6nrfOO5_B7UBwfH=at911+7=DGXrAt3p7HWOML-ug@mail.gmail.com>
	<BN3PR02MB1255D0D909681CA87A652FFB9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<CAOVZBjCRfk3x3YCCwkcp8B9wkzUNfnqNQBcumigqQZVmY8eodA@mail.gmail.com>
	<BN3PR02MB12550AC814AEFC577E6EFB249B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<CAOVZBjBmYtnTSC1koWaEKWe1s8amgbCWVh1CaATEXCru4FwJGQ@mail.gmail.com>
	<BN3PR02MB12550701E04310D6530C38ED9B4B0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<CAOVZBjCOPHmiv8pLrVfOnH2=2YypDc+WCQEH26qD0WxjVLMoeQ@mail.gmail.com>
Message-ID: <CAOVZBjA-s4wVyT4fxJCHfrWqVE81VLNo+SUph=vyt+ZxBL34MA@mail.gmail.com>

Hi list,

Finally, my problem evolved and is I hope solved.
Ellie, you correctly pinpointed that the file [sprout]
/etc/clearwater/cluster_setting was badly filed by etcd with my Bono host
address!

However in order for my cluster to work again, stop all clearwater-*
services did not work out and I had to remove and install again all
clearwater-* packages. (from
http://clearwater.readthedocs.org/en/stable/Manual_Install/index.html).

Also, given that I did not run an apt-get autoremove, some remaining files
were still in my systems.
Indeed, running "python create_numbers.py --start 6505550000 --count 1000"
returned that the numbers was already in database (Created 0 numbers, 1000
already present in database).

So I ran "python create_numbers.py --start 6505552000 --count 1000" that
returned the following: Created 1000 numbers, 0 already present in database.

Now, 90% of the tests are said to be passed!
I still have some 403 errors but it's another story.
I now have to understand how I can see whether the sipp testing is OK or
not.

Thank you so much for your time Ellie!

Yours sincerely,
Austin


2015-10-02 12:56 GMT+02:00 Austin Marston <marstonaustin at gmail.com>:

> Hello Ellie,
>
> The local_config file is ok.
> The cluster_setting file is now well configured automatically with etcd.
>
>
> Here are the results of the command :
> *[sprout]*user at cw-002:~$ sudo monit summary
>
> The Monit daemon 5.8.1 uptime: 1h 34m
>
> Process 'sprout_process'            Running
> Program 'poll_sprout_sip'           Status ok
> Program 'poll_sprout_http'          Status ok
> Process 'snmpd_process'             Running
> Process 'ntp_process'               Running
> System 'node-cw-002.svc.laas.fr'    Running
> Process 'memcached_process'         Running
> Program 'poll_memcached'            Status ok
> Process 'etcd_process'              Running
> Program 'poll_etcd_cluster'         Waiting
> Program 'poll_etcd'                 Status ok
> Process 'clearwater_diags_monitor_process' Running
> Process 'clearwater_config_manager' Running
> *Process 'clearwater_cluster_manager' Execution failed*
> Process 'chronos_process'           Running
> Program 'poll_chronos'              Status ok
> Process 'astaire_process'           Running
>
> *[homestead]*user at cw-004:~$ sudo monit summary
> [sudo] password for user:
> The Monit daemon 5.8.1 uptime: 1h 38m
>
> Process 'snmpd_process'             Running
> Process 'ntp_process'               Running
> System 'node-cw-004.svc.laas.fr'    Running
> Process 'nginx_process'             Running
> Process 'homestead_process'         Running
> Program 'poll_homestead'            Status ok
> Process 'homestead-prov_process'    Running
> Program 'poll_homestead-prov'       Status ok
> *Process 'etcd_process'              Does not exist*
> Program 'poll_etcd_cluster'         Initializing
> Program 'poll_etcd'                 Initializing
> Process 'clearwater_diags_monitor_process' Running
> Process 'clearwater_config_manager' Running
> *Process 'clearwater_cluster_manager' Running*
> Process 'cassandra_process'         Running
>
> The cluster_manager is also failed on Ellis and Bono.
>
> Austin
>
> 2015-10-02 12:02 GMT+02:00 Eleanor Merry <Eleanor.Merry at metaswitch.com>:
>
>> The cluster_settings file is created from the values in
>> /etc/clearwater/local_config. Can you check that the values in your
>> local_config file are correct for the Sprout node, and change the cluster
>> settings file to use Sprout?s local IP, not Bono?s.
>>
>>
>>
>> Also, what?s the result of running ?sudo monit summary? on your Sprout
>> node?
>>
>>
>>
>> Ellie
>>
>>
>>
>> *From:* Austin Marston [mailto:marstonaustin at gmail.com]
>> *Sent:* 01 October 2015 16:23
>>
>> *To:* Eleanor Merry
>> *Subject:* Re: [Clearwater] Manual installation does not pass any live
>> test
>>
>>
>>
>> I do not use cluster for the moment and I did not configured
>> clearwater-cluster-manager.
>>
>> I followed the most simple deployment following this page
>> http://clearwater.readthedocs.org/en/stable/Manual_Install/index.html without
>> followinf the Larger Scale Deployment part.
>>
>>
>>
>> The /etc/clearwater/cluster_settings file was created automatically and
>> with the 172.16.1.20 address like you found, which is my Bono host.
>>
>>
>>
>>
>>
>> Austin
>>
>>
>>
>> 2015-10-01 17:02 GMT+02:00 Eleanor Merry <Eleanor.Merry at metaswitch.com>:
>>
>> Hi Austin,
>>
>>
>>
>> You?re seeing repeated 401s in the tests because Sprout can?t write the
>> authentication vectors to memcached.
>>
>>
>>
>> On an initial register, Sprout gets an authentication vector from
>> Homestead, stores it in memcached, then responds with a 401 challenge to
>> the client. The client then should send it a new REGISTER with an
>> authorization header. Sprout will then test the supplied authorisation
>> credentials against the authentication vector it?s just stored, and if it
>> succeeds respond with a 200.
>>
>>
>>
>> In this case, Sprout doesn?t have a stored authentication vector (as the
>> write to memcached failed), so it repeats the first step.
>>
>>
>>
>> Looking at the logs in detail, we see:
>>
>> 01-10-2015 12:36:06.700 UTC Debug memcachedstore.cpp:916: ADD/CAS
>> returned rc = 5 (WRITE FAILURE)
>>
>> (19914928) CONNECTION FAILURE(Connection refused),  host:
>> 172.16.1.20:11211 -> libmemcached/connect.cc:131
>>
>>
>>
>> This is attempting to connect to host 172.16.1.20 ? this doesn?t match
>> the Sprout IP address you?ve put below. Can you check that the value in
>> /etc/clearwater/cluster_settings are correct for your current system (you
>> can see more details at
>> http://clearwater.readthedocs.org/en/latest/Old_Manual_Install/index.html#larger-scale-deployments,
>> Clustering Sprout).
>>
>>
>>
>> Also, are you using our automatic clustering tool
>> (clearwater-cluster-manager)?
>>
>>
>>
>> Ellie
>>
>>
>>
>> *From:* Austin Marston [mailto:marstonaustin at gmail.com]
>> *Sent:* 01 October 2015 15:13
>> *To:* Eleanor Merry
>> *Subject:* Re: [Clearwater] Manual installation does not pass any live
>> test
>>
>>
>>
>> ??Yes I run the create_number.py script.
>>
>> Indeed I can create users manually if I want to, on Ellis' interface.
>>
>> I created one 2 users as my first tests before trying the live tests.
>>
>>
>>
>> Actually, the access files are really light, but the
>> sprout/homestead_date files are huge so here they are cut.
>>
>>
>>
>> Thanks,
>>
>> Austin
>>
>>
>>
>> 2015-10-01 15:18 GMT+02:00 Eleanor Merry <Eleanor.Merry at metaswitch.com>:
>>
>> The file is a bit too large. Can you just send the homestead_<date> and
>> sprout_<date> logs (so not the access logs)?
>>
>>
>>
>> Also, the clearwater-live-tests create their own subscribers ? you
>> shouldn?t create any yourself (although you do need to run the
>> create_numbers.py script on Ellis so that it has a pool of number it can
>> provision subscribers from during the live tests ? see
>> https://github.com/Metaswitch/ellis/blob/dev/docs/create-numbers.md for
>> more details).
>>
>>
>>
>> Ellie
>>
>>
>>
>> *From:* Austin Marston [mailto:marstonaustin at gmail.com]
>> *Sent:* 01 October 2015 14:10
>> *To:* Eleanor Merry
>> *Subject:* Re: [Clearwater] Manual installation does not pass any live
>> test
>>
>>
>>
>> ?
>>
>> Hi Ellis,
>>
>>
>>
>> You'll find as attachments in google drive the Homestead and Sprout logs
>> corresponding to a new try of the live tests.
>>
>> They are the most recent log files (a total of 4 files) in both Sprout
>> and Homestead VMs.
>>
>> -The test was run between 12:36 and 12:42 on 10/01/2015.
>>
>> -172.16.1.28 is my test client
>>
>> -172.16.1.21 is Sprout.
>>
>> -172.16.1.23 is Homestead.
>>
>> -176.16.1.29 is the DNS server.
>>
>>
>>
>> The log level 5 was already up in Homestead and not in Sprout.
>>
>>
>>
>> Two more notes :
>>
>> -> I just found that while running all the tests, at the very end of the
>> execution, I see this output on my test client:
>>
>> Failed to delete leaked number, check Ellis logs
>>
>> -> I also have issue running the sipp testing. Even thought I created the
>> users following all steps of the "Bulk-provisioning numbers" page and run
>> the scripts I get some 403 and 401 errors in Bono and nothing is created in
>> the Ellis database.
>>
>>
>>
>>
>>
>> Thank you so much for your help!
>>
>>
>>
>> Austin
>>
>>
>>
>> 2015-10-01 12:42 GMT+02:00 Eleanor Merry <Eleanor.Merry at metaswitch.com>:
>>
>> Hi Austin,
>>
>>
>>
>> Can you please send me the full Sprout and Homestead logs for this (as
>> attachments). Can you also turn on debug logging - to do so create/edit the
>> file /etc/clearwater/user_settings, add log_level=5 and then restart
>> Sprout/Homestead (service <sprout/homestead> stop ? they?re automatically
>> restarted by monit).
>>
>>
>>
>> Thanks,
>>
>>
>>
>> Ellie
>>
>>
>>
>> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
>> *On Behalf Of *Austin Marston
>> *Sent:* 30 September 2015 09:04
>> *To:* clearwater at lists.projectclearwater.org
>> *Subject:* [Clearwater] Manual installation does not pass any live test
>>
>>
>>
>> Hi list,
>>
>>
>> I just manually deployed over a VMware platform five VMs respectively
>> with bono, homer,sprout,homestead, and ellis, with no HSS lookups nor
>> I-CSCF function.
>> I tried to test it with clearwater-live-test but every tests fail with
>> the following error:
>> RuntimeError thrown: - Expected 200, got
>> 401
>>
>>
>> I understood from old messages from the list that "401s are expected. The
>> first time a client registers with Clearwater it will not provide
>> authentication credentials. Sprout rejects the REGISTER message with a 401
>> response, containing an authentication challenge. The client then sends
>> another REGISTER with credentials."
>> But still, I cannot get what I'm missing here even with level 5
>> logs.
>> Indeed, Bono seems to fail with both 403 and 401
>> errors.
>>
>>
>> In the remaining of this email I put parts of the logs of each VM to
>> illustrate my problem.
>>
>>
>> *Sprout
>> logs:                                                                     *
>> 29-09-2015 16:31:33.994 UTC Error hssconnection.cpp:147: Failed to get
>> Authentication Vector for 2010076236 at my.local.domain
>> 29-09-2015 16:31:34.005 UTC Error httpconnection.cpp:743: cURL failure
>> with cURL error code 0 (see man 3 libcurl-errors) and HTTP error code 404
>> 29-09-2015 16:31:34.005 UTC Error hssconnection.cpp:147: Failed to get
>> Authentication Vector for 2010018950 at my.local.domain
>>
>> [...]
>> 29-09-2015 16:31:34.145 UTC Error hssconnection.cpp:147: Failed to get
>> Authentication Vector for 2010024050 at my.local.domain
>> 29-09-2015 16:31:39.414 UTC Status load_monitor.cpp:237: Maximum incoming
>> request rate/second increased to 1675.491699 (based on a smoothed mean
>> latency of 2810 and 0 upstream overload responses)
>> 29-09-2015 16:31:41.596 UTC Error memcachedstore.cpp:719: Failed to write
>> data for av\\6505550473 at my.local.domain\0826351d69f0f550 to 1 replicas
>> 29-09-2015 16:31:41.596 UTC Error avstore.cpp:84: Failed to write
>> Authentication Vector for private_id 6505550473 at my.local.domain
>> 29-09-2015 16:31:41.830 UTC Error memcachedstore.cpp:514: Failed to read
>> data for av\\6505550473 at my.local.domain\0826351d69f0f550 from 1 replicas
>> 29-09-2015 16:31:41.830 UTC Warning authentication.cpp:242: Received an
>> authentication request for 6505550473 at my.local.domain with nonce
>> 0826351d69f0f550, but no matching AV found
>>
>>
>>
>>
>> *Homer
>> logs:                                                                      *
>> 29-09-2015 16:31:33.566 UTC DEBUG base.py:283: Writing response body:
>> {}
>> 29-09-2015 16:31:33.566 UTC INFO base.py:269: Sending 200 response to
>> localhost for PUT
>> http://http_homer/org.etsi.ngn.simservs/users/sip%3A6505550980%40my.local.domain/simservs.xml
>> 29-09-2015 16:31:39.797 UTC INFO base.py:256: Received request from
>> localhost - PUT
>> http://http_homer/org.etsi.ngn.simservs/users/sip%3A6505550758%40my.local.domain/simservs.xml
>> 29-09-2015 16:31:39.798 UTC INFO xsd.py:76: Performing XSD
>> validation
>> 29-09-2015 16:31:39.801 UTC DEBUG base.py:283: Writing response body:
>> {}
>> 29-09-2015 16:31:39.802 UTC INFO base.py:269: Sending 200 response to
>> localhost for PUT
>> http://http_homer/org.etsi.ngn.simservs/users/sip%3A6505550758%40my.local.domain/simservs.xml
>> 29-09-2015 16:31:43.436 UTC INFO base.py:256: Received request from
>> localhost - PUT
>> http://http_homer/org.etsi.ngn.simservs/users/sip%3A6505550048%40my.local.domain/simservs.xml
>> 29-09-2015 16:31:43.436 UTC INFO xsd.py:76: Performing XSD
>> validation
>> 29-09-2015 16:31:43.439 UTC DEBUG base.py:283: Writing response body:
>> {}
>>
>>
>>
>>
>> *Homestead-prov
>> logs:                                                             *
>> 29-09-2015 16:31:33.538 UTC INFO base.py:269: Sending 200 response to
>> localhost for PUT
>> http://http_homestead_prov/irs/1004b727-c478-4123-98f6-9b63802c6a8c/service_profiles/19e5a738-f3ec-4664-bd2b-958ab5a8310a/filter_criteria
>> 29-09-2015 16:31:34.183 UTC INFO base.py:256: Received request from
>> localhost - PUT
>> http://http_homestead_prov/private/6505550473%40my.local.domain
>> 29-09-2015 16:31:34.184 UTC DEBUG models.py:303: Create private ID
>> 6505550473 at my.local.domain
>> 29-09-2015 16:31:34.185 UTC DEBUG cache.py:85: Put private ID '6505550473 at my.local.domain'
>> into cache with AV: {'digest': {'ha1': u'33accced67a669ca7b3d7e96640ef7f4',
>> 'realm': u'my.local.domain', 'qop': 'auth'}}
>> 29-09-2015 16:31:34.187 UTC INFO base.py:269: Sending 200 response to
>> localhost for PUT
>> http://http_homestead_prov/private/6505550473%40my.local.domain
>>
>>
>> *Homestead
>> logs:                                                                  *
>> 29-09-2015 16:31:33.999 UTC Debug cassandra_store.cpp:731: Failed ONE
>> read for get_columns. Try QUORUM
>> 29-09-2015 16:31:34.000 UTC Debug cassandra_store.cpp:432: Cassandra
>> request failed: rc=2, Row 2010018950 at my.local.domain not present in
>> column_family impi
>> 29-09-2015 16:31:34.000 UTC Debug handlers.cpp:196: Cache query failed -
>> reject request
>> 29-09-2015 16:31:34.000 UTC Debug handlers.cpp:201: No cached av found
>> for private ID 2010018950 at my.local.domain, public ID
>> sip:2010018950 at my.local.domain - reject
>> 29-09-2015 16:31:34.000 UTC Verbose httpstack.cpp:69: Sending response
>> 404 to request for URL /impi/2010018950%40my.local.domain/av, args
>> impu=sip%3A2010018950%40my.local.domain
>> 29-09-2015 16:31:34.000 UTC Verbose httpstack.cpp:286: Process request
>> for URL /impi/2010070984%40my.local.domain/av, args
>> impu=sip%3A2010070984%40my.local.domain
>> 29-09-2015 16:31:34.000 UTC Debug handlers.cpp:147: Parsed HTTP request:
>> private ID 2010070984 at my.local.domain, public ID
>> sip:2010070984 at my.local.domain, scheme Unknown, authorization
>> 29-09-2015 16:31:34.000 UTC Debug handlers.cpp:167: Querying cache for
>> authentication vector for
>> 2010070984 at my.local.domain/sip:2010070984 at my.local.domain
>> 29-09-2015 16:31:34.000 UTC Debug cassandra_store.cpp:279: Getting
>> thread-local Client
>> 29-09-2015 16:31:34.000 UTC Debug cache.cpp:673: Looking for
>> authentication vector for 2010070984 at my.local.domain
>> 29-09-2015 16:31:34.000 UTC Debug cache.cpp:686: Checking public ID
>> sip:2010070984 at my.local.domain
>> 29-09-2015 16:31:34.000 UTC Debug cache.cpp:696: Issuing cache
>> query
>> 29-09-2015 16:31:34.000 UTC Debug cassandra_store.cpp:731: Failed ONE
>> read for get_columns. Try QUORUM
>> 29-09-2015 16:31:34.001 UTC Verbose httpstack.cpp:286: Process request
>> for URL /impi/2010052484%40my.local.domain/av, args
>> impu=sip%3A2010052484%40my.local.domain
>> 29-09-2015 16:31:34.001 UTC Debug handlers.cpp:147: Parsed HTTP request:
>> private ID 2010052484 at my.local.domain, public ID
>> sip:2010052484 at my.local.domain, scheme Unknown, authorization
>>
>>
>>
>>
>> *Bono
>> logs:                                                                       *
>> 29-09-2015 16:31:33.996 UTC Call-Disconnected: CALL_ID=2010076236///
>> 401485-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.008 UTC Call-Disconnected: CALL_ID=2010018950///
>> 419639-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.008 UTC Call-Disconnected: CALL_ID=2010070984///
>> 456772-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.008 UTC Call-Disconnected: CALL_ID=2010052484///
>> 428115-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.013 UTC Call-Disconnected: CALL_ID=2010023276///
>> 389993-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.013 UTC Call-Disconnected: CALL_ID=2010004256///
>> 394841-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.014 UTC Call-Disconnected: CALL_ID=2010076066///
>> 463823-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.014 UTC Call-Disconnected: CALL_ID=2010040994///
>> 436539-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.020 UTC Call-Disconnected: CALL_ID=2010047262///
>> 392603-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.021 UTC Call-Disconnected: CALL_ID=2010070408///
>> 400941-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.024 UTC Call-Disconnected: CALL_ID=2010001186///
>> 425962-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.030 UTC Call-Disconnected: CALL_ID=2010080220///
>> 434892-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.037 UTC Call-Disconnected: CALL_ID=2010026340///
>> 453694-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.049 UTC Call-Disconnected: CALL_ID=2010059662///
>> 396784-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.051 UTC Call-Disconnected: CALL_ID=2010007964///
>> 407878-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.059 UTC Call-Disconnected: CALL_ID=2010001412///
>> 417601-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.084 UTC Call-Disconnected: CALL_ID=2010048846///
>> 437109-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.085 UTC Call-Disconnected: CALL_ID=2010031554///
>> 390144-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.092 UTC Call-Disconnected: CALL_ID=2010083556///
>> 425110-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.092 UTC Call-Disconnected: CALL_ID=2010025210///
>> 446867-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.110 UTC Call-Disconnected: CALL_ID=2010060732///
>> 434889-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.112 UTC Call-Disconnected: CALL_ID=2010020690///
>> 399131-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.120 UTC Call-Disconnected: CALL_ID=2010065600///
>> 423893-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.123 UTC Call-Disconnected: CALL_ID=2010058752///
>> 460196-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.129 UTC Call-Disconnected: CALL_ID=2010070682///
>> 453479-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.129 UTC Call-Disconnected: CALL_ID=2010046438///
>> 394971-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.142 UTC Call-Disconnected: CALL_ID=2010049994///
>> 458825-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.144 UTC Call-Disconnected: CALL_ID=2010070470///
>> 420696-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:34.146 UTC Call-Disconnected: CALL_ID=2010024050///
>> 437499-9633 at 172.16.1.29 REASON=403
>> 29-09-2015 16:31:41.598 UTC Call-Disconnected:
>> CALL_ID=17745eb3f9e50ecb8155e83aea81edae REASON=401
>> 29-09-2015 16:31:41.835 UTC Call-Disconnected:
>> CALL_ID=17745eb3f9e50ecb8155e83aea81edae REASON=401
>> 29-09-2015 16:31:42.136 UTC Call-Disconnected:
>> CALL_ID=17745eb3f9e50ecb8155e83aea81edae REASON=401
>> 29-09-2015 16:31:42.437 UTC Call-Disconnected:
>> CALL_ID=17745eb3f9e50ecb8155e83aea81edae REASON=401
>> 29-09-2015 16:31:45.312 UTC Call-Disconnected:
>> CALL_ID=88a59417552efe9582128c1c9d134862 REASON=401
>> 29-09-2015 16:31:45.320 UTC Call-Disconnected:
>> CALL_ID=88a59417552efe9582128c1c9d134862 REASON=401
>> 29-09-2015 16:31:45.327 UTC Call-Disconnected:
>> CALL_ID=88a59417552efe9582128c1c9d134862 REASON=401
>> 29-09-2015 16:31:45.334 UTC Call-Disconnected:
>> CALL_ID=88a59417552efe9582128c1c9d134862 REASON=401
>> 29-09-2015 16:31:51.173 UTC Call-Disconnected:
>> CALL_ID=a716eed59a862336ae71d2a329b77400 REASON=401
>> 29-09-2015 16:31:51.465 UTC Call-Disconnected:
>> CALL_ID=a716eed59a862336ae71d2a329b77400 REASON=401
>> 29-09-2015 16:31:51.766 UTC Call-Disconnected:
>> CALL_ID=a716eed59a862336ae71d2a329b77400 REASON=401
>>
>>
>>
>>
>> *Ellis
>> logs:                                                                      *
>> 29-09-2015 16:31:32.727 UTC DEBUG utils.py:94: Still expecting 0
>> callbacks
>> 29-09-2015 16:31:34.324 UTC DEBUG numbers.py:121: Number allocation API
>> call (PSTN = false)
>> 29-09-2015 16:31:34.650 UTC DEBUG numbers.py:134: Allocating a non-PSTN
>> number
>> 29-09-2015 16:31:34.653 UTC DEBUG numbers.py:148: Fetched
>> e7d0bf0c-e00a-41c8-8bc7-5377aa76dc97
>> 29-09-2015 16:31:35.426 UTC DEBUG numbers.py:160: Updated the
>> owner
>> 29-09-2015 16:31:35.427 UTC DEBUG numbers.py:130: SIP URI sip:6505550758
>> @my.local.domain
>> 29-09-2015 16:31:39.686 UTC DEBUG numbers.py:159: Populating other
>> servers...
>> 29-09-2015 16:31:39.686 UTC DEBUG numbers.py:170: About to create private
>> ID at Homestead
>> 29-09-2015 16:31:39.686 UTC WARNING homestead.py:279: Passing SIP
>> password in the clear over http
>> 29-09-2015 16:31:39.698 UTC WARNING homestead.py:279: Passing SIP
>> password in the clear over http
>> 29-09-2015 16:31:39.704 UTC WARNING homestead.py:279: Passing SIP
>> password in the clear over http
>> 29-09-2015 16:31:39.720 UTC DEBUG numbers.py:175: Created private ID at
>> Homestead
>> 29-09-2015 16:31:39.721 UTC WARNING homestead.py:279: Passing SIP
>> password in the clear over http
>> 29-09-2015 16:31:39.794 UTC DEBUG utils.py:84: OK HTTP response.
>> HTTPResponse(code=200,request_time=0.015350818634033203,buffer=<_io.BytesIO
>> object at
>> 0x7ff1a10d6f50>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest
>> object at 0x7ff1a1070d50>,effective_url='
>> http://hs.my.local.domain:8889/private/6505550758%40my.local.domain/associated_implicit_registration_sets/1ed1c2eb-745d-4d53-8be2-68116882268e',headers={'Date':
>> 'Tue, 29 Sep 2015 16:31:39 GMT', 'Content-Length': '0', 'Content-Type':
>> 'text/html; charset=UTF-8', 'Connection': 'close', 'Server': 'nginx/1.4.6
>> (Ubuntu)'},error=None)
>> 29-09-2015 16:31:39.794 UTC DEBUG utils.py:94: Still expecting 2
>> callbacks
>> 29-09-2015 16:31:39.803 UTC DEBUG utils.py:94: Still expecting 1
>> callbacks
>> 29-09-2015 16:31:39.823 UTC DEBUG utils.py:84: OK HTTP response.
>> HTTPResponse(code=200,request_time=0.030350208282470703,buffer=<_io.BytesIO
>> object at
>> 0x7ff1a10d6d10>,_body=None,time_info={},request=<tornado.httpclient.HTTPRequest
>> object at 0x7ff1a10705d0>,effective_url='
>> http://hs.my.local.domain:8889/irs/1ed1c2eb-745d-4d53-8be2-68116882268e/service_profiles/113003da-f6f3-4efa-8612-bcb4f5be908b/filter_criteria',headers={'Date':
>> 'Tue, 29 Sep 2015 16:31:39 GMT', 'Content-Length': '0', 'Content-Type':
>> 'text/html; charset=UTF-8', 'Connection': 'close', 'Server': 'nginx/1.4.6
>> (Ubuntu)'},error=None)
>> 29-09-2015 16:31:39.823 UTC DEBUG utils.py:92: All requests
>> successful.
>>
>>
>> Many thanks for your help
>> !
>>
>>
>> PS: Let me know if the logs are not clear enough or if I should put them
>> in attachment.
>>
>>
>>
>>
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151002/3edc024c/attachment.html>

From shayn at gigaspaces.com  Fri Oct  2 10:26:47 2015
From: shayn at gigaspaces.com (Shay Naeh)
Date: Fri, 2 Oct 2015 14:26:47 +0000
Subject: [Clearwater] Deploying Clearwater with Haet
In-Reply-To: <CY1PR0201MB0924E05F204B2DB63A4E8A66F44C0@CY1PR0201MB0924.namprd02.prod.outlook.com>
References: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
	<BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069A6C79056AFC6762139BDC74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB125571D8C63B36001E324CD99B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069FD082C705B75402A4DA2C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB1255BC0AB2D26E2CBF1ADC849B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069C562DDF2E1B656EE7AD7C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<CY1PR0201MB0924E05F204B2DB63A4E8A66F44C0@CY1PR0201MB0924.namprd02.prod.outlook.com>
Message-ID: <AMXPR04MB069478BA1E007031294568DC74B0@AMXPR04MB069.eurprd04.prod.outlook.com>

Hi Rob and Ellie,

1.       Sudo service homer restart didn't help, but Sudo monit start homer_proccess worked just fine.

2.       Regarding ellis I played with it and what helped bringing up the Login screen was after provisioning additional phone numbers (don't understand why it is related).

3.       Now I encounter an issue after the login, I get the error 'failed to update the server (see detailed diagnostics in the developer console)". I think I saw an error like that that was related to the Cassandra DB tables but I suspect that some configuration regarding IPs or FQDNS is wrong. In ellis log files I find the following uncaught exception.
Thanks,
Shay

02-10-2015 14:15:22.080 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.18ms
02-10-2015 14:16:02.117 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.37ms
02-10-2015 14:16:42.169 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.46ms
02-10-2015 14:17:22.213 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.28ms
02-10-2015 14:17:40.600 UTC INFO web.py:1447: 200 GET /css/bootstrap-responsive.css (0.0.0.0) 2.12ms
02-10-2015 14:17:40.822 UTC INFO web.py:1447: 200 GET /css/style.css (0.0.0.0) 1.49ms
02-10-2015 14:17:40.954 UTC INFO web.py:1447: 200 GET /js/jquery.cookie.js (0.0.0.0) 0.66ms
02-10-2015 14:17:41.133 UTC INFO web.py:1447: 200 GET /js/app.js (0.0.0.0) 5.38ms
02-10-2015 14:17:43.362 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:03.366 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:23.371 UTC WARNING utils.py:78: Non-OK HTTP response. HTTP 599: Timeout
02-10-2015 14:18:23.371 UTC WARNING numbers.py:195: Failed to update all the backends
02-10-2015 14:18:23.371 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:23.372 UTC WARNING utils.py:78: Non-OK HTTP response. HTTP 599: Timeout
02-10-2015 14:18:23.373 UTC ERROR iostream.py:307: Uncaught exception, closing connection.
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/iostream.py", line 304, in wrapper
    callback(*args)
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/httpserver.py", line 243, in _on_headers
    remote_ip = self.address[0]
IndexError: string index out of range
02-10-2015 14:18:23.373 UTC ERROR ioloop.py:435: Exception in callback <tornado.stack_context._StackContextWrapper object at 0x7fc2624aedb8>
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/ioloop.py", line 421, in _run_callback
    callback()
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/iostream.py", line 304, in wrapper
    callback(*args)
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/httpserver.py", line 243, in _on_headers
    remote_ip = self.address[0]

From: Robert Day [mailto:Robert.Day at metaswitch.com]
Sent: Thursday, October 1, 2015 10:41 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

Good news that your Homestead node is now OK!

For your Homer node, do the log files in /var/log/homer contain any errors that help explain why it isn't starting? What happens if you run "sudo service homer restart" manually - does that print any errors?

On the Ellis node, the /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config script should create the Nginx config to redirect it to Ellis. Running 'sudo service clearwater-infrastructure' should have run that script, but what happens if you run /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config manually? Does it create a /etc/nginx/sites-available/ellis file? If not, can you send us the contents of /etc/clearwater/config, /etc/clearwater/shared_config and /etc/clearwater/local_config?

Thanks,
Rob

--
Rob Day
Software Engineer, Project Clearwater

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 01 October 2015 19:57
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] FW: Deploying Clearwater with Haet

Hi ellie,

1.       I had to copy the shared_config from ellis to homestead and now it is ok

2.       I had the same problem on homer but after copying the shared_config and restarting Clearwater-infrastructure it tells me when doing 'sudo monit status' thar homer_process failed

3.       When I point with the browser to Ellis IP address I get the welcome message of Nginx but no login screen as I am used to get. What application should run on top of Nginx?

Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:59 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi,

Is your etcd cluster healthy? What's the result of running sudo clearwater-etcdctl cluster-health? If these look OK then can you also please try running 'sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config' on the Ellis node, then the apply_shared_config script again on Homestead.

If this still doesn't work, you can copy over the shared_config file on Ellis to Homestead, run 'sudo service clearwater infrastructure' and this should unblock you. I'd be interested to dig further into why the apply_shared_config isn't working though.

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:48
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

It doesn't exist on Homestead but exists on Ellis.
I did clearwater-infrastructure restart but it didn't help.
Can we conduct a short webex, do you want me to send you webex details?
Thanks,
Shay
From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:46 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Does the /etc/clearwater/shared_config file exist on your Homestead node? Does it exist on the Ellis node?

If the shared_config file does exist, can you run 'sudo service clearwater-infrastructure restart'?

Ellie


From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:22
To: Eleanor Merry; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Thanks Eleanor,

I run the /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config

But still I get the following errors when running 'sudo monit status'

Process 'homestead_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59

Program 'poll_homestead'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Thu, 01 Oct 2015 16:10:49

Process 'homestead-prov_process'
  status                            Execution failed
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59



Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 12:54 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

It doesn't look like /etc/clearwater/shared_config has successfully propagated to the Homestead node.

In order to set up the diameter configuration file (used by Homestead) and the local_settings file (used by Homestead-prov) correctly, Homestead/Homestead-prov need information from the /etc/clearwater/shared_config file (e.g. the hs_hostname). If the shared_config file is missing; the install will succeed, but Homestead/Homestead-prov won't be functional until they've got their configuration files set up.

Looking at the template, we install the clearwater-management package after we install the Homestead/Homestead-prov packages, but don't then do anything to apply the shared configuration to the node. I've raised an issue (https://github.com/Metaswitch/clearwater-heat/issues/15) to fix this; in the meantime can you please run "sudo /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config" on all your nodes?

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 29 September 2015 16:10
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Deploying Clearwater with Haet

Hello,
I am trying deploying Clearwater on my OpenStack using the Heat templates as defined in here https://github.com/Metaswitch/clearwater-heat
I am running into issues with Homestead where I get (see below):

1.       Why LOCAL_IP=MUST BE CONFIGURED is not configured?

2.       Regarding the Diameter stack I saw an open issue https://github.com/Metaswitch/homestead/issues/73 but when deleting the certificates and starting the Clearwater-infrastructure it fails again on the same problem.

Thanks,
Shay



4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:39:55 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:39:55 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:39:55 homestead-0 monit:     exec code in run_globals
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:39:55 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:39:55 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:39:55 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:39:55 homestead-0 monit:    ...fail!
Sep 29 06:40:01 homestead-0 CRON[15030]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:40:22 homestead-0 monit: CMD /etc/init.d/homestead-prov start
Sep 29 06:40:26 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:40:26 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:40:26 homestead-0 monit:     exec code in run_globals
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:40:26 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:40:26 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:40:26 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:41:01 homestead-0 CRON[15050]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:41:03 homestead-0 monit: CMD /etc/init.d/homestead-prov restart
Sep 29 06:41:03 homestead-0 monit:  * Restarting homestead-prov homestead-prov
Sep 29 06:41:03 homestead-0 homestead[15074]: 4005 - Description: Homestead started. @@Cause: The Homestead application is starting. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1001 - Description: Diameter stack is starting. @@Cause: Diameter stack is beginning initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1002 - Description: Diameter stack initialization completed. @@Cause: Diameter stack has completed initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:41:06 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:41:06 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:41:06 homestead-0 monit:     exec code in run_globals
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151002/adb6c56a/attachment.html>

From Eleanor.Merry at metaswitch.com  Fri Oct  2 11:07:27 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 2 Oct 2015 15:07:27 +0000
Subject: [Clearwater] Deploying Clearwater with Haet
In-Reply-To: <AMXPR04MB069478BA1E007031294568DC74B0@AMXPR04MB069.eurprd04.prod.outlook.com>
References: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
	<BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069A6C79056AFC6762139BDC74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB125571D8C63B36001E324CD99B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069FD082C705B75402A4DA2C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB1255BC0AB2D26E2CBF1ADC849B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069C562DDF2E1B656EE7AD7C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<CY1PR0201MB0924E05F204B2DB63A4E8A66F44C0@CY1PR0201MB0924.namprd02.prod.outlook.com>
	<AMXPR04MB069478BA1E007031294568DC74B0@AMXPR04MB069.eurprd04.prod.outlook.com>
Message-ID: <BLUPR02MB1251451BCECFA22EEE0EAB129B4B0@BLUPR02MB1251.namprd02.prod.outlook.com>

Hi,

It looks like there is a corrupted number in the Ellis/Homestead databases. Can you please remove the numbers? You can do this by running:

To delete all numbers on Ellis, run the following:
sudo mysql
USE ellis
UPDATE numbers SET owner_id=NULL;

To delete all numbers on Homestead, run the following:

cqlsh
USE homestead_provisioning;
TRUNCATE public;
TRUNCATE private;
TRUNCATE service_profiles;
TRUNCATE implicit_registration_sets;
USE homestead_cache;
TRUNCATE impi;
TRUNCATE impu;
TRUNCATE impi_mapping;

To delete all numbers on Homer, run the following:

cqlsh
USE homer;
TRUNCATE simservs;

Can you also please confirm that the shared_configuration file is present on all your nodes?

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 02 October 2015 15:27
To: Robert Day; clearwater at lists.projectclearwater.org
Cc: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

Hi Rob and Ellie,

1.       Sudo service homer restart didn't help, but Sudo monit start homer_proccess worked just fine.

2.       Regarding ellis I played with it and what helped bringing up the Login screen was after provisioning additional phone numbers (don't understand why it is related).

3.       Now I encounter an issue after the login, I get the error 'failed to update the server (see detailed diagnostics in the developer console)". I think I saw an error like that that was related to the Cassandra DB tables but I suspect that some configuration regarding IPs or FQDNS is wrong. In ellis log files I find the following uncaught exception.
Thanks,
Shay

02-10-2015 14:15:22.080 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.18ms
02-10-2015 14:16:02.117 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.37ms
02-10-2015 14:16:42.169 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.46ms
02-10-2015 14:17:22.213 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.28ms
02-10-2015 14:17:40.600 UTC INFO web.py:1447: 200 GET /css/bootstrap-responsive.css (0.0.0.0) 2.12ms
02-10-2015 14:17:40.822 UTC INFO web.py:1447: 200 GET /css/style.css (0.0.0.0) 1.49ms
02-10-2015 14:17:40.954 UTC INFO web.py:1447: 200 GET /js/jquery.cookie.js (0.0.0.0) 0.66ms
02-10-2015 14:17:41.133 UTC INFO web.py:1447: 200 GET /js/app.js (0.0.0.0) 5.38ms
02-10-2015 14:17:43.362 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:03.366 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:23.371 UTC WARNING utils.py:78: Non-OK HTTP response. HTTP 599: Timeout
02-10-2015 14:18:23.371 UTC WARNING numbers.py:195: Failed to update all the backends
02-10-2015 14:18:23.371 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:23.372 UTC WARNING utils.py:78: Non-OK HTTP response. HTTP 599: Timeout
02-10-2015 14:18:23.373 UTC ERROR iostream.py:307: Uncaught exception, closing connection.
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/iostream.py", line 304, in wrapper
    callback(*args)
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/httpserver.py", line 243, in _on_headers
    remote_ip = self.address[0]
IndexError: string index out of range
02-10-2015 14:18:23.373 UTC ERROR ioloop.py:435: Exception in callback <tornado.stack_context._StackContextWrapper object at 0x7fc2624aedb8>
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/ioloop.py", line 421, in _run_callback
    callback()
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/iostream.py", line 304, in wrapper
    callback(*args)
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/httpserver.py", line 243, in _on_headers
    remote_ip = self.address[0]

From: Robert Day [mailto:Robert.Day at metaswitch.com]
Sent: Thursday, October 1, 2015 10:41 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

Good news that your Homestead node is now OK!

For your Homer node, do the log files in /var/log/homer contain any errors that help explain why it isn't starting? What happens if you run "sudo service homer restart" manually - does that print any errors?

On the Ellis node, the /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config script should create the Nginx config to redirect it to Ellis. Running 'sudo service clearwater-infrastructure' should have run that script, but what happens if you run /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config manually? Does it create a /etc/nginx/sites-available/ellis file? If not, can you send us the contents of /etc/clearwater/config, /etc/clearwater/shared_config and /etc/clearwater/local_config?

Thanks,
Rob

--
Rob Day
Software Engineer, Project Clearwater

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 01 October 2015 19:57
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] FW: Deploying Clearwater with Haet

Hi ellie,

1.       I had to copy the shared_config from ellis to homestead and now it is ok

2.       I had the same problem on homer but after copying the shared_config and restarting Clearwater-infrastructure it tells me when doing 'sudo monit status' thar homer_process failed

3.       When I point with the browser to Ellis IP address I get the welcome message of Nginx but no login screen as I am used to get. What application should run on top of Nginx?

Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:59 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi,

Is your etcd cluster healthy? What's the result of running sudo clearwater-etcdctl cluster-health? If these look OK then can you also please try running 'sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config' on the Ellis node, then the apply_shared_config script again on Homestead.

If this still doesn't work, you can copy over the shared_config file on Ellis to Homestead, run 'sudo service clearwater infrastructure' and this should unblock you. I'd be interested to dig further into why the apply_shared_config isn't working though.

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:48
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

It doesn't exist on Homestead but exists on Ellis.
I did clearwater-infrastructure restart but it didn't help.
Can we conduct a short webex, do you want me to send you webex details?
Thanks,
Shay
From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:46 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Does the /etc/clearwater/shared_config file exist on your Homestead node? Does it exist on the Ellis node?

If the shared_config file does exist, can you run 'sudo service clearwater-infrastructure restart'?

Ellie


From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:22
To: Eleanor Merry; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Thanks Eleanor,

I run the /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config

But still I get the following errors when running 'sudo monit status'

Process 'homestead_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59

Program 'poll_homestead'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Thu, 01 Oct 2015 16:10:49

Process 'homestead-prov_process'
  status                            Execution failed
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59



Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 12:54 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

It doesn't look like /etc/clearwater/shared_config has successfully propagated to the Homestead node.

In order to set up the diameter configuration file (used by Homestead) and the local_settings file (used by Homestead-prov) correctly, Homestead/Homestead-prov need information from the /etc/clearwater/shared_config file (e.g. the hs_hostname). If the shared_config file is missing; the install will succeed, but Homestead/Homestead-prov won't be functional until they've got their configuration files set up.

Looking at the template, we install the clearwater-management package after we install the Homestead/Homestead-prov packages, but don't then do anything to apply the shared configuration to the node. I've raised an issue (https://github.com/Metaswitch/clearwater-heat/issues/15) to fix this; in the meantime can you please run "sudo /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config" on all your nodes?

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 29 September 2015 16:10
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Deploying Clearwater with Haet

Hello,
I am trying deploying Clearwater on my OpenStack using the Heat templates as defined in here https://github.com/Metaswitch/clearwater-heat
I am running into issues with Homestead where I get (see below):

1.       Why LOCAL_IP=MUST BE CONFIGURED is not configured?

2.       Regarding the Diameter stack I saw an open issue https://github.com/Metaswitch/homestead/issues/73 but when deleting the certificates and starting the Clearwater-infrastructure it fails again on the same problem.

Thanks,
Shay



4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:39:55 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:39:55 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:39:55 homestead-0 monit:     exec code in run_globals
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:39:55 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:39:55 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:39:55 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:39:55 homestead-0 monit:    ...fail!
Sep 29 06:40:01 homestead-0 CRON[15030]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:40:22 homestead-0 monit: CMD /etc/init.d/homestead-prov start
Sep 29 06:40:26 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:40:26 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:40:26 homestead-0 monit:     exec code in run_globals
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:40:26 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:40:26 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:40:26 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:41:01 homestead-0 CRON[15050]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:41:03 homestead-0 monit: CMD /etc/init.d/homestead-prov restart
Sep 29 06:41:03 homestead-0 monit:  * Restarting homestead-prov homestead-prov
Sep 29 06:41:03 homestead-0 homestead[15074]: 4005 - Description: Homestead started. @@Cause: The Homestead application is starting. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1001 - Description: Diameter stack is starting. @@Cause: Diameter stack is beginning initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1002 - Description: Diameter stack initialization completed. @@Cause: Diameter stack has completed initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:41:06 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:41:06 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:41:06 homestead-0 monit:     exec code in run_globals
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151002/4086a89d/attachment.html>

From shayn at gigaspaces.com  Fri Oct  2 11:25:17 2015
From: shayn at gigaspaces.com (Shay Naeh)
Date: Fri, 2 Oct 2015 15:25:17 +0000
Subject: [Clearwater] Deploying Clearwater with Haet
In-Reply-To: <BLUPR02MB1251451BCECFA22EEE0EAB129B4B0@BLUPR02MB1251.namprd02.prod.outlook.com>
References: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
	<BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069A6C79056AFC6762139BDC74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB125571D8C63B36001E324CD99B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069FD082C705B75402A4DA2C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB1255BC0AB2D26E2CBF1ADC849B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069C562DDF2E1B656EE7AD7C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<CY1PR0201MB0924E05F204B2DB63A4E8A66F44C0@CY1PR0201MB0924.namprd02.prod.outlook.com>
	<AMXPR04MB069478BA1E007031294568DC74B0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BLUPR02MB1251451BCECFA22EEE0EAB129B4B0@BLUPR02MB1251.namprd02.prod.outlook.com>
Message-ID: <AMXPR04MB06922DB70876C9D75BD2ADCC74B0@AMXPR04MB069.eurprd04.prod.outlook.com>

I've deleted all the numbers and when login to ellis it tells me you have no numbers, create one.
When trying to create I get the same error failed to update the server.
And yes I have shared_config on all nodes

Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Friday, October 2, 2015 6:07 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org
Subject: RE: Deploying Clearwater with Haet

Hi,

It looks like there is a corrupted number in the Ellis/Homestead databases. Can you please remove the numbers? You can do this by running:

To delete all numbers on Ellis, run the following:
sudo mysql
USE ellis
UPDATE numbers SET owner_id=NULL;

To delete all numbers on Homestead, run the following:

cqlsh
USE homestead_provisioning;
TRUNCATE public;
TRUNCATE private;
TRUNCATE service_profiles;
TRUNCATE implicit_registration_sets;
USE homestead_cache;
TRUNCATE impi;
TRUNCATE impu;
TRUNCATE impi_mapping;

To delete all numbers on Homer, run the following:

cqlsh
USE homer;
TRUNCATE simservs;

Can you also please confirm that the shared_configuration file is present on all your nodes?

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 02 October 2015 15:27
To: Robert Day; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

Hi Rob and Ellie,

1.       Sudo service homer restart didn't help, but Sudo monit start homer_proccess worked just fine.

2.       Regarding ellis I played with it and what helped bringing up the Login screen was after provisioning additional phone numbers (don't understand why it is related).

3.       Now I encounter an issue after the login, I get the error 'failed to update the server (see detailed diagnostics in the developer console)". I think I saw an error like that that was related to the Cassandra DB tables but I suspect that some configuration regarding IPs or FQDNS is wrong. In ellis log files I find the following uncaught exception.
Thanks,
Shay

02-10-2015 14:15:22.080 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.18ms
02-10-2015 14:16:02.117 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.37ms
02-10-2015 14:16:42.169 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.46ms
02-10-2015 14:17:22.213 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.28ms
02-10-2015 14:17:40.600 UTC INFO web.py:1447: 200 GET /css/bootstrap-responsive.css (0.0.0.0) 2.12ms
02-10-2015 14:17:40.822 UTC INFO web.py:1447: 200 GET /css/style.css (0.0.0.0) 1.49ms
02-10-2015 14:17:40.954 UTC INFO web.py:1447: 200 GET /js/jquery.cookie.js (0.0.0.0) 0.66ms
02-10-2015 14:17:41.133 UTC INFO web.py:1447: 200 GET /js/app.js (0.0.0.0) 5.38ms
02-10-2015 14:17:43.362 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:03.366 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:23.371 UTC WARNING utils.py:78: Non-OK HTTP response. HTTP 599: Timeout
02-10-2015 14:18:23.371 UTC WARNING numbers.py:195: Failed to update all the backends
02-10-2015 14:18:23.371 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:23.372 UTC WARNING utils.py:78: Non-OK HTTP response. HTTP 599: Timeout
02-10-2015 14:18:23.373 UTC ERROR iostream.py:307: Uncaught exception, closing connection.
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/iostream.py", line 304, in wrapper
    callback(*args)
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/httpserver.py", line 243, in _on_headers
    remote_ip = self.address[0]
IndexError: string index out of range
02-10-2015 14:18:23.373 UTC ERROR ioloop.py:435: Exception in callback <tornado.stack_context._StackContextWrapper object at 0x7fc2624aedb8>
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/ioloop.py", line 421, in _run_callback
    callback()
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/iostream.py", line 304, in wrapper
    callback(*args)
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/httpserver.py", line 243, in _on_headers
    remote_ip = self.address[0]

From: Robert Day [mailto:Robert.Day at metaswitch.com]
Sent: Thursday, October 1, 2015 10:41 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

Good news that your Homestead node is now OK!

For your Homer node, do the log files in /var/log/homer contain any errors that help explain why it isn't starting? What happens if you run "sudo service homer restart" manually - does that print any errors?

On the Ellis node, the /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config script should create the Nginx config to redirect it to Ellis. Running 'sudo service clearwater-infrastructure' should have run that script, but what happens if you run /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config manually? Does it create a /etc/nginx/sites-available/ellis file? If not, can you send us the contents of /etc/clearwater/config, /etc/clearwater/shared_config and /etc/clearwater/local_config?

Thanks,
Rob

--
Rob Day
Software Engineer, Project Clearwater

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 01 October 2015 19:57
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] FW: Deploying Clearwater with Haet

Hi ellie,

1.       I had to copy the shared_config from ellis to homestead and now it is ok

2.       I had the same problem on homer but after copying the shared_config and restarting Clearwater-infrastructure it tells me when doing 'sudo monit status' thar homer_process failed

3.       When I point with the browser to Ellis IP address I get the welcome message of Nginx but no login screen as I am used to get. What application should run on top of Nginx?

Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:59 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi,

Is your etcd cluster healthy? What's the result of running sudo clearwater-etcdctl cluster-health? If these look OK then can you also please try running 'sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config' on the Ellis node, then the apply_shared_config script again on Homestead.

If this still doesn't work, you can copy over the shared_config file on Ellis to Homestead, run 'sudo service clearwater infrastructure' and this should unblock you. I'd be interested to dig further into why the apply_shared_config isn't working though.

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:48
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

It doesn't exist on Homestead but exists on Ellis.
I did clearwater-infrastructure restart but it didn't help.
Can we conduct a short webex, do you want me to send you webex details?
Thanks,
Shay
From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:46 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Does the /etc/clearwater/shared_config file exist on your Homestead node? Does it exist on the Ellis node?

If the shared_config file does exist, can you run 'sudo service clearwater-infrastructure restart'?

Ellie


From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:22
To: Eleanor Merry; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Thanks Eleanor,

I run the /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config

But still I get the following errors when running 'sudo monit status'

Process 'homestead_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59

Program 'poll_homestead'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Thu, 01 Oct 2015 16:10:49

Process 'homestead-prov_process'
  status                            Execution failed
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59



Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 12:54 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

It doesn't look like /etc/clearwater/shared_config has successfully propagated to the Homestead node.

In order to set up the diameter configuration file (used by Homestead) and the local_settings file (used by Homestead-prov) correctly, Homestead/Homestead-prov need information from the /etc/clearwater/shared_config file (e.g. the hs_hostname). If the shared_config file is missing; the install will succeed, but Homestead/Homestead-prov won't be functional until they've got their configuration files set up.

Looking at the template, we install the clearwater-management package after we install the Homestead/Homestead-prov packages, but don't then do anything to apply the shared configuration to the node. I've raised an issue (https://github.com/Metaswitch/clearwater-heat/issues/15) to fix this; in the meantime can you please run "sudo /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config" on all your nodes?

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 29 September 2015 16:10
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Deploying Clearwater with Haet

Hello,
I am trying deploying Clearwater on my OpenStack using the Heat templates as defined in here https://github.com/Metaswitch/clearwater-heat
I am running into issues with Homestead where I get (see below):

1.       Why LOCAL_IP=MUST BE CONFIGURED is not configured?

2.       Regarding the Diameter stack I saw an open issue https://github.com/Metaswitch/homestead/issues/73 but when deleting the certificates and starting the Clearwater-infrastructure it fails again on the same problem.

Thanks,
Shay



4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:39:55 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:39:55 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:39:55 homestead-0 monit:     exec code in run_globals
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:39:55 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:39:55 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:39:55 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:39:55 homestead-0 monit:    ...fail!
Sep 29 06:40:01 homestead-0 CRON[15030]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:40:22 homestead-0 monit: CMD /etc/init.d/homestead-prov start
Sep 29 06:40:26 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:40:26 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:40:26 homestead-0 monit:     exec code in run_globals
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:40:26 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:40:26 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:40:26 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:41:01 homestead-0 CRON[15050]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:41:03 homestead-0 monit: CMD /etc/init.d/homestead-prov restart
Sep 29 06:41:03 homestead-0 monit:  * Restarting homestead-prov homestead-prov
Sep 29 06:41:03 homestead-0 homestead[15074]: 4005 - Description: Homestead started. @@Cause: The Homestead application is starting. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1001 - Description: Diameter stack is starting. @@Cause: Diameter stack is beginning initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1002 - Description: Diameter stack initialization completed. @@Cause: Diameter stack has completed initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:41:06 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:41:06 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:41:06 homestead-0 monit:     exec code in run_globals
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151002/29185070/attachment.html>

From Eleanor.Merry at metaswitch.com  Fri Oct  2 11:35:07 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 2 Oct 2015 15:35:07 +0000
Subject: [Clearwater] Deploying Clearwater with Haet
In-Reply-To: <AMXPR04MB06922DB70876C9D75BD2ADCC74B0@AMXPR04MB069.eurprd04.prod.outlook.com>
References: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
	<BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069A6C79056AFC6762139BDC74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB125571D8C63B36001E324CD99B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069FD082C705B75402A4DA2C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB1255BC0AB2D26E2CBF1ADC849B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069C562DDF2E1B656EE7AD7C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<CY1PR0201MB0924E05F204B2DB63A4E8A66F44C0@CY1PR0201MB0924.namprd02.prod.outlook.com>
	<AMXPR04MB069478BA1E007031294568DC74B0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BLUPR02MB1251451BCECFA22EEE0EAB129B4B0@BLUPR02MB1251.namprd02.prod.outlook.com>
	<AMXPR04MB06922DB70876C9D75BD2ADCC74B0@AMXPR04MB069.eurprd04.prod.outlook.com>
Message-ID: <BLUPR02MB12514381B2F4D8066BCCA0149B4B0@BLUPR02MB1251.namprd02.prod.outlook.com>

Ok, can you please send me the debug logs from Ellis? This will help narrow down whether the problem is with Ellis, Homer or Homestead. To turn on debug logging for Ellis, write LOG_LEVEL = logging.DEBUG to the local_settings.py file (at /usr/share/clearwater/ellis/src/metaswitch/ellis/local_settings.py). Then restart clearwater-infrastructure (sudo service clearwater-infrastructure restart), and restart Ellis (sudo service ellis stop - it will be restarted by monit).

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 02 October 2015 16:25
To: Eleanor Merry; clearwater at lists.projectclearwater.org
Subject: RE: Deploying Clearwater with Haet

I've deleted all the numbers and when login to ellis it tells me you have no numbers, create one.
When trying to create I get the same error failed to update the server.
And yes I have shared_config on all nodes

Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Friday, October 2, 2015 6:07 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi,

It looks like there is a corrupted number in the Ellis/Homestead databases. Can you please remove the numbers? You can do this by running:

To delete all numbers on Ellis, run the following:
sudo mysql
USE ellis
UPDATE numbers SET owner_id=NULL;

To delete all numbers on Homestead, run the following:

cqlsh
USE homestead_provisioning;
TRUNCATE public;
TRUNCATE private;
TRUNCATE service_profiles;
TRUNCATE implicit_registration_sets;
USE homestead_cache;
TRUNCATE impi;
TRUNCATE impu;
TRUNCATE impi_mapping;

To delete all numbers on Homer, run the following:

cqlsh
USE homer;
TRUNCATE simservs;

Can you also please confirm that the shared_configuration file is present on all your nodes?

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 02 October 2015 15:27
To: Robert Day; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

Hi Rob and Ellie,

1.       Sudo service homer restart didn't help, but Sudo monit start homer_proccess worked just fine.

2.       Regarding ellis I played with it and what helped bringing up the Login screen was after provisioning additional phone numbers (don't understand why it is related).

3.       Now I encounter an issue after the login, I get the error 'failed to update the server (see detailed diagnostics in the developer console)". I think I saw an error like that that was related to the Cassandra DB tables but I suspect that some configuration regarding IPs or FQDNS is wrong. In ellis log files I find the following uncaught exception.
Thanks,
Shay

02-10-2015 14:15:22.080 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.18ms
02-10-2015 14:16:02.117 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.37ms
02-10-2015 14:16:42.169 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.46ms
02-10-2015 14:17:22.213 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.28ms
02-10-2015 14:17:40.600 UTC INFO web.py:1447: 200 GET /css/bootstrap-responsive.css (0.0.0.0) 2.12ms
02-10-2015 14:17:40.822 UTC INFO web.py:1447: 200 GET /css/style.css (0.0.0.0) 1.49ms
02-10-2015 14:17:40.954 UTC INFO web.py:1447: 200 GET /js/jquery.cookie.js (0.0.0.0) 0.66ms
02-10-2015 14:17:41.133 UTC INFO web.py:1447: 200 GET /js/app.js (0.0.0.0) 5.38ms
02-10-2015 14:17:43.362 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:03.366 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:23.371 UTC WARNING utils.py:78: Non-OK HTTP response. HTTP 599: Timeout
02-10-2015 14:18:23.371 UTC WARNING numbers.py:195: Failed to update all the backends
02-10-2015 14:18:23.371 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:23.372 UTC WARNING utils.py:78: Non-OK HTTP response. HTTP 599: Timeout
02-10-2015 14:18:23.373 UTC ERROR iostream.py:307: Uncaught exception, closing connection.
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/iostream.py", line 304, in wrapper
    callback(*args)
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/httpserver.py", line 243, in _on_headers
    remote_ip = self.address[0]
IndexError: string index out of range
02-10-2015 14:18:23.373 UTC ERROR ioloop.py:435: Exception in callback <tornado.stack_context._StackContextWrapper object at 0x7fc2624aedb8>
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/ioloop.py", line 421, in _run_callback
    callback()
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/iostream.py", line 304, in wrapper
    callback(*args)
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/httpserver.py", line 243, in _on_headers
    remote_ip = self.address[0]

From: Robert Day [mailto:Robert.Day at metaswitch.com]
Sent: Thursday, October 1, 2015 10:41 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

Good news that your Homestead node is now OK!

For your Homer node, do the log files in /var/log/homer contain any errors that help explain why it isn't starting? What happens if you run "sudo service homer restart" manually - does that print any errors?

On the Ellis node, the /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config script should create the Nginx config to redirect it to Ellis. Running 'sudo service clearwater-infrastructure' should have run that script, but what happens if you run /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config manually? Does it create a /etc/nginx/sites-available/ellis file? If not, can you send us the contents of /etc/clearwater/config, /etc/clearwater/shared_config and /etc/clearwater/local_config?

Thanks,
Rob

--
Rob Day
Software Engineer, Project Clearwater

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 01 October 2015 19:57
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] FW: Deploying Clearwater with Haet

Hi ellie,

1.       I had to copy the shared_config from ellis to homestead and now it is ok

2.       I had the same problem on homer but after copying the shared_config and restarting Clearwater-infrastructure it tells me when doing 'sudo monit status' thar homer_process failed

3.       When I point with the browser to Ellis IP address I get the welcome message of Nginx but no login screen as I am used to get. What application should run on top of Nginx?

Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:59 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi,

Is your etcd cluster healthy? What's the result of running sudo clearwater-etcdctl cluster-health? If these look OK then can you also please try running 'sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config' on the Ellis node, then the apply_shared_config script again on Homestead.

If this still doesn't work, you can copy over the shared_config file on Ellis to Homestead, run 'sudo service clearwater infrastructure' and this should unblock you. I'd be interested to dig further into why the apply_shared_config isn't working though.

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:48
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

It doesn't exist on Homestead but exists on Ellis.
I did clearwater-infrastructure restart but it didn't help.
Can we conduct a short webex, do you want me to send you webex details?
Thanks,
Shay
From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:46 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Does the /etc/clearwater/shared_config file exist on your Homestead node? Does it exist on the Ellis node?

If the shared_config file does exist, can you run 'sudo service clearwater-infrastructure restart'?

Ellie


From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:22
To: Eleanor Merry; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Thanks Eleanor,

I run the /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config

But still I get the following errors when running 'sudo monit status'

Process 'homestead_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59

Program 'poll_homestead'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Thu, 01 Oct 2015 16:10:49

Process 'homestead-prov_process'
  status                            Execution failed
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59



Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 12:54 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

It doesn't look like /etc/clearwater/shared_config has successfully propagated to the Homestead node.

In order to set up the diameter configuration file (used by Homestead) and the local_settings file (used by Homestead-prov) correctly, Homestead/Homestead-prov need information from the /etc/clearwater/shared_config file (e.g. the hs_hostname). If the shared_config file is missing; the install will succeed, but Homestead/Homestead-prov won't be functional until they've got their configuration files set up.

Looking at the template, we install the clearwater-management package after we install the Homestead/Homestead-prov packages, but don't then do anything to apply the shared configuration to the node. I've raised an issue (https://github.com/Metaswitch/clearwater-heat/issues/15) to fix this; in the meantime can you please run "sudo /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config" on all your nodes?

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 29 September 2015 16:10
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Deploying Clearwater with Haet

Hello,
I am trying deploying Clearwater on my OpenStack using the Heat templates as defined in here https://github.com/Metaswitch/clearwater-heat
I am running into issues with Homestead where I get (see below):

1.       Why LOCAL_IP=MUST BE CONFIGURED is not configured?

2.       Regarding the Diameter stack I saw an open issue https://github.com/Metaswitch/homestead/issues/73 but when deleting the certificates and starting the Clearwater-infrastructure it fails again on the same problem.

Thanks,
Shay



4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:39:55 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:39:55 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:39:55 homestead-0 monit:     exec code in run_globals
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:39:55 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:39:55 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:39:55 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:39:55 homestead-0 monit:    ...fail!
Sep 29 06:40:01 homestead-0 CRON[15030]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:40:22 homestead-0 monit: CMD /etc/init.d/homestead-prov start
Sep 29 06:40:26 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:40:26 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:40:26 homestead-0 monit:     exec code in run_globals
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:40:26 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:40:26 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:40:26 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:41:01 homestead-0 CRON[15050]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:41:03 homestead-0 monit: CMD /etc/init.d/homestead-prov restart
Sep 29 06:41:03 homestead-0 monit:  * Restarting homestead-prov homestead-prov
Sep 29 06:41:03 homestead-0 homestead[15074]: 4005 - Description: Homestead started. @@Cause: The Homestead application is starting. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1001 - Description: Diameter stack is starting. @@Cause: Diameter stack is beginning initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1002 - Description: Diameter stack initialization completed. @@Cause: Diameter stack has completed initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:41:06 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:41:06 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:41:06 homestead-0 monit:     exec code in run_globals
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151002/0652eb73/attachment.html>

From Eleanor.Merry at metaswitch.com  Mon Oct  5 05:48:55 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Mon, 5 Oct 2015 09:48:55 +0000
Subject: [Clearwater] Deploying Clearwater with Haet
In-Reply-To: <AMXPR04MB06927AC3C7B4F50B0DB4B67C74A0@AMXPR04MB069.eurprd04.prod.outlook.com>
References: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
	<BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069A6C79056AFC6762139BDC74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB125571D8C63B36001E324CD99B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069FD082C705B75402A4DA2C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB1255BC0AB2D26E2CBF1ADC849B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069C562DDF2E1B656EE7AD7C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<CY1PR0201MB0924E05F204B2DB63A4E8A66F44C0@CY1PR0201MB0924.namprd02.prod.outlook.com>
	<AMXPR04MB069478BA1E007031294568DC74B0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BLUPR02MB1251451BCECFA22EEE0EAB129B4B0@BLUPR02MB1251.namprd02.prod.outlook.com>
	<AMXPR04MB06922DB70876C9D75BD2ADCC74B0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BLUPR02MB12514381B2F4D8066BCCA0149B4B0@BLUPR02MB1251.namprd02.prod.outlook.com>
	<AMXPR04MB069ED45FB03A281222549F0C74B0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BLUPR02MB1251E1F76437D8E91DAC24619B4B0@BLUPR02MB1251.namprd02.prod.outlook.com>
	<AMXPR04MB06927AC3C7B4F50B0DB4B67C74A0@AMXPR04MB069.eurprd04.prod.outlook.com>
Message-ID: <BN3PR02MB1255D314BC784A319C5F09809B480@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Shay,

I've put answers to your points below:

1. Just to check, you did run 'sudo service clearwater-infrastructure restart' then 'sudo service ellis stop' after you changed the local_settings.py file? What's the value of LOG_LEVEL in '/usr/share/clearwater/ellis/env/lib/python2.7/site-packages/ellis-0.1-py2.7.egg/metaswitch/ellis/local_settings.py'?

2. 'cw-ngv.com' is the default value Ellis uses for the domain. It's overridden by the value of home_domain in the /etc/clearwater/shared_config file - to trigger the overriding you do need to run the 'sudo service clearwater-infrastructure restart' then 'sudo service ellis stop' commands (same as for turning on debug logging).

3. Did you run the '/usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config' script on Bono? You need to do this for Bono to pick up the changes (this runs 'sudo service clearwater-infrastructure restart' then 'sudo service bono stop' under the covers, so it's the same steps that are necessary for Ellis).

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 03 October 2015 23:39
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

Hi Ellie,

1.       I checked the flag and its value in local_settings.py is "LOG_LEVEL = logging.DEBUG" don't understand why we don't get debug info

2.       You were right regarding the timeouts I fixed the connectivity issue and I am able to provision new subscribes (after truncating the DBs). It provision them with the domain 6505550570 at cw-ngv.com<mailto:6505550570 at cw-ngv.com> cw-ngv.com, why not example.com?

3.       I have issues with bono. Shared_config was missing in bono and I copied it from ellis. When running 'sudo monit status' it says bono_process does not exist and in the /var/log/syslog I find the following error:



Oct  3 06:45:02 bono-0 bono[13215]: 2006 - Description: Fatal - Must enable P-CSCF, S-CSCF or I-CSCF in /etc/clearwater/config. @@Cause: Neither a P-CSCF, a S-CSCF nor an I-CSCF was configured in /etc/clearwater/config. @@Effect: The application will exit and restart until the problem is fixed. @@Action: The P-CSCF is configured by setting the pcscf=<port> option. The S-CSCF is configured by setting the scscf=<port> option. The I-CSCF is configured by setting the icscf=<port> option.

Oct  3 06:45:12 bono-0 bono[13239]: 2005 - Description: Application started. @@Cause: The application is starting. @@Effect: Normal. @@Action: None.

Thanks,
Shay



From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Friday, October 2, 2015 7:47 PM
To: Shay Naeh
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

There's still no debug logs. Can you confirm that you edited the local settings file, restarted clearwater-infrastructure and restarted Ellis?

In the logs though there's a lot of timeouts to hs.example.com:8889 and homer.example.com:7888. Can you confirm that these are the correct hostnames for your Homestead and Homer nodes, and that the Ellis node can successfully talk to the Homestead node over port 8889, and to the Homer node at port 7888.

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 02 October 2015 17:37
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

Logs tar file attached.
Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Friday, October 2, 2015 6:35 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Ok, can you please send me the debug logs from Ellis? This will help narrow down whether the problem is with Ellis, Homer or Homestead. To turn on debug logging for Ellis, write LOG_LEVEL = logging.DEBUG to the local_settings.py file (at /usr/share/clearwater/ellis/src/metaswitch/ellis/local_settings.py). Then restart clearwater-infrastructure (sudo service clearwater-infrastructure restart), and restart Ellis (sudo service ellis stop - it will be restarted by monit).

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 02 October 2015 16:25
To: Eleanor Merry; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

I've deleted all the numbers and when login to ellis it tells me you have no numbers, create one.
When trying to create I get the same error failed to update the server.
And yes I have shared_config on all nodes

Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Friday, October 2, 2015 6:07 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi,

It looks like there is a corrupted number in the Ellis/Homestead databases. Can you please remove the numbers? You can do this by running:

To delete all numbers on Ellis, run the following:
sudo mysql
USE ellis
UPDATE numbers SET owner_id=NULL;

To delete all numbers on Homestead, run the following:

cqlsh
USE homestead_provisioning;
TRUNCATE public;
TRUNCATE private;
TRUNCATE service_profiles;
TRUNCATE implicit_registration_sets;
USE homestead_cache;
TRUNCATE impi;
TRUNCATE impu;
TRUNCATE impi_mapping;

To delete all numbers on Homer, run the following:

cqlsh
USE homer;
TRUNCATE simservs;

Can you also please confirm that the shared_configuration file is present on all your nodes?

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 02 October 2015 15:27
To: Robert Day; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

Hi Rob and Ellie,

1.       Sudo service homer restart didn't help, but Sudo monit start homer_proccess worked just fine.

2.       Regarding ellis I played with it and what helped bringing up the Login screen was after provisioning additional phone numbers (don't understand why it is related).

3.       Now I encounter an issue after the login, I get the error 'failed to update the server (see detailed diagnostics in the developer console)". I think I saw an error like that that was related to the Cassandra DB tables but I suspect that some configuration regarding IPs or FQDNS is wrong. In ellis log files I find the following uncaught exception.
Thanks,
Shay

02-10-2015 14:15:22.080 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.18ms
02-10-2015 14:16:02.117 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.37ms
02-10-2015 14:16:42.169 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.46ms
02-10-2015 14:17:22.213 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.28ms
02-10-2015 14:17:40.600 UTC INFO web.py:1447: 200 GET /css/bootstrap-responsive.css (0.0.0.0) 2.12ms
02-10-2015 14:17:40.822 UTC INFO web.py:1447: 200 GET /css/style.css (0.0.0.0) 1.49ms
02-10-2015 14:17:40.954 UTC INFO web.py:1447: 200 GET /js/jquery.cookie.js (0.0.0.0) 0.66ms
02-10-2015 14:17:41.133 UTC INFO web.py:1447: 200 GET /js/app.js (0.0.0.0) 5.38ms
02-10-2015 14:17:43.362 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:03.366 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:23.371 UTC WARNING utils.py:78: Non-OK HTTP response. HTTP 599: Timeout
02-10-2015 14:18:23.371 UTC WARNING numbers.py:195: Failed to update all the backends
02-10-2015 14:18:23.371 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:23.372 UTC WARNING utils.py:78: Non-OK HTTP response. HTTP 599: Timeout
02-10-2015 14:18:23.373 UTC ERROR iostream.py:307: Uncaught exception, closing connection.
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/iostream.py", line 304, in wrapper
    callback(*args)
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/httpserver.py", line 243, in _on_headers
    remote_ip = self.address[0]
IndexError: string index out of range
02-10-2015 14:18:23.373 UTC ERROR ioloop.py:435: Exception in callback <tornado.stack_context._StackContextWrapper object at 0x7fc2624aedb8>
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/ioloop.py", line 421, in _run_callback
    callback()
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/iostream.py", line 304, in wrapper
    callback(*args)
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/httpserver.py", line 243, in _on_headers
    remote_ip = self.address[0]

From: Robert Day [mailto:Robert.Day at metaswitch.com]
Sent: Thursday, October 1, 2015 10:41 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

Good news that your Homestead node is now OK!

For your Homer node, do the log files in /var/log/homer contain any errors that help explain why it isn't starting? What happens if you run "sudo service homer restart" manually - does that print any errors?

On the Ellis node, the /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config script should create the Nginx config to redirect it to Ellis. Running 'sudo service clearwater-infrastructure' should have run that script, but what happens if you run /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config manually? Does it create a /etc/nginx/sites-available/ellis file? If not, can you send us the contents of /etc/clearwater/config, /etc/clearwater/shared_config and /etc/clearwater/local_config?

Thanks,
Rob

--
Rob Day
Software Engineer, Project Clearwater

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 01 October 2015 19:57
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] FW: Deploying Clearwater with Haet

Hi ellie,

1.       I had to copy the shared_config from ellis to homestead and now it is ok

2.       I had the same problem on homer but after copying the shared_config and restarting Clearwater-infrastructure it tells me when doing 'sudo monit status' thar homer_process failed

3.       When I point with the browser to Ellis IP address I get the welcome message of Nginx but no login screen as I am used to get. What application should run on top of Nginx?

Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:59 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi,

Is your etcd cluster healthy? What's the result of running sudo clearwater-etcdctl cluster-health? If these look OK then can you also please try running 'sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config' on the Ellis node, then the apply_shared_config script again on Homestead.

If this still doesn't work, you can copy over the shared_config file on Ellis to Homestead, run 'sudo service clearwater infrastructure' and this should unblock you. I'd be interested to dig further into why the apply_shared_config isn't working though.

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:48
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

It doesn't exist on Homestead but exists on Ellis.
I did clearwater-infrastructure restart but it didn't help.
Can we conduct a short webex, do you want me to send you webex details?
Thanks,
Shay
From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:46 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Does the /etc/clearwater/shared_config file exist on your Homestead node? Does it exist on the Ellis node?

If the shared_config file does exist, can you run 'sudo service clearwater-infrastructure restart'?

Ellie


From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:22
To: Eleanor Merry; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Thanks Eleanor,

I run the /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config

But still I get the following errors when running 'sudo monit status'

Process 'homestead_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59

Program 'poll_homestead'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Thu, 01 Oct 2015 16:10:49

Process 'homestead-prov_process'
  status                            Execution failed
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59



Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 12:54 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

It doesn't look like /etc/clearwater/shared_config has successfully propagated to the Homestead node.

In order to set up the diameter configuration file (used by Homestead) and the local_settings file (used by Homestead-prov) correctly, Homestead/Homestead-prov need information from the /etc/clearwater/shared_config file (e.g. the hs_hostname). If the shared_config file is missing; the install will succeed, but Homestead/Homestead-prov won't be functional until they've got their configuration files set up.

Looking at the template, we install the clearwater-management package after we install the Homestead/Homestead-prov packages, but don't then do anything to apply the shared configuration to the node. I've raised an issue (https://github.com/Metaswitch/clearwater-heat/issues/15) to fix this; in the meantime can you please run "sudo /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config" on all your nodes?

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 29 September 2015 16:10
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Deploying Clearwater with Haet

Hello,
I am trying deploying Clearwater on my OpenStack using the Heat templates as defined in here https://github.com/Metaswitch/clearwater-heat
I am running into issues with Homestead where I get (see below):

1.       Why LOCAL_IP=MUST BE CONFIGURED is not configured?

2.       Regarding the Diameter stack I saw an open issue https://github.com/Metaswitch/homestead/issues/73 but when deleting the certificates and starting the Clearwater-infrastructure it fails again on the same problem.

Thanks,
Shay



4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:39:55 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:39:55 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:39:55 homestead-0 monit:     exec code in run_globals
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:39:55 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:39:55 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:39:55 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:39:55 homestead-0 monit:    ...fail!
Sep 29 06:40:01 homestead-0 CRON[15030]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:40:22 homestead-0 monit: CMD /etc/init.d/homestead-prov start
Sep 29 06:40:26 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:40:26 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:40:26 homestead-0 monit:     exec code in run_globals
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:40:26 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:40:26 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:40:26 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:41:01 homestead-0 CRON[15050]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:41:03 homestead-0 monit: CMD /etc/init.d/homestead-prov restart
Sep 29 06:41:03 homestead-0 monit:  * Restarting homestead-prov homestead-prov
Sep 29 06:41:03 homestead-0 homestead[15074]: 4005 - Description: Homestead started. @@Cause: The Homestead application is starting. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1001 - Description: Diameter stack is starting. @@Cause: Diameter stack is beginning initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1002 - Description: Diameter stack initialization completed. @@Cause: Diameter stack has completed initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:41:06 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:41:06 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:41:06 homestead-0 monit:     exec code in run_globals
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151005/bc69f49b/attachment.html>

From shayn at gigaspaces.com  Mon Oct  5 08:05:23 2015
From: shayn at gigaspaces.com (Shay Naeh)
Date: Mon, 5 Oct 2015 12:05:23 +0000
Subject: [Clearwater] Deploying Clearwater with Haet
In-Reply-To: <BN3PR02MB1255D314BC784A319C5F09809B480@BN3PR02MB1255.namprd02.prod.outlook.com>
References: <VI1PR04MB1407ACA5737D12995C11391BC74E0@VI1PR04MB1407.eurprd04.prod.outlook.com>
	<BN3PR02MB12557BE42C46C5051B73C29C9B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069A6C79056AFC6762139BDC74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB125571D8C63B36001E324CD99B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069FD082C705B75402A4DA2C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB1255BC0AB2D26E2CBF1ADC849B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<AMXPR04MB069C562DDF2E1B656EE7AD7C74C0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<CY1PR0201MB0924E05F204B2DB63A4E8A66F44C0@CY1PR0201MB0924.namprd02.prod.outlook.com>
	<AMXPR04MB069478BA1E007031294568DC74B0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BLUPR02MB1251451BCECFA22EEE0EAB129B4B0@BLUPR02MB1251.namprd02.prod.outlook.com>
	<AMXPR04MB06922DB70876C9D75BD2ADCC74B0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BLUPR02MB12514381B2F4D8066BCCA0149B4B0@BLUPR02MB1251.namprd02.prod.outlook.com>
	<AMXPR04MB069ED45FB03A281222549F0C74B0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BLUPR02MB1251E1F76437D8E91DAC24619B4B0@BLUPR02MB1251.namprd02.prod.outlook.com>
	<AMXPR04MB06927AC3C7B4F50B0DB4B67C74A0@AMXPR04MB069.eurprd04.prod.outlook.com>
	<BN3PR02MB1255D314BC784A319C5F09809B480@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <AMXPR04MB069593FF9F6AD0CC985CEB9C7480@AMXPR04MB069.eurprd04.prod.outlook.com>

Hi Ellie,

1.       LOG_LEVEL=logging.DEBUG and yes I run it.

2.       This is my shared config, I run the two commands to restart infrastructure and stop ellis and still it generates subscribers with the domain 'cw-ngv.com'

# Deployment definitions

home_domain=example.com

sprout_hostname=sprout.example.com

hs_hostname=hs.example.com:8888

hs_provisioning_hostname=hs.example.com:8889

ralf_hostname=ralf.example.com:10888

xdms_hostname=homer.example.com:7888



# Email server configuration

smtp_smarthost=localhost

smtp_username=username

smtp_password=password

email_recovery_sender=clearwater at example.org



# Keys

signup_key=secret

turn_workaround=secret

ellis_api_key=secret

ellis_cookie_key=secret

3.       Regarding Bono I did what you suggested but it says bono_process does not exists, no process is listening on port 5060 or 5062 whaen running 'netstat -na' and in /var/log/syslog there is the following error -

Oct  5 11:59:29 bono-0 bono[20299]: 2006 - Description: Fatal - Must enable P-CSCF, S-CSCF or I-CSCF in /etc/clearwater/config. @@Cause: Neither a P-CSCF, a S-CSCF nor an I-CSCF was configured in /etc/clearwater/config. @@Effect: The application will exit and restart until the problem is fixed. @@Action: The P-CSCF is configured by setting the pcscf=<port> option. The S-CSCF is configured by setting the scscf=<port> option. The I-CSCF is configured by setting the icscf=<port> option.

~

I saw something similar to that here in sprout - Error main.cpp:887: Must enable P-CSCF, S-CSCF or I-CSCF in https://github.com/Metaswitch/sprout/issues/450

I copied the shared_config to bono from ellis since there was no shared_config created in bono.



What is urgent for me is to be able to provision subscribers (I thing that part is done except for the cw-ngv.coma domain and I'd like to have example.com) and be able to connect tow Jitsi clients and demonstrate a video call.

Thanks,
Shay





From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Monday, October 5, 2015 12:49 PM
To: Shay Naeh
Cc: clearwater at lists.projectclearwater.org
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

I've put answers to your points below:

1. Just to check, you did run 'sudo service clearwater-infrastructure restart' then 'sudo service ellis stop' after you changed the local_settings.py file? What's the value of LOG_LEVEL in '/usr/share/clearwater/ellis/env/lib/python2.7/site-packages/ellis-0.1-py2.7.egg/metaswitch/ellis/local_settings.py'?

2. 'cw-ngv.com' is the default value Ellis uses for the domain. It's overridden by the value of home_domain in the /etc/clearwater/shared_config file - to trigger the overriding you do need to run the 'sudo service clearwater-infrastructure restart' then 'sudo service ellis stop' commands (same as for turning on debug logging).

3. Did you run the '/usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config' script on Bono? You need to do this for Bono to pick up the changes (this runs 'sudo service clearwater-infrastructure restart' then 'sudo service bono stop' under the covers, so it's the same steps that are necessary for Ellis).

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 03 October 2015 23:39
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

Hi Ellie,

1.       I checked the flag and its value in local_settings.py is "LOG_LEVEL = logging.DEBUG" don't understand why we don't get debug info

2.       You were right regarding the timeouts I fixed the connectivity issue and I am able to provision new subscribes (after truncating the DBs). It provision them with the domain 6505550570 at cw-ngv.com<mailto:6505550570 at cw-ngv.com> cw-ngv.com, why not example.com?

3.       I have issues with bono. Shared_config was missing in bono and I copied it from ellis. When running 'sudo monit status' it says bono_process does not exist and in the /var/log/syslog I find the following error:



Oct  3 06:45:02 bono-0 bono[13215]: 2006 - Description: Fatal - Must enable P-CSCF, S-CSCF or I-CSCF in /etc/clearwater/config. @@Cause: Neither a P-CSCF, a S-CSCF nor an I-CSCF was configured in /etc/clearwater/config. @@Effect: The application will exit and restart until the problem is fixed. @@Action: The P-CSCF is configured by setting the pcscf=<port> option. The S-CSCF is configured by setting the scscf=<port> option. The I-CSCF is configured by setting the icscf=<port> option.

Oct  3 06:45:12 bono-0 bono[13239]: 2005 - Description: Application started. @@Cause: The application is starting. @@Effect: Normal. @@Action: None.

Thanks,
Shay



From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Friday, October 2, 2015 7:47 PM
To: Shay Naeh
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

There's still no debug logs. Can you confirm that you edited the local settings file, restarted clearwater-infrastructure and restarted Ellis?

In the logs though there's a lot of timeouts to hs.example.com:8889 and homer.example.com:7888. Can you confirm that these are the correct hostnames for your Homestead and Homer nodes, and that the Ellis node can successfully talk to the Homestead node over port 8889, and to the Homer node at port 7888.

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 02 October 2015 17:37
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

Logs tar file attached.
Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Friday, October 2, 2015 6:35 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Ok, can you please send me the debug logs from Ellis? This will help narrow down whether the problem is with Ellis, Homer or Homestead. To turn on debug logging for Ellis, write LOG_LEVEL = logging.DEBUG to the local_settings.py file (at /usr/share/clearwater/ellis/src/metaswitch/ellis/local_settings.py). Then restart clearwater-infrastructure (sudo service clearwater-infrastructure restart), and restart Ellis (sudo service ellis stop - it will be restarted by monit).

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 02 October 2015 16:25
To: Eleanor Merry; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

I've deleted all the numbers and when login to ellis it tells me you have no numbers, create one.
When trying to create I get the same error failed to update the server.
And yes I have shared_config on all nodes

Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Friday, October 2, 2015 6:07 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi,

It looks like there is a corrupted number in the Ellis/Homestead databases. Can you please remove the numbers? You can do this by running:

To delete all numbers on Ellis, run the following:
sudo mysql
USE ellis
UPDATE numbers SET owner_id=NULL;

To delete all numbers on Homestead, run the following:

cqlsh
USE homestead_provisioning;
TRUNCATE public;
TRUNCATE private;
TRUNCATE service_profiles;
TRUNCATE implicit_registration_sets;
USE homestead_cache;
TRUNCATE impi;
TRUNCATE impu;
TRUNCATE impi_mapping;

To delete all numbers on Homer, run the following:

cqlsh
USE homer;
TRUNCATE simservs;

Can you also please confirm that the shared_configuration file is present on all your nodes?

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 02 October 2015 15:27
To: Robert Day; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

Hi Rob and Ellie,

1.       Sudo service homer restart didn't help, but Sudo monit start homer_proccess worked just fine.

2.       Regarding ellis I played with it and what helped bringing up the Login screen was after provisioning additional phone numbers (don't understand why it is related).

3.       Now I encounter an issue after the login, I get the error 'failed to update the server (see detailed diagnostics in the developer console)". I think I saw an error like that that was related to the Cassandra DB tables but I suspect that some configuration regarding IPs or FQDNS is wrong. In ellis log files I find the following uncaught exception.
Thanks,
Shay

02-10-2015 14:15:22.080 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 17.18ms
02-10-2015 14:16:02.117 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.37ms
02-10-2015 14:16:42.169 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.46ms
02-10-2015 14:17:22.213 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.28ms
02-10-2015 14:17:40.600 UTC INFO web.py:1447: 200 GET /css/bootstrap-responsive.css (0.0.0.0) 2.12ms
02-10-2015 14:17:40.822 UTC INFO web.py:1447: 200 GET /css/style.css (0.0.0.0) 1.49ms
02-10-2015 14:17:40.954 UTC INFO web.py:1447: 200 GET /js/jquery.cookie.js (0.0.0.0) 0.66ms
02-10-2015 14:17:41.133 UTC INFO web.py:1447: 200 GET /js/app.js (0.0.0.0) 5.38ms
02-10-2015 14:17:43.362 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:03.366 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:23.371 UTC WARNING utils.py:78: Non-OK HTTP response. HTTP 599: Timeout
02-10-2015 14:18:23.371 UTC WARNING numbers.py:195: Failed to update all the backends
02-10-2015 14:18:23.371 UTC WARNING homestead.py:309: Passing SIP password in the clear over http
02-10-2015 14:18:23.372 UTC WARNING utils.py:78: Non-OK HTTP response. HTTP 599: Timeout
02-10-2015 14:18:23.373 UTC ERROR iostream.py:307: Uncaught exception, closing connection.
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/iostream.py", line 304, in wrapper
    callback(*args)
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/httpserver.py", line 243, in _on_headers
    remote_ip = self.address[0]
IndexError: string index out of range
02-10-2015 14:18:23.373 UTC ERROR ioloop.py:435: Exception in callback <tornado.stack_context._StackContextWrapper object at 0x7fc2624aedb8>
Traceback (most recent call last):
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/ioloop.py", line 421, in _run_callback
    callback()
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/iostream.py", line 304, in wrapper
    callback(*args)
  File "/usr/share/clearwater/ellis/env/local/lib/python2.7/site-packages/tornado-2.3-py2.7.egg/tornado/httpserver.py", line 243, in _on_headers
    remote_ip = self.address[0]

From: Robert Day [mailto:Robert.Day at metaswitch.com]
Sent: Thursday, October 1, 2015 10:41 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

Good news that your Homestead node is now OK!

For your Homer node, do the log files in /var/log/homer contain any errors that help explain why it isn't starting? What happens if you run "sudo service homer restart" manually - does that print any errors?

On the Ellis node, the /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config script should create the Nginx config to redirect it to Ellis. Running 'sudo service clearwater-infrastructure' should have run that script, but what happens if you run /usr/share/clearwater/infrastructure/scripts/create-ellis-nginx-config manually? Does it create a /etc/nginx/sites-available/ellis file? If not, can you send us the contents of /etc/clearwater/config, /etc/clearwater/shared_config and /etc/clearwater/local_config?

Thanks,
Rob

--
Rob Day
Software Engineer, Project Clearwater

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 01 October 2015 19:57
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] FW: Deploying Clearwater with Haet

Hi ellie,

1.       I had to copy the shared_config from ellis to homestead and now it is ok

2.       I had the same problem on homer but after copying the shared_config and restarting Clearwater-infrastructure it tells me when doing 'sudo monit status' thar homer_process failed

3.       When I point with the browser to Ellis IP address I get the welcome message of Nginx but no login screen as I am used to get. What application should run on top of Nginx?

Thanks,
Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:59 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi,

Is your etcd cluster healthy? What's the result of running sudo clearwater-etcdctl cluster-health? If these look OK then can you also please try running 'sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config' on the Ellis node, then the apply_shared_config script again on Homestead.

If this still doesn't work, you can copy over the shared_config file on Ellis to Homestead, run 'sudo service clearwater infrastructure' and this should unblock you. I'd be interested to dig further into why the apply_shared_config isn't working though.

Ellie

From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:48
To: Eleanor Merry
Subject: RE: Deploying Clearwater with Haet

It doesn't exist on Homestead but exists on Ellis.
I did clearwater-infrastructure restart but it didn't help.
Can we conduct a short webex, do you want me to send you webex details?
Thanks,
Shay
From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 7:46 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Does the /etc/clearwater/shared_config file exist on your Homestead node? Does it exist on the Ellis node?

If the shared_config file does exist, can you run 'sudo service clearwater-infrastructure restart'?

Ellie


From: Shay Naeh [mailto:shayn at gigaspaces.com]
Sent: 01 October 2015 17:22
To: Eleanor Merry; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Thanks Eleanor,

I run the /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config

But still I get the following errors when running 'sudo monit status'

Process 'homestead_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59

Program 'poll_homestead'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Thu, 01 Oct 2015 16:10:49

Process 'homestead-prov_process'
  status                            Execution failed
  monitoring status                 Monitored
  data collected                    Thu, 01 Oct 2015 16:13:59



Shay

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: Thursday, October 1, 2015 12:54 PM
To: Shay Naeh; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: Deploying Clearwater with Haet

Hi Shay,

It doesn't look like /etc/clearwater/shared_config has successfully propagated to the Homestead node.

In order to set up the diameter configuration file (used by Homestead) and the local_settings file (used by Homestead-prov) correctly, Homestead/Homestead-prov need information from the /etc/clearwater/shared_config file (e.g. the hs_hostname). If the shared_config file is missing; the install will succeed, but Homestead/Homestead-prov won't be functional until they've got their configuration files set up.

Looking at the template, we install the clearwater-management package after we install the Homestead/Homestead-prov packages, but don't then do anything to apply the shared configuration to the node. I've raised an issue (https://github.com/Metaswitch/clearwater-heat/issues/15) to fix this; in the meantime can you please run "sudo /usr/share/clearwater/clearwater-config-manager/scripts/apply_shared_config" on all your nodes?

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shay Naeh
Sent: 29 September 2015 16:10
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] Deploying Clearwater with Haet

Hello,
I am trying deploying Clearwater on my OpenStack using the Heat templates as defined in here https://github.com/Metaswitch/clearwater-heat
I am running into issues with Homestead where I get (see below):

1.       Why LOCAL_IP=MUST BE CONFIGURED is not configured?

2.       Regarding the Diameter stack I saw an open issue https://github.com/Metaswitch/homestead/issues/73 but when deleting the certificates and starting the Clearwater-infrastructure it fails again on the same problem.

Thanks,
Shay



4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:39:55 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:39:55 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:39:55 homestead-0 monit:     exec code in run_globals
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:39:55 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:39:55 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:39:55 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:39:55 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:39:55 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:39:55 homestead-0 monit:    ...fail!
Sep 29 06:40:01 homestead-0 CRON[15030]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:40:22 homestead-0 monit: CMD /etc/init.d/homestead-prov start
Sep 29 06:40:26 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:40:26 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:40:26 homestead-0 monit:     exec code in run_globals
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:40:26 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
Sep 29 06:40:26 homestead-0 monit:     execfile(_local_settings_file)
Sep 29 06:40:26 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/local_settings.py", line 55, in <module>
Sep 29 06:40:26 homestead-0 monit:     LOCAL_IP = MUST_BE_CONFIGURED
Sep 29 06:40:26 homestead-0 monit: NameError: name 'MUST_BE_CONFIGURED' is not defined
Sep 29 06:41:01 homestead-0 CRON[15050]: (root) CMD (/usr/lib/sysstat/sadc 1 1 /var/log/sysstat/clearwater-sa`date +%d` > /dev/null 2>&1)
Sep 29 06:41:03 homestead-0 monit: CMD /etc/init.d/homestead-prov restart
Sep 29 06:41:03 homestead-0 monit:  * Restarting homestead-prov homestead-prov
Sep 29 06:41:03 homestead-0 homestead[15074]: 4005 - Description: Homestead started. @@Cause: The Homestead application is starting. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1001 - Description: Diameter stack is starting. @@Cause: Diameter stack is beginning initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 1002 - Description: Diameter stack initialization completed. @@Cause: Diameter stack has completed initialization. @@Effect: Normal. @@Action: None.
Sep 29 06:41:03 homestead-0 homestead[15074]: 4007 - Description: Fatal - Failed to initialize Diameter stack in function fd_core_parseconf with error 2. @@Cause: The Diameter interface could not be initialized or encountered an error while running. @@Effect: The application will exit and restart until the problem is fixed. @@Action: (1). Check the configuration for the Diameter destination hosts. (2). Check the connectivity to the Diameter host using Wireshark.
Sep 29 06:41:06 homestead-0 monit: Traceback (most recent call last):
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
Sep 29 06:41:06 homestead-0 monit:     "__main__", fname, loader, pkg_name)
Sep 29 06:41:06 homestead-0 monit:   File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
Sep 29 06:41:06 homestead-0 monit:     exec code in run_globals
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/main.py", line 50, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import api
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/__init__.py", line 35, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest.api import base
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/api/base.py", line 48, in <module>
Sep 29 06:41:06 homestead-0 monit:     from metaswitch.crest import settings
Sep 29 06:41:06 homestead-0 monit:   File "/usr/share/clearwater/homestead/env/local/lib/python2.7/site-packages/crest-0.1-py2.7.egg/metaswitch/crest/settings.py", line 139, in <module>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151005/6214bb03/attachment.html>

From atva_angel at yahoo.com  Mon Oct  5 14:18:59 2015
From: atva_angel at yahoo.com (atva angel)
Date: Mon, 5 Oct 2015 18:18:59 +0000 (UTC)
Subject: [Clearwater] Regarding to IMPU and PSI on Clearwater
In-Reply-To: <1854759891.115247.1442159232411.JavaMail.yahoo@mail.yahoo.com>
References: <1854759891.115247.1442159232411.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <868760952.761583.1444069139164.JavaMail.yahoo@mail.yahoo.com>

Hi all,
Could you please help to take a look and share your advice?
Thanks,Tam 


     On Sunday, September 13, 2015 10:47 PM, atva angel <atva_angel at yahoo.com> wrote:
   

 Hi community,
Could you please help to clarify some cases as below?
1. As normal, I can define the IMPU on Ellis with SIP-URI format, i.e: 
IMS user 12345678 has IMPI: 12345678 at example.com, IMPU - sip:12345678 at example.com. I wonder if Ellis/Clearwater could support TEL-URI format for the IMPU of IMS user or not, i.e: tel:+1123456782. I can not find where to define the IMS PSI on Ellis/Clearwater. Could you please to share your advice for this case?
Thanks,Tam




   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151005/6daf4181/attachment.html>

From jcoimbra at daitangroup.com  Mon Oct  5 16:17:56 2015
From: jcoimbra at daitangroup.com (Juliano Medeiros Coimbra)
Date: Mon, 5 Oct 2015 20:17:56 +0000
Subject: [Clearwater] Is CW-AIO losing registered SIP URIs?
In-Reply-To: <BN3PR02MB125563822E6987D0A83B1FC19B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
References: <CP1PR80MB214A25200573C8CAA47D4BBCE420@CP1PR80MB214.lamprd80.prod.outlook.com>,
	<BN3PR02MB125563822E6987D0A83B1FC19B4C0@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <CP1PR80MB21425E46002FC8E79C29189CE480@CP1PR80MB214.lamprd80.prod.outlook.com>

Hello Eleanor,


Thank you for pinpointing all the way to discover if I have issues in Clearwater.


As you (and the documentation) suggests, I just moved to the current version of All-in-one EC2 AMI.


I am going to reply following your orientation if I get any news from now on.


Best


Juliano Medeiros Coimbra, Architect
T: +55.19.3112-1200 ext. 1454
DaitanGroup | www.daitangroup.com<http://www.daitangroup.com/> | Highly Reliable Outsourcing. Value Added Services Worldwide.
Privileged and confidential. If this message has been received in error, please notify sender and delete it immediately.
Conte?do confidencial. Se esta mensagem foi recebida por engano, favor avisar o remetente e apag?-la.



________________________________
De: Eleanor Merry <Eleanor.Merry at metaswitch.com>
Enviado: quinta-feira, 1 de outubro de 2015 07:28
Para: Juliano Medeiros Coimbra; clearwater at lists.projectclearwater.org
Assunto: RE: Is CW-AIO losing registered SIP URIs?


Hi Juliano,



Do the users still exist on Homestead - can you check in the Cassandra database? Do the Homestead logs (in /var/log/homestead) give any more details when attempting to get the AV for a subscriber? Do the Sprout logs (in /var/log/sprout) indicate why the call is being rejected? You may need to turn on debug logging for this - to do so create/edit the file /etc/clearwater/user_settings, add log_level=5 and then restart Sprout/Homestead (service <sprout/homestead> stop - they're automatically restarted by monit).



Also, I've noticed that you're running a fairly old version of Clearwater - we strongly recommend upgrading regularly (we release every two weeks). In this case, I recommend creating a new CW-AIO box as we've changed what Ubuntu version we support (it's now 14.04).



Ellie



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Juliano Medeiros Coimbra
Sent: 25 September 2015 17:42
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] Is CW-AIO losing registered SIP URIs?



Hello,



I am using a CW-AIO image in the following version:



$ dpkg -l sprout homer homestead

Desired=Unknown/Install/Remove/Purge/Hold

| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend

|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)

||/ Name                       Version                    Description

+++-==========================-==========================-====================================================================

ii  homer                      1.0-150313.142251          homer, the Cassandra powered XDMS

ii  homestead                  1.0-150313.140357          homestead, the HSS Cache/Gateway

ii  sprout                     1.0-150313.150041          sprout, the SIP Router



I've created a few users using Ellis (of course with no "demo account" option) and also using Homer and Homestead APIs, but after some time (sorry, I don't know how much time, but it seems to be days) those users got the following state:



$ curl -X GET http://localhost:8889/impi/6505550716%40example.com/av

{"status": 404, "message": "Not Found", "reason": "unknown", "detail": {}, "error": true}



Therefore they also doesn't get registered through Sprout. The other scenario is a user that is found through this command above, but Sprout starts signaling "SIP/2.0 403 Forbidden".



Am I missing a configuration parameter? Or just messing around with the APIs?



I am sorry if this is a trivial question, but I am quite newbie with Project Clearwater. [??]



Thanks



Juliano Medeiros Coimbra, Architect

T: +55.19.3112-1200 ext. 1454

DaitanGroup | www.daitangroup.com<http://www.daitangroup.com/> | Highly Reliable Outsourcing. Value Added Services Worldwide.

Privileged and confidential. If this message has been received in error, please notify sender and delete it immediately.

Conte?do confidencial. Se esta mensagem foi recebida por engano, favor avisar o remetente e apag?-la.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151005/929b4674/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 488 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151005/929b4674/attachment.png>

From Eleanor.Merry at metaswitch.com  Wed Oct  7 06:54:40 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Wed, 7 Oct 2015 10:54:40 +0000
Subject: [Clearwater] Regarding to IMPU and PSI on Clearwater
In-Reply-To: <868760952.761583.1444069139164.JavaMail.yahoo@mail.yahoo.com>
References: <1854759891.115247.1442159232411.JavaMail.yahoo@mail.yahoo.com>
	<868760952.761583.1444069139164.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <BLUPR02MB125179CE04DA8A37D49A7BFD9B360@BLUPR02MB1251.namprd02.prod.outlook.com>

Hi Tam,

We don?t support provisioning Tel URIs through Ellis/Homestead-prov. If you use an external HSS (see http://clearwater.readthedocs.org/en/latest/External_HSS_Integration/index.html for details) then you can create Tel URI IMPUs in that. Clearwater will deal correctly with Tel URIs if it receives requests using them.

We also don?t support provisioning PSIs through Ellis/Homestead-prov.

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of atva angel
Sent: 05 October 2015 19:19
To: Clearwater Mail
Subject: Re: [Clearwater] Regarding to IMPU and PSI on Clearwater

Hi all,


Could you please help to take a look and share your advice?


Thanks,
Tam


On Sunday, September 13, 2015 10:47 PM, atva angel <atva_angel at yahoo.com<mailto:atva_angel at yahoo.com>> wrote:

Hi community,

Could you please help to clarify some cases as below?
1. As normal, I can define the IMPU on Ellis with SIP-URI format, i.e:
IMS user 12345678 has IMPI: 12345678 at example.com<mailto:12345678 at example.com>, IMPU - sip:12345678 at example.com. I wonder if Ellis/Clearwater could support TEL-URI format for the IMPU of IMS user or not, i.e: tel:+112345678
2. I can not find where to define the IMS PSI on Ellis/Clearwater. Could you please to share your advice for this case?

Thanks,
Tam



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151007/b66742f6/attachment.html>

From Andrew.Caldwell at metaswitch.com  Wed Oct  7 07:39:56 2015
From: Andrew.Caldwell at metaswitch.com (Andrew Caldwell)
Date: Wed, 7 Oct 2015 11:39:56 +0000
Subject: [Clearwater] Regarding to IMPU and PSI on Clearwater
In-Reply-To: <BLUPR02MB125179CE04DA8A37D49A7BFD9B360@BLUPR02MB1251.namprd02.prod.outlook.com>
References: <1854759891.115247.1442159232411.JavaMail.yahoo@mail.yahoo.com>
	<868760952.761583.1444069139164.JavaMail.yahoo@mail.yahoo.com>
	<BLUPR02MB125179CE04DA8A37D49A7BFD9B360@BLUPR02MB1251.namprd02.prod.outlook.com>
Message-ID: <BLUPR0201MB1588DACB3A86FABC1BEE357E87360@BLUPR0201MB1588.namprd02.prod.outlook.com>

Hi Tam,

While it?s true that Ellis/Homestead-prov don?t support PSI?s in their standard implementation (Clearwater requires an external HSS to support this), it is possible to implement a PSI as a special type of IMPU:


1.       Creating an IMPU that will act as the PSI (you can pick a specific number for the IMPU if you like by using the homestead provisioning API directly or using the specific number provisioning API on Ellis, documented at https://github.com/Metaswitch/ellis/blob/dev/docs/api.md#provisioning-specific-numbers)

2.       Configuring your service?s application server(s) to be triggered for that IMPU in session case TERMINATING_UNREGISTERED

3.       Don?t register any UEs with that IMPU (feel free to forget the SIP password entirely if you like ?)

Now this IMPU will invoke your Application Server when it is called (as if the IMPU were a standard PSI).

Hope this helps,

Andy

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Eleanor Merry
Sent: 07 October 2015 11:55
To: atva angel; Clearwater Mail
Subject: Re: [Clearwater] Regarding to IMPU and PSI on Clearwater

Hi Tam,

We don?t support provisioning Tel URIs through Ellis/Homestead-prov. If you use an external HSS (see http://clearwater.readthedocs.org/en/latest/External_HSS_Integration/index.html for details) then you can create Tel URI IMPUs in that. Clearwater will deal correctly with Tel URIs if it receives requests using them.

We also don?t support provisioning PSIs through Ellis/Homestead-prov.

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of atva angel
Sent: 05 October 2015 19:19
To: Clearwater Mail
Subject: Re: [Clearwater] Regarding to IMPU and PSI on Clearwater

Hi all,

Could you please help to take a look and share your advice?

Thanks,
Tam


On Sunday, September 13, 2015 10:47 PM, atva angel <atva_angel at yahoo.com<mailto:atva_angel at yahoo.com>> wrote:

Hi community,

Could you please help to clarify some cases as below?
1. As normal, I can define the IMPU on Ellis with SIP-URI format, i.e:
IMS user 12345678 has IMPI: 12345678 at example.com<mailto:12345678 at example.com>, IMPU - sip:12345678 at example.com. I wonder if Ellis/Clearwater could support TEL-URI format for the IMPU of IMS user or not, i.e: tel:+112345678
2. I can not find where to define the IMS PSI on Ellis/Clearwater. Could you please to share your advice for this case?

Thanks,
Tam



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151007/944361bd/attachment.html>

From longnv at viosoft.com  Thu Oct  8 23:42:28 2015
From: longnv at viosoft.com (Nguyen Van Long)
Date: Fri, 9 Oct 2015 03:42:28 +0000 (UTC)
Subject: [Clearwater] Clearwater with Freeswitch
Message-ID: <1270094656.1255165.1444362148136.JavaMail.zimbra@viosoft.com>

Dear Clearwater team, 
I am working one Clearwater and Freeswitch. I have successfully installed all-in-one Clearwater and Freeswitch one my system. My scenario is that I have 2 servers, I install Clearwater and Freeswitch on each one. I would like to use Freeswitch as SBC (Session Border Control) to connect the two Clearwater of two server but I don't know how to configure to archive my purpose. Could you have any ideal or quick guide for this issue. 
(My scenario something like this: SIP <-------> SBC<Freeswitch> <-------> Clearwater1 <-------> SBC<Freeswitch> <-----------> Clearwater2 <--------> SIP) 

Thank in advance! 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151009/5f00e338/attachment.html>

From nicola.principe at datatronics.es  Fri Oct  9 06:37:50 2015
From: nicola.principe at datatronics.es (Nicola Principe)
Date: Fri, 9 Oct 2015 12:37:50 +0200
Subject: [Clearwater]  - Sprout_process does not exit, elastic scaling
Message-ID: <004501d1027e$8c1164a0$a4342de0$@principe@datatronics.es>

Hi community,

 

I have a PCW deployment with 3 Sprouts and 2 Homesteads.

I have tried to add a 4th Sprout following the automatic clustering scaling
instructions, but it does not work.

 

On one of my Sprout already in the deployment I see this (10.4.0.130 is the
new Sprout):

 

Describing the Sprout Memcached cluster in site site1:

  The local node is in this cluster

  The cluster is *not* stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

   10.4.0.130 is in state joining, acknowledged change

 

Describing the Sprout Chronos cluster in site site1:

  The local node is in this cluster

  The cluster is *not* stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

    10.4.0.130 is in state joining, acknowledged change

 

But on the new Sprout node the sprout_process does not exist:

 

[sprout]manager at sprout-4:/var/log/sprout$ sudo monit status

The Monit daemon 5.8.1 uptime: 7m

 

Process 'sprout_process'

  status                            Does not exist

  monitoring status                 Monitored

  data collected                    Fri, 09 Oct 2015 12:23:25

 

Program 'poll_sprout_sip'

  status                            Initializing

  monitoring status                 Initializing

  data collected                    Fri, 09 Oct 2015 12:15:54

 

Program 'poll_sprout_http'

  status                            Initializing

  monitoring status                 Initializing

  data collected                    Fri, 09 Oct 2015 12:15:54

 

In the logs I can see the following:

09-10-2015 10:22:33.009 UTC Error memcached_config.cpp:133: Failed to open
'/etc/clearwater/cluster_settings'

09-10-2015 10:22:33.009 UTC Error memcachedstore.cpp:184: Failed to read
config, keeping previous settings

09-10-2015 10:22:33.010 UTC Error main.cpp:1885: Cluster settings file
'/etc/clearwater/cluster_settings' does not contain a valid set of servers

 

...but the cluster_settings file has been generated by etcd automatically.

 

Do you have any suggestion to sort this out?

 

Thanks,

Nicola

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151009/e85d2959/attachment.html>

From nicola.principe at datatronics.es  Fri Oct  9 08:19:55 2015
From: nicola.principe at datatronics.es (Nicola Principe)
Date: Fri, 9 Oct 2015 14:19:55 +0200
Subject: [Clearwater] - Sprout_process does not exit, elastic scaling
In-Reply-To: <004501d1027e$8c1164a0$a4342de0$@principe@datatronics.es>
References: <004501d1027e$8c1164a0$a4342de0$@principe@datatronics.es>
Message-ID: <005601d1028c$ce75d200$6b617600$@principe@datatronics.es>

Hi community,

 

I have solved the "does not exist" issue (it was due to file permission),
and then the Sprout cluster was in a not stable state.

I have tried to decommission the new sprout, but clearwater-cluster-manager
is not able to complete the query: 

 

UTC ERROR common_etcd_synchronizer.py:139 (thread ChronosPlugin): 10.4.0.130
caught EtcdException("Unable to decode server response:
HTTPConnectionPool(host='10.4.0.130', port=4000): Read timed out.",) when
trying to read with index 1478037 - pause before retry

 

Is there a way to manually decommission a node from a deployment and leave
the cluster stable?

 

Thanks.

 

 

De: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] En
nombre de Nicola Principe
Enviado el: viernes, 09 de octubre de 2015 12:38
Para: clearwater at lists.projectclearwater.org
Asunto: [Clearwater] - Sprout_process does not exit, elastic scaling

 

Hi community,

 

I have a PCW deployment with 3 Sprouts and 2 Homesteads.

I have tried to add a 4th Sprout following the automatic clustering scaling
instructions, but it does not work.

 

On one of my Sprout already in the deployment I see this (10.4.0.130 is the
new Sprout):

 

Describing the Sprout Memcached cluster in site site1:

  The local node is in this cluster

  The cluster is *not* stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

   10.4.0.130 is in state joining, acknowledged change

 

Describing the Sprout Chronos cluster in site site1:

  The local node is in this cluster

  The cluster is *not* stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

    10.4.0.130 is in state joining, acknowledged change

 

But on the new Sprout node the sprout_process does not exist:

 

[sprout]manager at sprout-4:/var/log/sprout$ sudo monit status

The Monit daemon 5.8.1 uptime: 7m

 

Process 'sprout_process'

  status                            Does not exist

  monitoring status                 Monitored

  data collected                    Fri, 09 Oct 2015 12:23:25

 

Program 'poll_sprout_sip'

  status                            Initializing

  monitoring status                 Initializing

  data collected                    Fri, 09 Oct 2015 12:15:54

 

Program 'poll_sprout_http'

  status                            Initializing

  monitoring status                 Initializing

  data collected                    Fri, 09 Oct 2015 12:15:54

 

In the logs I can see the following:

09-10-2015 10:22:33.009 UTC Error memcached_config.cpp:133: Failed to open
'/etc/clearwater/cluster_settings'

09-10-2015 10:22:33.009 UTC Error memcachedstore.cpp:184: Failed to read
config, keeping previous settings

09-10-2015 10:22:33.010 UTC Error main.cpp:1885: Cluster settings file
'/etc/clearwater/cluster_settings' does not contain a valid set of servers

 

...but the cluster_settings file has been generated by etcd automatically.

 

Do you have any suggestion to sort this out?

 

Thanks,

Nicola

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151009/65c6aa17/attachment.html>

From Eleanor.Merry at metaswitch.com  Fri Oct  9 11:39:17 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 9 Oct 2015 15:39:17 +0000
Subject: [Clearwater] - Sprout_process does not exit, elastic scaling
In-Reply-To: <005601d1028c$ce75d200$6b617600$@principe@datatronics.es>
References: <004501d1027e$8c1164a0$a4342de0$@principe@datatronics.es>
	<005601d1028c$ce75d200$6b617600$@principe@datatronics.es>
Message-ID: <BN3PR02MB12554BE6FE27C2A7BDCCB3529B340@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Nicola,

It looks like the existing Sprout's aren't picking up the changes from the new Sprout. The clearwater-cluster-manager process on the new Sprout is waiting for the clearwater-cluster-manager processes on the other Sprouts to acknowledge its existence before it kicks the new Sprout process to reload its cluster_settings file (which is why Sprout hasn't recognised that the cluster_settings file is available yet).

Can you please send me the clearwater-cluster-manager logs from one of the old Sprouts (in /var/log/clearwater-cluster-manager/)?

To remove a node from the deployment, you should follow the docs at http://clearwater.readthedocs.org/en/latest/Clearwater_Elastic_Scaling/index.html#if-you-did-a-manual-install or   http://clearwater.readthedocs.org/en/latest/Handling_Failed_Nodes/index.html (to force remove the node). In this case though I think it's the existing nodes misbehaving though, so removing the new node won't help.

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Nicola Principe
Sent: 09 October 2015 13:20
To: clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] - Sprout_process does not exit, elastic scaling

Hi community,

I have solved the "does not exist" issue (it was due to file permission), and then the Sprout cluster was in a not stable state.
I have tried to decommission the new sprout, but clearwater-cluster-manager is not able to complete the query:

UTC ERROR common_etcd_synchronizer.py:139 (thread ChronosPlugin): 10.4.0.130 caught EtcdException("Unable to decode server response: HTTPConnectionPool(host='10.4.0.130', port=4000): Read timed out.",) when trying to read with index 1478037 - pause before retry

Is there a way to manually decommission a node from a deployment and leave the cluster stable?

Thanks.


De: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] En nombre de Nicola Principe
Enviado el: viernes, 09 de octubre de 2015 12:38
Para: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Asunto: [Clearwater] - Sprout_process does not exit, elastic scaling

Hi community,

I have a PCW deployment with 3 Sprouts and 2 Homesteads.
I have tried to add a 4th Sprout following the automatic clustering scaling instructions, but it does not work.

On one of my Sprout already in the deployment I see this (10.4.0.130 is the new Sprout):

Describing the Sprout Memcached cluster in site site1:
  The local node is in this cluster
  The cluster is *not* stable
    10.4.0.157 is in state normal
    10.4.0.156 is in state normal
    10.4.0.159 is in state normal
   10.4.0.130 is in state joining, acknowledged change

Describing the Sprout Chronos cluster in site site1:
  The local node is in this cluster
  The cluster is *not* stable
    10.4.0.157 is in state normal
    10.4.0.156 is in state normal
    10.4.0.159 is in state normal
    10.4.0.130 is in state joining, acknowledged change

But on the new Sprout node the sprout_process does not exist:

[sprout]manager at sprout-4:/var/log/sprout$ sudo monit status
The Monit daemon 5.8.1 uptime: 7m

Process 'sprout_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Fri, 09 Oct 2015 12:23:25

Program 'poll_sprout_sip'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Fri, 09 Oct 2015 12:15:54

Program 'poll_sprout_http'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Fri, 09 Oct 2015 12:15:54

In the logs I can see the following:
09-10-2015 10:22:33.009 UTC Error memcached_config.cpp:133: Failed to open '/etc/clearwater/cluster_settings'
09-10-2015 10:22:33.009 UTC Error memcachedstore.cpp:184: Failed to read config, keeping previous settings
09-10-2015 10:22:33.010 UTC Error main.cpp:1885: Cluster settings file '/etc/clearwater/cluster_settings' does not contain a valid set of servers

...but the cluster_settings file has been generated by etcd automatically.

Do you have any suggestion to sort this out?

Thanks,
Nicola

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151009/940e1ee5/attachment.html>

From nicola.principe at datatronics.es  Fri Oct  9 06:36:01 2015
From: nicola.principe at datatronics.es (Nicola Principe)
Date: Fri, 9 Oct 2015 12:36:01 +0200
Subject: [Clearwater]  - Sprout_process does not exit, elastic scaling
Message-ID: <003f01d1027e$4ae5bd00$e0b13700$@principe@datatronics.es>

Hi community,

 

I have a PCW deployment with 3 Sprouts and 2 Homesteads.

I have tried to add a 4th Sprout following the automatic clustering scaling
instructions, but it does not work.

 

On one of my Sprout already in the deployment I see this (10.4.0.130 is the
new Sprout):

 

Describing the Sprout Memcached cluster in site site1:

  The local node is in this cluster

  The cluster is *not* stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

   10.4.0.130 is in state joining, acknowledged change

 

Describing the Sprout Chronos cluster in site site1:

  The local node is in this cluster

  The cluster is *not* stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

    10.4.0.130 is in state joining, acknowledged change

 

But on the new Sprout node the sprout_process does not exist:

 

[sprout]manager at sprout-4:/var/log/sprout$ sudo monit status

The Monit daemon 5.8.1 uptime: 7m

 

Process 'sprout_process'

  status                            Does not exist

  monitoring status                 Monitored

  data collected                    Fri, 09 Oct 2015 12:23:25

 

Program 'poll_sprout_sip'

  status                            Initializing

  monitoring status                 Initializing

  data collected                    Fri, 09 Oct 2015 12:15:54

 

Program 'poll_sprout_http'

  status                            Initializing

  monitoring status                 Initializing

  data collected                    Fri, 09 Oct 2015 12:15:54

 

In the logs I can see the following:

09-10-2015 10:22:33.009 UTC Error memcached_config.cpp:133: Failed to open
'/etc/clearwater/cluster_settings'

09-10-2015 10:22:33.009 UTC Error memcachedstore.cpp:184: Failed to read
config, keeping previous settings

09-10-2015 10:22:33.010 UTC Error main.cpp:1885: Cluster settings file
'/etc/clearwater/cluster_settings' does not contain a valid set of servers

 

...but the cluster_settings file has been generated by etcd automatically.

 

Do you have any suggestion to sort this out?

 

Thanks,

Nicola

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151009/3d77ad00/attachment.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sprout.log.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151009/3d77ad00/attachment.txt>

From nicola.principe at datatronics.es  Tue Oct 13 03:42:55 2015
From: nicola.principe at datatronics.es (Nicola Principe)
Date: Tue, 13 Oct 2015 09:42:55 +0200
Subject: [Clearwater] - Sprout_process does not exit, elastic scaling
In-Reply-To: <BN3PR02MB12554BE6FE27C2A7BDCCB3529B340@BN3PR02MB1255.namprd02.prod.outlook.com>
References: <004501d1027e$8c1164a0$a4342de0$@principe@datatronics.es>
	<005601d1028c$ce75d200$6b617600$@principe@datatronics.es>
	<BN3PR02MB12554BE6FE27C2A7BDCCB3529B340@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <002401d1058a$c95f0b50$5c1d21f0$@principe@datatronics.es>

Hi Ellie,

 

I do not have the logs at the moment because I have already cleaned the
scenario and started again with my old deployment (3 sprouts) trying to add
a 4th one.

 

I found myself again in the same situation described below, but this time,
thanks to your suggestions, I checked the cluster-manager logs of the
existing Sprouts and noticed that they were just empty!

I then restarted clearwater-cluster-manager on my 3 existing Sprouts and
also applied the shared config on all the 4. It worked.

 

Describing the Sprout Memcached cluster in site site1:

  The local node is in this cluster

  The cluster is stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

    10.4.0.130 is in state normal

 

Describing the Sprout Chronos cluster in site site1:

  The local node is in this cluster

  The cluster is stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

    10.4.0.130 is in state normal

 

It's strange because I tested the elastic scaling with Sprout version
1.0-150717.1xxxxx and it just worked. Now I'm with 1.0-150928.173306.

 

On the other side, etcd_cluster variable on local_config should containt the
existing deployment IPs. Now, my first sprout was 10.4.0.156; I then added 3
more. 

My question: when adding the 5th Sprout, shall I include in the etcd_cluster
all the already existing Sprouts, or setting the first original Sprout's IP
would be sufficient?

 

Thanks,

Nicola

 

 

 

 

De: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com] 
Enviado el: viernes, 09 de octubre de 2015 17:39
Para: Datatronics - Nicola Principe; clearwater at lists.projectclearwater.org
Asunto: RE: [Clearwater] - Sprout_process does not exit, elastic scaling

 

Hi Nicola, 

 

It looks like the existing Sprout's aren't picking up the changes from the
new Sprout. The clearwater-cluster-manager process on the new Sprout is
waiting for the clearwater-cluster-manager processes on the other Sprouts to
acknowledge its existence before it kicks the new Sprout process to reload
its cluster_settings file (which is why Sprout hasn't recognised that the
cluster_settings file is available yet).  

 

Can you please send me the clearwater-cluster-manager logs from one of the
old Sprouts (in /var/log/clearwater-cluster-manager/)?

 

To remove a node from the deployment, you should follow the docs at
http://clearwater.readthedocs.org/en/latest/Clearwater_Elastic_Scaling/index
.html#if-you-did-a-manual-install or
http://clearwater.readthedocs.org/en/latest/Handling_Failed_Nodes/index.html
(to force remove the node). In this case though I think it's the existing
nodes misbehaving though, so removing the new node won't help.

 

Ellie

 

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On
Behalf Of Nicola Principe
Sent: 09 October 2015 13:20
To: clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] - Sprout_process does not exit, elastic scaling

 

Hi community,

 

I have solved the "does not exist" issue (it was due to file permission),
and then the Sprout cluster was in a not stable state.

I have tried to decommission the new sprout, but clearwater-cluster-manager
is not able to complete the query: 

 

UTC ERROR common_etcd_synchronizer.py:139 (thread ChronosPlugin): 10.4.0.130
caught EtcdException("Unable to decode server response:
HTTPConnectionPool(host='10.4.0.130', port=4000): Read timed out.",) when
trying to read with index 1478037 - pause before retry

 

Is there a way to manually decommission a node from a deployment and leave
the cluster stable?

 

Thanks.

 

 

De: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] En
nombre de Nicola Principe
Enviado el: viernes, 09 de octubre de 2015 12:38
Para: clearwater at lists.projectclearwater.org
Asunto: [Clearwater] - Sprout_process does not exit, elastic scaling

 

Hi community,

 

I have a PCW deployment with 3 Sprouts and 2 Homesteads.

I have tried to add a 4th Sprout following the automatic clustering scaling
instructions, but it does not work.

 

On one of my Sprout already in the deployment I see this (10.4.0.130 is the
new Sprout):

 

Describing the Sprout Memcached cluster in site site1:

  The local node is in this cluster

  The cluster is *not* stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

   10.4.0.130 is in state joining, acknowledged change

 

Describing the Sprout Chronos cluster in site site1:

  The local node is in this cluster

  The cluster is *not* stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

    10.4.0.130 is in state joining, acknowledged change

 

But on the new Sprout node the sprout_process does not exist:

 

[sprout]manager at sprout-4:/var/log/sprout$ sudo monit status

The Monit daemon 5.8.1 uptime: 7m

 

Process 'sprout_process'

  status                            Does not exist

  monitoring status                 Monitored

  data collected                    Fri, 09 Oct 2015 12:23:25

 

Program 'poll_sprout_sip'

  status                            Initializing

  monitoring status                 Initializing

  data collected                    Fri, 09 Oct 2015 12:15:54

 

Program 'poll_sprout_http'

  status                            Initializing

  monitoring status                 Initializing

  data collected                    Fri, 09 Oct 2015 12:15:54

 

In the logs I can see the following:

09-10-2015 10:22:33.009 UTC Error memcached_config.cpp:133: Failed to open
'/etc/clearwater/cluster_settings'

09-10-2015 10:22:33.009 UTC Error memcachedstore.cpp:184: Failed to read
config, keeping previous settings

09-10-2015 10:22:33.010 UTC Error main.cpp:1885: Cluster settings file
'/etc/clearwater/cluster_settings' does not contain a valid set of servers

 

...but the cluster_settings file has been generated by etcd automatically.

 

Do you have any suggestion to sort this out?

 

Thanks,

Nicola

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151013/30bbb10c/attachment.html>

From Robert.Day at metaswitch.com  Tue Oct 13 06:02:32 2015
From: Robert.Day at metaswitch.com (Robert Day)
Date: Tue, 13 Oct 2015 10:02:32 +0000
Subject: [Clearwater] bono on two interfaces for public and private
 network
In-Reply-To: <22131571.790321.1443755113232.JavaMail.root@vznit170060.mailsrvcs.net>
References: <22131571.790321.1443755113232.JavaMail.root@vznit170060.mailsrvcs.net>
Message-ID: <CY1PR0201MB09240238FCE95A2C51F513FFF4300@CY1PR0201MB0924.namprd02.prod.outlook.com>

Hi Keller,

You'll be pleased to know that multiple network support does work - we have a test system running on two separate networks, which we update hourly with the latest Project Clearwater code to help make sure we don't break this function.

I agree that our documentation on this topic isn't as clear as it could be, particularly if you're setting the system up from scratch to have multiple networks rather than converting it - we're working on some docs updates at the moment to clarify this.

That said, here are a few useful rules of thumb for now:
- local_ip and public_ip should always be on the signalling network
- public_hostname should resolve to public_ip (so should also be on the signalling network
- management_local_ip should be the management IP address (note that Clearwater nodes don't generally need to know the management-network public IP or hostname)
- the IP addresses used for the etcd_cluster option should be management IP addresses
- Ellis is the exception, as Ellis doesn't need a signalling interface - in this case, local_ip, public_ip and public_hostname should all be on the management network

Hope that helps,
Rob

--
Rob Day
Software Engineer, Project Clearwater

-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of vze10n7ur at verizon.net
Sent: 02 October 2015 04:05
To: clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] bono on two interfaces for public and private network

 
 Does multiple network support actually work?  I tried a few weeks ago on OpenStack and I could not get it to play along.  I wanted my VMs' eth0 to be management, their eth1 (via a namespace) to be inter-VM traffic, and some sort of floating IP arrangement for the nodes that had to expose themselves to UEs and browsers (so bono, ellis and homer).  I wanted everything nameable in DNS with specific domains for each (e.g. the floating IP for bono would resolve to, say, bono01.public.example.com whereas it's mgmt interface would be bono01.mgmt.example.com).


I read and re-read the documentation but I could not find the right combination of parameters in the configuration files to get all the nodes to talk as they should.  It feels like my case would be not too outlandish?


--keller

 
On 10/01/15, Eleanor Merry<Eleanor.Merry at metaswitch.com> wrote:
 
Hi, 

You can bind Bono to two interfaces for management and signalling (e.g. ssh access on the management network, all SIP processing on the signalling network). You can see details of how to set this up at: http://clearwater.readthedocs.org/en/stable/Multiple_Network_Support/index.html. 
[cut]

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



From peter.skrzynski at nec.co.nz  Tue Oct 13 22:08:46 2015
From: peter.skrzynski at nec.co.nz (Peter Skrzynski)
Date: Wed, 14 Oct 2015 02:08:46 +0000
Subject: [Clearwater] SNMP alarms
Message-ID: <2a06daaa30f141cf895b2a482be00825@ex15w.necnz.internal>

Hi Clearwater.
I was interested in using SNMP alarms for my Clearwater IMS (manual install of bono/sprout/homestead/ibcf/dns).
But I am puzzled, because the documentation says that the external SNMP destination is configured.
That would be the snmp_ip config item?
?  snmp_ip - the IP address to send alarms to (defaults to being unset). If this is set then Sprout, Ralf, Homestead and Chronos will send alarms - more details on the alarms are here.
BUT, when I look at the code, the snmp_ip is only passed to the app as a flag, not an IP address. Namely ??alarms-enabled?.
So my question is?
How does Sprout know where to send the SNMP alarms, if the IP address is not passed into the software?
Is there anything else that needs to be done / installed to get SNMP alarms working?
Thanks,
Peter.




Peter Skrzynski

Technical Lead
NEC New Zealand Limited
NEC House, Level 6, 40 Taranaki Street, PO Box 1936, Wellington 6011, New Zealand
T: 043816257, M: 0274849530, F: +6443811110
peter.skrzynski at nec.co.nz<mailto:peter.skrzynski at nec.co.nz>
nz.nec.com<http://nz.nec.com>

[cid:imagea09aaf.JPG at 21352791.4e9d9d41]

Please consider the environment before printing this email

Attention:
The information contained in this message and or attachments is intended only for the person or entity to which it is addressed and may contain confidential and/or privileged material.  Any review, retransmission, dissemination, copying or other use of, or taking of any action in reliance upon, this information by persons or entities other than the intended recipient is prohibited. If you received this in error, please contact the sender and delete the material from any system and destroy any copies. NEC has no liability for any act or omission in reliance on the email or any attachment.  Before opening this email or any attachment(s), please check them for viruses. NEC is not responsible for any viruses in this email or any attachment(s); any changes made to this  email or any attachment(s) after they are sent; or any effects this email or any attachment(s) have on your network or computer system.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151014/ede2fb89/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: imagea09aaf.JPG
Type: image/jpeg
Size: 59501 bytes
Desc: imagea09aaf.JPG
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151014/ede2fb89/attachment.jpe>

From Eleanor.Merry at metaswitch.com  Wed Oct 14 06:14:33 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Wed, 14 Oct 2015 10:14:33 +0000
Subject: [Clearwater] - Sprout_process does not exit, elastic scaling
In-Reply-To: <002401d1058a$c95f0b50$5c1d21f0$@principe@datatronics.es>
References: <004501d1027e$8c1164a0$a4342de0$@principe@datatronics.es>
	<005601d1028c$ce75d200$6b617600$@principe@datatronics.es>
	<BN3PR02MB12554BE6FE27C2A7BDCCB3529B340@BN3PR02MB1255.namprd02.prod.outlook.com>
	<002401d1058a$c95f0b50$5c1d21f0$@principe@datatronics.es>
Message-ID: <BN3PR02MB12557E33D8176FC86A6053049B3F0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Nicola,

You should have the IP addresses of all four Sprouts in the etcd_cluster value on your fifth Sprout - the etcd_cluster is only used at start of day when the node first joins the cluster, and it should contain the IP addresses of any node that is currently a member of the etcd_cluster.

Do you know if your cluster-manger process was running during the scale up on your old Sprouts? Would you be able to send me your /var/log/monit.log? Were there any logs in /var/log/clearwater-cluster-manager/cluster-manager.output.log or in /var/log/clearwater-cluster-manager/cluster-manager_<timestamp>.log?

Ellie

From: Nicola Principe [mailto:nicola.principe at datatronics.es]
Sent: 13 October 2015 08:43
To: Eleanor Merry
Cc: clearwater at lists.projectclearwater.org
Subject: RE: [Clearwater] - Sprout_process does not exit, elastic scaling

Hi Ellie,

I do not have the logs at the moment because I have already cleaned the scenario and started again with my old deployment (3 sprouts) trying to add a 4th one.

I found myself again in the same situation described below, but this time, thanks to your suggestions, I checked the cluster-manager logs of the existing Sprouts and noticed that they were just empty!
I then restarted clearwater-cluster-manager on my 3 existing Sprouts and also applied the shared config on all the 4. It worked.

Describing the Sprout Memcached cluster in site site1:
  The local node is in this cluster
  The cluster is stable
    10.4.0.157 is in state normal
    10.4.0.156 is in state normal
    10.4.0.159 is in state normal
    10.4.0.130 is in state normal

Describing the Sprout Chronos cluster in site site1:
  The local node is in this cluster
  The cluster is stable
    10.4.0.157 is in state normal
    10.4.0.156 is in state normal
    10.4.0.159 is in state normal
    10.4.0.130 is in state normal

It's strange because I tested the elastic scaling with Sprout version 1.0-150717.1xxxxx and it just worked. Now I'm with 1.0-150928.173306.

On the other side, etcd_cluster variable on local_config should containt the existing deployment IPs. Now, my first sprout was 10.4.0.156; I then added 3 more.
My question: when adding the 5th Sprout, shall I include in the etcd_cluster all the already existing Sprouts, or setting the first original Sprout's IP would be sufficient?

Thanks,
Nicola




De: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Enviado el: viernes, 09 de octubre de 2015 17:39
Para: Datatronics - Nicola Principe; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Asunto: RE: [Clearwater] - Sprout_process does not exit, elastic scaling

Hi Nicola,

It looks like the existing Sprout's aren't picking up the changes from the new Sprout. The clearwater-cluster-manager process on the new Sprout is waiting for the clearwater-cluster-manager processes on the other Sprouts to acknowledge its existence before it kicks the new Sprout process to reload its cluster_settings file (which is why Sprout hasn't recognised that the cluster_settings file is available yet).

Can you please send me the clearwater-cluster-manager logs from one of the old Sprouts (in /var/log/clearwater-cluster-manager/)?

To remove a node from the deployment, you should follow the docs at http://clearwater.readthedocs.org/en/latest/Clearwater_Elastic_Scaling/index.html#if-you-did-a-manual-install or   http://clearwater.readthedocs.org/en/latest/Handling_Failed_Nodes/index.html (to force remove the node). In this case though I think it's the existing nodes misbehaving though, so removing the new node won't help.

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Nicola Principe
Sent: 09 October 2015 13:20
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Clearwater] - Sprout_process does not exit, elastic scaling

Hi community,

I have solved the "does not exist" issue (it was due to file permission), and then the Sprout cluster was in a not stable state.
I have tried to decommission the new sprout, but clearwater-cluster-manager is not able to complete the query:

UTC ERROR common_etcd_synchronizer.py:139 (thread ChronosPlugin): 10.4.0.130 caught EtcdException("Unable to decode server response: HTTPConnectionPool(host='10.4.0.130', port=4000): Read timed out.",) when trying to read with index 1478037 - pause before retry

Is there a way to manually decommission a node from a deployment and leave the cluster stable?

Thanks.


De: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] En nombre de Nicola Principe
Enviado el: viernes, 09 de octubre de 2015 12:38
Para: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Asunto: [Clearwater] - Sprout_process does not exit, elastic scaling

Hi community,

I have a PCW deployment with 3 Sprouts and 2 Homesteads.
I have tried to add a 4th Sprout following the automatic clustering scaling instructions, but it does not work.

On one of my Sprout already in the deployment I see this (10.4.0.130 is the new Sprout):

Describing the Sprout Memcached cluster in site site1:
  The local node is in this cluster
  The cluster is *not* stable
    10.4.0.157 is in state normal
    10.4.0.156 is in state normal
    10.4.0.159 is in state normal
   10.4.0.130 is in state joining, acknowledged change

Describing the Sprout Chronos cluster in site site1:
  The local node is in this cluster
  The cluster is *not* stable
    10.4.0.157 is in state normal
    10.4.0.156 is in state normal
    10.4.0.159 is in state normal
    10.4.0.130 is in state joining, acknowledged change

But on the new Sprout node the sprout_process does not exist:

[sprout]manager at sprout-4:/var/log/sprout$ sudo monit status
The Monit daemon 5.8.1 uptime: 7m

Process 'sprout_process'
  status                            Does not exist
  monitoring status                 Monitored
  data collected                    Fri, 09 Oct 2015 12:23:25

Program 'poll_sprout_sip'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Fri, 09 Oct 2015 12:15:54

Program 'poll_sprout_http'
  status                            Initializing
  monitoring status                 Initializing
  data collected                    Fri, 09 Oct 2015 12:15:54

In the logs I can see the following:
09-10-2015 10:22:33.009 UTC Error memcached_config.cpp:133: Failed to open '/etc/clearwater/cluster_settings'
09-10-2015 10:22:33.009 UTC Error memcachedstore.cpp:184: Failed to read config, keeping previous settings
09-10-2015 10:22:33.010 UTC Error main.cpp:1885: Cluster settings file '/etc/clearwater/cluster_settings' does not contain a valid set of servers

...but the cluster_settings file has been generated by etcd automatically.

Do you have any suggestion to sort this out?

Thanks,
Nicola

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151014/2dc5fddf/attachment.html>

From Eleanor.Merry at metaswitch.com  Wed Oct 14 06:14:37 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Wed, 14 Oct 2015 10:14:37 +0000
Subject: [Clearwater] SNMP alarms
In-Reply-To: <2a06daaa30f141cf895b2a482be00825@ex15w.necnz.internal>
References: <2a06daaa30f141cf895b2a482be00825@ex15w.necnz.internal>
Message-ID: <BN3PR02MB125503203F66F472CF1668359B3F0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Peter,

You?ll need to install ?clearwater-snmp-alarm-agent? (part of the https://github.com/Metaswitch/clearwater-snmp-handlers code). It?s this process that?s responsible for raising alarms externally (so it uses the actual value of snmp_ip). Sprout only uses the value of snmp_ip to determine if alarms are enabled on the node, and so whether it should send the alarms internally to the alarm agent.

Our documentation on how to set up alarm support isn?t very good ? we?re looking at improving this.

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Peter Skrzynski
Sent: 14 October 2015 03:09
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] SNMP alarms

Hi Clearwater.
I was interested in using SNMP alarms for my Clearwater IMS (manual install of bono/sprout/homestead/ibcf/dns).
But I am puzzled, because the documentation says that the external SNMP destination is configured.
That would be the snmp_ip config item?
?  snmp_ip - the IP address to send alarms to (defaults to being unset). If this is set then Sprout, Ralf, Homestead and Chronos will send alarms - more details on the alarms are here.
BUT, when I look at the code, the snmp_ip is only passed to the app as a flag, not an IP address. Namely ??alarms-enabled?.
So my question is?
How does Sprout know where to send the SNMP alarms, if the IP address is not passed into the software?
Is there anything else that needs to be done / installed to get SNMP alarms working?
Thanks,
Peter.




Peter Skrzynski
Technical Lead
NEC New Zealand Limited
NEC House, Level 6, 40 Taranaki Street, PO Box 1936, Wellington 6011, New Zealand
T: 043816257, M: 0274849530, F: +6443811110
peter.skrzynski at nec.co.nz<mailto:peter.skrzynski at nec.co.nz>
nz.nec.com<http://nz.nec.com>

[cid:image002.jpg at 01D1066F.8D6C3330]

Please consider the environment before printing this email

Attention:
The information contained in this message and or attachments is intended only for the person or entity to which it is addressed and may contain confidential and/or privileged material.  Any review, retransmission, dissemination, copying or other use of, or taking of any action in reliance upon, this information by persons or entities other than the intended recipient is prohibited. If you received this in error, please contact the sender and delete the material from any system and destroy any copies. NEC has no liability for any act or omission in reliance on the email or any attachment.  Before opening this email or any attachment(s), please check them for viruses. NEC is not responsible for any viruses in this email or any attachment(s); any changes made to this  email or any attachment(s) after they are sent; or any effects this email or any attachment(s) have on your network or computer system.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151014/39431c84/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 8336 bytes
Desc: image002.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151014/39431c84/attachment.jpg>

From nicola.principe at datatronics.es  Wed Oct 14 06:36:54 2015
From: nicola.principe at datatronics.es (Nicola Principe)
Date: Wed, 14 Oct 2015 12:36:54 +0200
Subject: [Clearwater] - Sprout_process does not exit, elastic scaling
In-Reply-To: <BN3PR02MB12557E33D8176FC86A6053049B3F0@BN3PR02MB1255.namprd02.prod.outlook.com>
References: <004501d1027e$8c1164a0$a4342de0$@principe@datatronics.es>
	<005601d1028c$ce75d200$6b617600$@principe@datatronics.es>
	<BN3PR02MB12554BE6FE27C2A7BDCCB3529B340@BN3PR02MB1255.namprd02.prod.outlook.com>
	<002401d1058a$c95f0b50$5c1d21f0$@principe@datatronics.es>
	<BN3PR02MB12557E33D8176FC86A6053049B3F0@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <004501d1066c$3fac5bf0$bf0513d0$@principe@datatronics.es>

Hi Ellie,

 

Please find attached the logs, where you can see how I successfully added 5
more Sprouts.

I have 9 Sprouts now up, running, syncronized and healthy.

 

After some tests I decided to configure the etcd_cluster with one IP per
node and it works, even if I need to restart the clearwater-cluster-manager
manually on the existing Sprouts after installing the new one.

 

Regards,

Nicola

 

 

De: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com] 
Enviado el: mi?rcoles, 14 de octubre de 2015 12:15
Para: Datatronics - Nicola Principe
CC: clearwater at lists.projectclearwater.org
Asunto: RE: [Clearwater] - Sprout_process does not exit, elastic scaling

 

Hi Nicola,

 

You should have the IP addresses of all four Sprouts in the etcd_cluster
value on your fifth Sprout ? the etcd_cluster is only used at start of day
when the node first joins the cluster, and it should contain the IP
addresses of any node that is currently a member of the etcd_cluster. 

 

Do you know if your cluster-manger process was running during the scale up
on your old Sprouts? Would you be able to send me your /var/log/monit.log?
Were there any logs in
/var/log/clearwater-cluster-manager/cluster-manager.output.log or in
/var/log/clearwater-cluster-manager/cluster-manager_<timestamp>.log?

 

Ellie

 

From: Nicola Principe [mailto:nicola.principe at datatronics.es] 
Sent: 13 October 2015 08:43
To: Eleanor Merry
Cc: clearwater at lists.projectclearwater.org
Subject: RE: [Clearwater] - Sprout_process does not exit, elastic scaling

 

Hi Ellie,

 

I do not have the logs at the moment because I have already cleaned the
scenario and started again with my old deployment (3 sprouts) trying to add
a 4th one.

 

I found myself again in the same situation described below, but this time,
thanks to your suggestions, I checked the cluster-manager logs of the
existing Sprouts and noticed that they were just empty!

I then restarted clearwater-cluster-manager on my 3 existing Sprouts and
also applied the shared config on all the 4. It worked.

 

Describing the Sprout Memcached cluster in site site1:

  The local node is in this cluster

  The cluster is stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

    10.4.0.130 is in state normal

 

Describing the Sprout Chronos cluster in site site1:

  The local node is in this cluster

  The cluster is stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

    10.4.0.130 is in state normal

 

It's strange because I tested the elastic scaling with Sprout version
1.0-150717.1xxxxx and it just worked. Now I'm with 1.0-150928.173306.

 

On the other side, etcd_cluster variable on local_config should containt the
existing deployment IPs. Now, my first sprout was 10.4.0.156; I then added 3
more. 

My question: when adding the 5th Sprout, shall I include in the etcd_cluster
all the already existing Sprouts, or setting the first original Sprout's IP
would be sufficient?

 

Thanks,

Nicola

 

 

 

 

De: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com] 
Enviado el: viernes, 09 de octubre de 2015 17:39
Para: Datatronics - Nicola Principe; clearwater at lists.projectclearwater.org
Asunto: RE: [Clearwater] - Sprout_process does not exit, elastic scaling

 

Hi Nicola, 

 

It looks like the existing Sprout?s aren?t picking up the changes from the
new Sprout. The clearwater-cluster-manager process on the new Sprout is
waiting for the clearwater-cluster-manager processes on the other Sprouts to
acknowledge its existence before it kicks the new Sprout process to reload
its cluster_settings file (which is why Sprout hasn?t recognised that the
cluster_settings file is available yet).  

 

Can you please send me the clearwater-cluster-manager logs from one of the
old Sprouts (in /var/log/clearwater-cluster-manager/)?

 

To remove a node from the deployment, you should follow the docs at
http://clearwater.readthedocs.org/en/latest/Clearwater_Elastic_Scaling/index
.html#if-you-did-a-manual-install or
http://clearwater.readthedocs.org/en/latest/Handling_Failed_Nodes/index.html
(to force remove the node). In this case though I think it?s the existing
nodes misbehaving though, so removing the new node won?t help.

 

Ellie

 

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On
Behalf Of Nicola Principe
Sent: 09 October 2015 13:20
To: clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] - Sprout_process does not exit, elastic scaling

 

Hi community,

 

I have solved the "does not exist" issue (it was due to file permission),
and then the Sprout cluster was in a not stable state.

I have tried to decommission the new sprout, but clearwater-cluster-manager
is not able to complete the query: 

 

UTC ERROR common_etcd_synchronizer.py:139 (thread ChronosPlugin): 10.4.0.130
caught EtcdException("Unable to decode server response:
HTTPConnectionPool(host='10.4.0.130', port=4000): Read timed out.",) when
trying to read with index 1478037 - pause before retry

 

Is there a way to manually decommission a node from a deployment and leave
the cluster stable?

 

Thanks.

 

 

De: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] En
nombre de Nicola Principe
Enviado el: viernes, 09 de octubre de 2015 12:38
Para: clearwater at lists.projectclearwater.org
Asunto: [Clearwater] - Sprout_process does not exit, elastic scaling

 

Hi community,

 

I have a PCW deployment with 3 Sprouts and 2 Homesteads.

I have tried to add a 4th Sprout following the automatic clustering scaling
instructions, but it does not work.

 

On one of my Sprout already in the deployment I see this (10.4.0.130 is the
new Sprout):

 

Describing the Sprout Memcached cluster in site site1:

  The local node is in this cluster

  The cluster is *not* stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

   10.4.0.130 is in state joining, acknowledged change

 

Describing the Sprout Chronos cluster in site site1:

  The local node is in this cluster

  The cluster is *not* stable

    10.4.0.157 is in state normal

    10.4.0.156 is in state normal

    10.4.0.159 is in state normal

    10.4.0.130 is in state joining, acknowledged change

 

But on the new Sprout node the sprout_process does not exist:

 

[sprout]manager at sprout-4:/var/log/sprout$ sudo monit status

The Monit daemon 5.8.1 uptime: 7m

 

Process 'sprout_process'

  status                            Does not exist

  monitoring status                 Monitored

  data collected                    Fri, 09 Oct 2015 12:23:25

 

Program 'poll_sprout_sip'

  status                            Initializing

  monitoring status                 Initializing

  data collected                    Fri, 09 Oct 2015 12:15:54

 

Program 'poll_sprout_http'

  status                            Initializing

  monitoring status                 Initializing

  data collected                    Fri, 09 Oct 2015 12:15:54

 

In the logs I can see the following:

09-10-2015 10:22:33.009 UTC Error memcached_config.cpp:133: Failed to open
'/etc/clearwater/cluster_settings'

09-10-2015 10:22:33.009 UTC Error memcachedstore.cpp:184: Failed to read
config, keeping previous settings

09-10-2015 10:22:33.010 UTC Error main.cpp:1885: Cluster settings file
'/etc/clearwater/cluster_settings' does not contain a valid set of servers

 

...but the cluster_settings file has been generated by etcd automatically.

 

Do you have any suggestion to sort this out?

 

Thanks,

Nicola

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151014/733ee839/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sprout.tar.gz
Type: application/x-gzip
Size: 521319 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151014/733ee839/attachment.bin>

From Andrew.Caldwell at metaswitch.com  Thu Oct 15 07:29:53 2015
From: Andrew.Caldwell at metaswitch.com (Andrew Caldwell)
Date: Thu, 15 Oct 2015 11:29:53 +0000
Subject: [Clearwater] Pride and Prejudice Release Note
Message-ID: <BLUPR0201MB1588935C499DC6F1E513A147873E0@BLUPR0201MB1588.namprd02.prod.outlook.com>

The release for sprint "Pride and Prejudice" has been cut.  The code for this release is tagged as release-83 in github.

We've added one new feature this sprint:

*         Homestead and Ralf now honour SRV priorities when routing messages to the HSS/CDF as well as when connecting to the Diameter Realm.

This release includes the following bug fixes:

*         Bad BGCF config makes sprout cyclically crash (https://github.com/Metaswitch/sprout/issues/1208)

*         Call diversions by a B2BUA are not correlated with the original call in SAS (https://github.com/Metaswitch/sprout/issues/1202)

*         On call diversion, Sprout duplicates START ACRs (and doesn't send ENDs) (https://github.com/Metaswitch/sprout/issues/1199)

*         Sprout's implementation `Session-expires` and `Min-SE` headers does not match RFC 4028 (https://github.com/Metaswitch/sprout/issues/1194)

*         Ralf has no documentation (https://github.com/Metaswitch/ralf/issues/106)

*         If no cassandra schema is installed, cassandra_store fails to start with an unknown error (https://github.com/Metaswitch/cpp-common/issues/382)

*         clearwater-prov-tools doesn't install (https://github.com/Metaswitch/ellis/issues/129)

*         API docs include a confusing "username" field (https://github.com/Metaswitch/ellis/issues/72)

*         API docs on redirect are wrong (and maybe we should do something different anyway) (https://github.com/Metaswitch/ellis/issues/71)

*         Concept of a "PSTN number" is not clearly documented (https://github.com/Metaswitch/ellis/issues/70)

*         API docs say "gab_listed" is a field in the number creation response (https://github.com/Metaswitch/ellis/issues/69)

*         API docs say JSON can be used for resources that don't support it (https://github.com/Metaswitch/ellis/issues/67)

*         Warnings about "already initialized constant" when running any knife command. (https://github.com/Metaswitch/chef/issues/233)

*         The cassandra schemas aren't always being added (https://github.com/Metaswitch/clearwater-cassandra/issues/54)

*         Clearwater-Architecture needs up to date architecture diagram (https://github.com/Metaswitch/clearwater-docs/issues/17)

*         MG-SOFT MIB Browser cannot compile the release-82 PROJECT-CLEARWATER-MIB. (https://github.com/Metaswitch/clearwater-snmp-handlers/issues/92)

*         logrotated files can be very large (https://github.com/Metaswitch/clearwater-etcd/issues/190)

*         etcd is restarting every 30 minutes (https://github.com/Metaswitch/clearwater-etcd/issues/166)

*         apply_shared_config should optionally wait for completion (https://github.com/Metaswitch/clearwater-etcd/issues/129)

*         Ellis starts with wrong configuration (https://github.com/Metaswitch/clearwater-docker/issues/10)

For upgrading to this release, follow the instructions at http://clearwater.readthedocs.org/en/latest/Upgrading_a_Clearwater_deployment/index.html. If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.

Thanks,
Andy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151015/57799c89/attachment.html>

From pitchfm at amazon.co.uk  Thu Oct 15 12:49:09 2015
From: pitchfm at amazon.co.uk (Pitchford, Matt)
Date: Thu, 15 Oct 2015 16:49:09 +0000
Subject: [Clearwater] AWS Installation error eu-west-1
Message-ID: <f561662dbad342d881d022f8091f6b16@EX13D09EUA002.ant.amazon.com>

Hello,

A couple problems:


1.       When running the "knife deployment..." command in eu-west-1 I receive the following error  "knife deployment resize -E clearwater --ralf-count 1 -VERROR: Fog::Compute::AWS::Error: RequestLimitExceeded => Request limit exceeded."     I saw this in another thread happening in US-West.

2.       How do I change the install VPC in the deployment?  The deployment starts and creates all the security groups, but they are in the default VPC.  I've created another VPC for Clearwater but the installation isn't going there.

Thank you.

Matt Pitchford
Solutions Architect
Amazon Web Services<http://aws.amazon.com/>
pitchfm at amazon.com<mailto:pitchfm at amazon.com>
+44 7402 431 831

[SolutionsArchitect-Professional2]




Amazon Web Services UK Limited. Registered in England and Wales with registration number 08650665 and which has its registered office at 60 Holborn Viaduct, London EC1A 2FD, United Kingdom.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151015/4430f77d/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 8102 bytes
Desc: image001.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151015/4430f77d/attachment.jpg>

From Eleanor.Merry at metaswitch.com  Thu Oct 15 14:44:40 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Thu, 15 Oct 2015 18:44:40 +0000
Subject: [Clearwater] AWS Installation error eu-west-1
In-Reply-To: <f561662dbad342d881d022f8091f6b16@EX13D09EUA002.ant.amazon.com>
References: <f561662dbad342d881d022f8091f6b16@EX13D09EUA002.ant.amazon.com>
Message-ID: <BN3PR02MB12554C9C7F6AD711496D212A9B3E0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Matt,

Where in Chef processing are you hitting the request limit exceeded error, and what version of the code are you using?
We did have an issue where Chef would try to update the DNS records without any pacing in its requests (which meant EC2 rejected them - see https://github.com/Metaswitch/chef/issues/179 for more details); this should be fixed in the latest Project Clearwater release.

To set what VPC Chef uses you need  to update your Chef environment to set the VPC ID (and upload the environment to your Chef server) - you can find details of this at: https://github.com/Metaswitch/chef/blob/master/VPC-SUPPORT.md#updating-your-chef-environment

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Pitchford, Matt
Sent: 15 October 2015 17:49
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] AWS Installation error eu-west-1

Hello,

A couple problems:


1.       When running the "knife deployment..." command in eu-west-1 I receive the following error  "knife deployment resize -E clearwater --ralf-count 1 -VERROR: Fog::Compute::AWS::Error: RequestLimitExceeded => Request limit exceeded."     I saw this in another thread happening in US-West.

2.       How do I change the install VPC in the deployment?  The deployment starts and creates all the security groups, but they are in the default VPC.  I've created another VPC for Clearwater but the installation isn't going there.

Thank you.

Matt Pitchford
Solutions Architect
Amazon Web Services<http://aws.amazon.com/>
pitchfm at amazon.com<mailto:pitchfm at amazon.com>
+44 7402 431 831

[SolutionsArchitect-Professional2]

Amazon Web Services UK Limited. Registered in England and Wales with registration number 08650665 and which has its registered office at 60 Holborn Viaduct, London EC1A 2FD, United Kingdom.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151015/50b06143/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 8102 bytes
Desc: image001.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151015/50b06143/attachment.jpg>

From Eleanor.Merry at metaswitch.com  Fri Oct 16 13:03:00 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 16 Oct 2015 17:03:00 +0000
Subject: [Clearwater] AWS Installation error eu-west-1
In-Reply-To: <760a131af77140828a6838e3c22f0e62@EX13D09EUA002.ant.amazon.com>
References: <f561662dbad342d881d022f8091f6b16@EX13D09EUA002.ant.amazon.com>
	<BN3PR02MB12554C9C7F6AD711496D212A9B3E0@BN3PR02MB1255.namprd02.prod.outlook.com>
	<760a131af77140828a6838e3c22f0e62@EX13D09EUA002.ant.amazon.com>
Message-ID: <BN3PR02MB12553B0B1DADE67504E5A6229B3D0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Matt,

Can you send me the logs for this? I'd like the logs that are printed to the terminal, and any per-node logs (created in <chef_repo>/logs)

Thanks,

Ellie

From: Pitchford, Matt [mailto:pitchfm at amazon.co.uk]
Sent: 16 October 2015 11:43
To: Eleanor Merry; clearwater at lists.projectclearwater.org
Subject: RE: AWS Installation error eu-west-1

Hello,

Security groups now being created in VPC, thanks for that guidance.

Still seeing issue #1 though, getting the Request Limit Exceeded.  Everything has been downloaded in the past week, so I'm thinking I have the correct versions of all the software.  Trying to run this in eu-west-1.

-Matt

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: 15 October 2015 19:45
To: Pitchford, Matt; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: AWS Installation error eu-west-1

Hi Matt,

Where in Chef processing are you hitting the request limit exceeded error, and what version of the code are you using?
We did have an issue where Chef would try to update the DNS records without any pacing in its requests (which meant EC2 rejected them - see https://github.com/Metaswitch/chef/issues/179 for more details); this should be fixed in the latest Project Clearwater release.

To set what VPC Chef uses you need  to update your Chef environment to set the VPC ID (and upload the environment to your Chef server) - you can find details of this at: https://github.com/Metaswitch/chef/blob/master/VPC-SUPPORT.md#updating-your-chef-environment

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Pitchford, Matt
Sent: 15 October 2015 17:49
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] AWS Installation error eu-west-1

Hello,

A couple problems:


1.       When running the "knife deployment..." command in eu-west-1 I receive the following error  "knife deployment resize -E clearwater --ralf-count 1 -VERROR: Fog::Compute::AWS::Error: RequestLimitExceeded => Request limit exceeded."     I saw this in another thread happening in US-West.

2.       How do I change the install VPC in the deployment?  The deployment starts and creates all the security groups, but they are in the default VPC.  I've created another VPC for Clearwater but the installation isn't going there.

Thank you.

Matt Pitchford
Solutions Architect
Amazon Web Services<http://aws.amazon.com/>
pitchfm at amazon.com<mailto:pitchfm at amazon.com>
+44 7402 431 831

[SolutionsArchitect-Professional2]

Amazon Web Services UK Limited. Registered in England and Wales with registration number 08650665 and which has its registered office at 60 Holborn Viaduct, London EC1A 2FD, United Kingdom.
Amazon Web Services UK Limited. Registered in England and Wales with registration number 08650665 and which has its registered office at 60 Holborn Viaduct, London EC1A 2FD, United Kingdom.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151016/b8ff08ef/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 8102 bytes
Desc: image001.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151016/b8ff08ef/attachment.jpg>

From pitchfm at amazon.co.uk  Fri Oct 16 06:42:31 2015
From: pitchfm at amazon.co.uk (Pitchford, Matt)
Date: Fri, 16 Oct 2015 10:42:31 +0000
Subject: [Clearwater] AWS Installation error eu-west-1
In-Reply-To: <BN3PR02MB12554C9C7F6AD711496D212A9B3E0@BN3PR02MB1255.namprd02.prod.outlook.com>
References: <f561662dbad342d881d022f8091f6b16@EX13D09EUA002.ant.amazon.com>
	<BN3PR02MB12554C9C7F6AD711496D212A9B3E0@BN3PR02MB1255.namprd02.prod.outlook.com>
Message-ID: <760a131af77140828a6838e3c22f0e62@EX13D09EUA002.ant.amazon.com>

Hello,

Security groups now being created in VPC, thanks for that guidance.

Still seeing issue #1 though, getting the Request Limit Exceeded.  Everything has been downloaded in the past week, so I'm thinking I have the correct versions of all the software.  Trying to run this in eu-west-1.

-Matt

From: Eleanor Merry [mailto:Eleanor.Merry at metaswitch.com]
Sent: 15 October 2015 19:45
To: Pitchford, Matt; clearwater at lists.projectclearwater.org
Subject: RE: AWS Installation error eu-west-1

Hi Matt,

Where in Chef processing are you hitting the request limit exceeded error, and what version of the code are you using?
We did have an issue where Chef would try to update the DNS records without any pacing in its requests (which meant EC2 rejected them - see https://github.com/Metaswitch/chef/issues/179 for more details); this should be fixed in the latest Project Clearwater release.

To set what VPC Chef uses you need  to update your Chef environment to set the VPC ID (and upload the environment to your Chef server) - you can find details of this at: https://github.com/Metaswitch/chef/blob/master/VPC-SUPPORT.md#updating-your-chef-environment

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Pitchford, Matt
Sent: 15 October 2015 17:49
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] AWS Installation error eu-west-1

Hello,

A couple problems:


1.       When running the "knife deployment..." command in eu-west-1 I receive the following error  "knife deployment resize -E clearwater --ralf-count 1 -VERROR: Fog::Compute::AWS::Error: RequestLimitExceeded => Request limit exceeded."     I saw this in another thread happening in US-West.

2.       How do I change the install VPC in the deployment?  The deployment starts and creates all the security groups, but they are in the default VPC.  I've created another VPC for Clearwater but the installation isn't going there.

Thank you.

Matt Pitchford
Solutions Architect
Amazon Web Services<http://aws.amazon.com/>
pitchfm at amazon.com<mailto:pitchfm at amazon.com>
+44 7402 431 831

[SolutionsArchitect-Professional2]

Amazon Web Services UK Limited. Registered in England and Wales with registration number 08650665 and which has its registered office at 60 Holborn Viaduct, London EC1A 2FD, United Kingdom.



Amazon Web Services UK Limited. Registered in England and Wales with registration number 08650665 and which has its registered office at 60 Holborn Viaduct, London EC1A 2FD, United Kingdom.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151016/3d1dcef3/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 8102 bytes
Desc: image001.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151016/3d1dcef3/attachment.jpg>

From anil.jillella at netxcell.com  Tue Oct 20 10:06:42 2015
From: anil.jillella at netxcell.com (Anil Jillella)
Date: Tue, 20 Oct 2015 19:36:42 +0530
Subject: [Clearwater] create_numbers.py is missing
Message-ID: <F8C15A68638D0B42AAF9AA4D1B78E55D3D4E84@nxlmail10.netxcell.com>

Hi,

I am new to this group.

I have installed all-in-one image with OVF (VMWare). I downloaded  http://vm-images.cw-ngv.com/cw-aio.ova  and started the virtual machine, everything seems fine but while creating a new private identity, ellis is not providing a number. As per the logs it says "No available numbers". I tried to find create_numbers.py to run, but I couldn't find it. Is there anything that I am missing while installation

Anil Jillella, PMP
Solution Architect - Technology,
Netxcell Limited,
B-Block, 4th Floor, Wing1, Cybergateway,
Hitechcity, Madhapur, Hyderabad-500081
Mobile: +91 8790069880


________________________________
The information contained in or accompanying this e-mail is intended only for the sole use of the stated recipient and may contain information that is confidential and/or privileged. If the reader is not the intended recipient thereof, you are hereby notified that any dissemination, distribution, printing or copying of this e-mail is strictly prohibited and may constitute a breach of confidence and/or privilege. The sender does not accept responsibility for any loss, disruption or damage to your data or computer system which may occur whilst using data contained in, or transmitted with, this e-mail. If you have received this e-mail in error, please notify the sender immediately and destroy all copies of this message and any attachments. Any views or opinions presented are solely those of the author and do not necessarily represent those of the Netxcell Limited.

www.netxcell.com - An ISO 9001:2008 and ISO 27001:2005 Certified Company
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151020/ed014eb9/attachment.html>

From fmalka at modulo.co.il  Wed Oct 21 02:02:39 2015
From: fmalka at modulo.co.il (Franck Malka)
Date: Wed, 21 Oct 2015 09:02:39 +0300
Subject: [Clearwater] Ut interface and Homer
Message-ID: <CAKX=ev07joDj90Ja7n2MRE2804ej=Px6QR1aGaCQ_A6Lihnygw@mail.gmail.com>

Hi,

is there any documentation somewhere about the Ut interface?
i am looking to see how the clieny who set/get xml documents via homer.

i am also looking for some ways to add additiinal xml documents to the xdm
server.
ia there any page describing these functionalities?

Franck
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151021/29ec0da5/attachment.html>

From Andrew.Caldwell at metaswitch.com  Wed Oct 21 07:11:12 2015
From: Andrew.Caldwell at metaswitch.com (Andrew Caldwell)
Date: Wed, 21 Oct 2015 11:11:12 +0000
Subject: [Clearwater] create_numbers.py is missing
In-Reply-To: <F8C15A68638D0B42AAF9AA4D1B78E55D3D4E84@nxlmail10.netxcell.com>
References: <F8C15A68638D0B42AAF9AA4D1B78E55D3D4E84@nxlmail10.netxcell.com>
Message-ID: <BLUPR0201MB1588068028393C8B835475D887380@BLUPR0201MB1588.namprd02.prod.outlook.com>

Anil,

Good spot, the OVA we uploaded at the end of last sprint had an issue where Ellis was not being automatically provisioned with numbers.  You were correct to look for create_numbers.py and you could follow the instructions in https://clearwater.readthedocs.org/en/latest/Manual_Install/index.html#provision-telephone-numbers-in-ellis to provision the numbers (it's not quite as simple as running the script).  Alternatively, we've just uploaded an updated OVA with a fix for the issue to the standard place so, if you prefer, you can just download that image and deploy it instead.

Thanks for the report,

Andy

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Anil Jillella
Sent: 20 October 2015 15:07
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] create_numbers.py is missing

Hi,

I am new to this group.

I have installed all-in-one image with OVF (VMWare). I downloaded  http://vm-images.cw-ngv.com/cw-aio.ova  and started the virtual machine, everything seems fine but while creating a new private identity, ellis is not providing a number. As per the logs it says "No available numbers". I tried to find create_numbers.py to run, but I couldn't find it. Is there anything that I am missing while installation

Anil Jillella, PMP
Solution Architect - Technology,
Netxcell Limited,
B-Block, 4th Floor, Wing1, Cybergateway,
Hitechcity, Madhapur, Hyderabad-500081
Mobile: +91 8790069880


________________________________
The information contained in or accompanying this e-mail is intended only for the sole use of the stated recipient and may contain information that is confidential and/or privileged. If the reader is not the intended recipient thereof, you are hereby notified that any dissemination, distribution, printing or copying of this e-mail is strictly prohibited and may constitute a breach of confidence and/or privilege. The sender does not accept responsibility for any loss, disruption or damage to your data or computer system which may occur whilst using data contained in, or transmitted with, this e-mail. If you have received this e-mail in error, please notify the sender immediately and destroy all copies of this message and any attachments. Any views or opinions presented are solely those of the author and do not necessarily represent those of the Netxcell Limited.

www.netxcell.com<http://www.netxcell.com> - An ISO 9001:2008 and ISO 27001:2005 Certified Company
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151021/2c894922/attachment.html>

From marstonaustin at gmail.com  Thu Oct 22 11:21:51 2015
From: marstonaustin at gmail.com (Austin Marston)
Date: Thu, 22 Oct 2015 17:21:51 +0200
Subject: [Clearwater] SIP stress not working
Message-ID: <CAOVZBjBRjLfrX09x_SP462Eu1EqAp8C0Gk27+1PKV_J25uAA9A@mail.gmail.com>

Hi all,

I deployed manually clearwater infra with one
bono,sprout,ellis,ralph,homer, and hs.
My sip testing seem to be working fine but my stress testing is not working
at all.

I created a new node for sip testing, following
https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%20Numbers.md

Note:
Whenever I want to check what might be wrong I always get different status
for my nodes.
The clearwater cluster_manager seem to fail most of the time on bono and
sprout and when I check the cluster health results are always different.
Like for instance, when I ran clearwater-etcdctl cluster-health
I see that the cluster is healthy but sometimes my bono node (172-16-1-20)
or sprout node (172-16-1-20) are reported as not healthy.

[17:07:14][sprout]user at cw-002:/var/log/sprout$ clearwater-etcdctl
cluster-health
cluster is healthy
member 27a940d2104e9692 is unhealthy
member 2ea8f3a5eea05584 is healthy
member 5fdc25bd4ae527c0 is healthy
member d26088cb54745bbc is healthy
member e525c6a4ed161686 is healthy
member f5765a98a56e9c4a is healthy
[17:07:24]user at cw-002:/var/log/sprout$ clearwater-etcdctl member list
27a940d2104e9692: name=172-16-1-20 peerURLs=http://172.16.1.20:2380
clientURLs=http://172.16.1.20:4000
2ea8f3a5eea05584: name=172-16-1-22 peerURLs=http://172.16.1.22:2380
clientURLs=http://172.16.1.22:4000
5fdc25bd4ae527c0: name=172-16-1-25 peerURLs=http://172.16.1.25:2380
clientURLs=http://172.16.1.25:4000
d26088cb54745bbc: name=172-16-1-24 peerURLs=http://172.16.1.24:2380
clientURLs=http://172.16.1.24:4000
e525c6a4ed161686: name=172-16-1-21 peerURLs=http://172.16.1.21:2380
clientURLs=http://172.16.1.21:4000
f5765a98a56e9c4a: name=172-16-1-23 peerURLs=http://172.16.1.23:2380
clientURLs=http://172.16.1.23:4000
[17:10:00][sprout]user at cw-002:/var/log/sprout$ clearwater-etcdctl
cluster-health
cluster is healthy
member 27a940d2104e9692 is healthy
member 2ea8f3a5eea05584 is healthy
member 5fdc25bd4ae527c0 is healthy
member d26088cb54745bbc is healthy
member e525c6a4ed161686 is healthy
member f5765a98a56e9c4a is healthy

I attach my sip stress logs and my sprout logs. I was running the test
between 13:57 and 14:05 on the 22 of october.
If you have any idea about why this could go wrong. I certainly forgot
something that might be obvious but cannot catch it!

Thanks,
Austin?
 sprout_20151022T140000Z.txt
<https://drive.google.com/file/d/0BwD2rKlmArODQTNhWmNNanFDQmM/view?usp=drive_web>
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151022/837fd4d5/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: all_sip_log.tgz
Type: application/x-gzip
Size: 3462775 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151022/837fd4d5/attachment.bin>

From gavin.murphy at newnet.com  Thu Oct 22 11:44:07 2015
From: gavin.murphy at newnet.com (Gavin Murphy)
Date: Thu, 22 Oct 2015 12:44:07 -0300
Subject: [Clearwater] Routing to other networks
Message-ID: <56290447.3030305@newnet.com>

Hi,

    I have a all-in-one installation set up and seem to be confused
about how you set up routing to another network. I configured the
bgcf.json file to specify a route for a specific number prefix, but when
I send requests from an on-net user to a number with that prefix, I
always get a 404 Not Found. Is there something I'm missing in the setup
of routing to another network?

Thanks,

Gavin

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ This e-mail is intended only for the named recipient(s) and may contain information that is otherwise privileged, confidential and/or exempt from disclosure under applicable law. No waiver of privilege, confidence, or otherwise is intended by virtue of communication via the internet. Any unauthorized use, dissemination or copying is strictly prohibited. If you have received this e-mail in error, or are not the named as a recipient, please immediately notify the sender and destroy all copies of this e-mail. ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++



From gavin.murphy at newnet.com  Thu Oct 22 12:28:08 2015
From: gavin.murphy at newnet.com (Gavin Murphy)
Date: Thu, 22 Oct 2015 13:28:08 -0300
Subject: [Clearwater] Routing to other networks
In-Reply-To: <56290447.3030305@newnet.com>
References: <56290447.3030305@newnet.com>
Message-ID: <56290E98.9040206@newnet.com>

It was not long after I sent this question that I discovered that you
also need to set up ENUM. Requests are now being routed as I would expect.

Gavin

On 22/10/2015 12:44 PM, Gavin Murphy wrote:
> Hi,
>
>    I have a all-in-one installation set up and seem to be confused
> about how you set up routing to another network. I configured the
> bgcf.json file to specify a route for a specific number prefix, but when
> I send requests from an on-net user to a number with that prefix, I
> always get a 404 Not Found. Is there something I'm missing in the setup
> of routing to another network?
>
> Thanks,
>
> Gavin
>
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> This e-mail is intended only for the named recipient(s) and may
> contain information that is otherwise privileged, confidential and/or
> exempt from disclosure under applicable law. No waiver of privilege,
> confidence, or otherwise is intended by virtue of communication via
> the internet. Any unauthorized use, dissemination or copying is
> strictly prohibited. If you have received this e-mail in error, or are
> not the named as a recipient, please immediately notify the sender and
> destroy all copies of this e-mail.
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
>

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ This e-mail is intended only for the named recipient(s) and may contain information that is otherwise privileged, confidential and/or exempt from disclosure under applicable law. No waiver of privilege, confidence, or otherwise is intended by virtue of communication via the internet. Any unauthorized use, dissemination or copying is strictly prohibited. If you have received this e-mail in error, or are not the named as a recipient, please immediately notify the sender and destroy all copies of this e-mail. ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++



From marstonaustin at gmail.com  Fri Oct 23 02:21:46 2015
From: marstonaustin at gmail.com (Austin Marston)
Date: Fri, 23 Oct 2015 08:21:46 +0200
Subject: [Clearwater] SIP stress not working
Message-ID: <CAOVZBjBE0BBqbfu_rHwJ9F--ssh+x06fRurDC2Mhq6s-aPqttw@mail.gmail.com>

Hi all,

I deployed manually clearwater infra with one
bono,sprout,ellis,ralph,homer, and hs.
My sip testing seem to be working fine but my stress testing is not working
at all.

I created a new node for sip testing, following
https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%20Numbers.md

Note:
Whenever I want to check what might be wrong I always get different status
for my nodes.
The clearwater cluster_manager seem to fail most of the time on bono and
sprout and when I check the cluster health results are always different.
Like for instance, when I ran clearwater-etcdctl cluster-health
I see that the cluster is healthy but sometimes my bono node (172-16-1-20)
or sprout node (172-16-1-20) are reported as not healthy.

[17:07:14][sprout]user at cw-002:/var/log/sprout$ clearwater-etcdctl
cluster-health
cluster is healthy
member 27a940d2104e9692 is unhealthy
member 2ea8f3a5eea05584 is healthy
member 5fdc25bd4ae527c0 is healthy
member d26088cb54745bbc is healthy
member e525c6a4ed161686 is healthy
member f5765a98a56e9c4a is healthy
[17:07:24]user at cw-002:/var/log/sprout$ clearwater-etcdctl member list
27a940d2104e9692: name=172-16-1-20 peerURLs=http://172.16.1.20:2380
clientURLs=http://172.16.1.20:4000
2ea8f3a5eea05584: name=172-16-1-22 peerURLs=http://172.16.1.22:2380
clientURLs=http://172.16.1.22:4000
5fdc25bd4ae527c0: name=172-16-1-25 peerURLs=http://172.16.1.25:2380
clientURLs=http://172.16.1.25:4000
d26088cb54745bbc: name=172-16-1-24 peerURLs=http://172.16.1.24:2380
clientURLs=http://172.16.1.24:4000
e525c6a4ed161686: name=172-16-1-21 peerURLs=http://172.16.1.21:2380
clientURLs=http://172.16.1.21:4000
f5765a98a56e9c4a: name=172-16-1-23 peerURLs=http://172.16.1.23:2380
clientURLs=http://172.16.1.23:4000
[17:10:00][sprout]user at cw-002:/var/log/sprout$ clearwater-etcdctl
cluster-health
cluster is healthy
member 27a940d2104e9692 is healthy
member 2ea8f3a5eea05584 is healthy
member 5fdc25bd4ae527c0 is healthy
member d26088cb54745bbc is healthy
member e525c6a4ed161686 is healthy
member f5765a98a56e9c4a is healthy

I attach my sip stress logs and my sprout logs. I was running the test
between 13:57 and 14:05 on the 22 of october.
If you have any idea about why this could go wrong. I certainly forgot
something that might be obvious but cannot catch it!

Thanks,
Austin????
 sprout_20151022T140000Z.txt
<https://drive.google.com/file/d/0BwD2rKlmArODN2V3NnJMeC1Vcms/view?usp=drive_web>
????
 all_sip_log.tgz
<https://drive.google.com/file/d/0BwD2rKlmArODRXBsQXpySHhuSGc/view?usp=drive_web>
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151023/48e17f75/attachment.html>

From Eleanor.Merry at metaswitch.com  Fri Oct 23 15:42:14 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 23 Oct 2015 19:42:14 +0000
Subject: [Clearwater] Routing to other networks
In-Reply-To: <56290E98.9040206@newnet.com>
References: <56290447.3030305@newnet.com> <56290E98.9040206@newnet.com>
Message-ID: <BN3PR02MB1255DE72494C7038E747293F9B260@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Gavin, 

Glad to hear this is now working for you!

Ellie

-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Gavin Murphy
Sent: 22 October 2015 17:28
To: clearwater at lists.projectclearwater.org
Subject: Re: [Clearwater] Routing to other networks

It was not long after I sent this question that I discovered that you also need to set up ENUM. Requests are now being routed as I would expect.

Gavin

On 22/10/2015 12:44 PM, Gavin Murphy wrote:
> Hi,
>
>    I have a all-in-one installation set up and seem to be confused 
> about how you set up routing to another network. I configured the 
> bgcf.json file to specify a route for a specific number prefix, but 
> when I send requests from an on-net user to a number with that prefix, 
> I always get a 404 Not Found. Is there something I'm missing in the 
> setup of routing to another network?
>
> Thanks,
>
> Gavin
>
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> This e-mail is intended only for the named recipient(s) and may 
> contain information that is otherwise privileged, confidential and/or 
> exempt from disclosure under applicable law. No waiver of privilege, 
> confidence, or otherwise is intended by virtue of communication via 
> the internet. Any unauthorized use, dissemination or copying is 
> strictly prohibited. If you have received this e-mail in error, or are 
> not the named as a recipient, please immediately notify the sender and 
> destroy all copies of this e-mail.
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.p
> rojectclearwater.org
>

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ This e-mail is intended only for the named recipient(s) and may contain information that is otherwise privileged, confidential and/or exempt from disclosure under applicable law. No waiver of privilege, confidence, or otherwise is intended by virtue of communication via the internet. Any unauthorized use, dissemination or copying is strictly prohibited. If you have received this e-mail in error, or are not the named as a recipient, please immediately notify the sender and destroy all copies of this e-mail. ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



From Eleanor.Merry at metaswitch.com  Fri Oct 23 15:42:18 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 23 Oct 2015 19:42:18 +0000
Subject: [Clearwater] SIP stress not working
In-Reply-To: <CAOVZBjBE0BBqbfu_rHwJ9F--ssh+x06fRurDC2Mhq6s-aPqttw@mail.gmail.com>
References: <CAOVZBjBE0BBqbfu_rHwJ9F--ssh+x06fRurDC2Mhq6s-aPqttw@mail.gmail.com>
Message-ID: <BN3PR02MB1255A9DE7661631D0A207A3B9B260@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Austin,

It sounds like your etcd process could be regularly restarting. Can you send me the etcd logs (in /var/log/clearwater-etcd) and monit logs (/var/log/monit.log) from your Sprout node?
Thanks,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Austin Marston
Sent: 23 October 2015 07:22
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] SIP stress not working

Hi all,

I deployed manually clearwater infra with one bono,sprout,ellis,ralph,homer, and hs.
My sip testing seem to be working fine but my stress testing is not working at all.

I created a new node for sip testing, following https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%20Numbers.md

Note:
Whenever I want to check what might be wrong I always get different status for my nodes.
The clearwater cluster_manager seem to fail most of the time on bono and sprout and when I check the cluster health results are always different.
Like for instance, when I ran clearwater-etcdctl cluster-health
I see that the cluster is healthy but sometimes my bono node (172-16-1-20) or sprout node (172-16-1-20) are reported as not healthy.

[17:07:14][sprout]user at cw-002:/var/log/sprout$ clearwater-etcdctl cluster-health
cluster is healthy
member 27a940d2104e9692 is unhealthy
member 2ea8f3a5eea05584 is healthy
member 5fdc25bd4ae527c0 is healthy
member d26088cb54745bbc is healthy
member e525c6a4ed161686 is healthy
member f5765a98a56e9c4a is healthy
[17:07:24]user at cw-002:/var/log/sprout$ clearwater-etcdctl member list
27a940d2104e9692: name=172-16-1-20 peerURLs=http://172.16.1.20:2380<http://172.16.1.20:2380/>clientURLs=http://172.16.1.20:4000<http://172.16.1.20:4000/>
2ea8f3a5eea05584: name=172-16-1-22 peerURLs=http://172.16.1.22:2380<http://172.16.1.22:2380/>clientURLs=http://172.16.1.22:4000<http://172.16.1.22:4000/>
5fdc25bd4ae527c0: name=172-16-1-25 peerURLs=http://172.16.1.25:2380<http://172.16.1.25:2380/>clientURLs=http://172.16.1.25:4000<http://172.16.1.25:4000/>
d26088cb54745bbc: name=172-16-1-24 peerURLs=http://172.16.1.24:2380<http://172.16.1.24:2380/>clientURLs=http://172.16.1.24:4000<http://172.16.1.24:4000/>
e525c6a4ed161686: name=172-16-1-21 peerURLs=http://172.16.1.21:2380<http://172.16.1.21:2380/>clientURLs=http://172.16.1.21:4000<http://172.16.1.21:4000/>
f5765a98a56e9c4a: name=172-16-1-23 peerURLs=http://172.16.1.23:2380<http://172.16.1.23:2380/>clientURLs=http://172.16.1.23:4000<http://172.16.1.23:4000/>
[17:10:00][sprout]user at cw-002:/var/log/sprout$ clearwater-etcdctl cluster-health
cluster is healthy
member 27a940d2104e9692 is healthy
member 2ea8f3a5eea05584 is healthy
member 5fdc25bd4ae527c0 is healthy
member d26088cb54745bbc is healthy
member e525c6a4ed161686 is healthy
member f5765a98a56e9c4a is healthy

I attach my sip stress logs and my sprout logs. I was running the test between 13:57 and 14:05 on the 22 of october.
If you have any idea about why this could go wrong. I certainly forgot something that might be obvious but cannot catch it!

Thanks,
Austin????
[Image removed by sender.] sprout_20151022T140000Z.txt<https://drive.google.com/file/d/0BwD2rKlmArODN2V3NnJMeC1Vcms/view?usp=drive_web>
????
[Image removed by sender.] all_sip_log.tgz<https://drive.google.com/file/d/0BwD2rKlmArODRXBsQXpySHhuSGc/view?usp=drive_web>
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151023/9d9f2911/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ~WRD000.jpg
Type: image/jpeg
Size: 823 bytes
Desc: ~WRD000.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151023/9d9f2911/attachment.jpg>

From Eleanor.Merry at metaswitch.com  Fri Oct 23 16:09:26 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 23 Oct 2015 20:09:26 +0000
Subject: [Clearwater] Ut interface and Homer
In-Reply-To: <CAKX=ev07joDj90Ja7n2MRE2804ej=Px6QR1aGaCQ_A6Lihnygw@mail.gmail.com>
References: <CAKX=ev07joDj90Ja7n2MRE2804ej=Px6QR1aGaCQ_A6Lihnygw@mail.gmail.com>
Message-ID: <BN3PR02MB125550DF7DFC61AA479E62099B260@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Franck,

You can find our documentation on the API to Homer here (https://github.com/Metaswitch/crest/blob/dev/docs/homer_api.md).

Homer only implements a subset of the XDMS specification though ? see https://github.com/Metaswitch/crest/blob/dev/docs/homer_features.md for the details of what isn?t covered (and http://technical.openmobilealliance.org/Technical/release_program/docs/XDM/V2_0-20090810-C/OMA-TS-XDM_Core-V2_0-20090810-C.pdf for the full XDM specification).

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Franck Malka
Sent: 21 October 2015 07:03
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] Ut interface and Homer


Hi,

is there any documentation somewhere about the Ut interface?
i am looking to see how the clieny who set/get xml documents via homer.

i am also looking for some ways to add additiinal xml documents to the xdm server.
ia there any page describing these functionalities?

Franck
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151023/828b2518/attachment.html>

From tysonite at gmail.com  Tue Oct 27 05:35:29 2015
From: tysonite at gmail.com (Mikhail Kulinich)
Date: Tue, 27 Oct 2015 12:35:29 +0300
Subject: [Clearwater] RTP forwarding
Message-ID: <CAGY+F0L3+BYNJxJiMu2O44bkrad9C+PC6g+DLsParOXW5dBbow@mail.gmail.com>

Hello!

I found out that Bono requires some ports to be opened to external network.
Here is the extraction from the firewall documentation page (
http://clearwater.readthedocs.org/en/stable/Clearwater_IP_Port_Usage/index.html
):

   -

   RTP forwarding:

   UDP/32768-65535



As far as I understand, those ports may be used to carry RTP traffic
between 2 IMS clients if they are, for instance, on different sub-nets
(e.g. can't communicate directly with each other). Bono acts as RTP proxy.
Am I right? If not, could you please clarify what is RTP forwarding?

-- 
Thank you,
Mikhail
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151027/9d64ff25/attachment.html>

From Chris.Elford at metaswitch.com  Wed Oct 28 12:53:18 2015
From: Chris.Elford at metaswitch.com (Chris Elford)
Date: Wed, 28 Oct 2015 16:53:18 +0000
Subject: [Clearwater]  Quidditch Through the Ages Release Note
Message-ID: <BN1PR0201MB0801F41C7C97065E8BE27121E1210@BN1PR0201MB0801.namprd02.prod.outlook.com>

The release for sprint "Quidditch Through the Ages" has been cut. The code for this release is tagged as release-84 in github.

This release targets improved stability.

It includes the following bug fixes:

*         Sprouts continuously fail Monit poll and drop core when HSS is isolated from signaling network. (https://github.com/Metaswitch/sprout/issues/1224)

*         scribbler in stack.cpp init_stack() (https://github.com/Metaswitch/sprout/issues/1228)

*         Rf billing incorrect for Term Interim and Stop ACRs when using Sentinel VoLTE TAS. (https://github.com/Metaswitch/sprout/issues/1222)

*         Confusion about signature of on_tx_request() (https://github.com/Metaswitch/sprout/issues/1184)

*         scheme setup scripts always wait for the grace period during install (https://github.com/Metaswitch/homestead/issues/269)

*         Homestead crash while pegging diameter statistics (https://github.com/Metaswitch/homestead/issues/255)

*         Multiple Chronos crashes when attempting 50 registrations per second. (https://github.com/Metaswitch/chronos/issues/181)

*         Public ID query sends end chunk before all data is sent (https://github.com/Metaswitch/crest/issues/252)

*         cassandra is still running after decommission  (https://github.com/Metaswitch/clearwater-cassandra/issues/58)

*         Geographic redundancy doc misrepresents scalability (https://github.com/Metaswitch/clearwater-docs/issues/19)

*         Clearwater-socket-factory isn't always started by upstart (https://github.com/Metaswitch/clearwater-infrastructure/issues/260)

*         net-snmp proxying compares against the wrong range (https://github.com/Metaswitch/clearwater-snmp-handlers/issues/89)

*         *_NOT_YET_CLUSTERED alarms are never raised, but they're in the source and JSON (https://github.com/Metaswitch/clearwater-etcd/issues/213)

*         TOO_LONG_CLUSTERING_MINOR is using the wrong OID (https://github.com/Metaswitch/clearwater-etcd/issues/211)

*         mark_node_failed does not properly deal with cassandra clusters. (https://github.com/Metaswitch/clearwater-etcd/issues/205)

*         Hang when boxes try to leave a cluster (https://github.com/Metaswitch/clearwater-etcd/issues/204)

*         Plugin utility method join_cassandra_cluster is too destructive (https://github.com/Metaswitch/clearwater-etcd/issues/172)

*         Possible backward compatibility issues between release-79 and release-73 (https://github.com/Metaswitch/clearwater-etcd/issues/168)

*         Nodes can miss the shared configuration file (https://github.com/Metaswitch/clearwater-heat/issues/15)

*         Ellis numbers malformed (https://github.com/Metaswitch/clearwater-heat/issues/13)

To upgrade to this release, follow the instructions at http://clearwater.readthedocs.org/en/latest/Upgrading_a_Clearwater_deployment/index.html. If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.

Yours,

Chris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151028/729f1b69/attachment.html>

From marstonaustin at gmail.com  Thu Oct 29 05:48:55 2015
From: marstonaustin at gmail.com (Austin Marston)
Date: Thu, 29 Oct 2015 10:48:55 +0100
Subject: [Clearwater] Fwd:  SIP stress not working
In-Reply-To: <CAOVZBjBOTpz0ZdCVJVxC9OQgHj6pt8H6UQ_eLi97YfohgYxYPA@mail.gmail.com>
References: <CAOVZBjBE0BBqbfu_rHwJ9F--ssh+x06fRurDC2Mhq6s-aPqttw@mail.gmail.com>
	<BN3PR02MB1255A9DE7661631D0A207A3B9B260@BN3PR02MB1255.namprd02.prod.outlook.com>
	<CAOVZBjBOTpz0ZdCVJVxC9OQgHj6pt8H6UQ_eLi97YfohgYxYPA@mail.gmail.com>
Message-ID: <CAOVZBjAYaKYDeL64YvzgN_XysgMNyw+q2jXZZ5SgF0xwiXyMjg@mail.gmail.com>

Hello,

Thanks, you'll find attached the logs of sprout etc and monit from a brand
new run of the sip testing.
While I tested that, the sprout status of  etcd_process was "Does not
exist" and clearwater_cluster_manager was "Execution failed".

Thanks a lot,
Austin

2015-10-23 21:42 GMT+02:00 Eleanor Merry <Eleanor.Merry at metaswitch.com>:

> Hi Austin,
>
>
>
> It sounds like your etcd process could be regularly restarting. Can you
> send me the etcd logs (in /var/log/clearwater-etcd) and monit logs
> (/var/log/monit.log) from your Sprout node?
>
> Thanks,
>
>
>
> Ellie
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Austin Marston
> *Sent:* 23 October 2015 07:22
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Clearwater] SIP stress not working
>
>
>
> Hi all,
>
> I deployed manually clearwater infra with one
> bono,sprout,ellis,ralph,homer, and hs.
> My sip testing seem to be working fine but my stress testing is not
> working at all.
>
> I created a new node for sip testing, following
> https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%20Numbers.md
>
> Note:
> Whenever I want to check what might be wrong I always get different status
> for my nodes.
> The clearwater cluster_manager seem to fail most of the time on bono and
> sprout and when I check the cluster health results are always different.
> Like for instance, when I ran clearwater-etcdctl cluster-health
> I see that the cluster is healthy but sometimes my bono node (172-16-1-20)
> or sprout node (172-16-1-20) are reported as not healthy.
>
> [17:07:14][sprout]user at cw-002:/var/log/sprout$ clearwater-etcdctl
> cluster-health
> cluster is healthy
> member 27a940d2104e9692 is unhealthy
> member 2ea8f3a5eea05584 is healthy
> member 5fdc25bd4ae527c0 is healthy
> member d26088cb54745bbc is healthy
> member e525c6a4ed161686 is healthy
> member f5765a98a56e9c4a is healthy
> [17:07:24]user at cw-002:/var/log/sprout$ clearwater-etcdctl member list
> 27a940d2104e9692: name=172-16-1-20 peerURLs=http://172.16.1.20:2380
> clientURLs=http://172.16.1.20:4000
> 2ea8f3a5eea05584: name=172-16-1-22 peerURLs=http://172.16.1.22:2380
> clientURLs=http://172.16.1.22:4000
> 5fdc25bd4ae527c0: name=172-16-1-25 peerURLs=http://172.16.1.25:2380
> clientURLs=http://172.16.1.25:4000
> d26088cb54745bbc: name=172-16-1-24 peerURLs=http://172.16.1.24:2380
> clientURLs=http://172.16.1.24:4000
> e525c6a4ed161686: name=172-16-1-21 peerURLs=http://172.16.1.21:2380
> clientURLs=http://172.16.1.21:4000
> f5765a98a56e9c4a: name=172-16-1-23 peerURLs=http://172.16.1.23:2380
> clientURLs=http://172.16.1.23:4000
> [17:10:00][sprout]user at cw-002:/var/log/sprout$ clearwater-etcdctl
> cluster-health
> cluster is healthy
> member 27a940d2104e9692 is healthy
> member 2ea8f3a5eea05584 is healthy
> member 5fdc25bd4ae527c0 is healthy
> member d26088cb54745bbc is healthy
> member e525c6a4ed161686 is healthy
> member f5765a98a56e9c4a is healthy
>
> I attach my sip stress logs and my sprout logs. I was running the test
> between 13:57 and 14:05 on the 22 of october.
> If you have any idea about why this could go wrong. I certainly forgot
> something that might be obvious but cannot catch it!
>
> Thanks,
> Austin????
>
> *[image: Image removed by sender.] sprout_20151022T140000Z.txt
> <https://drive.google.com/file/d/0BwD2rKlmArODN2V3NnJMeC1Vcms/view?usp=drive_web>*
>
> ????
>
> *[image: Image removed by sender.] all_sip_log.tgz
> <https://drive.google.com/file/d/0BwD2rKlmArODRXBsQXpySHhuSGc/view?usp=drive_web>*
>
> ?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151029/9c287519/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ~WRD000.jpg
Type: image/jpeg
Size: 823 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151029/9c287519/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: monit.log.bckup
Type: application/octet-stream
Size: 1260019 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151029/9c287519/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: clearwater-etcd.log.bckup
Type: application/octet-stream
Size: 87953 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151029/9c287519/attachment-0001.obj>

From marstonaustin at gmail.com  Thu Oct 29 06:15:05 2015
From: marstonaustin at gmail.com (Austin Marston)
Date: Thu, 29 Oct 2015 11:15:05 +0100
Subject: [Clearwater] Fwd:  SIP stress not working
In-Reply-To: <CAOVZBjBOTpz0ZdCVJVxC9OQgHj6pt8H6UQ_eLi97YfohgYxYPA@mail.gmail.com>
References: <CAOVZBjBE0BBqbfu_rHwJ9F--ssh+x06fRurDC2Mhq6s-aPqttw@mail.gmail.com>
	<BN3PR02MB1255A9DE7661631D0A207A3B9B260@BN3PR02MB1255.namprd02.prod.outlook.com>
	<CAOVZBjBOTpz0ZdCVJVxC9OQgHj6pt8H6UQ_eLi97YfohgYxYPA@mail.gmail.com>
Message-ID: <CAOVZBjDhk30tDb95ZkMj1EwXH71CZUxPKsSMdzPWfu6xQVCy+g@mail.gmail.com>

Hello,

Thanks, you'll find attached the logs of sprout etc and monit from a brand
new run of the sip testing.
While I tested that, the sprout status of  etcd_process was "Does not
exist" and clearwater_cluster_manager was "Execution failed".

Thanks a lot,
Austin

2015-10-23 21:42 GMT+02:00 Eleanor Merry <Eleanor.Merry at metaswitch.com>:

> Hi Austin,
>
>
>
> It sounds like your etcd process could be regularly restarting. Can you
> send me the etcd logs (in /var/log/clearwater-etcd) and monit logs
> (/var/log/monit.log) from your Sprout node?
>
> Thanks,
>
>
>
> Ellie
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Austin Marston
> *Sent:* 23 October 2015 07:22
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Clearwater] SIP stress not working
>
>
>
> Hi all,
>
> I deployed manually clearwater infra with one
> bono,sprout,ellis,ralph,homer, and hs.
> My sip testing seem to be working fine but my stress testing is not
> working at all.
>
> I created a new node for sip testing, following
> https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%20Numbers.md
>
> Note:
> Whenever I want to check what might be wrong I always get different status
> for my nodes.
> The clearwater cluster_manager seem to fail most of the time on bono and
> sprout and when I check the cluster health results are always different.
> Like for instance, when I ran clearwater-etcdctl cluster-health
> I see that the cluster is healthy but sometimes my bono node (172-16-1-20)
> or sprout node (172-16-1-20) are reported as not healthy.
>
> [17:07:14][sprout]user at cw-002:/var/log/sprout$ clearwater-etcdctl
> cluster-health
> cluster is healthy
> member 27a940d2104e9692 is unhealthy
> member 2ea8f3a5eea05584 is healthy
> member 5fdc25bd4ae527c0 is healthy
> member d26088cb54745bbc is healthy
> member e525c6a4ed161686 is healthy
> member f5765a98a56e9c4a is healthy
> [17:07:24]user at cw-002:/var/log/sprout$ clearwater-etcdctl member list
> 27a940d2104e9692: name=172-16-1-20 peerURLs=http://172.16.1.20:2380
> clientURLs=http://172.16.1.20:4000
> 2ea8f3a5eea05584: name=172-16-1-22 peerURLs=http://172.16.1.22:2380
> clientURLs=http://172.16.1.22:4000
> 5fdc25bd4ae527c0: name=172-16-1-25 peerURLs=http://172.16.1.25:2380
> clientURLs=http://172.16.1.25:4000
> d26088cb54745bbc: name=172-16-1-24 peerURLs=http://172.16.1.24:2380
> clientURLs=http://172.16.1.24:4000
> e525c6a4ed161686: name=172-16-1-21 peerURLs=http://172.16.1.21:2380
> clientURLs=http://172.16.1.21:4000
> f5765a98a56e9c4a: name=172-16-1-23 peerURLs=http://172.16.1.23:2380
> clientURLs=http://172.16.1.23:4000
> [17:10:00][sprout]user at cw-002:/var/log/sprout$ clearwater-etcdctl
> cluster-health
> cluster is healthy
> member 27a940d2104e9692 is healthy
> member 2ea8f3a5eea05584 is healthy
> member 5fdc25bd4ae527c0 is healthy
> member d26088cb54745bbc is healthy
> member e525c6a4ed161686 is healthy
> member f5765a98a56e9c4a is healthy
>
> I attach my sip stress logs and my sprout logs. I was running the test
> between 13:57 and 14:05 on the 22 of october.
> If you have any idea about why this could go wrong. I certainly forgot
> something that might be obvious but cannot catch it!
>
> Thanks,
> Austin????
> ?
>  clearwater-etcd.log.bckup
> <https://drive.google.com/file/d/0BwD2rKlmArODcmRDZlZSWWRBcC1LUnBEYm9vMHhoaUczbGQw/view?usp=drive_web>
> ??
>  monit.log.bckup
> <https://drive.google.com/file/d/0BwD2rKlmArODM3ZTQ1J6OGZ6eDJNZHk3N1pSQUhaM1BuRU84/view?usp=drive_web>
> ?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151029/610bdf1e/attachment.html>

From peter.skrzynski at nec.co.nz  Thu Oct 29 23:47:23 2015
From: peter.skrzynski at nec.co.nz (Peter Skrzynski)
Date: Fri, 30 Oct 2015 03:47:23 +0000
Subject: [Clearwater] Bono node restarting under high load
Message-ID: <7fb48b648e4e47bebc9afce372b5c679@ex15w.necnz.internal>

Hi Clearwater,
I have a manual install (76 In Dubious Battle) of bono/sprout/homestead/ibcf/dns being used as a SIP proxy between Voip UE?s and a PSTN. I have a load tester doing hundreds of calls, and also hundreds of registrations. When the PSTN becomes unavailable, then my Bono goes into overload and henceforth continually restarts?
It seems that the Bono process starts consuming too much memory (over 80%), and this causes Monit to restart it.
Once I turn off load tester, and restore PSTN, and then restart load tester, then all is OK again.
Can anyone advise how to investigate this further, or is this a weakness of Bono?

Bono log shows something like?
signal 6 caught
basic stack dump:
/usr/share/Clearwater/bin/bono(_ZN6Logger9backtraceEPKc+0x6d)[0x4cdd0d]
/usr/share/Clearwater/bin/bono(_ZN3Log9backtraceEPKcz+0x10d)[0x58126d]
/usr/share/Clearwater/bin/bono(_Z14signal_handleri+0x2c)[0x5d8e7c]
/lib/x86_64-linux-gnu/libc.so.6(+0x36d40)[0x7f028782d40]
/lib/x86_64-linux-gnu/libpthread.so.0(sem_wait+0x2e)[0x7f028844f67e]
/usr/share/Clearwater/bin/bono(main+0x5bdc)[0x4cc91c]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf5)[0x7f028787dec5]
/usr/share/Clearwater/bin/bono[0x4cdbbc]
sdvanced stack dump (requires gdb):
sh: 1 /usr/bin/gdb: not found

?free ?m? command shows something like?
Mem:   total=1994   used=1662  free=371
And amount of free memory keeps dropping.


Also ?ps -eo pmem,pcpu,vsize,pid,cmd | sort -k 1 -nr | head -5? commands

shows that the bono process using the most memory.





Peter Skrzynski

Technical Lead
NEC New Zealand Limited
NEC House, Level 6, 40 Taranaki Street, PO Box 1936, Wellington 6011, New Zealand
T: 043816257, M: 0274849530, F: +6443811110
peter.skrzynski at nec.co.nz<mailto:peter.skrzynski at nec.co.nz>
nz.nec.com<http://nz.nec.com>

[cid:image7a9ca8.JPG at 5aecb01e.4da02883]

Please consider the environment before printing this email

Attention:
The information contained in this message and or attachments is intended only for the person or entity to which it is addressed and may contain confidential and/or privileged material.  Any review, retransmission, dissemination, copying or other use of, or taking of any action in reliance upon, this information by persons or entities other than the intended recipient is prohibited. If you received this in error, please contact the sender and delete the material from any system and destroy any copies. NEC has no liability for any act or omission in reliance on the email or any attachment.  Before opening this email or any attachment(s), please check them for viruses. NEC is not responsible for any viruses in this email or any attachment(s); any changes made to this  email or any attachment(s) after they are sent; or any effects this email or any attachment(s) have on your network or computer system.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151030/9decc4a3/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image7a9ca8.JPG
Type: image/jpeg
Size: 59501 bytes
Desc: image7a9ca8.JPG
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151030/9decc4a3/attachment.jpe>

From Eleanor.Merry at metaswitch.com  Fri Oct 30 15:34:43 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 30 Oct 2015 19:34:43 +0000
Subject: [Clearwater] Bono node restarting under high load
In-Reply-To: <7fb48b648e4e47bebc9afce372b5c679@ex15w.necnz.internal>
References: <7fb48b648e4e47bebc9afce372b5c679@ex15w.necnz.internal>
Message-ID: <BN3PR02MB12557D1D408000295758FA599B2F0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Peter,

Can you send across the advanced stack dump? You?ll need to install bono-dbg, then run this scenario again.

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Peter Skrzynski
Sent: 30 October 2015 03:47
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] Bono node restarting under high load

Hi Clearwater,
I have a manual install (76 In Dubious Battle) of bono/sprout/homestead/ibcf/dns being used as a SIP proxy between Voip UE?s and a PSTN. I have a load tester doing hundreds of calls, and also hundreds of registrations. When the PSTN becomes unavailable, then my Bono goes into overload and henceforth continually restarts?
It seems that the Bono process starts consuming too much memory (over 80%), and this causes Monit to restart it.
Once I turn off load tester, and restore PSTN, and then restart load tester, then all is OK again.
Can anyone advise how to investigate this further, or is this a weakness of Bono?

Bono log shows something like?
signal 6 caught
basic stack dump:
/usr/share/Clearwater/bin/bono(_ZN6Logger9backtraceEPKc+0x6d)[0x4cdd0d]
/usr/share/Clearwater/bin/bono(_ZN3Log9backtraceEPKcz+0x10d)[0x58126d]
/usr/share/Clearwater/bin/bono(_Z14signal_handleri+0x2c)[0x5d8e7c]
/lib/x86_64-linux-gnu/libc.so.6(+0x36d40)[0x7f028782d40]
/lib/x86_64-linux-gnu/libpthread.so.0(sem_wait+0x2e)[0x7f028844f67e]
/usr/share/Clearwater/bin/bono(main+0x5bdc)[0x4cc91c]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf5)[0x7f028787dec5]
/usr/share/Clearwater/bin/bono[0x4cdbbc]
sdvanced stack dump (requires gdb):
sh: 1 /usr/bin/gdb: not found

?free ?m? command shows something like?
Mem:   total=1994   used=1662  free=371
And amount of free memory keeps dropping.


Also ?ps -eo pmem,pcpu,vsize,pid,cmd | sort -k 1 -nr | head -5? commands

shows that the bono process using the most memory.





Peter Skrzynski
Technical Lead
NEC New Zealand Limited
NEC House, Level 6, 40 Taranaki Street, PO Box 1936, Wellington 6011, New Zealand
T: 043816257, M: 0274849530, F: +6443811110
peter.skrzynski at nec.co.nz<mailto:peter.skrzynski at nec.co.nz>
nz.nec.com<http://nz.nec.com>

[cid:image002.jpg at 01D11341.6AD0DD10]

Please consider the environment before printing this email

Attention:
The information contained in this message and or attachments is intended only for the person or entity to which it is addressed and may contain confidential and/or privileged material.  Any review, retransmission, dissemination, copying or other use of, or taking of any action in reliance upon, this information by persons or entities other than the intended recipient is prohibited. If you received this in error, please contact the sender and delete the material from any system and destroy any copies. NEC has no liability for any act or omission in reliance on the email or any attachment.  Before opening this email or any attachment(s), please check them for viruses. NEC is not responsible for any viruses in this email or any attachment(s); any changes made to this  email or any attachment(s) after they are sent; or any effects this email or any attachment(s) have on your network or computer system.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151030/5f1610b4/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 8344 bytes
Desc: image002.jpg
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151030/5f1610b4/attachment.jpg>

From Eleanor.Merry at metaswitch.com  Fri Oct 30 15:34:57 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 30 Oct 2015 19:34:57 +0000
Subject: [Clearwater] RTP forwarding
In-Reply-To: <CAGY+F0L3+BYNJxJiMu2O44bkrad9C+PC6g+DLsParOXW5dBbow@mail.gmail.com>
References: <CAGY+F0L3+BYNJxJiMu2O44bkrad9C+PC6g+DLsParOXW5dBbow@mail.gmail.com>
Message-ID: <BN3PR02MB125581460270C38314CF53DD9B2F0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Mikhail,

That?s correct!

Bono nodes run an instance of restund (http://www.creytiv.com/restund.html) which acts as a STUN/TURN server to help establish media flows if the endpoints support ICE (https://en.wikipedia.org/wiki/Interactive_Connectivity_Establishment).  In the event that the endpoints are behind restrictive NATs, the restund service may need to proxy the media streams to allow them to get through the NATs (using TURN).

If your endpoints are behind NATs but do not support ICE for media NAT traversal, you will need the P-CSCF to anchor the media on all calls.  Bono does not support this functionality, but our commercial P-CSCF, Perimeta (http://www.metaswitch.com/products/perimeta-session-border-controller), does.

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Mikhail Kulinich
Sent: 27 October 2015 09:35
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] RTP forwarding

Hello!

I found out that Bono requires some ports to be opened to external network.
Here is the extraction from the firewall documentation page (http://clearwater.readthedocs.org/en/stable/Clearwater_IP_Port_Usage/index.html):

?         RTP forwarding:

?         UDP/32768-65535

As far as I understand, those ports may be used to carry RTP traffic between 2 IMS clients if they are, for instance, on different sub-nets (e.g. can't communicate directly with each other). Bono acts as RTP proxy. Am I right? If not, could you please clarify what is RTP forwarding?

--
Thank you,
Mikhail
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151030/3913be5f/attachment.html>

From Eleanor.Merry at metaswitch.com  Fri Oct 30 15:35:04 2015
From: Eleanor.Merry at metaswitch.com (Eleanor Merry)
Date: Fri, 30 Oct 2015 19:35:04 +0000
Subject: [Clearwater] Fwd:  SIP stress not working
In-Reply-To: <CAOVZBjDhk30tDb95ZkMj1EwXH71CZUxPKsSMdzPWfu6xQVCy+g@mail.gmail.com>
References: <CAOVZBjBE0BBqbfu_rHwJ9F--ssh+x06fRurDC2Mhq6s-aPqttw@mail.gmail.com>
	<BN3PR02MB1255A9DE7661631D0A207A3B9B260@BN3PR02MB1255.namprd02.prod.outlook.com>
	<CAOVZBjBOTpz0ZdCVJVxC9OQgHj6pt8H6UQ_eLi97YfohgYxYPA@mail.gmail.com>
	<CAOVZBjDhk30tDb95ZkMj1EwXH71CZUxPKsSMdzPWfu6xQVCy+g@mail.gmail.com>
Message-ID: <BN3PR02MB125509BE65CF912343F32ADF9B2F0@BN3PR02MB1255.namprd02.prod.outlook.com>

Hi Austin,

The etcd processes (on all your nodes) are continually restarting. This is because monit is marking the etcd process as unresponsive, and so killing the etcd process.

The monit check for responsive used to write to etcd. We?ve since realised that this is a bad check to do, as it relies on the etcd leader being up. This can lead to the situation where monit continually kills all etcd processes as:


?         Something initially goes wrong and the current etcd leader fails

?         The surviving etcd processes attempt to elect a new leader

?         While the election is in process, they can?t write to etcd

?         Monit kills the rest of the etcd processes

?         This now can?t recover as the etcd processes won?t stay up long enough to elect a leader

We?ve fixed this in the latest Project Clearwater release. Can you try upgrading your system and running stress again?

Thanks,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Austin Marston
Sent: 29 October 2015 10:15
To: clearwater at lists.projectclearwater.org
Subject: [Clearwater] Fwd: SIP stress not working

Hello,

Thanks, you'll find attached the logs of sprout etc and monit from a brand new run of the sip testing.
While I tested that, the sprout status of  etcd_process was "Does not exist" and clearwater_cluster_manager was "Execution failed".

Thanks a lot,
Austin

2015-10-23 21:42 GMT+02:00 Eleanor Merry <Eleanor.Merry at metaswitch.com<mailto:Eleanor.Merry at metaswitch.com>>:
Hi Austin,

It sounds like your etcd process could be regularly restarting. Can you send me the etcd logs (in /var/log/clearwater-etcd) and monit logs (/var/log/monit.log) from your Sprout node?
Thanks,

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Austin Marston
Sent: 23 October 2015 07:22
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Clearwater] SIP stress not working

Hi all,

I deployed manually clearwater infra with one bono,sprout,ellis,ralph,homer, and hs.
My sip testing seem to be working fine but my stress testing is not working at all.

I created a new node for sip testing, following https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%20Numbers.md

Note:
Whenever I want to check what might be wrong I always get different status for my nodes.
The clearwater cluster_manager seem to fail most of the time on bono and sprout and when I check the cluster health results are always different.
Like for instance, when I ran clearwater-etcdctl cluster-health
I see that the cluster is healthy but sometimes my bono node (172-16-1-20) or sprout node (172-16-1-20) are reported as not healthy.

[17:07:14][sprout]user at cw-002:/var/log/sprout$ clearwater-etcdctl cluster-health
cluster is healthy
member 27a940d2104e9692 is unhealthy
member 2ea8f3a5eea05584 is healthy
member 5fdc25bd4ae527c0 is healthy
member d26088cb54745bbc is healthy
member e525c6a4ed161686 is healthy
member f5765a98a56e9c4a is healthy
[17:07:24]user at cw-002:/var/log/sprout$ clearwater-etcdctl member list
27a940d2104e9692: name=172-16-1-20 peerURLs=http://172.16.1.20:2380<http://172.16.1.20:2380/>clientURLs=http://172.16.1.20:4000<http://172.16.1.20:4000/>
2ea8f3a5eea05584: name=172-16-1-22 peerURLs=http://172.16.1.22:2380<http://172.16.1.22:2380/>clientURLs=http://172.16.1.22:4000<http://172.16.1.22:4000/>
5fdc25bd4ae527c0: name=172-16-1-25 peerURLs=http://172.16.1.25:2380<http://172.16.1.25:2380/>clientURLs=http://172.16.1.25:4000<http://172.16.1.25:4000/>
d26088cb54745bbc: name=172-16-1-24 peerURLs=http://172.16.1.24:2380<http://172.16.1.24:2380/>clientURLs=http://172.16.1.24:4000<http://172.16.1.24:4000/>
e525c6a4ed161686: name=172-16-1-21 peerURLs=http://172.16.1.21:2380<http://172.16.1.21:2380/>clientURLs=http://172.16.1.21:4000<http://172.16.1.21:4000/>
f5765a98a56e9c4a: name=172-16-1-23 peerURLs=http://172.16.1.23:2380<http://172.16.1.23:2380/>clientURLs=http://172.16.1.23:4000<http://172.16.1.23:4000/>
[17:10:00][sprout]user at cw-002:/var/log/sprout$ clearwater-etcdctl cluster-health
cluster is healthy
member 27a940d2104e9692 is healthy
member 2ea8f3a5eea05584 is healthy
member 5fdc25bd4ae527c0 is healthy
member d26088cb54745bbc is healthy
member e525c6a4ed161686 is healthy
member f5765a98a56e9c4a is healthy

I attach my sip stress logs and my sprout logs. I was running the test between 13:57 and 14:05 on the 22 of october.
If you have any idea about why this could go wrong. I certainly forgot something that might be obvious but cannot catch it!

Thanks,
Austin????
?
[https://ssl.gstatic.com/docs/doclist/images/icon_10_generic_list.png] clearwater-etcd.log.bckup<https://drive.google.com/file/d/0BwD2rKlmArODcmRDZlZSWWRBcC1LUnBEYm9vMHhoaUczbGQw/view?usp=drive_web>
??
[https://ssl.gstatic.com/docs/doclist/images/icon_10_generic_list.png] monit.log.bckup<https://drive.google.com/file/d/0BwD2rKlmArODM3ZTQ1J6OGZ6eDJNZHk3N1pSQUhaM1BuRU84/view?usp=drive_web>
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20151030/6bc2ff86/attachment.html>

