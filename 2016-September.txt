From skhalup at virtuozzo.com  Thu Sep  1 05:48:42 2016
From: skhalup at virtuozzo.com (Stanislav Khalup)
Date: Thu, 1 Sep 2016 09:48:42 +0000
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress test
Message-ID: <HE1PR0802MB25074DC687820CFBE4CF5C6CA1E20@HE1PR0802MB2507.eurprd08.prod.outlook.com>

Hello all,

We've been trying to perform IMS stress testing for some time now but it seems that we are really unlucky. When we perform sip test we experience constant bono/sprout crashes which affects results of our performance evaluation. The thing is we do know that generally our deployment is working (we managed to perform calls and run tests). At first we manually deployed IMS cluster but after crashes we decided to try all in one VM but we still experience sprout crashes (bono crashes are mostly fixed after setting 1CPU/1Worker). Could you please look at the dumps: https://www.dropbox.com/sh/qjdja9eowgvo1zc/AADm25_pwKNs3gWwBmb0Pzhpa?dl=0 because for now we have no clue for what is happening.

BR,
Stanislav
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160901/7a394443/attachment.html>

From michaelkatsoulis88 at gmail.com  Fri Sep  2 03:35:28 2016
From: michaelkatsoulis88 at gmail.com (=?UTF-8?B?zpzOuc+HzqzOu863z4IgzprOsc+Ez4POv8+NzrvOt8+C?=)
Date: Fri, 2 Sep 2016 10:35:28 +0300
Subject: [Project Clearwater]  Increase stress test load
Message-ID: <CAJG3f9MuVGuZsWtF9oJ-QAsF4BiVg42kijPeBcitHgjXWLiwqQ@mail.gmail.com>

Hi all,

I have successfully manually installed Clearwater in 6 virtual machines and
tested my deployment using SIP clients (following instructions from
http://clearwater.readthedocs.io/en/stable/Making_your_first_call.html.).
I also created a SIP Stress node and everything seems to be working great.
The instructions in
http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html
mention that a pair of subscribers is registered  every 5 minutes and then
making a call every 30 minutes.
How is it possible that i can increase the number of registrations and
calls per minute to test the performance of my deployment ?
Could anyone help?

Thank you in advance,
Michael Katsoulis
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/def6a606/attachment.html>

From Richard.Whitehouse at metaswitch.com  Fri Sep  2 05:14:00 2016
From: Richard.Whitehouse at metaswitch.com (Richard Whitehouse)
Date: Fri, 2 Sep 2016 09:14:00 +0000
Subject: [Project Clearwater] Increase stress test load
In-Reply-To: <CAJG3f9MuVGuZsWtF9oJ-QAsF4BiVg42kijPeBcitHgjXWLiwqQ@mail.gmail.com>
References: <CAJG3f9MuVGuZsWtF9oJ-QAsF4BiVg42kijPeBcitHgjXWLiwqQ@mail.gmail.com>
Message-ID: <BY2PR0201MB1816792F48D0CA279F9C1CF1F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>

Michael,

The easiest way to increase the load on the system is to add more subscribers. For each 30,000 subscribers, we recommend you have a separate SIP stress node.

Altering the number of subscribers is covered under the SIP Stress article which documents the settings you can change http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html

Alternatively, if you are looking at different scenarios, you can alter the stress script, which is at /usr/share/clearwater/sip-stress/sip-stress.xml on the stress nodes, or create a new one from scratch.


Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of ??????? ?ats?????
Sent: 02 September 2016 08:35
To: Clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Increase stress test load

Hi all,

I have successfully manually installed Clearwater in 6 virtual machines and tested my deployment using SIP clients (following instructions from http://clearwater.readthedocs.io/en/stable/Making_your_first_call.html.).
I also created a SIP Stress node and everything seems to be working great.
The instructions in http://clearwater.readthedocs.io/en/stable/Clearwater_stress_testing.html
mention that a pair of subscribers is registered  every 5 minutes and then making a call every 30 minutes.
How is it possible that i can increase the number of registrations and calls per minute to test the performance of my deployment ?
Could anyone help?

Thank you in advance,
Michael Katsoulis
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/bf9f08b2/attachment.html>

From Graeme.Robertson at metaswitch.com  Fri Sep  2 06:51:12 2016
From: Graeme.Robertson at metaswitch.com (Graeme Robertson)
Date: Fri, 2 Sep 2016 10:51:12 +0000
Subject: [Project Clearwater] Lapras release note
In-Reply-To: <CY4PR02MB2616496D24B3CC12317786F0E3E20@CY4PR02MB2616.namprd02.prod.outlook.com>
References: <CY4PR02MB2616496D24B3CC12317786F0E3E20@CY4PR02MB2616.namprd02.prod.outlook.com>
Message-ID: <CY4PR02MB2616725F8B26FDD0D61E8366E3E50@CY4PR02MB2616.namprd02.prod.outlook.com>

The release for Project Clearwater sprint "Lapras" has been cut. The code for this release is tagged as release-105 in github.

This release includes the following bug fixes:

*         Privacy header options delimiter is incorrect (https://github.com/Metaswitch/sprout/issues/1514)

*         The --default-tel-uri-translation option is marked as requiring an argument, but doesn't need one (https://github.com/Metaswitch/sprout/issues/1511)

*         sprout can't talk to chronos in an IPv6 environment (https://github.com/Metaswitch/sprout/issues/1510)

*         Restund can't be started by monit (https://github.com/Metaswitch/sprout/issues/1509)

*         IPv6 Homestead fails to connect to Cassandra. (https://github.com/Metaswitch/homestead/issues/13)

*         Stuck RALF_PROCESS_FAIL alarm (https://github.com/Metaswitch/ralf/issues/238)

*         URLs not formatted correctly for IPv6 (https://github.com/Metaswitch/chronos/issues/282)

*         homestead and ralf wait up to 120s before retrying Diameter connection (https://github.com/Metaswitch/cpp-common/issues/106)

*         adding an ibcf node to a deployment doesn't create a route53 entry for it (https://github.com/Metaswitch/chef/issues/288)

*         Clearwater AIO node hostname is "cwaio" not "cw-aio" (https://github.com/Metaswitch/clearwater-infrastructure/issues/363)

*         Astaire crashes in the FV tests (https://github.com/Metaswitch/astaire/issues/70)

*         LZ4 compressing and decompressing of large messages doesn't work (https://github.com/Metaswitch/sas-client/issues/68)


To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html. If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.



Graeme
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/3fa84bc5/attachment.html>

From kosalayb at gmail.com  Fri Sep  2 09:56:37 2016
From: kosalayb at gmail.com (Kosala)
Date: Fri, 2 Sep 2016 14:56:37 +0100
Subject: [Project Clearwater] Stress Testing
Message-ID: <CAGEe17aSXE9EC=A+yOcFopZK0r81mzQf-Co=SErRaccMnCRB4w@mail.gmail.com>

Hi Team,

I could manage to do stress testing and I can see the following results in
log.
Does this mean , it creates and do successfully 500 outgoing calls in 32.07
s ?

Log is as follows:

^[[2J------------------------------ Scenario Screen -------- [1-9]: Change
Screen --^M
     Users (length)   Port   Total-time  Total-calls  Remote-host^M
         500 (0 ms)   5060      32.07 s          500  10.0.140.153:5060
(TCP)^M
^M
  Call limit reached (-m 500), 1.001 s period  1 ms scheduler resolution^M
  500 calls (limit 500)                  Peak was 500 calls, after 0 s^M
  0 Running, 503 Paused, 3 Woken up^M
  0 dead call msg (discarded)            0 out-of-call msg
(discarded)        ^M
  28 open sockets                       ^M
^M
                                 Messages  Retrans   Timeout
Unexpected-Msg^M
       Pause [0ms/10:00]         500                           0        ^M
    REGISTER ---------->         26        0                            ^M
         401 <----------         26        0         0         0        ^M
    REGISTER ---------->         26        0                            ^M
         200 <----------         26        0         0         0        ^M
    REGISTER ---------->         26        0                            ^M
         401 <----------         26        0         0         0        ^M
    REGISTER ---------->         26        0                            ^M
         200 <----------         26        0         0         0        ^M
       Pause [     4:50]         22                            0        ^M
    REGISTER ---------->  B-RTD1 0         0                            ^M
         200 <----------  E-RTD1 0         0         0         0        ^M
    REGISTER ---------->  B-RTD1 0         0                            ^M
         200 <----------  E-RTD1 0         0         0         0        ^M
       Pause [     4:50]         0                             0        ^M
       Pause [$pre_call_delay]   4                             0        ^M
      INVITE ---------->  B-RTD2 1         0                            ^M
         100 <----------         0         0         0         0        ^M
      INVITE <----------         0         0         0         0        ^M
         100 <----------         0         0         0         0        ^M
      INVITE <----------         0         0         0         0        ^M
         100 ---------->         0         0                            ^M
         180 ---------->         0         0                            ^M
         180 <----------         0         0         0         0        ^M
       Pause [   6000ms]         0                             0        ^M
         200 ---------->         0         0                            ^M
         200 <----------         0         0         0         0        ^M
         ACK ---------->         0         0                            ^M
         ACK <----------         0         0         0         0        ^M
      UPDATE ---------->         0         0                            ^M
      UPDATE <----------         0         0         0         0        ^M
         200 ---------->         0         0                            ^M
         200 <----------  E-RTD2 0         0         0         0        ^M
       Pause [    24.0s]         0                             0        ^M
         BYE ---------->  B-RTD3 0         0                            ^M
         BYE <----------         0         0         0         0        ^M
         200 ---------->         0         0                            ^M
         200 <----------  E-RTD3 0         0         0         0        ^M
       Pause [$post_call_delay]  0                             0        ^M
------- Waiting for active calls to end. Press [q] again to force exit.
-------^M
^M
^M------------------------------ Scenario Screen -------- [1-9]: Change
Screen --^M
     Users (length)   Port   Total-time  Total-calls  Remote-host^M
         500 (0 ms)   5060      32.56 s          500  10.0.140.153:5060
(TCP)^M
^M
  Call limit reached (-m 500), 0.494 s period  1 ms scheduler resolution^M
  500 calls (limit 500)                  Peak was 500 calls, after 0 s^M

Any help is appreciated.

Many Thanks
Kosala
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/28180970/attachment.html>

From Richard.Whitehouse at metaswitch.com  Fri Sep  2 10:19:06 2016
From: Richard.Whitehouse at metaswitch.com (Richard Whitehouse)
Date: Fri, 2 Sep 2016 14:19:06 +0000
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress
	test
In-Reply-To: <HE1PR0802MB25074DC687820CFBE4CF5C6CA1E20@HE1PR0802MB2507.eurprd08.prod.outlook.com>
References: <HE1PR0802MB25074DC687820CFBE4CF5C6CA1E20@HE1PR0802MB2507.eurprd08.prod.outlook.com>
Message-ID: <BY2PR0201MB181634D7856F9265C4D217E4F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>

Stanislav,

I've taken a look at the Sprout crash. It looks like you have are hitting a crash in the Net SNMP library we use for alarms and statistics. I've raised an issue to track this - https://github.com/Metaswitch/sprout/issues/1527

We've seen similar looking stacks for Bono before on multi-core VMs - e.g. under http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/2015-January/001986.html

Historically we've scaled up Sprout and Bono by running many single or dual-core instances rather than running fewer larger instances - this is because
- we've seen virtualization environments impose per-VM limits on TCP connection counts, and obviously Bono has large numbers of TCP connections in a real-world scenario
- we only support a single transport thread and, since Bono performs relatively little processing per message, and Sprout needs to perform some processing per message, it is this that ends up being the bottleneck quite quickly.

Generally we've run single core Bono nodes, and dual core Sprout nodes.

Having said that, we should look into why it's crashing when you're running more cores.

Can you give us some description of the scenario you are running under when you see this?

You might also find it useful to subscribe to the mailing list so that you receive updates when we push out updated releases.

Thanks,

Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Stanislav Khalup
Sent: 01 September 2016 10:49
To: clearwater at lists.projectclearwater.org
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com>
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress test

Hello all,

We've been trying to perform IMS stress testing for some time now but it seems that we are really unlucky. When we perform sip test we experience constant bono/sprout crashes which affects results of our performance evaluation. The thing is we do know that generally our deployment is working (we managed to perform calls and run tests). At first we manually deployed IMS cluster but after crashes we decided to try all in one VM but we still experience sprout crashes (bono crashes are mostly fixed after setting 1CPU/1Worker). Could you please look at the dumps: https://www.dropbox.com/sh/qjdja9eowgvo1zc/AADm25_pwKNs3gWwBmb0Pzhpa?dl=0 because for now we have no clue for what is happening.

BR,
Stanislav
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/2f928211/attachment.html>

From richard.whitehouse at projectclearwater.org  Fri Sep  2 10:31:36 2016
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Fri, 2 Sep 2016 14:31:36 +0000
Subject: [Project Clearwater] Is Bono a multi-thread program?
In-Reply-To: <CAM_v+eRWO3_2YGEJPQEedywLmwtbBZuBNJ-01PpV5CnAS40x2A@mail.gmail.com>
References: <CAM_v+eT6WPxEny2EiCPbw1-EjN=p6OwzoUuAzA9rKRS-fgwBeQ@mail.gmail.com>
	<BY2PR0201MB1816C068F7DA89FC69C28971F0E30@BY2PR0201MB1816.namprd02.prod.outlook.com>
	<CAM_v+eRDTt1jBj-e-X7vBftodarv3Rsv955sap=5T4RAvxP8Kg@mail.gmail.com>
	<CAM_v+eRWO3_2YGEJPQEedywLmwtbBZuBNJ-01PpV5CnAS40x2A@mail.gmail.com>
Message-ID: <BY2PR0201MB18165D98A3AC3DB81AF9021FF0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>

Morris,

The thread dispatcher calls pjsip_endpt_process_rx_data to continue processing of the message - http://www.pjsip.org/pjsip/docs/html/group__PJSIP__ENDPT.htm - this passes it to the other registered modules e.g. the stateful proxy module.

If you are creating a thread and it immediately dies, then it?s possible that it?s run out of work to do. Normally threads will programmed to run in a loop handling events until terminated.

Thanks,

Richard

From: yan morris [mailto:morrisyan123 at gmail.com]
Sent: 02 September 2016 07:05
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: Re: [Project Clearwater] Is Bono a multi-thread program?

Hi , Richard

Please forgive me that I misunderstand your reply.

I find the thread_dispatcher.cpp file , however , I cannot understand how the mod_thread_dispatcher communicate with stateful_proxy_module. Could you explain it for me?

In addition , I create the threads , but it would end immediately , do you know the reason about this problem?

Thank for your time !

Morris



2016-09-01 1:27 GMT+08:00 yan morris <morrisyan123 at gmail.com<mailto:morrisyan123 at gmail.com>>:
Hi , Richard

Thank for your reply.

Could you please tell me more detailed about implementing multi-thread on Bono ?

Where I can find the codes about creating thread? Because I see the Bono.cpp , I only find the high level process

request or response function . Does Clearwater use PJSIP API to make it multi-threaded or "mod_stateful_proxy"

is implemented by multi-thread already?

I am trying to use PJSIP write my own sip server as Clearwater's Application Server , and I want it multi-threaded .

Therefore I think Bono's source code may help me , but I don't see the codes about this.


Hope you can give me a hand.

Thank you very much.

Morris


2016-09-01 0:16 GMT+08:00 Richard Whitehouse (projectclearwater.org<http://projectclearwater.org>) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>:
Morris,

Yes, Bono is multi-threaded. It?s architected with a transport thread which handles incoming PJSIP requests and a number of worker threads, which SIP messages are dispatched onto.

The worker threads are created by thread_dispatcher. By default bono has one worker thread per CPU core.

The threads are created using PJSIP?s pj_thread functions.

Thanks,

Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of yan morris
Sent: 31 August 2016 17:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Is Bono a multi-thread program?

Hi ,

I want to know , if Bono is a multi-thread program or not.

I see the source code of Bono , I look at the function "proxy_on_rx_request"

which process incoming request , but I don't see any code is creating a new thread when request income .

Therefore ,  I want to know if Bono processes request using multi-thread ?

If it does , could you tell where I can find the codes which concerned about?

(or It is implemented by PJSIP , please let me know , too)

Thank you very much

Morris





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/2cf2bf0d/attachment.html>

From richard.whitehouse at projectclearwater.org  Fri Sep  2 10:35:27 2016
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Fri, 2 Sep 2016 14:35:27 +0000
Subject: [Project Clearwater] problem on how to enable services in cw-aio
In-Reply-To: <CAAimRk6wq2HQx5yMVMCd2boT0Z1zF=nrwZeSKXavTeXii5WBDg@mail.gmail.com>
References: <CAAimRk7jC6yQ67tJhhgF3WBAmSAatjzX6v9JwQZC=eSLPLOvCQ@mail.gmail.com>
	<CY1PR0201MB1818ED5B399307BE5EA01E4BF0E00@CY1PR0201MB1818.namprd02.prod.outlook.com>
	<CAAimRk6wq2HQx5yMVMCd2boT0Z1zF=nrwZeSKXavTeXii5WBDg@mail.gmail.com>
Message-ID: <BY2PR0201MB18168AB49B63149F5B8CA28EF0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>

Abdulaziz Fahad,
Have you managed to get any desktop clients working against your Clearwater server? Typically mobile clients are the most complicated to get working because the network setup is more complicated.

Is your iPhone client connected to the same network as your Clearwater Server? It might be useful to check that you can access Ellis via your iPhone by visiting the webpage in Safari on your iPhone.

Richard

From: Abdulaziz a [mailto:aalshabawi at gmail.com]
Sent: 01 September 2016 10:17
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com>
Subject: Re: [Project Clearwater] problem on how to enable services in cw-aio

Thank you for your replay

My infrastructure is VMware and i followed the instruction in all in one OFV installation . And I assigned an ip address for example (192.168.X.X), and enter that ip address in to my Firefox browser to access to my Clearwater server. And create user name and password for the server to access it and configure private identity Ex.(SIP:6505550216) . i followed this link to download a Zoiper IOS http://clearwater.readthedocs.io/en/stable/Configuring_Zoiper_Android_iOS_Client.html but it show to us Registration status: Failed i don?t know why. could you please advise me for this kind of problem.

Best wishes


2016-08-30 20:11 GMT+03:00 Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>:
Abdulaziz Fahad,

SSH, HTTP and SIP should be available once the image has finished booting ? you shouldn?t need to manually enable them.

Have you checked that you?ve setup appropriate Firewall configuration to allow the relevant ports to be open? The configuration you?ll need is documented under http://clearwater.readthedocs.io/en/stable/Clearwater_IP_Port_Usage.html

What infrastructure are you using to run the all in one node ? Amazon EC2, OpenStack, Virtual Box or VMWare?


Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Abdulaziz Fahad
Sent: 29 August 2016 09:57
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] problem on how to enable services in cw-aio

Dear clearwater staff ,,
i have download the image cw-aio in all in one OVF installation process , but i have problem in how to enable SSH ,HTTP and SIP .
thank you in advance
Best wishes



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/029d6af0/attachment.html>

From richard.whitehouse at projectclearwater.org  Fri Sep  2 10:40:02 2016
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Fri, 2 Sep 2016 14:40:02 +0000
Subject: [Project Clearwater] SIP node for stress testing
In-Reply-To: <CAGEe17Yq8rLJ_tvij_4hOPo4FEF=71Jt3XcgnfowE0mh0d2u5Q@mail.gmail.com>
References: <CAGEe17Yq8rLJ_tvij_4hOPo4FEF=71Jt3XcgnfowE0mh0d2u5Q@mail.gmail.com>
Message-ID: <BY2PR0201MB181679ADB033FFA1C2EFE19DF0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>

Kosala,

As Matt mentioned before, you can run stress using the All In One node, but we wouldn?t recommend it ? the results won?t really mean anything. It?ll be even worse if you run stress on the same node, as doing so will use up the same capacity you are trying to test.

Our recommended model for running stress is against a full Clearwater deployment, with separate nodes for all of the major components, and separate nodes for the SIP stress generating nodes.

Thanks,

Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Kosala
Sent: 31 August 2016 15:04
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP node for stress testing


Hi Team,
Can I use all-in-one image as a SIP node?

Firstly, I made changes to /etc/clearwater/local_config and /etc/clearwater/shared_config as described in Manual stress runs (http://clearwater.readthedocs.io/en/latest/Clearwater_stress_testing.html#manual-i-e-non-chef-stress-runs).

Secondly, Run > sudo apt-get install clearwater-sip-stress

Thanks
Kosala
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/676670f6/attachment.html>

From Richard.Whitehouse at metaswitch.com  Fri Sep  2 11:27:52 2016
From: Richard.Whitehouse at metaswitch.com (Richard Whitehouse)
Date: Fri, 2 Sep 2016 15:27:52 +0000
Subject: [Project Clearwater] access to memcached data in sprout node
In-Reply-To: <4912_1472633390_57C69A2E_4912_1270_7_3D8CCA413E49D6439A7E95F805D77A4C2025145A@OPEXCLILM43.corporate.adroot.infra.ftgroup>
References: <4912_1472633390_57C69A2E_4912_1270_7_3D8CCA413E49D6439A7E95F805D77A4C2025145A@OPEXCLILM43.corporate.adroot.infra.ftgroup>
Message-ID: <BY2PR0201MB1816CEA37AB286E316238B01F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>

Gilles,

We don't have any straightforward command line tools to interrogate this information, but it's fairly straightforward to use memcached's text protocol over telnet.

You can query for registration binding information by doing:

     telnet <sprout IP address> 11211
     get reg\\sip:<user>@<domain name>
     quit

This will give you registration bindings which are replicated on this node. If you query all three of your Sprout Nodes, you'll should see of two copies of each active binding.


Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of gilles.lecorgne at orange.com
Sent: 31 August 2016 09:50
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] access to memcached data in sprout node

Hello,

We have installed Clearwater nodes in our lab. We have 3 sprout nodes deployed in our environment, and the IMS traffic is load-balanced between the 3 nodes. We are interested  about the registration process in IMS Clearwater and how data session is distributed over the different sprout nodes (in memcached database). Is there a script or command line to access/read the data (registration and session context) on the different nodes?

Thank you for your support,

Best regards,

Gilles



_________________________________________________________________________________________________________________________



Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc

pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler

a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,

Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.



This message and its attachments may contain confidential or privileged information that may be protected by law;

they should not be distributed, used or copied without authorisation.

If you have received this email in error, please notify the sender and delete this message and its attachments.

As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.

Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/cc4f25fd/attachment.html>

From Richard.Whitehouse at metaswitch.com  Fri Sep  2 11:38:38 2016
From: Richard.Whitehouse at metaswitch.com (Richard Whitehouse)
Date: Fri, 2 Sep 2016 15:38:38 +0000
Subject: [Project Clearwater] Stress Testing
In-Reply-To: <CAGEe17aSXE9EC=A+yOcFopZK0r81mzQf-Co=SErRaccMnCRB4w@mail.gmail.com>
References: <CAGEe17aSXE9EC=A+yOcFopZK0r81mzQf-Co=SErRaccMnCRB4w@mail.gmail.com>
Message-ID: <BY2PR0201MB18168B47FC58A137D6EF1D42F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>

Kosala,

This means that in 32.07s, 500 outgoing calls were made successfully.

It does not mean that more than 500 calls could not have been made, it?s just the number that it was configured to make.


Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Kosala
Sent: 02 September 2016 14:57
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Stress Testing

Hi Team,
I could manage to do stress testing and I can see the following results in log.
Does this mean , it creates and do successfully 500 outgoing calls in 32.07 s ?
Log is as follows:

^[[2J------------------------------ Scenario Screen -------- [1-9]: Change Screen --^M
     Users (length)   Port   Total-time  Total-calls  Remote-host^M
         500 (0 ms)   5060      32.07 s          500  10.0.140.153:5060(TCP)^M
^M
  Call limit reached (-m 500), 1.001 s period  1 ms scheduler resolution^M
  500 calls (limit 500)                  Peak was 500 calls, after 0 s^M
  0 Running, 503 Paused, 3 Woken up^M
  0 dead call msg (discarded)            0 out-of-call msg (discarded)        ^M
  28 open sockets                       ^M
^M
                                 Messages  Retrans   Timeout   Unexpected-Msg^M
       Pause [0ms/10:00]         500                           0        ^M
    REGISTER ---------->         26        0                            ^M
         401 <----------         26        0         0         0        ^M
    REGISTER ---------->         26        0                            ^M
         200 <----------         26        0         0         0        ^M
    REGISTER ---------->         26        0                            ^M
         401 <----------         26        0         0         0        ^M
    REGISTER ---------->         26        0                            ^M
         200 <----------         26        0         0         0        ^M
       Pause [     4:50]         22                            0        ^M
    REGISTER ---------->  B-RTD1 0         0                            ^M
         200 <----------  E-RTD1 0         0         0         0        ^M
    REGISTER ---------->  B-RTD1 0         0                            ^M
         200 <----------  E-RTD1 0         0         0         0        ^M
       Pause [     4:50]         0                             0        ^M
       Pause [$pre_call_delay]   4                             0        ^M
      INVITE ---------->  B-RTD2 1         0                            ^M
         100 <----------         0         0         0         0        ^M
      INVITE <----------         0         0         0         0        ^M
         100 <----------         0         0         0         0        ^M
      INVITE <----------         0         0         0         0        ^M
         100 ---------->         0         0                            ^M
         180 ---------->         0         0                            ^M
         180 <----------         0         0         0         0        ^M
       Pause [   6000ms]         0                             0        ^M
         200 ---------->         0         0                            ^M
         200 <----------         0         0         0         0        ^M
         ACK ---------->         0         0                            ^M
         ACK <----------         0         0         0         0        ^M
      UPDATE ---------->         0         0                            ^M
      UPDATE <----------         0         0         0         0        ^M
         200 ---------->         0         0                            ^M
         200 <----------  E-RTD2 0         0         0         0        ^M
       Pause [    24.0s]         0                             0        ^M
         BYE ---------->  B-RTD3 0         0                            ^M
         BYE <----------         0         0         0         0        ^M
         200 ---------->         0         0                            ^M
         200 <----------  E-RTD3 0         0         0         0        ^M
       Pause [$post_call_delay]  0                             0        ^M
------- Waiting for active calls to end. Press [q] again to force exit. -------^M
^M
^M------------------------------ Scenario Screen -------- [1-9]: Change Screen --^M
     Users (length)   Port   Total-time  Total-calls  Remote-host^M
         500 (0 ms)   5060      32.56 s          500  10.0.140.153:5060(TCP)^M
^M
  Call limit reached (-m 500), 0.494 s period  1 ms scheduler resolution^M
  500 calls (limit 500)                  Peak was 500 calls, after 0 s^M
Any help is appreciated.
Many Thanks
Kosala
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/23a44fd5/attachment.html>

From gilles.lecorgne at orange.com  Fri Sep  2 12:00:32 2016
From: gilles.lecorgne at orange.com (gilles.lecorgne at orange.com)
Date: Fri, 2 Sep 2016 16:00:32 +0000
Subject: [Project Clearwater] stress-testing
Message-ID: <1698_1472832034_57C9A221_1698_11478_1_3D8CCA413E49D6439A7E95F805D77A4C2025FD97@OPEXCLILM43.corporate.adroot.infra.ftgroup>

Hi,

I installed 'stress-test' VM with stress testing tool. When I tried to start the script, there is the following output:

[cid:image001.png at 01D20543.E58F09B0]

Do you think there is something wrong as there is the message: 'zmq_msg_recv: Resource temporarily unavailable' ? If yes, do you know how to solve it?

Thank you for your support,

Gilles

_________________________________________________________________________________________________________________________

Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.

This message and its attachments may contain confidential or privileged information that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and delete this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
Thank you.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/600e2b33/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11418 bytes
Desc: image001.png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/600e2b33/attachment.png>

From skhalup at virtuozzo.com  Fri Sep  2 12:20:54 2016
From: skhalup at virtuozzo.com (Stanislav Khalup)
Date: Fri, 2 Sep 2016 16:20:54 +0000
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress
	test
In-Reply-To: <BY2PR0201MB181634D7856F9265C4D217E4F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
References: <HE1PR0802MB25074DC687820CFBE4CF5C6CA1E20@HE1PR0802MB2507.eurprd08.prod.outlook.com>
	<BY2PR0201MB181634D7856F9265C4D217E4F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
Message-ID: <HE1PR0802MB250794395DC79E8E08D4E42BA1E50@HE1PR0802MB2507.eurprd08.prod.outlook.com>

Richard,

Thank you very much for your response. Let me add some details.  Initially we tested all component in VMs with 2 vCPUs each but after reading the list we changed bono config to 1 vCPU, for sprout we left 2 vCPUs. As for bono the crashes were somehow resolved but for sprout the situation is the same. We experience the same kind of crashes with all-in-one image (with 8vCPU).
As for SNMP statistics I don't know whether this is related or not but we couldn't get bono/sprout functional statistics like: The number of incoming requests, indexed by time period or The number of requests rejected due to overload, indexed by time period. - those metrics were always zero.
I've added a pair of chronos dumps to dropbox folder.  Maybe they can shed some more light on the problem: https://www.dropbox.com/sh/qjdja9eowgvo1zc/AADm25_pwKNs3gWwBmb0Pzhpa?dl=0

BR,
Stanislav Khalup

From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Sent: Friday, September 2, 2016 5:19 PM
To: clearwater at lists.projectclearwater.org; Stanislav Khalup <skhalup at virtuozzo.com>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Stanislav,

I've taken a look at the Sprout crash. It looks like you have are hitting a crash in the Net SNMP library we use for alarms and statistics. I've raised an issue to track this - https://github.com/Metaswitch/sprout/issues/1527

We've seen similar looking stacks for Bono before on multi-core VMs - e.g. under http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/2015-January/001986.html

Historically we've scaled up Sprout and Bono by running many single or dual-core instances rather than running fewer larger instances - this is because
- we've seen virtualization environments impose per-VM limits on TCP connection counts, and obviously Bono has large numbers of TCP connections in a real-world scenario
- we only support a single transport thread and, since Bono performs relatively little processing per message, and Sprout needs to perform some processing per message, it is this that ends up being the bottleneck quite quickly.

Generally we've run single core Bono nodes, and dual core Sprout nodes.

Having said that, we should look into why it's crashing when you're running more cores.

Can you give us some description of the scenario you are running under when you see this?

You might also find it useful to subscribe to the mailing list so that you receive updates when we push out updated releases.

Thanks,

Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Stanislav Khalup
Sent: 01 September 2016 10:49
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress test

Hello all,

We've been trying to perform IMS stress testing for some time now but it seems that we are really unlucky. When we perform sip test we experience constant bono/sprout crashes which affects results of our performance evaluation. The thing is we do know that generally our deployment is working (we managed to perform calls and run tests). At first we manually deployed IMS cluster but after crashes we decided to try all in one VM but we still experience sprout crashes (bono crashes are mostly fixed after setting 1CPU/1Worker). Could you please look at the dumps: https://www.dropbox.com/sh/qjdja9eowgvo1zc/AADm25_pwKNs3gWwBmb0Pzhpa?dl=0 because for now we have no clue for what is happening.

BR,
Stanislav
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/4ef10f90/attachment.html>

From kosalayb at gmail.com  Fri Sep  2 12:22:07 2016
From: kosalayb at gmail.com (Kosala)
Date: Fri, 2 Sep 2016 17:22:07 +0100
Subject: [Project Clearwater] Stress Testing
In-Reply-To: <BY2PR0201MB18168B47FC58A137D6EF1D42F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
References: <CAGEe17aSXE9EC=A+yOcFopZK0r81mzQf-Co=SErRaccMnCRB4w@mail.gmail.com>
	<BY2PR0201MB18168B47FC58A137D6EF1D42F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
Message-ID: <CAGEe17bQdjigBUEO3uPvaiCc6AHbyw8oCUksu5QnftCO1iXmrA@mail.gmail.com>

Hi Richard,

Thanks for your response.
What is Register, it shows only 26. I thought it is only 26 successful
calls.
When I tail this log file , I can see, Register number is increasing and
some stage I am getting Timeout number increasing as well.

                                              Timeout
 REGISTER ------> 225       0                            ^M
...
 100 <----------         0         0         6         0        ^M
...

When I check my logs in bono, I can see as follows:

02-09-2016 16:08:43.380 UTC Call-Disconnected: CALL_ID=2010003917///
542-9365 at 10.0.22.164 REASON=401
02-09-2016 16:08:43.628 UTC Call-Disconnected: CALL_ID=2010000296///
2352-9365 at 10.0.22.164 REASON=401
02-09-2016 16:08:43.639 UTC Call-Disconnected: CALL_ID=2010000424///
2288-9365 at 10.0.22.164 REASON=401
02-09-2016 16:08:43.671 UTC Call-Disconnected: CALL_ID=2010000297///
2352-9365 at 10.0.22.164 REASON=401
02-09-2016 16:08:43.683 UTC Call-Disconnected: CALL_ID=2010000425///
2288-9365 at 10.0.22.164 REASON=401
02-09-2016 16:08:44.124 UTC Call-Disconnected: CALL_ID=2010001794///
1603-9365 at 10.0.22.164 REASON=401
02-09-2016 16:08:44.167 UTC Call-Disconnected: CALL_ID=2010001795///
1603-9365 at 10.0.22.164 REASON=401
02-09-2016 16:08:44.354 UTC Call-Disconnected: CALL_ID=2010002220///
1390-9365 at 10.0.22.164 REASON=401
02-09-2016 16:08:44.398 UTC Call-Disconnected: CALL_ID=2010002221///
1390-9365 at 10.0.22.164 REASON=401
02-09-2016 16:08:44.596 UTC Call-Disconnected: CALL_ID=2010001300///
1850-9365 at 10.0.22.164 REASON=401

I assume this call disconnected after stress testing each call. Is that
right?

Thanks
Kosala

On Fri, Sep 2, 2016 at 4:38 PM, Richard Whitehouse <
Richard.Whitehouse at metaswitch.com> wrote:

> Kosala,
>
>
>
> This means that in 32.07s, 500 outgoing calls were made successfully.
>
>
>
> It does not mean that more than 500 calls could not have been made, it?s
> just the number that it was configured to make.
>
>
>
>
>
> Richard
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Kosala
> *Sent:* 02 September 2016 14:57
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Stress Testing
>
>
>
> Hi Team,
>
> I could manage to do stress testing and I can see the following results in
> log.
>
> Does this mean , it creates and do successfully 500 outgoing calls in
> 32.07 s ?
>
> Log is as follows:
>
>
> ^[[2J------------------------------ Scenario Screen -------- [1-9]:
> Change Screen --^M
>      Users (length)   Port   Total-time  Total-calls  Remote-host^M
>          500 (0 ms)   5060      32.07 s          500  10.0.140.153:5060
> (TCP)^M
> ^M
>   Call limit reached (-m 500), 1.001 s period  1 ms scheduler resolution^M
>   500 calls (limit 500)                  Peak was 500 calls, after 0 s^M
>   0 Running, 503 Paused, 3 Woken up^M
>   0 dead call msg (discarded)            0 out-of-call msg
> (discarded)        ^M
>   28 open sockets                       ^M
> ^M
>                                  Messages  Retrans   Timeout
> Unexpected-Msg^M
>        Pause [0ms/10:00]         500                           0        ^M
>     REGISTER ---------->         26        0                            ^M
>          401 <----------         26        0         0         0        ^M
>     REGISTER ---------->         26        0                            ^M
>          200 <----------         26        0         0         0        ^M
>     REGISTER ---------->         26        0                            ^M
>          401 <----------         26        0         0         0        ^M
>     REGISTER ---------->         26        0                            ^M
>          200 <----------         26        0         0         0        ^M
>        Pause [     4:50]         22                            0        ^M
>     REGISTER ---------->  B-RTD1 0         0                            ^M
>          200 <----------  E-RTD1 0         0         0         0        ^M
>     REGISTER ---------->  B-RTD1 0         0                            ^M
>          200 <----------  E-RTD1 0         0         0         0        ^M
>        Pause [     4:50]         0                             0        ^M
>        Pause [$pre_call_delay]   4                             0        ^M
>       INVITE ---------->  B-RTD2 1         0                            ^M
>          100 <----------         0         0         0         0        ^M
>       INVITE <----------         0         0         0         0        ^M
>          100 <----------         0         0         0         0        ^M
>       INVITE <----------         0         0         0         0        ^M
>          100 ---------->         0         0                            ^M
>          180 ---------->         0         0                            ^M
>          180 <----------         0         0         0         0        ^M
>        Pause [   6000ms]         0                             0        ^M
>          200 ---------->         0         0                            ^M
>          200 <----------         0         0         0         0        ^M
>          ACK ---------->         0         0                            ^M
>          ACK <----------         0         0         0         0        ^M
>       UPDATE ---------->         0         0                            ^M
>       UPDATE <----------         0         0         0         0        ^M
>          200 ---------->         0         0                            ^M
>          200 <----------  E-RTD2 0         0         0         0        ^M
>        Pause [    24.0s]         0                             0        ^M
>          BYE ---------->  B-RTD3 0         0                            ^M
>          BYE <----------         0         0         0         0        ^M
>          200 ---------->         0         0                            ^M
>          200 <----------  E-RTD3 0         0         0         0        ^M
>        Pause [$post_call_delay]  0                             0        ^M
> ------- Waiting for active calls to end. Press [q] again to force exit.
> -------^M
> ^M
> ^M------------------------------ Scenario Screen -------- [1-9]: Change
> Screen --^M
>      Users (length)   Port   Total-time  Total-calls  Remote-host^M
>          500 (0 ms)   5060      32.56 s          500  10.0.140.153:5060
> (TCP)^M
> ^M
>   Call limit reached (-m 500), 0.494 s period  1 ms scheduler resolution^M
>   500 calls (limit 500)                  Peak was 500 calls, after 0 s^M
>
> Any help is appreciated.
>
> Many Thanks
>
> Kosala
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/89972b95/attachment.html>

From gilles.lecorgne at orange.com  Fri Sep  2 12:24:10 2016
From: gilles.lecorgne at orange.com (gilles.lecorgne at orange.com)
Date: Fri, 2 Sep 2016 16:24:10 +0000
Subject: [Project Clearwater] access to memcached data in sprout node
In-Reply-To: <BY2PR0201MB1816CEA37AB286E316238B01F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
References: <4912_1472633390_57C69A2E_4912_1270_7_3D8CCA413E49D6439A7E95F805D77A4C2025145A@OPEXCLILM43.corporate.adroot.infra.ftgroup>
	<BY2PR0201MB1816CEA37AB286E316238B01F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
Message-ID: <16462_1472833451_57C9A7AB_16462_3272_1_3D8CCA413E49D6439A7E95F805D77A4C2025FE0D@OPEXCLILM43.corporate.adroot.infra.ftgroup>

Hello Richard,

Thank you for your feedback and response. Is there a similar query for call/session information ?

Best regards,

Gilles

De : Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Envoy? : vendredi 2 septembre 2016 17:28
? : clearwater at lists.projectclearwater.org; LECORGNE Gilles IMT/OLN
Objet : RE: access to memcached data in sprout node

Gilles,

We don't have any straightforward command line tools to interrogate this information, but it's fairly straightforward to use memcached's text protocol over telnet.

You can query for registration binding information by doing:

     telnet <sprout IP address> 11211
     get reg\\sip:<user>@<domain name>
     quit

This will give you registration bindings which are replicated on this node. If you query all three of your Sprout Nodes, you'll should see of two copies of each active binding.


Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of gilles.lecorgne at orange.com<mailto:gilles.lecorgne at orange.com>
Sent: 31 August 2016 09:50
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] access to memcached data in sprout node

Hello,

We have installed Clearwater nodes in our lab. We have 3 sprout nodes deployed in our environment, and the IMS traffic is load-balanced between the 3 nodes. We are interested  about the registration process in IMS Clearwater and how data session is distributed over the different sprout nodes (in memcached database). Is there a script or command line to access/read the data (registration and session context) on the different nodes?

Thank you for your support,

Best regards,

Gilles



_________________________________________________________________________________________________________________________



Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc

pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler

a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,

Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.



This message and its attachments may contain confidential or privileged information that may be protected by law;

they should not be distributed, used or copied without authorisation.

If you have received this email in error, please notify the sender and delete this message and its attachments.

As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.

Thank you.

_________________________________________________________________________________________________________________________

Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.

This message and its attachments may contain confidential or privileged information that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and delete this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
Thank you.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160902/fdf7c9c2/attachment.html>

From michaelkatsoulis88 at gmail.com  Mon Sep  5 04:21:34 2016
From: michaelkatsoulis88 at gmail.com (=?UTF-8?B?zpzOuc+HzqzOu863z4IgzprOsc+Ez4POv8+NzrvOt8+C?=)
Date: Mon, 5 Sep 2016 11:21:34 +0300
Subject: [Project Clearwater] Increase stress test load
In-Reply-To: <BY2PR0201MB1816792F48D0CA279F9C1CF1F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
References: <CAJG3f9MuVGuZsWtF9oJ-QAsF4BiVg42kijPeBcitHgjXWLiwqQ@mail.gmail.com>
	<BY2PR0201MB1816792F48D0CA279F9C1CF1F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
Message-ID: <CAJG3f9O7hCXQ7RZY7q_KqiWO4H-P9n0Nn3LqDskMxO80XT+LQQ@mail.gmail.com>

Hi all,

i am trying to increase the load that Sip stress node produces by altering
the configuration in

/usr/share/clearwater/bin/sip-stress file. More specifically i change/add
some parameters in command :

*# Actually run sipp*.
  nice -n-20 /usr/share/clearwater/bin/sipp -i $local_ip -sf
/var/log/clearwater-sipp/sip-stress.xml $stress_target:5060 -t tn *-r 200
-rp 1s* -s $home_domain -inf
/usr/share/clearwater/sip-stress/users.csv.$index -users $num_users *-m
30000*  *-default_behaviors none*,-bye -max_socket 65000 -trace_stat
-trace_rtt -trace_counts -trace_err -max_reconnect -1 -reconnect_sleep 0
-reconnect_close 0 -send_timeout 4000 -recv_timeout 12000 -nostdin >>
/var/log/clearwater-sipp/sip-stress.$index.out 2>&1

I thought for example that by adding parameters  -r 200 -rp 1s would
produce 200 calls per second but both in logs and wireshark trace nothing
seems to change. Is there something  that i haven't understood?

I would appreciate your help!

Best Regards,
Michael Katsoulis



































2016-09-02 12:14 GMT+03:00 Richard Whitehouse <
Richard.Whitehouse at metaswitch.com>:

> Michael,
>
>
>
> The easiest way to increase the load on the system is to add more
> subscribers. For each 30,000 subscribers, we recommend you have a separate
> SIP stress node.
>
>
>
> Altering the number of subscribers is covered under the SIP Stress article
> which documents the settings you can change http://clearwater.readthedocs.
> io/en/stable/Clearwater_stress_testing.html
>
>
>
> Alternatively, if you are looking at different scenarios, you can alter
> the stress script, which is at /usr/share/clearwater/sip-stress/sip-stress.xml
> on the stress nodes, or create a new one from scratch.
>
>
>
>
>
> Richard
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *??????? ?ats?????
> *Sent:* 02 September 2016 08:35
> *To:* Clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Increase stress test load
>
>
>
> Hi all,
>
>
>
> I have successfully manually installed Clearwater in 6 virtual machines
> and tested my deployment using SIP clients (following instructions from
> http://clearwater.readthedocs.io/en/stable/Making_your_first_call.html.).
>
> I also created a SIP Stress node and everything seems to be working great.
>
> The instructions in http://clearwater.readthedocs.io/en/stable/
> Clearwater_stress_testing.html
>
> mention that a pair of subscribers is registered  every 5 minutes and then
> making a call every 30 minutes.
>
> How is it possible that i can increase the number of registrations and
> calls per minute to test the performance of my deployment ?
>
> Could anyone help?
>
>
>
> Thank you in advance,
>
> Michael Katsoulis
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160905/9272de4e/attachment.html>

From skhalup at virtuozzo.com  Mon Sep  5 10:29:14 2016
From: skhalup at virtuozzo.com (Stanislav Khalup)
Date: Mon, 5 Sep 2016 14:29:14 +0000
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress
	test
References: <HE1PR0802MB25074DC687820CFBE4CF5C6CA1E20@HE1PR0802MB2507.eurprd08.prod.outlook.com>
	<BY2PR0201MB181634D7856F9265C4D217E4F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
Message-ID: <HE1PR0802MB2507E4723F3D8927AB44AE5CA1E60@HE1PR0802MB2507.eurprd08.prod.outlook.com>

Hello all,

We continue our stress tests to understand some dependencies behind crashes but it seems there is no such thing. This time we installed the latest all-in-one node and limited it to 1 CPU. Then we ran many sipp stress tests. We believed that in this case we will get application crashes only when load hits some considerable level but it seems crashes and actual number of registration attempts/calls ongoing are not or poorly related. The latest dumps are place here: https://www.dropbox.com/sh/ckjr8oi3rll5y78/AAArJDu7WItDxJjipdDMx7TVa?dl=0 Is this kind of re-occurring crashes expected from all-in-one node?

BR,
Stanislav Khalup

From: Stanislav Khalup
Sent: Friday, September 2, 2016 7:21 PM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com>; clearwater at lists.projectclearwater.org
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Richard,

Thank you very much for your response. Let me add some details.  Initially we tested all component in VMs with 2 vCPUs each but after reading the list we changed bono config to 1 vCPU, for sprout we left 2 vCPUs. As for bono the crashes were somehow resolved but for sprout the situation is the same. We experience the same kind of crashes with all-in-one image (with 8vCPU).
As for SNMP statistics I don't know whether this is related or not but we couldn't get bono/sprout functional statistics like: The number of incoming requests, indexed by time period or The number of requests rejected due to overload, indexed by time period. - those metrics were always zero.
I've added a pair of chronos dumps to dropbox folder.  Maybe they can shed some more light on the problem: https://www.dropbox.com/sh/qjdja9eowgvo1zc/AADm25_pwKNs3gWwBmb0Pzhpa?dl=0

BR,
Stanislav Khalup

From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Sent: Friday, September 2, 2016 5:19 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Stanislav Khalup <skhalup at virtuozzo.com<mailto:skhalup at virtuozzo.com>>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Stanislav,

I've taken a look at the Sprout crash. It looks like you have are hitting a crash in the Net SNMP library we use for alarms and statistics. I've raised an issue to track this - https://github.com/Metaswitch/sprout/issues/1527

We've seen similar looking stacks for Bono before on multi-core VMs - e.g. under http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/2015-January/001986.html

Historically we've scaled up Sprout and Bono by running many single or dual-core instances rather than running fewer larger instances - this is because
- we've seen virtualization environments impose per-VM limits on TCP connection counts, and obviously Bono has large numbers of TCP connections in a real-world scenario
- we only support a single transport thread and, since Bono performs relatively little processing per message, and Sprout needs to perform some processing per message, it is this that ends up being the bottleneck quite quickly.

Generally we've run single core Bono nodes, and dual core Sprout nodes.

Having said that, we should look into why it's crashing when you're running more cores.

Can you give us some description of the scenario you are running under when you see this?

You might also find it useful to subscribe to the mailing list so that you receive updates when we push out updated releases.

Thanks,

Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Stanislav Khalup
Sent: 01 September 2016 10:49
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress test

Hello all,

We've been trying to perform IMS stress testing for some time now but it seems that we are really unlucky. When we perform sip test we experience constant bono/sprout crashes which affects results of our performance evaluation. The thing is we do know that generally our deployment is working (we managed to perform calls and run tests). At first we manually deployed IMS cluster but after crashes we decided to try all in one VM but we still experience sprout crashes (bono crashes are mostly fixed after setting 1CPU/1Worker). Could you please look at the dumps: https://www.dropbox.com/sh/qjdja9eowgvo1zc/AADm25_pwKNs3gWwBmb0Pzhpa?dl=0 because for now we have no clue for what is happening.

BR,
Stanislav
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160905/400ef89d/attachment.html>

From arvindas at hpe.com  Tue Sep  6 03:31:51 2016
From: arvindas at hpe.com (Shirabur, Aravind Ashok (CMS))
Date: Tue, 6 Sep 2016 07:31:51 +0000
Subject: [Project Clearwater] Registration Timeout in 300 sec
Message-ID: <TU4PR84MB00640776ACA54B4DA9D430AEDCF90@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>

Hi,

We are testing project clear water (clearwater-infrastructure  1.0-160518.173307) , we observed that IMS core network registration expiry time is always 300 sec; it's not considering the Expiry timer in the successful Registration message. Any configuration to set the Registration Expiry timer value.

Thanks
Aravind
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160906/f6d27833/attachment.html>

From skhalup at virtuozzo.com  Tue Sep  6 11:14:19 2016
From: skhalup at virtuozzo.com (Stanislav Khalup)
Date: Tue, 6 Sep 2016 15:14:19 +0000
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress
	test
References: <HE1PR0802MB25074DC687820CFBE4CF5C6CA1E20@HE1PR0802MB2507.eurprd08.prod.outlook.com>
	<BY2PR0201MB181634D7856F9265C4D217E4F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
Message-ID: <HE1PR0802MB250746E0B2624876C95FD50DA1F90@HE1PR0802MB2507.eurprd08.prod.outlook.com>

Hello again,

I'm sorry for spamming  the mail list but we continue our testing and come across new crashes. This time we took all-in-one VM and limited CPU to 1 core (to avoid Bono crashes). We found out that crashes begin when we hit ~15k open live sockets. The thing is we expect that Clearwater IMS will stop processing registration/call requests but will preserve ongoing calls but as we saw during sipp test but instead most connections are forcefully closed and dump is generated. I am adding some more dumps (homer this time) and sip logs. Could you please tell me if this kind of behavior during testing is kind of normal, expected behavior?

Dumps: https://www.dropbox.com/sh/bl5ghgwrpum6pq9/AAA_UQV7v9NfG3y8q0lOP0Xra?dl=0

BR,
Stanislav Khalup

From: Stanislav Khalup
Sent: Monday, September 5, 2016 5:29 PM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com>; 'clearwater at lists.projectclearwater.org' <clearwater at lists.projectclearwater.org>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Hello all,

We continue our stress tests to understand some dependencies behind crashes but it seems there is no such thing. This time we installed the latest all-in-one node and limited it to 1 CPU. Then we ran many sipp stress tests. We believed that in this case we will get application crashes only when load hits some considerable level but it seems crashes and actual number of registration attempts/calls ongoing are not or poorly related. The latest dumps are place here: https://www.dropbox.com/sh/ckjr8oi3rll5y78/AAArJDu7WItDxJjipdDMx7TVa?dl=0 Is this kind of re-occurring crashes expected from all-in-one node?

BR,
Stanislav Khalup

From: Stanislav Khalup
Sent: Friday, September 2, 2016 7:21 PM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Richard,

Thank you very much for your response. Let me add some details.  Initially we tested all component in VMs with 2 vCPUs each but after reading the list we changed bono config to 1 vCPU, for sprout we left 2 vCPUs. As for bono the crashes were somehow resolved but for sprout the situation is the same. We experience the same kind of crashes with all-in-one image (with 8vCPU).
As for SNMP statistics I don't know whether this is related or not but we couldn't get bono/sprout functional statistics like: The number of incoming requests, indexed by time period or The number of requests rejected due to overload, indexed by time period. - those metrics were always zero.
I've added a pair of chronos dumps to dropbox folder.  Maybe they can shed some more light on the problem: https://www.dropbox.com/sh/qjdja9eowgvo1zc/AADm25_pwKNs3gWwBmb0Pzhpa?dl=0

BR,
Stanislav Khalup

From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Sent: Friday, September 2, 2016 5:19 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Stanislav Khalup <skhalup at virtuozzo.com<mailto:skhalup at virtuozzo.com>>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Stanislav,

I've taken a look at the Sprout crash. It looks like you have are hitting a crash in the Net SNMP library we use for alarms and statistics. I've raised an issue to track this - https://github.com/Metaswitch/sprout/issues/1527

We've seen similar looking stacks for Bono before on multi-core VMs - e.g. under http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/2015-January/001986.html

Historically we've scaled up Sprout and Bono by running many single or dual-core instances rather than running fewer larger instances - this is because
- we've seen virtualization environments impose per-VM limits on TCP connection counts, and obviously Bono has large numbers of TCP connections in a real-world scenario
- we only support a single transport thread and, since Bono performs relatively little processing per message, and Sprout needs to perform some processing per message, it is this that ends up being the bottleneck quite quickly.

Generally we've run single core Bono nodes, and dual core Sprout nodes.

Having said that, we should look into why it's crashing when you're running more cores.

Can you give us some description of the scenario you are running under when you see this?

You might also find it useful to subscribe to the mailing list so that you receive updates when we push out updated releases.

Thanks,

Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Stanislav Khalup
Sent: 01 September 2016 10:49
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress test

Hello all,

We've been trying to perform IMS stress testing for some time now but it seems that we are really unlucky. When we perform sip test we experience constant bono/sprout crashes which affects results of our performance evaluation. The thing is we do know that generally our deployment is working (we managed to perform calls and run tests). At first we manually deployed IMS cluster but after crashes we decided to try all in one VM but we still experience sprout crashes (bono crashes are mostly fixed after setting 1CPU/1Worker). Could you please look at the dumps: https://www.dropbox.com/sh/qjdja9eowgvo1zc/AADm25_pwKNs3gWwBmb0Pzhpa?dl=0 because for now we have no clue for what is happening.

BR,
Stanislav
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160906/8b47228c/attachment.html>

From richard.whitehouse at projectclearwater.org  Tue Sep  6 12:31:41 2016
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Tue, 6 Sep 2016 16:31:41 +0000
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress
	test
In-Reply-To: <HE1PR0802MB250746E0B2624876C95FD50DA1F90@HE1PR0802MB2507.eurprd08.prod.outlook.com>
References: <HE1PR0802MB25074DC687820CFBE4CF5C6CA1E20@HE1PR0802MB2507.eurprd08.prod.outlook.com>
	<BY2PR0201MB181634D7856F9265C4D217E4F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
	<HE1PR0802MB250746E0B2624876C95FD50DA1F90@HE1PR0802MB2507.eurprd08.prod.outlook.com>
Message-ID: <BY2PR0201MB18160BD81061DFE13C9F963BF0F90@BY2PR0201MB1816.namprd02.prod.outlook.com>

Stanislav,

Sorry to hear you are having these problems. We run stress regularly against Project Clearwater deployments, and we don't see such problems, so it's down to a difference in your setup.

We wouldn't ever expect to run stress against an all in one node - it's not designed for any particular capacity, it's designed to be use to manual trying out Clearwater performance, and trying it out initially.

Instead, as per the Stress Testing instructions, we'd expect stress to either be run against a deployment done using Chef, or using a Manual Install, with Clearwater deployed on at least six separate boxes with Sprout, Homer, Homestead, Ralf, Bono and Ellis all instead on separate servers, with separate servers for the Sip Stress Node. We'd expect all of the VMs to have around 1VCPU and 2GB of RAM as documented in the Manual Install instructions. If greater performance is required, we'd expect this to be achieved by increasing the number of deployed VMs, based on where the system was stressed, rather than increasing the resources assigned to each VM.

The latest dumps you have sent don't represent crashes, they represent processes being killed because they become unresponsive to the polling mechanism, due to the amount of load on the VM, and the process being unable to serve the monitoring request in time.

Can you provide some background on what you are trying to find out from the testing you are doing?


Thanks,

Richard

From: Stanislav Khalup [mailto:skhalup at virtuozzo.com]
Sent: 06 September 2016 16:14
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com>; clearwater at lists.projectclearwater.org
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Hello again,

I'm sorry for spamming  the mail list but we continue our testing and come across new crashes. This time we took all-in-one VM and limited CPU to 1 core (to avoid Bono crashes). We found out that crashes begin when we hit ~15k open live sockets. The thing is we expect that Clearwater IMS will stop processing registration/call requests but will preserve ongoing calls but as we saw during sipp test but instead most connections are forcefully closed and dump is generated. I am adding some more dumps (homer this time) and sip logs. Could you please tell me if this kind of behavior during testing is kind of normal, expected behavior?

Dumps: https://www.dropbox.com/sh/bl5ghgwrpum6pq9/AAA_UQV7v9NfG3y8q0lOP0Xra?dl=0

BR,
Stanislav Khalup

From: Stanislav Khalup
Sent: Monday, September 5, 2016 5:29 PM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>; 'clearwater at lists.projectclearwater.org' <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Hello all,

We continue our stress tests to understand some dependencies behind crashes but it seems there is no such thing. This time we installed the latest all-in-one node and limited it to 1 CPU. Then we ran many sipp stress tests. We believed that in this case we will get application crashes only when load hits some considerable level but it seems crashes and actual number of registration attempts/calls ongoing are not or poorly related. The latest dumps are place here: https://www.dropbox.com/sh/ckjr8oi3rll5y78/AAArJDu7WItDxJjipdDMx7TVa?dl=0 Is this kind of re-occurring crashes expected from all-in-one node?

BR,
Stanislav Khalup

From: Stanislav Khalup
Sent: Friday, September 2, 2016 7:21 PM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Richard,

Thank you very much for your response. Let me add some details.  Initially we tested all component in VMs with 2 vCPUs each but after reading the list we changed bono config to 1 vCPU, for sprout we left 2 vCPUs. As for bono the crashes were somehow resolved but for sprout the situation is the same. We experience the same kind of crashes with all-in-one image (with 8vCPU).
As for SNMP statistics I don't know whether this is related or not but we couldn't get bono/sprout functional statistics like: The number of incoming requests, indexed by time period or The number of requests rejected due to overload, indexed by time period. - those metrics were always zero.
I've added a pair of chronos dumps to dropbox folder.  Maybe they can shed some more light on the problem: https://www.dropbox.com/sh/qjdja9eowgvo1zc/AADm25_pwKNs3gWwBmb0Pzhpa?dl=0

BR,
Stanislav Khalup

From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Sent: Friday, September 2, 2016 5:19 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Stanislav Khalup <skhalup at virtuozzo.com<mailto:skhalup at virtuozzo.com>>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Stanislav,

I've taken a look at the Sprout crash. It looks like you have are hitting a crash in the Net SNMP library we use for alarms and statistics. I've raised an issue to track this - https://github.com/Metaswitch/sprout/issues/1527

We've seen similar looking stacks for Bono before on multi-core VMs - e.g. under http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/2015-January/001986.html

Historically we've scaled up Sprout and Bono by running many single or dual-core instances rather than running fewer larger instances - this is because
- we've seen virtualization environments impose per-VM limits on TCP connection counts, and obviously Bono has large numbers of TCP connections in a real-world scenario
- we only support a single transport thread and, since Bono performs relatively little processing per message, and Sprout needs to perform some processing per message, it is this that ends up being the bottleneck quite quickly.

Generally we've run single core Bono nodes, and dual core Sprout nodes.

Having said that, we should look into why it's crashing when you're running more cores.

Can you give us some description of the scenario you are running under when you see this?

You might also find it useful to subscribe to the mailing list so that you receive updates when we push out updated releases.

Thanks,

Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Stanislav Khalup
Sent: 01 September 2016 10:49
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress test

Hello all,

We've been trying to perform IMS stress testing for some time now but it seems that we are really unlucky. When we perform sip test we experience constant bono/sprout crashes which affects results of our performance evaluation. The thing is we do know that generally our deployment is working (we managed to perform calls and run tests). At first we manually deployed IMS cluster but after crashes we decided to try all in one VM but we still experience sprout crashes (bono crashes are mostly fixed after setting 1CPU/1Worker). Could you please look at the dumps: https://www.dropbox.com/sh/qjdja9eowgvo1zc/AADm25_pwKNs3gWwBmb0Pzhpa?dl=0 because for now we have no clue for what is happening.

BR,
Stanislav
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160906/c8a9256a/attachment.html>

From richard.whitehouse at projectclearwater.org  Tue Sep  6 12:39:24 2016
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Tue, 6 Sep 2016 16:39:24 +0000
Subject: [Project Clearwater] Registration Timeout in 300 sec
In-Reply-To: <TU4PR84MB00640776ACA54B4DA9D430AEDCF90@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>
References: <TU4PR84MB00640776ACA54B4DA9D430AEDCF90@TU4PR84MB0064.NAMPRD84.PROD.OUTLOOK.COM>
Message-ID: <BY2PR0201MB18165A29DE8ACF828404EC22F0F90@BY2PR0201MB1816.namprd02.prod.outlook.com>

Aravind,

The maximum expiry time that a subscriber can register for is configurable. By default, it's 300 seconds, but it can be configured by changing the value of reg_max_expires which can be shared in Clearwater's shared configuration. There's further documentation on this at http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html.

Registering clients can provide a registration of any time up to the maximum configured registration expiry.

Thanks,

Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Shirabur, Aravind Ashok (CMS)
Sent: 06 September 2016 08:32
To: Clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Registration Timeout in 300 sec

Hi,

We are testing project clear water (clearwater-infrastructure  1.0-160518.173307) , we observed that IMS core network registration expiry time is always 300 sec; it's not considering the Expiry timer in the successful Registration message. Any configuration to set the Registration Expiry timer value.

Thanks
Aravind
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160906/f09ff57f/attachment.html>

From richard.whitehouse at projectclearwater.org  Tue Sep  6 12:58:20 2016
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Tue, 6 Sep 2016 16:58:20 +0000
Subject: [Project Clearwater] Is Bono a multi-thread program?
In-Reply-To: <CAM_v+eTR4dNrWBX3siuZJVU48i=GRAgn_3pp35MerywYse9huw@mail.gmail.com>
References: <CAM_v+eT6WPxEny2EiCPbw1-EjN=p6OwzoUuAzA9rKRS-fgwBeQ@mail.gmail.com>
	<BY2PR0201MB1816C068F7DA89FC69C28971F0E30@BY2PR0201MB1816.namprd02.prod.outlook.com>
	<CAM_v+eRDTt1jBj-e-X7vBftodarv3Rsv955sap=5T4RAvxP8Kg@mail.gmail.com>
	<CAM_v+eRWO3_2YGEJPQEedywLmwtbBZuBNJ-01PpV5CnAS40x2A@mail.gmail.com>
	<BY2PR0201MB18165D98A3AC3DB81AF9021FF0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
	<CAM_v+eTR4dNrWBX3siuZJVU48i=GRAgn_3pp35MerywYse9huw@mail.gmail.com>
Message-ID: <BY2PR0201MB181688459C8CAFAE0B853D67F0F90@BY2PR0201MB1816.namprd02.prod.outlook.com>

Morris,

Yes that is possible. You could either spawn a new thread for each request, or dispatch it to a worker pool. What you choose to do is really an implementation detail of the application server you write, and not really related to how Clearwater is structured.

Richard

From: yan morris [mailto:morrisyan123 at gmail.com]
Sent: 06 September 2016 05:30
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: Re: [Project Clearwater] Is Bono a multi-thread program?

Hi , Richard

Thanks for your help .

I understand the architecture of modules .

However I have a question , I want to my stateful_proxy_server can process multiple request or response in the same time.

Could I just create threads , and distribute incoming request to particular thread?

That is , without creating new module to do multi-thread .

Is that possible ?

Thank you very much.

Morris

2016-09-02 22:31 GMT+08:00 Richard Whitehouse (projectclearwater.org<http://projectclearwater.org>) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>:
Morris,

The thread dispatcher calls pjsip_endpt_process_rx_data to continue processing of the message - http://www.pjsip.org/pjsip/docs/html/group__PJSIP__ENDPT.htm - this passes it to the other registered modules e.g. the stateful proxy module.

If you are creating a thread and it immediately dies, then it?s possible that it?s run out of work to do. Normally threads will programmed to run in a loop handling events until terminated.

Thanks,

Richard

From: yan morris [mailto:morrisyan123 at gmail.com<mailto:morrisyan123 at gmail.com>]
Sent: 02 September 2016 07:05
To: Richard Whitehouse (projectclearwater.org<http://projectclearwater.org>) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: Re: [Project Clearwater] Is Bono a multi-thread program?

Hi , Richard

Please forgive me that I misunderstand your reply.

I find the thread_dispatcher.cpp file , however , I cannot understand how the mod_thread_dispatcher communicate with stateful_proxy_module. Could you explain it for me?

In addition , I create the threads , but it would end immediately , do you know the reason about this problem?

Thank for your time !

Morris


2016-09-01 1:27 GMT+08:00 yan morris <morrisyan123 at gmail.com<mailto:morrisyan123 at gmail.com>>:
Hi , Richard

Thank for your reply.

Could you please tell me more detailed about implementing multi-thread on Bono ?

Where I can find the codes about creating thread? Because I see the Bono.cpp , I only find the high level process

request or response function . Does Clearwater use PJSIP API to make it multi-threaded or "mod_stateful_proxy"

is implemented by multi-thread already?

I am trying to use PJSIP write my own sip server as Clearwater's Application Server , and I want it multi-threaded .

Therefore I think Bono's source code may help me , but I don't see the codes about this.


Hope you can give me a hand.

Thank you very much.

Morris

2016-09-01 0:16 GMT+08:00 Richard Whitehouse (projectclearwater.org<http://projectclearwater.org>) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>:
Morris,

Yes, Bono is multi-threaded. It?s architected with a transport thread which handles incoming PJSIP requests and a number of worker threads, which SIP messages are dispatched onto.

The worker threads are created by thread_dispatcher. By default bono has one worker thread per CPU core.

The threads are created using PJSIP?s pj_thread functions.

Thanks,

Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of yan morris
Sent: 31 August 2016 17:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Is Bono a multi-thread program?

Hi ,

I want to know , if Bono is a multi-thread program or not.

I see the source code of Bono , I look at the function "proxy_on_rx_request"

which process incoming request , but I don't see any code is creating a new thread when request income .

Therefore ,  I want to know if Bono processes request using multi-thread ?

If it does , could you tell where I can find the codes which concerned about?

(or It is implemented by PJSIP , please let me know , too)

Thank you very much

Morris






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160906/e1404f68/attachment.html>

From richard.whitehouse at projectclearwater.org  Tue Sep  6 13:10:49 2016
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Tue, 6 Sep 2016 17:10:49 +0000
Subject: [Project Clearwater] Signup at Project Clearwater
In-Reply-To: <DM2PR02MB543150FECC83E40217509B0AEE60@DM2PR02MB543.namprd02.prod.outlook.com>
References: <0e9b778c06a5a253097a3380c3311a41@www.projectclearwater.org>
	<DM2PR02MB543150FECC83E40217509B0AEE60@DM2PR02MB543.namprd02.prod.outlook.com>
Message-ID: <BY2PR0201MB18164E0FAD132A697ABA2EE0F0F90@BY2PR0201MB1816.namprd02.prod.outlook.com>

Franz,

Yes, it is possible to use the MMtel AS as a basis for developing a SIP application server. You may also find Gemini and Memento useful, which provide different application server functionality.

I'd recommend looking at the Sproutlets architecture which the MMTel AS is built upon which is described in a blog post at http://www.projectclearwater.org/tadhack-mini-london-and-sproutlets/.

You may also want to sign up to the Project Clearwater Mailing List - http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



Richard

-----Original Message-----
From: Franz Edler [mailto:franz.edler at technikum-wien.at] 
Sent: 04 September 2016 12:33
To: Simon Dredge <Simon.Dredge at metaswitch.com>; Clearwater Contact <contact at projectclearwater.org>
Subject: Signup at Project Clearwater

From: Franz Edler <franz.edler at technikum-wien.at>
Company: University of Applied Sciences

Message Body:
Is it possible to use the Clearwater platform in particular the MMtel AS implementation (Sprout) as a basis for development of further applications?

--
This mail is sent via contact form on Project Clearwater http://www.projectclearwater.org


From kate3018250 at gmail.com  Tue Sep  6 22:43:54 2016
From: kate3018250 at gmail.com (=?UTF-8?B?546L5L+K5Lit?=)
Date: Wed, 7 Sep 2016 10:43:54 +0800
Subject: [Project Clearwater] Is Bono a multi-thread program?
In-Reply-To: <BY2PR0201MB181688459C8CAFAE0B853D67F0F90@BY2PR0201MB1816.namprd02.prod.outlook.com>
References: <CAM_v+eT6WPxEny2EiCPbw1-EjN=p6OwzoUuAzA9rKRS-fgwBeQ@mail.gmail.com>
	<BY2PR0201MB1816C068F7DA89FC69C28971F0E30@BY2PR0201MB1816.namprd02.prod.outlook.com>
	<CAM_v+eRDTt1jBj-e-X7vBftodarv3Rsv955sap=5T4RAvxP8Kg@mail.gmail.com>
	<CAM_v+eRWO3_2YGEJPQEedywLmwtbBZuBNJ-01PpV5CnAS40x2A@mail.gmail.com>
	<BY2PR0201MB18165D98A3AC3DB81AF9021FF0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
	<CAM_v+eTR4dNrWBX3siuZJVU48i=GRAgn_3pp35MerywYse9huw@mail.gmail.com>
	<BY2PR0201MB181688459C8CAFAE0B853D67F0F90@BY2PR0201MB1816.namprd02.prod.outlook.com>
Message-ID: <CAMKvRk8CHE60hOEDAbSzAwLX3m5yvGFKvx13czAUSw9JoxwQPw@mail.gmail.com>

Hi , Richard

I know that is not very concerned about Clearwater , Thanks for your time
to answer my question .

Thank you very much .

Morris

2016-09-07 0:58 GMT+08:00 Richard Whitehouse (projectclearwater.org) <
richard.whitehouse at projectclearwater.org>:

> Morris,
>
>
>
> Yes that is possible. You could either spawn a new thread for each
> request, or dispatch it to a worker pool. What you choose to do is really
> an implementation detail of the application server you write, and not
> really related to how Clearwater is structured.
>
>
>
> Richard
>
>
>
> *From:* yan morris [mailto:morrisyan123 at gmail.com]
> *Sent:* 06 September 2016 05:30
> *To:* Richard Whitehouse (projectclearwater.org) <richard.whitehouse@
> projectclearwater.org>
> *Subject:* Re: [Project Clearwater] Is Bono a multi-thread program?
>
>
>
> Hi , Richard
>
> Thanks for your help .
>
>
> I understand the architecture of modules .
>
> However I have a question , I want to my stateful_proxy_server can process
> multiple request or response in the same time.
>
> Could I just create threads , and distribute incoming request to
> particular thread?
>
> That is , without creating new module to do multi-thread .
>
>
>
> Is that possible ?
>
> Thank you very much.
>
> Morris
>
>
>
> 2016-09-02 22:31 GMT+08:00 Richard Whitehouse (projectclearwater.org) <
> richard.whitehouse at projectclearwater.org>:
>
> Morris,
>
>
>
> The thread dispatcher calls pjsip_endpt_process_rx_data to continue
> processing of the message - http://www.pjsip.org/pjsip/
> docs/html/group__PJSIP__ENDPT.htm - this passes it to the other
> registered modules e.g. the stateful proxy module.
>
>
>
> If you are creating a thread and it immediately dies, then it?s possible
> that it?s run out of work to do. Normally threads will programmed to run in
> a loop handling events until terminated.
>
>
>
> Thanks,
>
>
>
> Richard
>
>
>
> *From:* yan morris [mailto:morrisyan123 at gmail.com]
> *Sent:* 02 September 2016 07:05
> *To:* Richard Whitehouse (projectclearwater.org) <richard.whitehouse@
> projectclearwater.org>
> *Subject:* Re: [Project Clearwater] Is Bono a multi-thread program?
>
>
>
> Hi , Richard
>
> Please forgive me that I misunderstand your reply.
>
> I find the thread_dispatcher.cpp file , however , I cannot understand how
> the mod_thread_dispatcher communicate with stateful_proxy_module. Could you
> explain it for me?
>
> In addition , I create the threads , but it would end immediately , do you
> know the reason about this problem?
>
> Thank for your time !
>
> Morris
>
>
>
> 2016-09-01 1:27 GMT+08:00 yan morris <morrisyan123 at gmail.com>:
>
> Hi , Richard
>
> Thank for your reply.
>
> Could you please tell me more detailed about implementing multi-thread on
> Bono ?
>
> Where I can find the codes about creating thread? Because I see the
> Bono.cpp , I only find the high level process
>
> request or response function . Does Clearwater use PJSIP API to make it
> multi-threaded or "mod_stateful_proxy"
>
> is implemented by multi-thread already?
>
> I am trying to use PJSIP write my own sip server as Clearwater's
> Application Server , and I want it multi-threaded .
>
> Therefore I think Bono's source code may help me , but I don't see the
> codes about this.
>
>
> Hope you can give me a hand.
>
> Thank you very much.
>
> Morris
>
>
>
> 2016-09-01 0:16 GMT+08:00 Richard Whitehouse (projectclearwater.org) <
> richard.whitehouse at projectclearwater.org>:
>
> Morris,
>
>
>
> Yes, Bono is multi-threaded. It?s architected with a transport thread
> which handles incoming PJSIP requests and a number of worker threads, which
> SIP messages are dispatched onto.
>
>
>
> The worker threads are created by thread_dispatcher. By default bono has
> one worker thread per CPU core.
>
>
>
> The threads are created using PJSIP?s pj_thread functions.
>
>
>
> Thanks,
>
>
>
> Richard
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *yan morris
> *Sent:* 31 August 2016 17:05
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Is Bono a multi-thread program?
>
>
>
> Hi ,
>
> I want to know , if Bono is a multi-thread program or not.
>
> I see the source code of Bono , I look at the function
> "proxy_on_rx_request"
>
>
>
> which process incoming request , but I don't see any code is creating a
> new thread when request income .
>
>
> Therefore ,  I want to know if Bono processes request using multi-thread ?
>
> If it does , could you tell where I can find the codes which concerned
> about?
>
> (or It is implemented by PJSIP , please let me know , too)
>
> Thank you very much
>
> Morris
>
>
>
>
>
>
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160907/f137f759/attachment.html>

From skhalup at virtuozzo.com  Wed Sep  7 05:20:11 2016
From: skhalup at virtuozzo.com (Stanislav Khalup)
Date: Wed, 7 Sep 2016 09:20:11 +0000
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress
	test
In-Reply-To: <BY2PR0201MB18160BD81061DFE13C9F963BF0F90@BY2PR0201MB1816.namprd02.prod.outlook.com>
References: <HE1PR0802MB25074DC687820CFBE4CF5C6CA1E20@HE1PR0802MB2507.eurprd08.prod.outlook.com>
	<BY2PR0201MB181634D7856F9265C4D217E4F0E50@BY2PR0201MB1816.namprd02.prod.outlook.com>
	<HE1PR0802MB250746E0B2624876C95FD50DA1F90@HE1PR0802MB2507.eurprd08.prod.outlook.com>
	<BY2PR0201MB18160BD81061DFE13C9F963BF0F90@BY2PR0201MB1816.namprd02.prod.outlook.com>
Message-ID: <HE1PR0802MB250727710EEEFD02F6F0AB85A1F80@HE1PR0802MB2507.eurprd08.prod.outlook.com>

Hello Richard,

Thank you very much for your responses. Initially we had deployed Clearwater manually in VMs in accordance with recommendations but we came across Sprout/Chronos crashes that I attached before. In order to eliminate the chance of mistake in deployment we tried all-in-one image (we do not expect any kind of performance data from it - just verification that Clearwater QoS based balancing is working). So what we wanted to see is that when number of concurrent calls reaches some limit the IMS starts to refuse new connections without dropping ongoing calls. We couldn't see this because at 15k sockets they all are being dropped fast.

The goal behind our investigation for full-scale deployment is comparing performance of IMS in VMs and containers. We expect at least 10-15% growth in number of concurrent calls before system starts rejecting new connections but we can't perform testing because we see this kind of behavior in VMs: when we reach some load the sockets are being closed to some level regardless of calls being processed and then crash dump is generated. The most interesting thing is that calls themselves are working. We tried calling each other and adding sipp load in the background and then at some point our call was dropped. (this test was done against full deployment).

BR,
Stanislav Khalup

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Tuesday, September 6, 2016 7:32 PM
To: Stanislav Khalup <skhalup at virtuozzo.com>; clearwater at lists.projectclearwater.org
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Stanislav,

Sorry to hear you are having these problems. We run stress regularly against Project Clearwater deployments, and we don't see such problems, so it's down to a difference in your setup.

We wouldn't ever expect to run stress against an all in one node - it's not designed for any particular capacity, it's designed to be use to manual trying out Clearwater performance, and trying it out initially.

Instead, as per the Stress Testing instructions, we'd expect stress to either be run against a deployment done using Chef, or using a Manual Install, with Clearwater deployed on at least six separate boxes with Sprout, Homer, Homestead, Ralf, Bono and Ellis all instead on separate servers, with separate servers for the Sip Stress Node. We'd expect all of the VMs to have around 1VCPU and 2GB of RAM as documented in the Manual Install instructions. If greater performance is required, we'd expect this to be achieved by increasing the number of deployed VMs, based on where the system was stressed, rather than increasing the resources assigned to each VM.

The latest dumps you have sent don't represent crashes, they represent processes being killed because they become unresponsive to the polling mechanism, due to the amount of load on the VM, and the process being unable to serve the monitoring request in time.

Can you provide some background on what you are trying to find out from the testing you are doing?


Thanks,

Richard

From: Stanislav Khalup [mailto:skhalup at virtuozzo.com]
Sent: 06 September 2016 16:14
To: Richard Whitehouse <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Hello again,

I'm sorry for spamming  the mail list but we continue our testing and come across new crashes. This time we took all-in-one VM and limited CPU to 1 core (to avoid Bono crashes). We found out that crashes begin when we hit ~15k open live sockets. The thing is we expect that Clearwater IMS will stop processing registration/call requests but will preserve ongoing calls but as we saw during sipp test but instead most connections are forcefully closed and dump is generated. I am adding some more dumps (homer this time) and sip logs. Could you please tell me if this kind of behavior during testing is kind of normal, expected behavior?

Dumps: https://www.dropbox.com/sh/bl5ghgwrpum6pq9/AAA_UQV7v9NfG3y8q0lOP0Xra?dl=0

BR,
Stanislav Khalup

From: Stanislav Khalup
Sent: Monday, September 5, 2016 5:29 PM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>; 'clearwater at lists.projectclearwater.org' <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Hello all,

We continue our stress tests to understand some dependencies behind crashes but it seems there is no such thing. This time we installed the latest all-in-one node and limited it to 1 CPU. Then we ran many sipp stress tests. We believed that in this case we will get application crashes only when load hits some considerable level but it seems crashes and actual number of registration attempts/calls ongoing are not or poorly related. The latest dumps are place here: https://www.dropbox.com/sh/ckjr8oi3rll5y78/AAArJDu7WItDxJjipdDMx7TVa?dl=0 Is this kind of re-occurring crashes expected from all-in-one node?

BR,
Stanislav Khalup

From: Stanislav Khalup
Sent: Friday, September 2, 2016 7:21 PM
To: 'Richard Whitehouse' <Richard.Whitehouse at metaswitch.com<mailto:Richard.Whitehouse at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Richard,

Thank you very much for your response. Let me add some details.  Initially we tested all component in VMs with 2 vCPUs each but after reading the list we changed bono config to 1 vCPU, for sprout we left 2 vCPUs. As for bono the crashes were somehow resolved but for sprout the situation is the same. We experience the same kind of crashes with all-in-one image (with 8vCPU).
As for SNMP statistics I don't know whether this is related or not but we couldn't get bono/sprout functional statistics like: The number of incoming requests, indexed by time period or The number of requests rejected due to overload, indexed by time period. - those metrics were always zero.
I've added a pair of chronos dumps to dropbox folder.  Maybe they can shed some more light on the problem: https://www.dropbox.com/sh/qjdja9eowgvo1zc/AADm25_pwKNs3gWwBmb0Pzhpa?dl=0

BR,
Stanislav Khalup

From: Richard Whitehouse [mailto:Richard.Whitehouse at metaswitch.com]
Sent: Friday, September 2, 2016 5:19 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Stanislav Khalup <skhalup at virtuozzo.com<mailto:skhalup at virtuozzo.com>>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: RE: Sprout/Bono/Chronos crashes under stress test

Stanislav,

I've taken a look at the Sprout crash. It looks like you have are hitting a crash in the Net SNMP library we use for alarms and statistics. I've raised an issue to track this - https://github.com/Metaswitch/sprout/issues/1527

We've seen similar looking stacks for Bono before on multi-core VMs - e.g. under http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/2015-January/001986.html

Historically we've scaled up Sprout and Bono by running many single or dual-core instances rather than running fewer larger instances - this is because
- we've seen virtualization environments impose per-VM limits on TCP connection counts, and obviously Bono has large numbers of TCP connections in a real-world scenario
- we only support a single transport thread and, since Bono performs relatively little processing per message, and Sprout needs to perform some processing per message, it is this that ends up being the bottleneck quite quickly.

Generally we've run single core Bono nodes, and dual core Sprout nodes.

Having said that, we should look into why it's crashing when you're running more cores.

Can you give us some description of the scenario you are running under when you see this?

You might also find it useful to subscribe to the mailing list so that you receive updates when we push out updated releases.

Thanks,

Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Stanislav Khalup
Sent: 01 September 2016 10:49
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: [Project Clearwater] Sprout/Bono/Chronos crashes under stress test

Hello all,

We've been trying to perform IMS stress testing for some time now but it seems that we are really unlucky. When we perform sip test we experience constant bono/sprout crashes which affects results of our performance evaluation. The thing is we do know that generally our deployment is working (we managed to perform calls and run tests). At first we manually deployed IMS cluster but after crashes we decided to try all in one VM but we still experience sprout crashes (bono crashes are mostly fixed after setting 1CPU/1Worker). Could you please look at the dumps: https://www.dropbox.com/sh/qjdja9eowgvo1zc/AADm25_pwKNs3gWwBmb0Pzhpa?dl=0 because for now we have no clue for what is happening.

BR,
Stanislav
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160907/2894c66b/attachment.html>

From surender.s at hcl.com  Thu Sep  8 09:03:55 2016
From: surender.s at hcl.com (Surender Singh)
Date: Thu, 8 Sep 2016 13:03:55 +0000
Subject: [Project Clearwater] IPPBX Integration with Clearwater IMS
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107DFD21E@NDA-HCLT-MBS02.hclt.corp.hcl.in>

Hi,

I have deployed the All-in-one image solution  using VMware vSphere ESXi and I able to create user using Ebill with default domain example.com.

I did not installed the ebill separately  I just log-in  the using my IMS IP 10.112.87.60 (got from DHCP at time of deployment ) .

And also able to registered and make the calls using Zoiper(means IMS to IMS calls are connecting )

I follow  automated process for deployment

Please suggest on below my queries .


1)      can I change the default domain name example.com to zyx.com ,if yes please share procedure for All-in-one image .

2)      Can I connect the IPPBX (third party) with IMS networking if yes please share the procedure

3)      How we will see all 6 nodes are up or down in aio solution .

4)      Please suggest if any configuration is required for aio solution .

Quick Revert  will be appreciable

Regards
Surender Singh
8826292018



::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160908/a291208b/attachment.html>

From richard.whitehouse at projectclearwater.org  Thu Sep  8 10:09:50 2016
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Thu, 8 Sep 2016 14:09:50 +0000
Subject: [Project Clearwater] IPPBX Integration with Clearwater IMS
In-Reply-To: <16AE9ED83DBA5B4D85B058EAF39C918107DFD21E@NDA-HCLT-MBS02.hclt.corp.hcl.in>
References: <16AE9ED83DBA5B4D85B058EAF39C918107DFD21E@NDA-HCLT-MBS02.hclt.corp.hcl.in>
Message-ID: <BY2PR0201MB1816678983E13BE057B4081AF0FB0@BY2PR0201MB1816.namprd02.prod.outlook.com>

Surender,

I presume you mean 'ellis' instead of 'ebill'?

You can change the default domain by changing the value of the home_domain configuration option. Changing this is described in the documentation at http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options. You might want to review the other configuration options listed in that article to see if there is anything else you want to configure.

You can configure multiple domains using http://clearwater.readthedocs.io/en/stable/Multiple_Domains.html

You can monitor the different functions installed on an all-in-one node by running `sudo monit summary` when logged in to the box as the default user. You can also use more advanced ways of monitoring the node such as SNMP statistics and alarms, which are documented in the docs - e.g. at http://clearwater.readthedocs.io/en/stable/SNMP_Alarms.html

Regarding connecting the IPPBX to Project Clearwater, I think you'll need to know more details before it's possible to know whether it'll work. What type of IPPBX do you have? Does it support registering against an IMS Core, or does it expect to be configured, and not registered?

Finally, it's important to note that the all-in-one node is for really for trying Project Clearwater out, rather than something we'd expect to be deployed in a production environment. We'd really expect one of the other install methods to be used for a production solution.



Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 08 September 2016 14:04
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] IPPBX Integration with Clearwater IMS

Hi,

I have deployed the All-in-one image solution  using VMware vSphere ESXi and I able to create user using Ebill with default domain example.com.

I did not installed the ebill separately  I just log-in  the using my IMS IP 10.112.87.60 (got from DHCP at time of deployment ) .

And also able to registered and make the calls using Zoiper(means IMS to IMS calls are connecting )

I follow  automated process for deployment

Please suggest on below my queries .


1)      can I change the default domain name example.com to zyx.com ,if yes please share procedure for All-in-one image .

2)      Can I connect the IPPBX (third party) with IMS networking if yes please share the procedure

3)      How we will see all 6 nodes are up or down in aio solution .

4)      Please suggest if any configuration is required for aio solution .



Quick Revert  will be appreciable

Regards
Surender Singh
8826292018



::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160908/8cb9a472/attachment.html>

From richard.whitehouse at projectclearwater.org  Thu Sep  8 10:22:04 2016
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Thu, 8 Sep 2016 14:22:04 +0000
Subject: [Project Clearwater] IPPBX Integration with Clearwater IMS
In-Reply-To: <BY2PR0201MB1816678983E13BE057B4081AF0FB0@BY2PR0201MB1816.namprd02.prod.outlook.com>
References: <16AE9ED83DBA5B4D85B058EAF39C918107DFD21E@NDA-HCLT-MBS02.hclt.corp.hcl.in>
	<BY2PR0201MB1816678983E13BE057B4081AF0FB0@BY2PR0201MB1816.namprd02.prod.outlook.com>
Message-ID: <BY2PR0201MB181669F79020B3C320FB8E80F0FB0@BY2PR0201MB1816.namprd02.prod.outlook.com>

Surender,

As a follow up, it's worth noting that home_domain is being changed, we strongly recommend that the deployment is recreated from scratch to ensure that the settings are being consistently applied - see http://clearwater.readthedocs.io/en/stable/Modifying_Clearwater_settings.html.

Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org)
Sent: 08 September 2016 15:10
To: clearwater at lists.projectclearwater.org; Surender Singh <surender.s at hcl.com>
Subject: Re: [Project Clearwater] IPPBX Integration with Clearwater IMS

Surender,

I presume you mean 'ellis' instead of 'ebill'?

You can change the default domain by changing the value of the home_domain configuration option. Changing this is described in the documentation at http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options. You might want to review the other configuration options listed in that article to see if there is anything else you want to configure.

You can configure multiple domains using http://clearwater.readthedocs.io/en/stable/Multiple_Domains.html

You can monitor the different functions installed on an all-in-one node by running `sudo monit summary` when logged in to the box as the default user. You can also use more advanced ways of monitoring the node such as SNMP statistics and alarms, which are documented in the docs - e.g. at http://clearwater.readthedocs.io/en/stable/SNMP_Alarms.html

Regarding connecting the IPPBX to Project Clearwater, I think you'll need to know more details before it's possible to know whether it'll work. What type of IPPBX do you have? Does it support registering against an IMS Core, or does it expect to be configured, and not registered?

Finally, it's important to note that the all-in-one node is for really for trying Project Clearwater out, rather than something we'd expect to be deployed in a production environment. We'd really expect one of the other install methods to be used for a production solution.



Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 08 September 2016 14:04
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] IPPBX Integration with Clearwater IMS

Hi,

I have deployed the All-in-one image solution  using VMware vSphere ESXi and I able to create user using Ebill with default domain example.com.

I did not installed the ebill separately  I just log-in  the using my IMS IP 10.112.87.60 (got from DHCP at time of deployment ) .

And also able to registered and make the calls using Zoiper(means IMS to IMS calls are connecting )

I follow  automated process for deployment

Please suggest on below my queries .


1)      can I change the default domain name example.com to zyx.com ,if yes please share procedure for All-in-one image .

2)      Can I connect the IPPBX (third party) with IMS networking if yes please share the procedure

3)      How we will see all 6 nodes are up or down in aio solution .

4)      Please suggest if any configuration is required for aio solution .



Quick Revert  will be appreciable

Regards
Surender Singh
8826292018



::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160908/9edd8941/attachment.html>

From kosalayb at gmail.com  Thu Sep  8 11:42:50 2016
From: kosalayb at gmail.com (Kosala)
Date: Thu, 8 Sep 2016 16:42:50 +0100
Subject: [Project Clearwater] SIP calls are not stable
Message-ID: <CAGEe17YnYmDSbegk1iypaAMEBqC82f7RxtY5jfSnAX2gwUApfQ@mail.gmail.com>

Hi Team,

I have two sip clients installed; Zoiper in an ubuntu VM and Jitsi in a
windows environment. I have clearwater installed in separate VMs in Orange
Box.

When I am trying to make calls from sip clients, sometimes it works and
sometimes it is not working. I need to try a several times to get calls
working. It looks not stable. Can I make this stable?


Thanks in advance,
Kosala
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160908/fbb8c708/attachment.html>

From surender.s at hcl.com  Fri Sep  9 01:40:13 2016
From: surender.s at hcl.com (Surender Singh)
Date: Fri, 9 Sep 2016 05:40:13 +0000
Subject: [Project Clearwater] IPPBX Integration with Clearwater IMS
In-Reply-To: <BY2PR0201MB1816678983E13BE057B4081AF0FB0@BY2PR0201MB1816.namprd02.prod.outlook.com>
References: <16AE9ED83DBA5B4D85B058EAF39C918107DFD21E@NDA-HCLT-MBS02.hclt.corp.hcl.in>
	<BY2PR0201MB1816678983E13BE057B4081AF0FB0@BY2PR0201MB1816.namprd02.prod.outlook.com>
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107DFE085@NDA-HCLT-MBS02.hclt.corp.hcl.in>

Hi Richard,

Thanks for prompt response .

Please reply on my queries to consider ALL-IN-ONE NODE solution only.

I means Ellis instead of Ebill

PBX Connectivity:


1)       I just using the all in one node for testing setup not for production .


2)       I want create the sip trunk between Mitel IPPBX and IMS using Bono nodes



Hostname Change issue:

           I also try  to change the shared-config  but this is not working when I trying to login Ellis portal using IMS ip (10.112.87.60). I also restart the all infra services using sudo service clearwater-infrastructure restart

One more query here ,whether all IMS node (Sprout Bono, Homestaed, Homer, Ralf, Eliis and Memento) having same IP Address with different port , if yes then  how I can see the all nodes and its IP.

Regards
Surender Singh
8826292018

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: 08 September 2016 19:40
To: clearwater at lists.projectclearwater.org; Surender Singh <surender.s at hcl.com>
Subject: RE: IPPBX Integration with Clearwater IMS

Surender,

I presume you mean 'ellis' instead of 'ebill'?

You can change the default domain by changing the value of the home_domain configuration option. Changing this is described in the documentation at http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options. You might want to review the other configuration options listed in that article to see if there is anything else you want to configure.

You can configure multiple domains using http://clearwater.readthedocs.io/en/stable/Multiple_Domains.html

You can monitor the different functions installed on an all-in-one node by running `sudo monit summary` when logged in to the box as the default user. You can also use more advanced ways of monitoring the node such as SNMP statistics and alarms, which are documented in the docs - e.g. at http://clearwater.readthedocs.io/en/stable/SNMP_Alarms.html

Regarding connecting the IPPBX to Project Clearwater, I think you'll need to know more details before it's possible to know whether it'll work. What type of IPPBX do you have? Does it support registering against an IMS Core, or does it expect to be configured, and not registered?

Finally, it's important to note that the all-in-one node is for really for trying Project Clearwater out, rather than something we'd expect to be deployed in a production environment. We'd really expect one of the other install methods to be used for a production solution.



Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 08 September 2016 14:04
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] IPPBX Integration with Clearwater IMS

Hi,

I have deployed the All-in-one image solution  using VMware vSphere ESXi and I able to create user using Ebill with default domain example.com.

I did not installed the ebill separately  I just log-in  the using my IMS IP 10.112.87.60 (got from DHCP at time of deployment ) .

And also able to registered and make the calls using Zoiper(means IMS to IMS calls are connecting )

I follow  automated process for deployment

Please suggest on below my queries .


1)      can I change the default domain name example.com to zyx.com ,if yes please share procedure for All-in-one image .

2)      Can I connect the IPPBX (third party) with IMS networking if yes please share the procedure

3)      How we will see all 6 nodes are up or down in aio solution .

4)      Please suggest if any configuration is required for aio solution .



Quick Revert  will be appreciable

Regards
Surender Singh
8826292018



::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160909/979ff457/attachment.html>

From skhalup at virtuozzo.com  Fri Sep  9 11:09:09 2016
From: skhalup at virtuozzo.com (Stanislav Khalup)
Date: Fri, 9 Sep 2016 15:09:09 +0000
Subject: [Project Clearwater] Stress test results
Message-ID: <HE1PR0802MB250785953406BF517971A543A1FA0@HE1PR0802MB2507.eurprd08.prod.outlook.com>

Hello team,

At last after updating to the latest release, limiting all VMs to 1CPU/2GB Ram and setting all settings to default we managed to stop IMS cluster from crashes. The thing is we see the same results while performing stress tests. After some time the system closes socket regardless of ongoing calls. Is this kind of behavior normal for Clearwater IMS. I attach the full sipp logs and screens archive. Please look at graphic for network usage that shows downfall of traffic when sockets are being closed: https://www.dropbox.com/s/q2l3nsaxyu6sf32/sipp_stress.tar.gz?dl=0

Another question that is bothering me is how to interpret the results of stress testing? How can you evaluate that your deployment is hitting the limit? All the orchestration demos out there show that new nodes are added when CPU utilization reaches some 30% but in our test we could never see such loads even with 1-1 Bono and Sprout. Judging from the bucket algorithm description we assumed that refused connections metric should indicate that the system is at the limit but this snmp statistics is always zero in all our tests. So, how are you processing the results of stress test? What metrics are you looking at?

BR,
Stanislav Khalup
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160909/87d270b3/attachment.html>

From surender.s at hcl.com  Mon Sep 12 01:49:50 2016
From: surender.s at hcl.com (Surender Singh)
Date: Mon, 12 Sep 2016 05:49:50 +0000
Subject: [Project Clearwater] IPPBX Integration with Clearwater
	IMS_all-in-one node
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E0298A@NDA-HCLT-MBS02.hclt.corp.hcl.in>

Hi Richard,

Your help is very much required here..

I am not able to change the Host name from example.com to xyz.com .

Below is the procedure which I used to deployed the aio image and till now.


1)      Deployed the aio image file in VMware machine.

2)      Post deployment received the ip address 10.112.87.60 from DHCP .

3)      I typed the ip 10.112.87.60 in IE URL to go Ellis and create the Two number from Ellis Portal .

4)      After creating the number I registered  both number through Zopier and registered successfully  and call also connecting with SIP uri.

Now first of all , I want to change the Host name and create the subscriber manually .

As per your provided link, I am not able to change the host name.

http://clearwater.readthedocs.io/en/stable/Modifying_Clearwater_settings.html.

Note: This All in one image  solution I am going to use only for  testing purpose currently ,later on I will use Whole solution .


Regards
Surender Singh
8826292018

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: 08 September 2016 19:52
To: clearwater at lists.projectclearwater.org; Surender Singh <surender.s at hcl.com>
Subject: RE: IPPBX Integration with Clearwater IMS

Surender,

As a follow up, it's worth noting that home_domain is being changed, we strongly recommend that the deployment is recreated from scratch to ensure that the settings are being consistently applied - see http://clearwater.readthedocs.io/en/stable/Modifying_Clearwater_settings.html.

Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org)
Sent: 08 September 2016 15:10
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Surender Singh <surender.s at hcl.com<mailto:surender.s at hcl.com>>
Subject: Re: [Project Clearwater] IPPBX Integration with Clearwater IMS

Surender,

I presume you mean 'ellis' instead of 'ebill'?

You can change the default domain by changing the value of the home_domain configuration option. Changing this is described in the documentation at http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html#core-options. You might want to review the other configuration options listed in that article to see if there is anything else you want to configure.

You can configure multiple domains using http://clearwater.readthedocs.io/en/stable/Multiple_Domains.html

You can monitor the different functions installed on an all-in-one node by running `sudo monit summary` when logged in to the box as the default user. You can also use more advanced ways of monitoring the node such as SNMP statistics and alarms, which are documented in the docs - e.g. at http://clearwater.readthedocs.io/en/stable/SNMP_Alarms.html

Regarding connecting the IPPBX to Project Clearwater, I think you'll need to know more details before it's possible to know whether it'll work. What type of IPPBX do you have? Does it support registering against an IMS Core, or does it expect to be configured, and not registered?

Finally, it's important to note that the all-in-one node is for really for trying Project Clearwater out, rather than something we'd expect to be deployed in a production environment. We'd really expect one of the other install methods to be used for a production solution.



Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 08 September 2016 14:04
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] IPPBX Integration with Clearwater IMS

Hi,

I have deployed the All-in-one image solution  using VMware vSphere ESXi and I able to create user using Ebill with default domain example.com.

I did not installed the ebill separately  I just log-in  the using my IMS IP 10.112.87.60 (got from DHCP at time of deployment ) .

And also able to registered and make the calls using Zoiper(means IMS to IMS calls are connecting )

I follow  automated process for deployment

Please suggest on below my queries .


1)      can I change the default domain name example.com to zyx.com ,if yes please share procedure for All-in-one image .

2)      Can I connect the IPPBX (third party) with IMS networking if yes please share the procedure

3)      How we will see all 6 nodes are up or down in aio solution .

4)      Please suggest if any configuration is required for aio solution .



Quick Revert  will be appreciable

Regards
Surender Singh
8826292018



::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160912/76eedc94/attachment.html>

From morrisyan123 at gmail.com  Mon Sep 12 07:51:05 2016
From: morrisyan123 at gmail.com (yan morris)
Date: Mon, 12 Sep 2016 19:51:05 +0800
Subject: [Project Clearwater] iFC of Sip-Stress
Message-ID: <CAM_v+eSWUFo_Abu4GxgdvJyAmGsaAWwyDyMEeBCsjDqdV6x39w@mail.gmail.com>

Hi ,

I want to use Sip-Stress to test my Application Server,

In order to let all test calls go through my Application server ,

In small number test , I can use Web UI of Ellis to make it ,

but if there are many accounts , what should I do to modify the accounts
settings , or

where I can get some concerned information about this issue?


Thank you

Morris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160912/64ef8b35/attachment.html>

From kosalayb at gmail.com  Mon Sep 12 08:36:53 2016
From: kosalayb at gmail.com (Kosala)
Date: Mon, 12 Sep 2016 13:36:53 +0100
Subject: [Project Clearwater] bracket-ipv6-address: not found
Message-ID: <CAGEe17ZaKtQz53XSgG7V0j6Xe85OH+_d=Hb5hP_arAuCjrpJjw@mail.gmail.com>

Hi Team,

when I am trying to restart homestead, I am getting an error:

/etc/init.d/homestead: 132: /etc/init.d/homestead:
/usr/share/clearwater/bin/bracket-ipv6-address: not found

Any clue on how to solve this issue?

Thanks
Kosala
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160912/3db4a07c/attachment.html>

From surender.s at hcl.com  Tue Sep 13 02:48:22 2016
From: surender.s at hcl.com (Surender Singh)
Date: Tue, 13 Sep 2016 06:48:22 +0000
Subject: [Project Clearwater] Getting 503 service unavailable_All in one
	nodes
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E04E9F@NDA-HCLT-MBS02.hclt.corp.hcl.in>

An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160913/dd20e958/attachment.html>

From surender.s at hcl.com  Tue Sep 13 08:52:34 2016
From: surender.s at hcl.com (Surender Singh)
Date: Tue, 13 Sep 2016 12:52:34 +0000
Subject: [Project Clearwater] Getting 503 service unavailable_All in one
	nodes
In-Reply-To: <16AE9ED83DBA5B4D85B058EAF39C918107E04E9F@NDA-HCLT-MBS02.hclt.corp.hcl.in>
References: <16AE9ED83DBA5B4D85B058EAF39C918107E04E9F@NDA-HCLT-MBS02.hclt.corp.hcl.in>
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E05F13@NDA-HCLT-MBS02.hclt.corp.hcl.in>

An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160913/78b4d45c/attachment.html>

From graeme at projectclearwater.org  Tue Sep 13 09:57:51 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Tue, 13 Sep 2016 13:57:51 +0000
Subject: [Project Clearwater] bracket-ipv6-address: not found
References: <CAGEe17ZaKtQz53XSgG7V0j6Xe85OH+_d=Hb5hP_arAuCjrpJjw@mail.gmail.com>
Message-ID: <CY4PR02MB2616651BD47EFE083B25FDFEE3FE0@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Kosala,

The bracket-ipv6-address executable is installed by the clearwater-infrastructure package<https://github.com/Metaswitch/clearwater-infrastructure/blob/566ccbb9913ccb9dfbb452a9185b7bf93c32bc1d/debian/clearwater-infrastructure.install#L4>. However, it used to be a python script. I?m wondering whether you are running with an old version of the clearwater-infrastructure package (that still references the python script).

Does /usr/share/clearwater/bin contain a file called bracket_ipv6_address.py (rather than bracket-ipv6-address)?
What versions of homestead and clearwater-infrastructure are you using? You can run dpkg -s homestead and dpkg -s clearwater-infrastructure to find out.

If it turns out that you are running different versions of homestead and clearwater-infrastructure, it would be good to understand how that happened. How did you create your deployment?

Thanks!
Graeme

________________________________
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Kosala
Sent: 12 September 2016 13:37
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] bracket-ipv6-address: not found

Hi Team,
when I am trying to restart homestead, I am getting an error:

/etc/init.d/homestead: 132: /etc/init.d/homestead: /usr/share/clearwater/bin/bracket-ipv6-address: not found
Any clue on how to solve this issue?
Thanks
Kosala
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160913/a6443b30/attachment.html>

From graeme at projectclearwater.org  Tue Sep 13 10:00:07 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Tue, 13 Sep 2016 14:00:07 +0000
Subject: [Project Clearwater] Getting 503 service unavailable_All in
	one	nodes
References: <16AE9ED83DBA5B4D85B058EAF39C918107E04E9F@NDA-HCLT-MBS02.hclt.corp.hcl.in>
Message-ID: <CY4PR02MB2616A3C0F7E77218D95363EAE3FE0@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Surender,

Have you restarted the Sprout process since you changed host_domain? What is --domain set to if you run ps -eaf | grep /usr/share/clearwater/bin/sprout? After changing configuration all the Clearwater processes need to be restarted, and you can do that by running sudo monit restart all. My guess is that Sprout is receiving a SIP message to xxx at ims.hcl.com<mailto:xxx at ims.hcl.com>, and it doesn't yet recognise ims.hcl.com as its home domain, so its trying to figure out where to proxy the message onto rather than processing it locally.

You shouldn't need any DNS entries to run an AIO node, but for more information on Clearwater DNS and setting up your own DNS, see http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 13 September 2016 07:48
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Getting 503 service unavailable_All in one nodes


Hi,



Can anyone support me to resolve  the below problem .



I am using AIO image at  Vmware Esxi platform .Post deployement  the image i was able to registered the client using Zoiper and call was also conneting .



I am using this AIO for Demo purpose only.





But post change the Host_domain form example.com to ims.hcl.com , i unable to get registered  client .



As per logs error found var/log/sprout/sprout_curent.txt



13-09-2016 12:04:30.118 UTC Error dnscachedresolver.cpp:607: Failed to retrieve record for ims.hcl.com: Domain name not found

13-09-2016 12:05:02.223 UTC Error dnscachedresolver.cpp:607: Failed to retrieve record for _sip._tcp.ims.hcl.com: Domain name not found





I also added the 1000 more number using below command and this refleting in Ellis potal.



sudo usr/src/metaswitch/ellis/tools/create_numbers.py --start 6505550000 --count 1000 --realm ims.hcl.com



Now Please suggest me where i need to make DNS entry and share configuration for aio solution



Regards

Surender


::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160913/b7cbfab0/attachment.html>

From graeme at projectclearwater.org  Tue Sep 13 10:20:49 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Tue, 13 Sep 2016 14:20:49 +0000
Subject: [Project Clearwater] iFC of Sip-Stress
In-Reply-To: <CAM_v+eSWUFo_Abu4GxgdvJyAmGsaAWwyDyMEeBCsjDqdV6x39w@mail.gmail.com>
References: <CAM_v+eSWUFo_Abu4GxgdvJyAmGsaAWwyDyMEeBCsjDqdV6x39w@mail.gmail.com>
Message-ID: <CY4PR02MB2616092C2FA35AB5A681DFA9E3FE0@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Morris,

It sounds like you want to provision lots of subscribers with specific Initial Filter Criteria (pointing at your Application Server). We have a process for bulk provisioning<https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%20Numbers.md>, but the Initial Filter Criteria are not configurable ? the bulk_create.py<https://github.com/Metaswitch/crest/blob/320e224e5a30df30cd53ed2d660d817cdf3e2ff4/src/metaswitch/crest/tools/bulk_create.py#L102> script just calls generate_ifcs<https://github.com/Metaswitch/python-common/blob/dev/metaswitch/common/ifcs.py#L36>, which contains a single hardcoded rule for the MMTel on INVITEs. However, you could quite easily tweak the bulk_create.py script  on your node before running it to set initial_filter_xml to whatever you want. Let me know if that doesn?t make sense!

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of yan morris
Sent: 12 September 2016 12:51
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] iFC of Sip-Stress

Hi ,

I want to use Sip-Stress to test my Application Server,

In order to let all test calls go through my Application server ,

In small number test , I can use Web UI of Ellis to make it ,

but if there are many accounts , what should I do to modify the accounts settings , or

where I can get some concerned information about this issue?


Thank you

Morris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160913/c7b8c816/attachment.html>

From trey.ormsbee at interoptechnologies.com  Tue Sep 13 11:54:32 2016
From: trey.ormsbee at interoptechnologies.com (Trey Ormsbee)
Date: Tue, 13 Sep 2016 10:54:32 -0500
Subject: [Project Clearwater] latency to homestead causes curl connection
	fatal errors
Message-ID: <1473782072.6400.111.camel@interoptechnologies.com>

I am trying to simulate a higher latency connection to sprout and I have
inserted latency ranging from 50ms-1000ms on the sprout signaling
adapter.  I have an issue that occurs fairly quickly with homestead
queries (even with as low as 50ms) here is a snippet of the log:

12-09-2016 21:14:12.125 UTC Debug connection_pool.h:225: Request for
connection to IP: xxx.xxx.xxx.51, port: 8888
12-09-2016 21:14:12.125 UTC Debug connection_pool.h:244: No existing
connection in pool, create one
12-09-2016 21:14:12.126 UTC Debug http_connection_pool.cpp:56: Allocated
CURL handle 0x7fd0d000ec00
12-09-2016 21:14:12.126 UTC Debug connection_pool.h:246: Created new
connection 0x7fd0d0017a80
12-09-2016 21:14:12.126 UTC Debug httpclient.cpp:466: Sending HTTP
request : http://homestead.example.com:8888/impi/%2B15557775555%
40example.com/av?impu=sip%3A%2B15557775555%40example.com (trying
xxx.xxx.xxx.51)
12-09-2016 21:14:12.177 UTC Error httpclient.cpp:487:
http://homestead.example.com:8888/impi/%2B15557775555%
40example.com/av?impu=sip%3A%2B15557775555%40example.com failed at
server xxx.xxx.xxx.51 : Timeout was reached (28) : fatal
12-09-2016 21:14:12.177 UTC Debug baseresolver.cpp:498: Add
xxx.xxx.xxx.51:8888 transport 6 to blacklist for 30 seconds, graylist
for 30 seconds
12-09-2016 21:14:12.177 UTC Debug connection_pool.h:261: Release
connection to IP: xxx.xxx.xxx.51, port: 8888 and destroy
12-09-2016 21:14:12.177 UTC Debug connection_pool.h:225: Request for
connection to IP: xxx.xxx.xxx.52, port: 8888
12-09-2016 21:14:12.177 UTC Debug connection_pool.h:244: No existing
connection in pool, create one
12-09-2016 21:14:12.178 UTC Debug http_connection_pool.cpp:56: Allocated
CURL handle 0x7fd0d000ec00
12-09-2016 21:14:12.178 UTC Debug connection_pool.h:246: Created new
connection 0x7fd0d0019ef0
12-09-2016 21:14:12.178 UTC Debug httpclient.cpp:466: Sending HTTP
request : http://homestead.example.com:8888/impi/%2B15557775555%
40example.com/av?impu=sip%3A%2B15557775555%40example.com (trying
xxx.xxx.xxx.52)
12-09-2016 21:14:12.229 UTC Error httpclient.cpp:487:
http://homestead.example.com:8888/impi/%2B15557775555%
40example.com/av?impu=sip%3A%2B15557775555%40example.com failed at
server xxx.xxx.xxx.52 : Timeout was reached (28) : fatal
12-09-2016 21:14:12.229 UTC Debug baseresolver.cpp:498: Add
xxx.xxx.xxx.52:8888 transport 6 to blacklist for 30 seconds, graylist
for 30 seconds
12-09-2016 21:14:12.229 UTC Debug connection_pool.h:261: Release
connection to IP: xxx.xxx.xxx.52, port: 8888 and destroy
12-09-2016 21:14:12.229 UTC Debug communicationmonitor.cpp:82: Checking
communication changes - successful attempts 0, failures 1
12-09-2016 21:14:12.229 UTC Error httpclient.cpp:623: cURL failure with
cURL error code 28 (see man 3 libcurl-errors) and HTTP error code 500
12-09-2016 21:14:12.229 UTC Error hssconnection.cpp:149: Failed to get
Authentication Vector for +15557775555 at example.com
12-09-2016 21:14:12.229 UTC Debug authentication.cpp:638: Failed to get
Authentication vector

This is happening on release-105 and is fairly easily recreate on a busy
system by adding latency with tc. I used:

 ip netns exec signaling tc qdisc add dev eth1 root netem delay 400ms
200ms

After 5 or 6 successful registrations,  the above issue starts.  Once
that issue starts the only way to correct it is to remove the latency.
Any ideas what might be causing this,  does anyone else experience the
same issues? I found this old issue logged that seems be a similar
issue: https://github.com/Metaswitch/sprout/issues/144.

Thanks,
Trey
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160913/abb8c024/attachment.html>

From graeme at projectclearwater.org  Tue Sep 13 12:59:31 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Tue, 13 Sep 2016 16:59:31 +0000
Subject: [Project Clearwater] latency to homestead causes curl
	connection	fatal errors
In-Reply-To: <1473782072.6400.111.camel@interoptechnologies.com>
References: <1473782072.6400.111.camel@interoptechnologies.com>
Message-ID: <CY4PR02MB2616FA741B53AA462B65C620E3FE0@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Trey,

Have you seen this working on previous releases? We expect to be able to create HTTP connections from Sprout to Homestead very quickly (i.e. in under 10ms) so our HTTP connection timeout is set to 50ms (see here<https://github.com/Metaswitch/cpp-common/blob/1477ab58ca02d9b342554f887af75836720efbb5/include/http_connection_pool.h#L68>), and I think that?s the timeout you?re hitting. I suspect everything initially continues working after you add the latency because Sprout is using connections to Homestead that have already been set up, and you only run into the timeout problem when Sprout needs to set up new connections.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Trey Ormsbee
Sent: 13 September 2016 16:55
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] latency to homestead causes curl connection fatal errors

I am trying to simulate a higher latency connection to sprout and I have inserted latency ranging from 50ms-1000ms on the sprout signaling adapter.  I have an issue that occurs fairly quickly with homestead queries (even with as low as 50ms) here is a snippet of the log:

12-09-2016 21:14:12.125 UTC Debug connection_pool.h:225: Request for connection to IP: xxx.xxx.xxx.51, port: 8888
12-09-2016 21:14:12.125 UTC Debug connection_pool.h:244: No existing connection in pool, create one
12-09-2016 21:14:12.126 UTC Debug http_connection_pool.cpp:56: Allocated CURL handle 0x7fd0d000ec00
12-09-2016 21:14:12.126 UTC Debug connection_pool.h:246: Created new connection 0x7fd0d0017a80
12-09-2016 21:14:12.126 UTC Debug httpclient.cpp:466: Sending HTTP request : http://homestead.example.com:8888/impi/%2B15557775555%40example.com/av?impu=sip%3A%2B15557775555%40example.com (trying xxx.xxx.xxx.51)
12-09-2016 21:14:12.177 UTC Error httpclient.cpp:487: http://homestead.example.com:8888/impi/%2B15557775555%40example.com/av?impu=sip%3A%2B15557775555%40example.com failed at server xxx.xxx.xxx.51 : Timeout was reached (28) : fatal
12-09-2016 21:14:12.177 UTC Debug baseresolver.cpp:498: Add xxx.xxx.xxx.51:8888 transport 6 to blacklist for 30 seconds, graylist for 30 seconds
12-09-2016 21:14:12.177 UTC Debug connection_pool.h:261: Release connection to IP: xxx.xxx.xxx.51, port: 8888 and destroy
12-09-2016 21:14:12.177 UTC Debug connection_pool.h:225: Request for connection to IP: xxx.xxx.xxx.52, port: 8888
12-09-2016 21:14:12.177 UTC Debug connection_pool.h:244: No existing connection in pool, create one
12-09-2016 21:14:12.178 UTC Debug http_connection_pool.cpp:56: Allocated CURL handle 0x7fd0d000ec00
12-09-2016 21:14:12.178 UTC Debug connection_pool.h:246: Created new connection 0x7fd0d0019ef0
12-09-2016 21:14:12.178 UTC Debug httpclient.cpp:466: Sending HTTP request : http://homestead.example.com:8888/impi/%2B15557775555%40example.com/av?impu=sip%3A%2B15557775555%40example.com (trying xxx.xxx.xxx.52)
12-09-2016 21:14:12.229 UTC Error httpclient.cpp:487: http://homestead.example.com:8888/impi/%2B15557775555%40example.com/av?impu=sip%3A%2B15557775555%40example.com failed at server xxx.xxx.xxx.52 : Timeout was reached (28) : fatal
12-09-2016 21:14:12.229 UTC Debug baseresolver.cpp:498: Add xxx.xxx.xxx.52:8888 transport 6 to blacklist for 30 seconds, graylist for 30 seconds
12-09-2016 21:14:12.229 UTC Debug connection_pool.h:261: Release connection to IP: xxx.xxx.xxx.52, port: 8888 and destroy
12-09-2016 21:14:12.229 UTC Debug communicationmonitor.cpp:82: Checking communication changes - successful attempts 0, failures 1
12-09-2016 21:14:12.229 UTC Error httpclient.cpp:623: cURL failure with cURL error code 28 (see man 3 libcurl-errors) and HTTP error code 500
12-09-2016 21:14:12.229 UTC Error hssconnection.cpp:149: Failed to get Authentication Vector for +15557775555 at example.com<mailto:+15557775555 at example.com>
12-09-2016 21:14:12.229 UTC Debug authentication.cpp:638: Failed to get Authentication vector

This is happening on release-105 and is fairly easily recreate on a busy system by adding latency with tc. I used:

ip netns exec signaling tc qdisc add dev eth1 root netem delay 400ms 200ms

After 5 or 6 successful registrations,  the above issue starts.  Once that issue starts the only way to correct it is to remove the latency.  Any ideas what might be causing this,  does anyone else experience the same issues? I found this old issue logged that seems be a similar issue: https://github.com/Metaswitch/sprout/issues/144.

Thanks,
Trey
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160913/6df756bb/attachment.html>

From trey.ormsbee at interoptechnologies.com  Tue Sep 13 14:07:27 2016
From: trey.ormsbee at interoptechnologies.com (Trey Ormsbee)
Date: Tue, 13 Sep 2016 13:07:27 -0500
Subject: [Project Clearwater] latency to homestead causes curl
 connection	fatal errors
In-Reply-To: <CY4PR02MB2616FA741B53AA462B65C620E3FE0@CY4PR02MB2616.namprd02.prod.outlook.com>
References: <1473782072.6400.111.camel@interoptechnologies.com>
	<CY4PR02MB2616FA741B53AA462B65C620E3FE0@CY4PR02MB2616.namprd02.prod.outlook.com>
Message-ID: <1473790047.6400.112.camel@interoptechnologies.com>

No it does not work on previous releases,  but that answers the question
perfectly.  I'll have to adjust my test to ensure latency is not added
the homestead connections.

Thanks,
Trey

-----Original Message-----From: Graeme Robertson (projectclearwater.org)
<graeme at projectclearwater.org>
Reply-to: "clearwater at lists.projectclearwater.org"
<clearwater at lists.projectclearwater.org>
To: clearwater at lists.projectclearwater.org
<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] latency to homestead causes curl
connection	fatal errors
Date: Tue, 13 Sep 2016 12:59:31 -0400

Hi Trey,

 

Have you seen this working on previous releases? We expect to be able to
create HTTP connections from Sprout to Homestead very quickly (i.e. in
under 10ms) so our HTTP connection timeout is set to 50ms (see here),
and I think that?s the timeout you?re hitting. I suspect everything
initially continues working after you add the latency because Sprout is
using connections to Homestead that have already been set up, and you
only run into the timeout problem when Sprout needs to set up new
connections.

 

Thanks,

Graeme

 

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
On Behalf Of Trey Ormsbee
Sent: 13 September 2016 16:55
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] latency to homestead causes curl
connection fatal errors


 

I am trying to simulate a higher latency connection to sprout and I have
inserted latency ranging from 50ms-1000ms on the sprout signaling
adapter.  I have an issue that occurs fairly quickly with homestead
queries (even with as low as 50ms) here is a snippet of the log:

12-09-2016 21:14:12.125 UTC Debug connection_pool.h:225: Request for
connection to IP: xxx.xxx.xxx.51, port: 8888
12-09-2016 21:14:12.125 UTC Debug connection_pool.h:244: No existing
connection in pool, create one
12-09-2016 21:14:12.126 UTC Debug http_connection_pool.cpp:56: Allocated
CURL handle 0x7fd0d000ec00
12-09-2016 21:14:12.126 UTC Debug connection_pool.h:246: Created new
connection 0x7fd0d0017a80
12-09-2016 21:14:12.126 UTC Debug httpclient.cpp:466: Sending HTTP
request : http://homestead.example.com:8888/impi/%2B15557775555%
40example.com/av?impu=sip%3A%2B15557775555%40example.com (trying
xxx.xxx.xxx.51)
12-09-2016 21:14:12.177 UTC Error httpclient.cpp:487:
http://homestead.example.com:8888/impi/%2B15557775555%
40example.com/av?impu=sip%3A%2B15557775555%40example.com failed at
server xxx.xxx.xxx.51 : Timeout was reached (28) : fatal
12-09-2016 21:14:12.177 UTC Debug baseresolver.cpp:498: Add
xxx.xxx.xxx.51:8888 transport 6 to blacklist for 30 seconds, graylist
for 30 seconds
12-09-2016 21:14:12.177 UTC Debug connection_pool.h:261: Release
connection to IP: xxx.xxx.xxx.51, port: 8888 and destroy
12-09-2016 21:14:12.177 UTC Debug connection_pool.h:225: Request for
connection to IP: xxx.xxx.xxx.52, port: 8888
12-09-2016 21:14:12.177 UTC Debug connection_pool.h:244: No existing
connection in pool, create one
12-09-2016 21:14:12.178 UTC Debug http_connection_pool.cpp:56: Allocated
CURL handle 0x7fd0d000ec00
12-09-2016 21:14:12.178 UTC Debug connection_pool.h:246: Created new
connection 0x7fd0d0019ef0
12-09-2016 21:14:12.178 UTC Debug httpclient.cpp:466: Sending HTTP
request : http://homestead.example.com:8888/impi/%2B15557775555%
40example.com/av?impu=sip%3A%2B15557775555%40example.com (trying
xxx.xxx.xxx.52)
12-09-2016 21:14:12.229 UTC Error httpclient.cpp:487:
http://homestead.example.com:8888/impi/%2B15557775555%
40example.com/av?impu=sip%3A%2B15557775555%40example.com failed at
server xxx.xxx.xxx.52 : Timeout was reached (28) : fatal
12-09-2016 21:14:12.229 UTC Debug baseresolver.cpp:498: Add
xxx.xxx.xxx.52:8888 transport 6 to blacklist for 30 seconds, graylist
for 30 seconds
12-09-2016 21:14:12.229 UTC Debug connection_pool.h:261: Release
connection to IP: xxx.xxx.xxx.52, port: 8888 and destroy
12-09-2016 21:14:12.229 UTC Debug communicationmonitor.cpp:82: Checking
communication changes - successful attempts 0, failures 1
12-09-2016 21:14:12.229 UTC Error httpclient.cpp:623: cURL failure with
cURL error code 28 (see man 3 libcurl-errors) and HTTP error code 500
12-09-2016 21:14:12.229 UTC Error hssconnection.cpp:149: Failed to get
Authentication Vector for +15557775555 at example.com
12-09-2016 21:14:12.229 UTC Debug authentication.cpp:638: Failed to get
Authentication vector

This is happening on release-105 and is fairly easily recreate on a busy
system by adding latency with tc. I used:

ip netns exec signaling tc qdisc add dev eth1 root netem delay 400ms
200ms

After 5 or 6 successful registrations,  the above issue starts.  Once
that issue starts the only way to correct it is to remove the latency.
Any ideas what might be causing this,  does anyone else experience the
same issues? I found this old issue logged that seems be a similar
issue: https://github.com/Metaswitch/sprout/issues/144.

Thanks,
Trey 



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160913/d3819002/attachment.html>

From surender.s at hcl.com  Wed Sep 14 05:54:59 2016
From: surender.s at hcl.com (Surender Singh)
Date: Wed, 14 Sep 2016 09:54:59 +0000
Subject: [Project Clearwater] Getting 503 service unavailable_All
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E0D702@NDA-HCLT-MBS02.hclt.corp.hcl.in>

Hi Graeme,

Thanks for support ..

Now this  issue got resolved post restart the sprout services and earlier also restart the services.

    Sprout - sudo service sprout quiesce
    Bono - sudo service bono quiesce
    Homestead - sudo service homestead stop && sudo service homestead-prov stop
    Homer - sudo service homer stop
    Ralf -sudo service ralf stop
    Ellis - sudo service ellis stop
    Memento - sudo service memento stop

Regards
Surender Singh
8826292018




-
Date: Tue, 13 Sep 2016 14:00:07 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Getting 503 service unavailable_All
	in	one	nodes
Message-ID:
	<CY4PR02MB2616A3C0F7E77218D95363EAE3FE0 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="us-ascii"

Hi Surender,

Have you restarted the Sprout process since you changed host_domain? What is --domain set to if you run ps -eaf | grep /usr/share/clearwater/bin/sprout? After changing configuration all the Clearwater processes need to be restarted, and you can do that by running sudo monit restart all. My guess is that Sprout is receiving a SIP message to xxx at ims.hcl.com<mailto:xxx at ims.hcl.com>, and it doesn't yet recognise ims.hcl.com as its home domain, so its trying to figure out where to proxy the message onto rather than processing it locally.

You shouldn't need any DNS entries to run an AIO node, but for more information on Clearwater DNS and setting up your own DNS, see http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 13 September 2016 07:48
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Getting 503 service unavailable_All in one nodes


Hi,



Can anyone support me to resolve  the below problem .



I am using AIO image at  Vmware Esxi platform .Post deployement  the image i was able to registered the client using Zoiper and call was also conneting .



I am using this AIO for Demo purpose only.





But post change the Host_domain form example.com to ims.hcl.com , i unable to get registered  client .



As per logs error found var/log/sprout/sprout_curent.txt



13-09-2016 12:04:30.118 UTC Error dnscachedresolver.cpp:607: Failed to retrieve record for ims.hcl.com: Domain name not found

13-09-2016 12:05:02.223 UTC Error dnscachedresolver.cpp:607: Failed to retrieve record for _sip._tcp.ims.hcl.com: Domain name not found





I also added the 1000 more number using below command and this refleting in Ellis potal.



sudo usr/src/metaswitch/ellis/tools/create_numbers.py --start 6505550000 --count 1000 --realm ims.hcl.com



Now Please suggest me where i need to make DNS entry and share configuration for aio solution



Regards

Surender


::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents (with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification, distribution and / or publication of this message without the prior written consent of authorized representative of HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160913/b7cbfab0/attachment-0001.html>

------------------------------

Message: 2
Date: Tue, 13 Sep 2016 14:20:49 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] iFC of Sip-Stress
Message-ID:
	<CY4PR02MB2616092C2FA35AB5A681DFA9E3FE0 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="utf-8"

Hi Morris,

It sounds like you want to provision lots of subscribers with specific Initial Filter Criteria (pointing at your Application Server). We have a process for bulk provisioning<https://github.com/Metaswitch/crest/blob/dev/docs/Bulk-Provisioning%20Numbers.md>, but the Initial Filter Criteria are not configurable ? the bulk_create.py<https://github.com/Metaswitch/crest/blob/320e224e5a30df30cd53ed2d660d817cdf3e2ff4/src/metaswitch/crest/tools/bulk_create.py#L102> script just calls generate_ifcs<https://github.com/Metaswitch/python-common/blob/dev/metaswitch/common/ifcs.py#L36>, which contains a single hardcoded rule for the MMTel on INVITEs. However, you could quite easily tweak the bulk_create.py script  on your node before running it to set initial_filter_xml to whatever you want. Let me know if that doesn?t make sense!

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of yan morris
Sent: 12 September 2016 12:51
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] iFC of Sip-Stress

Hi ,

I want to use Sip-Stress to test my Application Server,

In order to let all test calls go through my Application server ,

In small number test , I can use Web UI of Ellis to make it ,

but if there are many accounts , what should I do to modify the accounts settings , or

where I can get some concerned information about this issue?


Thank you

Morris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160913/c7b8c816/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 41, Issue 20
******************************************



From surender.s at hcl.com  Wed Sep 14 07:55:59 2016
From: surender.s at hcl.com (Surender Singh)
Date: Wed, 14 Sep 2016 11:55:59 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E0DF51@NDA-HCLT-MBS02.hclt.corp.hcl.in>

Hi team,

I deployed the aio solution in my Vmware Exsi  and made the connectivity with Mitel IP-PBX .

As per doc and mailing list  I made the Following entry .


1)      As per mailing IBCF and BGCF functionality are provided by BONO

2)      I create the one user-settings file (already not created ) under the path =/etc/clearwater/user_settings  with entry like trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>" . Here 10.112.87.177 is my PBX ip.

3)      After that I make the entry for bfcg.json and enum.json under the path =/etc/clearwater/sample. And then run the command sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_bgcf_json and enum_json. Post this command both files created the path under /etc/Clearwater. Files contents are


-----------BGCF.JSON------------------


   {
      "routes" : [
        {   "name" : "Clearwater AIO_TO_PBX",
          "domain" : "10.112.87.177",
          "route" : ["sip:example.com:5058","sip:10.112.87.177:5060"]
        }
    ]
  }

------ENUM.JSON---------------------

{
    "number_blocks" : [
        {
           "name" : "Internal numbers",
           "prefix" : "650555",
           "regex" : "!(^.*$)!sip:\\1 at 10.112.87.250!"
        },

        {
            "name" : "External numbers",
            "prefix" : "",
            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"
        }
    ]
}



My IP Scheme are:

10.112.87.177 : PBX IP address
10.112.87.250: IMS IP address and Domain is example.com

10.112.123.187 : my System ip where Zoper client insall


My call Flow is:-

SIP Client A(Registered in PBX) calls to -> SIP Client B (Registered in ims with example.com domain )

When I make the call from A to B then call coming in IMS network but getting error 'Not found 404' because B number registered with xxxxxx at exmaple.com .


Please find the attached wireshark traces and BONO, Sprout logs



Note : I also not able to call from from B to A Also

Regards
Surender Singh





::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160914/990d232d/attachment.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sprout_20160914T160000Z.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160914/990d232d/attachment.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: bono_20160914T160000Z.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160914/990d232d/attachment-0001.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: log_current.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160914/990d232d/attachment-0002.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sprout_20160914T160000Z.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160914/990d232d/attachment-0003.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: bono_20160914T160000Z.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160914/990d232d/attachment-0004.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: log_current.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160914/990d232d/attachment-0005.txt>

From skhalup at virtuozzo.com  Wed Sep 14 08:36:37 2016
From: skhalup at virtuozzo.com (Stanislav Khalup)
Date: Wed, 14 Sep 2016 12:36:37 +0000
Subject: [Project Clearwater] Stress test results
Message-ID: <HE1PR0802MB25075235021CB0E05C45CBFFA1F10@HE1PR0802MB2507.eurprd08.prod.outlook.com>

Hello all,

I believe I am not the only one who is conducting stress testing. Could you please share you results: I am particularly interested in amount of concurrent calls that could be achieved with the smallest deployment.

BR,
Stanislav Khalup

From: Stanislav Khalup
Sent: Friday, September 9, 2016 6:09 PM
To: clearwater at lists.projectclearwater.org
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com>
Subject: Stress test results

Hello team,

At last after updating to the latest release, limiting all VMs to 1CPU/2GB Ram and setting all settings to default we managed to stop IMS cluster from crashes. The thing is we see the same results while performing stress tests. After some time the system closes socket regardless of ongoing calls. Is this kind of behavior normal for Clearwater IMS. I attach the full sipp logs and screens archive. Please look at graphic for network usage that shows downfall of traffic when sockets are being closed: https://www.dropbox.com/s/q2l3nsaxyu6sf32/sipp_stress.tar.gz?dl=0

Another question that is bothering me is how to interpret the results of stress testing? How can you evaluate that your deployment is hitting the limit? All the orchestration demos out there show that new nodes are added when CPU utilization reaches some 30% but in our test we could never see such loads even with 1-1 Bono and Sprout. Judging from the bucket algorithm description we assumed that refused connections metric should indicate that the system is at the limit but this snmp statistics is always zero in all our tests. So, how are you processing the results of stress test? What metrics are you looking at?

BR,
Stanislav Khalup
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160914/ed032041/attachment.html>

From Andrew.Edmonds at metaswitch.com  Wed Sep 14 09:05:26 2016
From: Andrew.Edmonds at metaswitch.com (Andrew Edmonds)
Date: Wed, 14 Sep 2016 13:05:26 +0000
Subject: [Project Clearwater] Missingno release note
Message-ID: <BLUPR02MB437777BC8C580EF06442DDBE5F10@BLUPR02MB437.namprd02.prod.outlook.com>

The release for Project Clearwater sprint "Missingno" has been cut. The code for this release is tagged as release-106 in github.

In this release we have been working towards increasing the robustness of our IPv6 solution.

This release includes the following bug fixes:


*         Privacy header options delimiter is incorrect (https://github.com/Metaswitch/sprout/issues/1514)

*         Log spam in non-GR deployment (https://github.com/Metaswitch/sprout/issues/1503)

*         Perimeta Rejects Notify from Sprout with an Apparent Invalid AOR  (https://github.com/Metaswitch/sprout/issues/1274)

*         NP lookup always performed regardless of state of override_npdi option (https://github.com/Metaswitch/sprout/issues/1096)

*         Homestead appears to be checking the wrong returncode in SAAs (https://github.com/Metaswitch/homestead/issues/359)

*         GR (split-brain) callee's state is set to "unregistered" in HSS after cache failure (https://github.com/Metaswitch/homestead/issues/345)

*         Homestead doesn't handle Experimental-Result-Codes on MAAs/SAAs (https://github.com/Metaswitch/homestead/issues/306)

*         URLs not formatted correctly for IPv6 (https://github.com/Metaswitch/chronos/issues/282)

*         Can't change the logging level in Homestead-prov/Homer (https://github.com/Metaswitch/crest/issues/290)

*         Crest doesn't use user_settings for its log level (https://github.com/Metaswitch/crest/issues/272)

*         Repeat public ID queries causes overload and request rejections (https://github.com/Metaswitch/crest/issues/256)

*         sync_database.py fails due to missing pycurl dependancy (https://github.com/Metaswitch/ellis/issues/188)

*         SUBSCRIBE live tests occasionally fail erroneously due to inaccurate CSeq checking (https://github.com/Metaswitch/clearwater-live-test/issues/130)

*         Astaire crashes in the FV tests (https://github.com/Metaswitch/astaire/issues/70)

*         clearwater-etcd doesn't recover after running out of disk space (https://github.com/Metaswitch/clearwater-etcd/issues/293)

*         etcd is running but contents of pid file is wrong (https://github.com/Metaswitch/clearwater-etcd/issues/262)

To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html. If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.

Andrew
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160914/25322d33/attachment.html>

From graeme at projectclearwater.org  Wed Sep 14 17:27:03 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Wed, 14 Sep 2016 21:27:03 +0000
Subject: [Project Clearwater] Stress test results
In-Reply-To: <HE1PR0802MB25075235021CB0E05C45CBFFA1F10@HE1PR0802MB2507.eurprd08.prod.outlook.com>
References: <HE1PR0802MB25075235021CB0E05C45CBFFA1F10@HE1PR0802MB2507.eurprd08.prod.outlook.com>
Message-ID: <CY4PR02MB2616FC2C563068482DB0125DE3F10@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Stanislav,

Apologies for the delayed reply. Whilst internally we do ensure that performance doesn't drop between Project Clearwater releases, we don't provide performance numbers for Project Clearwater, and we don't guarantee any level of performance. However, no, that doesn't sound like normal behaviour for Project Clearwater - when we're running high call loads through our system we normally drive CPU usage up to about 60%, and that's how we tend to evaluate stress tests. It sounds like Sprout has become overloaded a long time before reaching 60% CPU, and has been restarted by our monitoring tool which deemed it unresponsive. This suggests that there is still something different about your environment, and Sprout's throttling mechanism is not tuned correctly. You may want to try tweaking some of the throttling options, such as reducing the value of max_tokens. See http://clearwater.readthedocs.io/en/stable/Clearwater_Configuration_Options_Reference.html for more information.

Metaswitch Networks do produce a hardened, supported version of Project Clearwater called Clearwater Core, and we do guarantee and provide performance numbers for each release of Clearwater Core.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Stanislav Khalup
Sent: 14 September 2016 13:37
To: clearwater at lists.projectclearwater.org
Cc: Denis Plotnikov
Subject: Re: [Project Clearwater] Stress test results

Hello all,

I believe I am not the only one who is conducting stress testing. Could you please share you results: I am particularly interested in amount of concurrent calls that could be achieved with the smallest deployment.

BR,
Stanislav Khalup

From: Stanislav Khalup
Sent: Friday, September 9, 2016 6:09 PM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Denis Plotnikov <dplotnikov at virtuozzo.com<mailto:dplotnikov at virtuozzo.com>>
Subject: Stress test results

Hello team,

At last after updating to the latest release, limiting all VMs to 1CPU/2GB Ram and setting all settings to default we managed to stop IMS cluster from crashes. The thing is we see the same results while performing stress tests. After some time the system closes socket regardless of ongoing calls. Is this kind of behavior normal for Clearwater IMS. I attach the full sipp logs and screens archive. Please look at graphic for network usage that shows downfall of traffic when sockets are being closed: https://www.dropbox.com/s/q2l3nsaxyu6sf32/sipp_stress.tar.gz?dl=0

Another question that is bothering me is how to interpret the results of stress testing? How can you evaluate that your deployment is hitting the limit? All the orchestration demos out there show that new nodes are added when CPU utilization reaches some 30% but in our test we could never see such loads even with 1-1 Bono and Sprout. Judging from the bucket algorithm description we assumed that refused connections metric should indicate that the system is at the limit but this snmp statistics is always zero in all our tests. So, how are you processing the results of stress test? What metrics are you looking at?

BR,
Stanislav Khalup
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160914/7229a125/attachment.html>

From surender.s at hcl.com  Thu Sep 15 08:07:27 2016
From: surender.s at hcl.com (Surender Singh)
Date: Thu, 15 Sep 2016 12:07:27 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E0F36D@NDA-HCLT-MBS02.hclt.corp.hcl.in>

Hi Team,

Kindly help

Please share the procedure to connect the SIP trunk with PBX.

I already tried many combination but  in/out calls failing in IMS.

Regards
Surender Singh

From: Surender Singh
Sent: 14 September 2016 17:26
To: clearwater at lists.projectclearwater.org
Subject: SIP Trunking With PBX_aio

Hi team,

I deployed the aio solution in my Vmware Exsi  and made the connectivity with Mitel IP-PBX .

As per doc and mailing list  I made the Following entry .


1)      As per mailing IBCF and BGCF functionality are provided by BONO

2)      I create the one user-settings file (already not created ) under the path =/etc/clearwater/user_settings  with entry like trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>" . Here 10.112.87.177 is my PBX ip.

3)      After that I make the entry for bfcg.json and enum.json under the path =/etc/clearwater/sample. And then run the command sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_bgcf_json and enum_json. Post this command both files created the path under /etc/Clearwater. Files contents are


-----------BGCF.JSON------------------


   {
      "routes" : [
        {   "name" : "Clearwater AIO_TO_PBX",
          "domain" : "10.112.87.177",
          "route" : ["sip:example.com:5058","sip:10.112.87.177:5060"]
        }
    ]
  }

------ENUM.JSON---------------------

{
    "number_blocks" : [
        {
           "name" : "Internal numbers",
           "prefix" : "650555",
           "regex" : "!(^.*$)!sip:\\1 at 10.112.87.250!"
        },

        {
            "name" : "External numbers",
            "prefix" : "",
            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"
        }
    ]
}



My IP Scheme are:

10.112.87.177 : PBX IP address
10.112.87.250: IMS IP address and Domain is example.com

10.112.123.187 : my System ip where Zoper client insall


My call Flow is:-

SIP Client A(Registered in PBX) calls to -> SIP Client B (Registered in ims with example.com domain )

When I make the call from A to B then call coming in IMS network but getting error 'Not found 404' because B number registered with xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com> .


Please find the attached wireshark traces and BONO, Sprout logs



Note : I also not able to call from from B to A Also

Regards
Surender Singh






::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160915/3902eaa9/attachment.html>

From surender.s at hcl.com  Thu Sep 15 08:24:31 2016
From: surender.s at hcl.com (Surender Singh)
Date: Thu, 15 Sep 2016 12:24:31 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E0F389@NDA-HCLT-MBS02.hclt.corp.hcl.in>


Hi Team,

Kindly help

Please share the procedure to connect the SIP trunk with PBX.

I already tried many combination but  in/out calls failing in IMS.

When I also checking IBCF functionality are enabled or not then showing below o/p

[cw-aio]ubuntu at cw-aio:~$ sudo /usr/share/clearwater/bin/bono
/usr/share/clearwater/bin/bono: error while loading shared libraries: libmemcached.so.11: cannot open shared object file: No such file or directory
[cw-aio]ubuntu at cw-aio:~$

Regards
Surender Singh

From: Surender Singh
Sent: 14 September 2016 17:26
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: SIP Trunking With PBX_aio

Hi team,

I deployed the aio solution in my Vmware Exsi  and made the connectivity with Mitel IP-PBX .

As per doc and mailing list  I made the Following entry .


1)      As per mailing IBCF and BGCF functionality are provided by BONO

2)      I create the one user-settings file (already not created ) under the path =/etc/clearwater/user_settings  with entry like trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>" . Here 10.112.87.177 is my PBX ip.

3)      After that I make the entry for bfcg.json and enum.json under the path =/etc/clearwater/sample. And then run the command sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_bgcf_json and enum_json. Post this command both files created the path under /etc/Clearwater. Files contents are


-----------BGCF.JSON------------------


   {
      "routes" : [
        {   "name" : "Clearwater AIO_TO_PBX",
          "domain" : "10.112.87.177",
          "route" : ["sip:example.com:5058","sip:10.112.87.177:5060"]
        }
    ]
  }

------ENUM.JSON---------------------

{
    "number_blocks" : [
        {
           "name" : "Internal numbers",
           "prefix" : "650555",
           "regex" : "!(^.*$)!sip:\\1 at 10.112.87.250!"
        },

        {
            "name" : "External numbers",
            "prefix" : "",
            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"
        }
    ]
}



My IP Scheme are:

10.112.87.177 : PBX IP address
10.112.87.250: IMS IP address and Domain is example.com

10.112.123.187 : my System ip where Zoper client insall


My call Flow is:-

SIP Client A(Registered in PBX) calls to -> SIP Client B (Registered in ims with example.com domain )

When I make the call from A to B then call coming in IMS network but getting error 'Not found 404' because B number registered with xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com> .


Please find the attached wireshark traces and BONO, Sprout logs



Note : I also not able to call from from B to A Also

Regards
Surender Singh






::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160915/b228a294/attachment.html>

From graeme at projectclearwater.org  Thu Sep 15 10:44:05 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Thu, 15 Sep 2016 14:44:05 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
In-Reply-To: <16AE9ED83DBA5B4D85B058EAF39C918107E0F389@NDA-HCLT-MBS02.hclt.corp.hcl.in>
References: <16AE9ED83DBA5B4D85B058EAF39C918107E0F389@NDA-HCLT-MBS02.hclt.corp.hcl.in>
Message-ID: <CY4PR02MB261679B6BC152EC59EC14C67E3F00@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Surender,

I noticed there are lots of instances of "Invalid ENUM response: sip:@example.com" so it looks as though something is misconfigured, but it's not immediately obvious what. I think it would be useful to turn on debug logging for Sprout in order to dig into this issue further. In order to do this, add the line 'log_level=5' to /etc/clearwater/user_settings (creating it if it doesn't exist) and restart Sprout (by running sudo service sprout restart). This will cause far more detailed logs to be written to the Sprout log files.

Incidentally, I thought (from an earlier thread) that you'd changed your home domain from example.com to ims.hcl.com - is that on a different deployment? I just wanted to make sure there was no confusion here.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 15 September 2016 13:25
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio


Hi Team,

Kindly help

Please share the procedure to connect the SIP trunk with PBX.

I already tried many combination but  in/out calls failing in IMS.

When I also checking IBCF functionality are enabled or not then showing below o/p

[cw-aio]ubuntu at cw-aio:~$ sudo /usr/share/clearwater/bin/bono
/usr/share/clearwater/bin/bono: error while loading shared libraries: libmemcached.so.11: cannot open shared object file: No such file or directory
[cw-aio]ubuntu at cw-aio:~$

Regards
Surender Singh

From: Surender Singh
Sent: 14 September 2016 17:26
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: SIP Trunking With PBX_aio

Hi team,

I deployed the aio solution in my Vmware Exsi  and made the connectivity with Mitel IP-PBX .

As per doc and mailing list  I made the Following entry .


1)      As per mailing IBCF and BGCF functionality are provided by BONO

2)      I create the one user-settings file (already not created ) under the path =/etc/clearwater/user_settings  with entry like trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>" . Here 10.112.87.177 is my PBX ip.

3)      After that I make the entry for bfcg.json and enum.json under the path =/etc/clearwater/sample. And then run the command sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_bgcf_json and enum_json. Post this command both files created the path under /etc/Clearwater. Files contents are


-----------BGCF.JSON------------------


   {
      "routes" : [
        {   "name" : "Clearwater AIO_TO_PBX",
          "domain" : "10.112.87.177",
          "route" : ["sip:example.com:5058","sip:10.112.87.177:5060"]
        }
    ]
  }

------ENUM.JSON---------------------

{
    "number_blocks" : [
        {
           "name" : "Internal numbers",
           "prefix" : "650555",
           "regex" : "!(^.*$)!sip:\\1 at 10.112.87.250!"
        },

        {
            "name" : "External numbers",
            "prefix" : "",
            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"
        }
    ]
}



My IP Scheme are:

10.112.87.177 : PBX IP address
10.112.87.250: IMS IP address and Domain is example.com

10.112.123.187 : my System ip where Zoper client insall


My call Flow is:-

SIP Client A(Registered in PBX) calls to -> SIP Client B (Registered in ims with example.com domain )

When I make the call from A to B then call coming in IMS network but getting error 'Not found 404' because B number registered with xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com> .


Please find the attached wireshark traces and BONO, Sprout logs



Note : I also not able to call from from B to A Also

Regards
Surender Singh






::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160915/ed1ee91d/attachment.html>

From surender.s at hcl.com  Fri Sep 16 02:18:14 2016
From: surender.s at hcl.com (Surender Singh)
Date: Fri, 16 Sep 2016 06:18:14 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E0F453@NDA-HCLT-MBS02.hclt.corp.hcl.in>

Hi Graeme,


I am not able to reply through mailing list as I already created the account.

I requesting you for quick response...


ims.hcl.com is another deployment.


I putted the said line in user_setting file and  restart the sprout but showing Log level set to 2 ,fine below logs and also find the attached sprout logs .


[cw-aio]root at cw-aio:~# sudo service sprout restart
* Restarting Sprout SIP Router sprout                                          Log directory set to /var/log/sprout
Log level set to 2
16-09-2016 11:10:01.436 UTC Status utils.cpp:494: Switching to daemon mode
                                                                         [ OK ]

-----user_settings file is under the path etc/Clearwater is----

trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>"
enum_file="enum.json"
log_level=5




I have one more query, what is meaning of config file under the path /etc/Clearwater

-------config  file------------------
if [ -f /etc/clearwater/shared_config ]
then
  . /etc/clearwater/shared_config
fi

. /etc/clearwater/local_config

if [ -f /etc/clearwater/user_settings ]
then
  . /etc/clearwater/user_settings
fi


Regards
Surender Singh

-------------------------------------------------------------

Hi Surender,

I noticed there are lots of instances of "Invalid ENUM response:
sip:@example.com" so it looks as though something is misconfigured, but it's
not immediately obvious what. I think it would be useful to turn on debug
logging for Sprout in order to dig into this issue further. In order to do
this, add the line 'log_level=5' to /etc/clearwater/user_settings (creating it
if it doesn't exist) and restart Sprout (by running sudo service sprout
restart). This will cause far more detailed logs to be written to the Sprout
log files.

Incidentally, I thought (from an earlier thread) that you'd changed your home
domain from example.com to ims.hcl.com - is that on a different deployment? I
just wanted to make sure there was no confusion here.

Thanks,
Graeme


From: Surender Singh
Sent: 15 September 2016 17:54
To: 'clearwater at lists.projectclearwater.org' <clearwater at lists.projectclearwater.org>
Subject: RE: SIP Trunking With PBX_aio


Hi Team,

Kindly help

Please share the procedure to connect the SIP trunk with PBX.

I already tried many combination but  in/out calls failing in IMS.

When I also checking IBCF functionality are enabled or not then showing below o/p

[cw-aio]ubuntu at cw-aio:~$ sudo /usr/share/clearwater/bin/bono
/usr/share/clearwater/bin/bono: error while loading shared libraries: libmemcached.so.11: cannot open shared object file: No such file or directory
[cw-aio]ubuntu at cw-aio:~$

Regards
Surender Singh

From: Surender Singh
Sent: 14 September 2016 17:26
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: SIP Trunking With PBX_aio

Hi team,

I deployed the aio solution in my Vmware Exsi  and made the connectivity with Mitel IP-PBX .

As per doc and mailing list  I made the Following entry .


1)      As per mailing IBCF and BGCF functionality are provided by BONO

2)      I create the one user-settings file (already not created ) under the path =/etc/clearwater/user_settings  with entry like trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>" . Here 10.112.87.177 is my PBX ip.

3)      After that I make the entry for bfcg.json and enum.json under the path =/etc/clearwater/sample. And then run the command sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_bgcf_json and enum_json. Post this command both files created the path under /etc/Clearwater. Files contents are


-----------BGCF.JSON------------------


   {
      "routes" : [
        {   "name" : "Clearwater AIO_TO_PBX",
          "domain" : "10.112.87.177",
          "route" : ["sip:example.com:5058","sip:10.112.87.177:5060"]
        }
    ]
  }

------ENUM.JSON---------------------

{
    "number_blocks" : [
        {
           "name" : "Internal numbers",
           "prefix" : "650555",
           "regex" : "!(^.*$)!sip:\\1 at 10.112.87.250!"
        },

        {
            "name" : "External numbers",
            "prefix" : "",
            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"
        }
    ]
}



My IP Scheme are:

10.112.87.177 : PBX IP address
10.112.87.250: IMS IP address and Domain is example.com

10.112.123.187 : my System ip where Zoper client insall


My call Flow is:-

SIP Client A(Registered in PBX) calls to -> SIP Client B (Registered in ims with example.com domain )

When I make the call from A to B then call coming in IMS network but getting error 'Not found 404' because B number registered with xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com> .


Please find the attached wireshark traces and BONO, Sprout logs



Note : I also not able to call from from B to A Also

Regards
Surender Singh






::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/eb3030a1/attachment.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sprout_current.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/eb3030a1/attachment.txt>

From chessmancaryl at gmail.com  Fri Sep 16 04:02:07 2016
From: chessmancaryl at gmail.com (chess man)
Date: Fri, 16 Sep 2016 09:02:07 +0100
Subject: [Project Clearwater] Mininet
Message-ID: <CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw@mail.gmail.com>

Hi everybody,

I would like to know if it is possible to install clearwater images on the
Virtual machines of mininet ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/978b47a5/attachment.html>

From michaelkatsoulis88 at gmail.com  Fri Sep 16 05:16:42 2016
From: michaelkatsoulis88 at gmail.com (=?UTF-8?B?zpzOuc+HzqzOu863z4IgzprOsc+Ez4POv8+NzrvOt8+C?=)
Date: Fri, 16 Sep 2016 12:16:42 +0300
Subject: [Project Clearwater]  Performance limit measurement
Message-ID: <CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA@mail.gmail.com>

Hi all,

we are running Stress Tests against our Clearwater Deployment using Sip
Stress node.
We have noticed that the results are not consistent as the number of
successfull calls changes during repetitions of the same test scenario.

We have tried to increase the values of max_tokens , init_token_rate,
min_token_rate and
target_latency_us but we did not observe any difference.

What is the proposed way to discover the deployment's limit on how many
requests per second can
be served?

Thanks in advance,
Michael Katsoulis
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/27a77df2/attachment.html>

From graeme at projectclearwater.org  Fri Sep 16 08:20:26 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Fri, 16 Sep 2016 12:20:26 +0000
Subject: [Project Clearwater] Mininet
In-Reply-To: <CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw@mail.gmail.com>
References: <CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw@mail.gmail.com>
Message-ID: <CY4PR02MB2616A144FD60BFC5F7796B1CE3F30@CY4PR02MB2616.namprd02.prod.outlook.com>

Hello,

There is no reason why it shouldn?t just work, but as far as I know nobody has ever tried to install Project Clearwater on Mininet. If our AIO OVA doesn?t just work then you may have to follow our manual install instructions<http://clearwater.readthedocs.io/en/stable/Manual_Install.html>.

It would be interesting to hear how you get on!

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of chess man
Sent: 16 September 2016 09:02
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Mininet

Hi everybody,
I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/782ad823/attachment.html>

From graeme at projectclearwater.org  Fri Sep 16 08:33:07 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Fri, 16 Sep 2016 12:33:07 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
In-Reply-To: <16AE9ED83DBA5B4D85B058EAF39C918107E0F453@NDA-HCLT-MBS02.hclt.corp.hcl.in>
References: <16AE9ED83DBA5B4D85B058EAF39C918107E0F453@NDA-HCLT-MBS02.hclt.corp.hcl.in>
Message-ID: <CY4PR02MB2616B3B7962F78B040BAAD8BE3F30@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Surender,

I'm not sure what you mean about not being able to reply through the mailing list - I think this email went to the mailing list :).

That is odd. What is log_level set to if you run ' log_level="";. /etc/clearwater/config; echo $log_level'? And what is it set to if you run 'log_level="";. /etc/clearwater/user_settings; echo $log_level'?

The /etc/clearwater/config file is the config file that all of our processes actually read from. As you can see, this file pulls in all of our other config files (shared_config, local_config and user_settings).

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 16 September 2016 07:18
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio

Hi Graeme,


I am not able to reply through mailing list as I already created the account.

I requesting you for quick response...


ims.hcl.com is another deployment.


I putted the said line in user_setting file and  restart the sprout but showing Log level set to 2 ,fine below logs and also find the attached sprout logs .


[cw-aio]root at cw-aio:~# sudo service sprout restart
* Restarting Sprout SIP Router sprout                                          Log directory set to /var/log/sprout
Log level set to 2
16-09-2016 11:10:01.436 UTC Status utils.cpp:494: Switching to daemon mode
                                                                         [ OK ]

-----user_settings file is under the path etc/Clearwater is----

trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>"
enum_file="enum.json"
log_level=5




I have one more query, what is meaning of config file under the path /etc/Clearwater

-------config  file------------------
if [ -f /etc/clearwater/shared_config ]
then
  . /etc/clearwater/shared_config
fi

. /etc/clearwater/local_config

if [ -f /etc/clearwater/user_settings ]
then
  . /etc/clearwater/user_settings
fi


Regards
Surender Singh

-------------------------------------------------------------

Hi Surender,

I noticed there are lots of instances of "Invalid ENUM response:
sip:@example.com" so it looks as though something is misconfigured, but it's
not immediately obvious what. I think it would be useful to turn on debug
logging for Sprout in order to dig into this issue further. In order to do
this, add the line 'log_level=5' to /etc/clearwater/user_settings (creating it
if it doesn't exist) and restart Sprout (by running sudo service sprout
restart). This will cause far more detailed logs to be written to the Sprout
log files.

Incidentally, I thought (from an earlier thread) that you'd changed your home
domain from example.com to ims.hcl.com - is that on a different deployment? I
just wanted to make sure there was no confusion here.

Thanks,
Graeme


From: Surender Singh
Sent: 15 September 2016 17:54
To: 'clearwater at lists.projectclearwater.org' <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>
Subject: RE: SIP Trunking With PBX_aio


Hi Team,

Kindly help

Please share the procedure to connect the SIP trunk with PBX.

I already tried many combination but  in/out calls failing in IMS.

When I also checking IBCF functionality are enabled or not then showing below o/p

[cw-aio]ubuntu at cw-aio:~$ sudo /usr/share/clearwater/bin/bono
/usr/share/clearwater/bin/bono: error while loading shared libraries: libmemcached.so.11: cannot open shared object file: No such file or directory
[cw-aio]ubuntu at cw-aio:~$

Regards
Surender Singh

From: Surender Singh
Sent: 14 September 2016 17:26
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: SIP Trunking With PBX_aio

Hi team,

I deployed the aio solution in my Vmware Exsi  and made the connectivity with Mitel IP-PBX .

As per doc and mailing list  I made the Following entry .


1)      As per mailing IBCF and BGCF functionality are provided by BONO

2)      I create the one user-settings file (already not created ) under the path =/etc/clearwater/user_settings  with entry like trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>" . Here 10.112.87.177 is my PBX ip.

3)      After that I make the entry for bfcg.json and enum.json under the path =/etc/clearwater/sample. And then run the command sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_bgcf_json and enum_json. Post this command both files created the path under /etc/Clearwater. Files contents are


-----------BGCF.JSON------------------


   {
      "routes" : [
        {   "name" : "Clearwater AIO_TO_PBX",
          "domain" : "10.112.87.177",
          "route" : ["sip:example.com:5058","sip:10.112.87.177:5060"]
        }
    ]
  }

------ENUM.JSON---------------------

{
    "number_blocks" : [
        {
           "name" : "Internal numbers",
           "prefix" : "650555",
           "regex" : "!(^.*$)!sip:\\1 at 10.112.87.250!"
        },

        {
            "name" : "External numbers",
            "prefix" : "",
            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"
        }
    ]
}



My IP Scheme are:

10.112.87.177 : PBX IP address
10.112.87.250: IMS IP address and Domain is example.com

10.112.123.187 : my System ip where Zoper client insall


My call Flow is:-

SIP Client A(Registered in PBX) calls to -> SIP Client B (Registered in ims with example.com domain )

When I make the call from A to B then call coming in IMS network but getting error 'Not found 404' because B number registered with xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com> .


Please find the attached wireshark traces and BONO, Sprout logs



Note : I also not able to call from from B to A Also

Regards
Surender Singh






::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/12d206af/attachment.html>

From surender.s at hcl.com  Fri Sep 16 08:53:21 2016
From: surender.s at hcl.com (Surender Singh)
Date: Fri, 16 Sep 2016 12:53:21 +0000
Subject: [Project Clearwater] Clearwater Digest, Vol 41, Issue 28
In-Reply-To: <mailman.255.1474029230.25028.clearwater_lists.projectclearwater.org@lists.projectclearwater.org>
References: <mailman.255.1474029230.25028.clearwater_lists.projectclearwater.org@lists.projectclearwater.org>
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E0F50A@NDA-HCLT-MBS02.hclt.corp.hcl.in>



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 16 September 2016 18:04
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 41, Issue 28

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. Mininet (chess man)
   2.  Performance limit measurement (??????? ?????????)
   3. Re: Mininet (Graeme Robertson (projectclearwater.org))
   4. Re: SIP Trunking With PBX_aio
      (Graeme Robertson (projectclearwater.org))


----------------------------------------------------------------------

Message: 1
Date: Fri, 16 Sep 2016 09:02:07 +0100
From: chess man <chessmancaryl at gmail.com>
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Mininet
Message-ID:
	<CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Hi everybody,

I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/978b47a5/attachment-0001.html>

------------------------------

Message: 2
Date: Fri, 16 Sep 2016 12:16:42 +0300
From: ??????? ????????? 	<michaelkatsoulis88 at gmail.com>
To: Clearwater at lists.projectclearwater.org
Subject: [Project Clearwater]  Performance limit measurement
Message-ID:
	<CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Hi all,

we are running Stress Tests against our Clearwater Deployment using Sip Stress node.
We have noticed that the results are not consistent as the number of successfull calls changes during repetitions of the same test scenario.

We have tried to increase the values of max_tokens , init_token_rate, min_token_rate and target_latency_us but we did not observe any difference.

What is the proposed way to discover the deployment's limit on how many requests per second can be served?

Thanks in advance,
Michael Katsoulis
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/27a77df2/attachment-0001.html>

------------------------------

Message: 3
Date: Fri, 16 Sep 2016 12:20:26 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Mininet
Message-ID:
	<CY4PR02MB2616A144FD60BFC5F7796B1CE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="utf-8"

Hello,

There is no reason why it shouldn?t just work, but as far as I know nobody has ever tried to install Project Clearwater on Mininet. If our AIO OVA doesn?t just work then you may have to follow our manual install instructions<http://clearwater.readthedocs.io/en/stable/Manual_Install.html>.

It would be interesting to hear how you get on!

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of chess man
Sent: 16 September 2016 09:02
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Mininet

Hi everybody,
I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/782ad823/attachment-0001.html>

------------------------------

Message: 4
Date: Fri, 16 Sep 2016 12:33:07 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID:
	<CY4PR02MB2616B3B7962F78B040BAAD8BE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="us-ascii"

Hi Surender,

I'm not sure what you mean about not being able to reply through the mailing list - I think this email went to the mailing list :).

That is odd. What is log_level set to if you run ' log_level="";. /etc/clearwater/config; echo $log_level'? And what is it set to if you run 'log_level="";. /etc/clearwater/user_settings; echo $log_level'?

The /etc/clearwater/config file is the config file that all of our processes actually read from. As you can see, this file pulls in all of our other config files (shared_config, local_config and user_settings).

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 16 September 2016 07:18
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio

Hi Graeme,


I am not able to reply through mailing list as I already created the account.

I requesting you for quick response...


ims.hcl.com is another deployment.


I putted the said line in user_setting file and  restart the sprout but showing Log level set to 2 ,fine below logs and also find the attached sprout logs .


[cw-aio]root at cw-aio:~# sudo service sprout restart
* Restarting Sprout SIP Router sprout                                          Log directory set to /var/log/sprout
Log level set to 2
16-09-2016 11:10:01.436 UTC Status utils.cpp:494: Switching to daemon mode
                                                                         [ OK ]

-----user_settings file is under the path etc/Clearwater is----

trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>"
enum_file="enum.json"
log_level=5




I have one more query, what is meaning of config file under the path /etc/Clearwater

-------config  file------------------
if [ -f /etc/clearwater/shared_config ]
then
  . /etc/clearwater/shared_config
fi

. /etc/clearwater/local_config

if [ -f /etc/clearwater/user_settings ]
then
  . /etc/clearwater/user_settings
fi


Regards
Surender Singh

-------------------------------------------------------------

Hi Surender,

I noticed there are lots of instances of "Invalid ENUM response:
sip:@example.com" so it looks as though something is misconfigured, but it's not immediately obvious what. I think it would be useful to turn on debug logging for Sprout in order to dig into this issue further. In order to do this, add the line 'log_level=5' to /etc/clearwater/user_settings (creating it if it doesn't exist) and restart Sprout (by running sudo service sprout restart). This will cause far more detailed logs to be written to the Sprout log files.

Incidentally, I thought (from an earlier thread) that you'd changed your home domain from example.com to ims.hcl.com - is that on a different deployment? I just wanted to make sure there was no confusion here.

Thanks,
Graeme


From: Surender Singh
Sent: 15 September 2016 17:54
To: 'clearwater at lists.projectclearwater.org' <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>
Subject: RE: SIP Trunking With PBX_aio


Hi Team,

Kindly help

Please share the procedure to connect the SIP trunk with PBX.

I already tried many combination but  in/out calls failing in IMS.

When I also checking IBCF functionality are enabled or not then showing below o/p

[cw-aio]ubuntu at cw-aio:~$ sudo /usr/share/clearwater/bin/bono
/usr/share/clearwater/bin/bono: error while loading shared libraries: libmemcached.so.11: cannot open shared object file: No such file or directory [cw-aio]ubuntu at cw-aio:~$

Regards
Surender Singh

From: Surender Singh
Sent: 14 September 2016 17:26
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: SIP Trunking With PBX_aio

Hi team,

I deployed the aio solution in my Vmware Exsi  and made the connectivity with Mitel IP-PBX .

As per doc and mailing list  I made the Following entry .


1)      As per mailing IBCF and BGCF functionality are provided by BONO

2)      I create the one user-settings file (already not created ) under the path =/etc/clearwater/user_settings  with entry like trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>" . Here 10.112.87.177 is my PBX ip.

3)      After that I make the entry for bfcg.json and enum.json under the path =/etc/clearwater/sample. And then run the command sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_bgcf_json and enum_json. Post this command both files created the path under /etc/Clearwater. Files contents are


-----------BGCF.JSON------------------


   {
      "routes" : [
        {   "name" : "Clearwater AIO_TO_PBX",
          "domain" : "10.112.87.177",
          "route" : ["sip:example.com:5058","sip:10.112.87.177:5060"]
        }
    ]
  }

------ENUM.JSON---------------------

{
    "number_blocks" : [
        {
           "name" : "Internal numbers",
           "prefix" : "650555",
           "regex" : "!(^.*$)!sip:\\1 at 10.112.87.250!"
        },

        {
            "name" : "External numbers",
            "prefix" : "",
            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"
        }
    ]
}



My IP Scheme are:

10.112.87.177 : PBX IP address
10.112.87.250: IMS IP address and Domain is example.com

10.112.123.187 : my System ip where Zoper client insall


My call Flow is:-

SIP Client A(Registered in PBX) calls to -> SIP Client B (Registered in ims with example.com domain )

When I make the call from A to B then call coming in IMS network but getting error 'Not found 404' because B number registered with xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com> .


Please find the attached wireshark traces and BONO, Sprout logs



Note : I also not able to call from from B to A Also

Regards
Surender Singh






::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents (with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification, distribution and / or publication of this message without the prior written consent of authorized representative of HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/12d206af/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 41, Issue 28
******************************************



From graeme at projectclearwater.org  Fri Sep 16 09:14:20 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Fri, 16 Sep 2016 13:14:20 +0000
Subject: [Project Clearwater] Performance limit measurement
In-Reply-To: <CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA@mail.gmail.com>
References: <CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA@mail.gmail.com>
Message-ID: <CY4PR02MB26168AC7A5C61A9DD32EB0C2E3F30@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Michael,

How many successes and failures are you seeing? We primarily use the clearwater-sip-stress package to check we haven?t introduced crashes under load, and to check we haven?t significantly regressed the performance of Project Clearwater. Unfortunately clearwater-sip-stress is not reliable enough to generate completely accurate performance numbers for Project Clearwater (and we don?t accurately measure Project Clearwater performance or provide numbers). We tend to see around 1% failures when running clearwater-sip-stress. If your failure numbers are fluctuating at around 1% then this is probably down to the test scripts not being completely reliable, and you won?t have actually hit the deployment?s limit until you start seeing more failures than this.

Thanks,
Graeme


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of ??????? ?ats?????
Sent: 16 September 2016 10:17
To: Clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Performance limit measurement

Hi all,

we are running Stress Tests against our Clearwater Deployment using Sip Stress node.
We have noticed that the results are not consistent as the number of successfull calls changes during repetitions of the same test scenario.

We have tried to increase the values of max_tokens , init_token_rate, min_token_rate and
target_latency_us but we did not observe any difference.

What is the proposed way to discover the deployment's limit on how many requests per second can
be served?

Thanks in advance,
Michael Katsoulis
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/82ce7c7a/attachment.html>

From surender.s at hcl.com  Fri Sep 16 09:28:02 2016
From: surender.s at hcl.com (Surender Singh)
Date: Fri, 16 Sep 2016 13:28:02 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E0F52E@NDA-HCLT-MBS02.hclt.corp.hcl.in>

Hi Graeme,

Sorry I am not  able to understand regarding logs, I just changes the value in user_settings file and restart the sprout like below.

log_level="5"

But now after adding the below line in shared_config file , enum query is happing and able rewrite URI . IBFG/BGCF also able forward the packet at my pbx ip 10.112.87.177 but getting release from PBX 404.

I also disabled the lr at pbx.

Find below sip logs


Frame 7: 1665 bytes on wire (13320 bits), 1665 bytes captured (13320 bits)
Linux cooked capture
Internet Protocol Version 4, Src: 10.112.87.250, Dst: 10.112.87.177
Transmission Control Protocol, Src Port: 59955, Dst Port: 5060, Seq: 1, Ack: 1, Len: 1597
Session Initiation Protocol (INVITE)
    Request-Line: INVITE sip:1234 at 10.112.87.177;transport=TCP SIP/2.0
    Message Header
        Via: SIP/2.0/TCP 10.112.87.250:59955;rport;branch=z9hG4bKPja-Y65hsmJxWUbzhJeod0RuunKpnx1Gu6
        Via: SIP/2.0/TCP 10.112.87.250:42127;rport=42127;received=10.112.87.250;branch=z9hG4bKPj6ze-NaIX4.JPNaszyxOgTN3gp5j7Ngwp
        Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
        Via: SIP/2.0/TCP 10.112.87.250:42466;rport=42466;received=10.112.87.250;branch=z9hG4bKPj5xhgQqIs5lRKtimI4E9pvUXW2EhTqghw
        Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
        Record-Route: <sip:LKP68ldZs3 at cw-aio:5060;transport=TCP;lr>
        Via: SIP/2.0/TCP 10.112.123.187:43259;received=10.112.123.187;branch=z9hG4bK-524287-1---c83e12d46aead571
        Max-Forwards: 66
        Contact: <sip:6505550962 at 10.112.123.187:43259;transport=tcp>
        To: <sip:1234 at example.com>
        From: <sip:6505550962 at example.com>;tag=eb40917f
        Call-ID: VAescKO7V6NoEuXLTrcYVg..
        CSeq: 1 INVITE
        Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
        Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
        User-Agent: Z 3.9.32144 r32121
        Allow-Events: presence, kpml
        P-Asserted-Identity: <sip:6505550962 at example.com>
        Session-Expires: 600
        P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg
        Content-Type: application/sdp
        Content-Length:   243
    Message Body
        Session Description Protocol

Regards
Surender Singh





-----------------Write in Shared_config file------------------
enum_file=enum.json



{
    "routes" : [
        {   "name" : "Clearwater AIO_TO_PBX",
          "domain" : "10.112.87.177",
          "route" : ["sip:cw-aio:5058"]
        }
    ]
  
}



{
    "number_blocks" : [
        {
           "name" : "Internal numbers",
           "prefix" : "650555",
           "regex" : "!(^.*$)!sip:\\1 at example.com!"
        },
        {
            "name" : "External numbers",
            "prefix" : "",
            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177;transport=TCP!"
        }
    ]
}


-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 16 September 2016 18:04
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 41, Issue 28

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. Mininet (chess man)
   2.  Performance limit measurement (??????? ?????????)
   3. Re: Mininet (Graeme Robertson (projectclearwater.org))
   4. Re: SIP Trunking With PBX_aio
      (Graeme Robertson (projectclearwater.org))


----------------------------------------------------------------------

Message: 1
Date: Fri, 16 Sep 2016 09:02:07 +0100
From: chess man <chessmancaryl at gmail.com>
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Mininet
Message-ID:
	<CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Hi everybody,

I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/978b47a5/attachment-0001.html>

------------------------------

Message: 2
Date: Fri, 16 Sep 2016 12:16:42 +0300
From: ??????? ????????? 	<michaelkatsoulis88 at gmail.com>
To: Clearwater at lists.projectclearwater.org
Subject: [Project Clearwater]  Performance limit measurement
Message-ID:
	<CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Hi all,

we are running Stress Tests against our Clearwater Deployment using Sip Stress node.
We have noticed that the results are not consistent as the number of successfull calls changes during repetitions of the same test scenario.

We have tried to increase the values of max_tokens , init_token_rate, min_token_rate and target_latency_us but we did not observe any difference.

What is the proposed way to discover the deployment's limit on how many requests per second can be served?

Thanks in advance,
Michael Katsoulis
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/27a77df2/attachment-0001.html>

------------------------------

Message: 3
Date: Fri, 16 Sep 2016 12:20:26 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Mininet
Message-ID:
	<CY4PR02MB2616A144FD60BFC5F7796B1CE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="utf-8"

Hello,

There is no reason why it shouldn?t just work, but as far as I know nobody has ever tried to install Project Clearwater on Mininet. If our AIO OVA doesn?t just work then you may have to follow our manual install instructions<http://clearwater.readthedocs.io/en/stable/Manual_Install.html>.

It would be interesting to hear how you get on!

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of chess man
Sent: 16 September 2016 09:02
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Mininet

Hi everybody,
I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/782ad823/attachment-0001.html>

------------------------------

Message: 4
Date: Fri, 16 Sep 2016 12:33:07 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID:
	<CY4PR02MB2616B3B7962F78B040BAAD8BE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="us-ascii"

Hi Surender,

I'm not sure what you mean about not being able to reply through the mailing list - I think this email went to the mailing list :).

That is odd. What is log_level set to if you run ' log_level="";. /etc/clearwater/config; echo $log_level'? And what is it set to if you run 'log_level="";. /etc/clearwater/user_settings; echo $log_level'?

The /etc/clearwater/config file is the config file that all of our processes actually read from. As you can see, this file pulls in all of our other config files (shared_config, local_config and user_settings).

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 16 September 2016 07:18
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio

Hi Graeme,


I am not able to reply through mailing list as I already created the account.

I requesting you for quick response...


ims.hcl.com is another deployment.


I putted the said line in user_setting file and  restart the sprout but showing Log level set to 2 ,fine below logs and also find the attached sprout logs .


[cw-aio]root at cw-aio:~# sudo service sprout restart
* Restarting Sprout SIP Router sprout                                          Log directory set to /var/log/sprout
Log level set to 2
16-09-2016 11:10:01.436 UTC Status utils.cpp:494: Switching to daemon mode
                                                                         [ OK ]

-----user_settings file is under the path etc/Clearwater is----

trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>"
enum_file="enum.json"
log_level=5




I have one more query, what is meaning of config file under the path /etc/Clearwater

-------config  file------------------
if [ -f /etc/clearwater/shared_config ]
then
  . /etc/clearwater/shared_config
fi

. /etc/clearwater/local_config

if [ -f /etc/clearwater/user_settings ]
then
  . /etc/clearwater/user_settings
fi


Regards
Surender Singh

-------------------------------------------------------------

Hi Surender,

I noticed there are lots of instances of "Invalid ENUM response:
sip:@example.com" so it looks as though something is misconfigured, but it's not immediately obvious what. I think it would be useful to turn on debug logging for Sprout in order to dig into this issue further. In order to do this, add the line 'log_level=5' to /etc/clearwater/user_settings (creating it if it doesn't exist) and restart Sprout (by running sudo service sprout restart). This will cause far more detailed logs to be written to the Sprout log files.

Incidentally, I thought (from an earlier thread) that you'd changed your home domain from example.com to ims.hcl.com - is that on a different deployment? I just wanted to make sure there was no confusion here.

Thanks,
Graeme


From: Surender Singh
Sent: 15 September 2016 17:54
To: 'clearwater at lists.projectclearwater.org' <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>
Subject: RE: SIP Trunking With PBX_aio


Hi Team,

Kindly help

Please share the procedure to connect the SIP trunk with PBX.

I already tried many combination but  in/out calls failing in IMS.

When I also checking IBCF functionality are enabled or not then showing below o/p

[cw-aio]ubuntu at cw-aio:~$ sudo /usr/share/clearwater/bin/bono
/usr/share/clearwater/bin/bono: error while loading shared libraries: libmemcached.so.11: cannot open shared object file: No such file or directory [cw-aio]ubuntu at cw-aio:~$

Regards
Surender Singh

From: Surender Singh
Sent: 14 September 2016 17:26
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: SIP Trunking With PBX_aio

Hi team,

I deployed the aio solution in my Vmware Exsi  and made the connectivity with Mitel IP-PBX .

As per doc and mailing list  I made the Following entry .


1)      As per mailing IBCF and BGCF functionality are provided by BONO

2)      I create the one user-settings file (already not created ) under the path =/etc/clearwater/user_settings  with entry like trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>" . Here 10.112.87.177 is my PBX ip.

3)      After that I make the entry for bfcg.json and enum.json under the path =/etc/clearwater/sample. And then run the command sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_bgcf_json and enum_json. Post this command both files created the path under /etc/Clearwater. Files contents are


-----------BGCF.JSON------------------


   {
      "routes" : [
        {   "name" : "Clearwater AIO_TO_PBX",
          "domain" : "10.112.87.177",
          "route" : ["sip:example.com:5058","sip:10.112.87.177:5060"]
        }
    ]
  }

------ENUM.JSON---------------------

{
    "number_blocks" : [
        {
           "name" : "Internal numbers",
           "prefix" : "650555",
           "regex" : "!(^.*$)!sip:\\1 at 10.112.87.250!"
        },

        {
            "name" : "External numbers",
            "prefix" : "",
            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"
        }
    ]
}



My IP Scheme are:

10.112.87.177 : PBX IP address
10.112.87.250: IMS IP address and Domain is example.com

10.112.123.187 : my System ip where Zoper client insall


My call Flow is:-

SIP Client A(Registered in PBX) calls to -> SIP Client B (Registered in ims with example.com domain )

When I make the call from A to B then call coming in IMS network but getting error 'Not found 404' because B number registered with xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com> .


Please find the attached wireshark traces and BONO, Sprout logs



Note : I also not able to call from from B to A Also

Regards
Surender Singh






::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents (with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification, distribution and / or publication of this message without the prior written consent of authorized representative of HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.
----------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/12d206af/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 41, Issue 28
******************************************
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 7777.pcap
Type: application/octet-stream
Size: 8192 bytes
Desc: 7777.pcap
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/d049f6e9/attachment.obj>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sprout_current.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/d049f6e9/attachment.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sprout_current.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/d049f6e9/attachment-0001.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sprout_20160916T170000Z.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/d049f6e9/attachment-0002.txt>

From michaelkatsoulis88 at gmail.com  Fri Sep 16 10:15:55 2016
From: michaelkatsoulis88 at gmail.com (=?UTF-8?B?zpzOuc+HzqzOu863z4IgzprOsc+Ez4POv8+NzrvOt8+C?=)
Date: Fri, 16 Sep 2016 17:15:55 +0300
Subject: [Project Clearwater] Performance limit measurement
In-Reply-To: <CY4PR02MB26168AC7A5C61A9DD32EB0C2E3F30@CY4PR02MB2616.namprd02.prod.outlook.com>
References: <CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA@mail.gmail.com>
	<CY4PR02MB26168AC7A5C61A9DD32EB0C2E3F30@CY4PR02MB2616.namprd02.prod.outlook.com>
Message-ID: <CAJG3f9NLorKJEbEJbDh+gEKfbLfdqB0CQR1sv6J6pFtnbmy=+g@mail.gmail.com>

Hi Graeme,

thanks a lot for your response.

In our scenario we are using the Stress node to generate 15000 calls in 60
seconds. The number of
unsuccessful calls varies from ~500 to ~5000 even in subsequent repetitions
of the same scenario.
According to wireshark the failures happen because of Sprout that does not
send the correct responses in time
and so we get "time-outs" and "unexpected messages" in the Stress node.
The Sprout node has sufficient CPU and memory resources.
What could be the reason of this instability in our deployment?

Thank you in advance,
Michael Katsoulis














2016-09-16 16:14 GMT+03:00 Graeme Robertson (projectclearwater.org) <
graeme at projectclearwater.org>:

> Hi Michael,
>
>
>
> How many successes and failures are you seeing? We primarily use the
> clearwater-sip-stress package to check we haven?t introduced crashes under
> load, and to check we haven?t significantly regressed the performance of
> Project Clearwater. Unfortunately clearwater-sip-stress is not reliable
> enough to generate completely accurate performance numbers for Project
> Clearwater (and we don?t accurately measure Project Clearwater performance
> or provide numbers). We tend to see around 1% failures when running
> clearwater-sip-stress. If your failure numbers are fluctuating at around 1%
> then this is probably down to the test scripts not being completely
> reliable, and you won?t have actually hit the deployment?s limit until you
> start seeing more failures than this.
>
>
>
> Thanks,
>
> Graeme
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *??????? ?ats?????
> *Sent:* 16 September 2016 10:17
> *To:* Clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Performance limit measurement
>
>
>
> Hi all,
>
>
>
> we are running Stress Tests against our Clearwater Deployment using Sip
> Stress node.
>
> We have noticed that the results are not consistent as the number of
> successfull calls changes during repetitions of the same test scenario.
>
>
>
> We have tried to increase the values of max_tokens , init_token_rate,
> min_token_rate and
>
> target_latency_us but we did not observe any difference.
>
>
>
> What is the proposed way to discover the deployment's limit on how many
> requests per second can
>
> be served?
>
>
>
> Thanks in advance,
>
> Michael Katsoulis
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/eebab7c9/attachment.html>

From graeme at projectclearwater.org  Fri Sep 16 14:25:56 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Fri, 16 Sep 2016 18:25:56 +0000
Subject: [Project Clearwater] Performance limit measurement
In-Reply-To: <CAJG3f9NLorKJEbEJbDh+gEKfbLfdqB0CQR1sv6J6pFtnbmy=+g@mail.gmail.com>
References: <CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA@mail.gmail.com>
	<CY4PR02MB26168AC7A5C61A9DD32EB0C2E3F30@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAJG3f9NLorKJEbEJbDh+gEKfbLfdqB0CQR1sv6J6pFtnbmy=+g@mail.gmail.com>
Message-ID: <CY4PR02MB26163BAD3AB2F60F7D1A74BAE3F30@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Michael,

Can you tell me more about your scenario? It sounds like you?re not using the clearwater-sip-stress package, or at least not in exactly the form we package up. If you?re not using the clearwater-sip-stress package then please can you send details of your stress scenario?

Depending on how powerful your Sprout node is, I would expect 15000 calls per second to be towards the upper limit of its performance powers. However, if the CPU is not particularly high then that would suggest that Sprout?s throttling controls might require further tuning. Do you know what return code the ?unexpected messages? have? 503s indicate that there is overload somewhere. Sprout does adjust its throttling controls to match the load its able to process, but that process is not immediate, and we recommend building stress up gradually rather than immediately firing 15000 calls per second into the system ? for more information on that, see http://www.projectclearwater.org/clearwater-performance-and-our-load-monitor/.

One final thought I had was that the node you?re running stress on might be overloaded. If the stress node is not responding to messages in a timely fashion then that will generate time outs and unexpected messages.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of ??????? ?ats?????
Sent: 16 September 2016 15:16
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Performance limit measurement

Hi Graeme,

thanks a lot for your response.

In our scenario we are using the Stress node to generate 15000 calls in 60 seconds. The number of
unsuccessful calls varies from ~500 to ~5000 even in subsequent repetitions of the same scenario.
According to wireshark the failures happen because of Sprout that does not send the correct responses in time
and so we get "time-outs" and "unexpected messages" in the Stress node.
The Sprout node has sufficient CPU and memory resources.
What could be the reason of this instability in our deployment?

Thank you in advance,
Michael Katsoulis














2016-09-16 16:14 GMT+03:00 Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>:
Hi Michael,

How many successes and failures are you seeing? We primarily use the clearwater-sip-stress package to check we haven?t introduced crashes under load, and to check we haven?t significantly regressed the performance of Project Clearwater. Unfortunately clearwater-sip-stress is not reliable enough to generate completely accurate performance numbers for Project Clearwater (and we don?t accurately measure Project Clearwater performance or provide numbers). We tend to see around 1% failures when running clearwater-sip-stress. If your failure numbers are fluctuating at around 1% then this is probably down to the test scripts not being completely reliable, and you won?t have actually hit the deployment?s limit until you start seeing more failures than this.

Thanks,
Graeme


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of ??????? ?ats?????
Sent: 16 September 2016 10:17
To: Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Performance limit measurement

Hi all,

we are running Stress Tests against our Clearwater Deployment using Sip Stress node.
We have noticed that the results are not consistent as the number of successfull calls changes during repetitions of the same test scenario.

We have tried to increase the values of max_tokens , init_token_rate, min_token_rate and
target_latency_us but we did not observe any difference.

What is the proposed way to discover the deployment's limit on how many requests per second can
be served?

Thanks in advance,
Michael Katsoulis

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/1475c27f/attachment.html>

From graeme at projectclearwater.org  Fri Sep 16 14:38:41 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Fri, 16 Sep 2016 18:38:41 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
In-Reply-To: <16AE9ED83DBA5B4D85B058EAF39C918107E0F52E@NDA-HCLT-MBS02.hclt.corp.hcl.in>
References: <16AE9ED83DBA5B4D85B058EAF39C918107E0F52E@NDA-HCLT-MBS02.hclt.corp.hcl.in>
Message-ID: <CY4PR02MB26169F0D2D19946AC837C595E3F30@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Surender,



It's great to hear that you're making progress.



It sounds like there's a problem with your user_settings file - is it definitely in the /etc/clearwater directory? You suggested earlier it might be in /etc/Clearwater but I assumed that was a typo at the time. Anyway, it sounds like we're past the problems with Sprout, which is good. Unfortunately, I'm afraid I don't completely understand what your current issue is - please can you be more specific? It sounds like the INVITE is successfully being routed out to your PBX via the IBCF and your PBX is rejecting the INVITE with a 404. The INVITE that's being sent to the PBX looks sensible to me, so I think you need to work out why the PBX is rejecting it.



Thanks,

Graeme



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 16 September 2016 14:28
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP Trunking With PBX_aio



Hi Graeme,



Sorry I am not  able to understand regarding logs, I just changes the value in user_settings file and restart the sprout like below.



log_level="5"



But now after adding the below line in shared_config file , enum query is happing and able rewrite URI . IBFG/BGCF also able forward the packet at my pbx ip 10.112.87.177 but getting release from PBX 404.



I also disabled the lr at pbx.



Find below sip logs





Frame 7: 1665 bytes on wire (13320 bits), 1665 bytes captured (13320 bits) Linux cooked capture Internet Protocol Version 4, Src: 10.112.87.250, Dst: 10.112.87.177 Transmission Control Protocol, Src Port: 59955, Dst Port: 5060, Seq: 1, Ack: 1, Len: 1597 Session Initiation Protocol (INVITE)

    Request-Line: INVITE sip:1234 at 10.112.87.177;transport=TCP SIP/2.0

    Message Header

        Via: SIP/2.0/TCP 10.112.87.250:59955;rport;branch=z9hG4bKPja-Y65hsmJxWUbzhJeod0RuunKpnx1Gu6

        Via: SIP/2.0/TCP 10.112.87.250:42127;rport=42127;received=10.112.87.250;branch=z9hG4bKPj6ze-NaIX4.JPNaszyxOgTN3gp5j7Ngwp

        Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

        Via: SIP/2.0/TCP 10.112.87.250:42466;rport=42466;received=10.112.87.250;branch=z9hG4bKPj5xhgQqIs5lRKtimI4E9pvUXW2EhTqghw

        Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

        Record-Route: <sip:LKP68ldZs3 at cw-aio:5060;transport=TCP;lr>

        Via: SIP/2.0/TCP 10.112.123.187:43259;received=10.112.123.187;branch=z9hG4bK-524287-1---c83e12d46aead571

        Max-Forwards: 66

        Contact: <sip:6505550962 at 10.112.123.187:43259;transport=tcp>

        To: <sip:1234 at example.com>

        From: <sip:6505550962 at example.com>;tag=eb40917f

        Call-ID: VAescKO7V6NoEuXLTrcYVg..

        CSeq: 1 INVITE

        Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE

        Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri

        User-Agent: Z 3.9.32144 r32121

        Allow-Events: presence, kpml

        P-Asserted-Identity: <sip:6505550962 at example.com>

        Session-Expires: 600

        P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg

        Content-Type: application/sdp

        Content-Length:   243

    Message Body

        Session Description Protocol



Regards

Surender Singh











-----------------Write in Shared_config file------------------ enum_file=enum.json







{

    "routes" : [

        {   "name" : "Clearwater AIO_TO_PBX",

          "domain" : "10.112.87.177",

          "route" : ["sip:cw-aio:5058"]

        }

    ]



}







{

    "number_blocks" : [

        {

           "name" : "Internal numbers",

           "prefix" : "650555",

           "regex" : "!(^.*$)!sip:\\1 at example.com!"

        },

        {

            "name" : "External numbers",

            "prefix" : "",

            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177;transport=TCP!"

        }

    ]

}





-----Original Message-----

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org<mailto:clearwater-request at lists.projectclearwater.org>

Sent: 16 September 2016 18:04

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: Clearwater Digest, Vol 41, Issue 28



Send Clearwater mailing list submissions to

                clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>



To subscribe or unsubscribe via the World Wide Web, visit

                http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



or, via email, send a message with subject or body 'help' to

                clearwater-request at lists.projectclearwater.org<mailto:clearwater-request at lists.projectclearwater.org>



You can reach the person managing the list at

                clearwater-owner at lists.projectclearwater.org<mailto:clearwater-owner at lists.projectclearwater.org>



When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."





Today's Topics:



   1. Mininet (chess man)

   2.  Performance limit measurement (??????? ?????????)

   3. Re: Mininet (Graeme Robertson (projectclearwater.org))

   4. Re: SIP Trunking With PBX_aio

      (Graeme Robertson (projectclearwater.org))





----------------------------------------------------------------------



Message: 1

Date: Fri, 16 Sep 2016 09:02:07 +0100

From: chess man <chessmancaryl at gmail.com<mailto:chessmancaryl at gmail.com>>

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: [Project Clearwater] Mininet

Message-ID:

                <CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw at mail.gmail.com<mailto:CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw at mail.gmail.com>>

Content-Type: text/plain; charset="utf-8"



Hi everybody,



I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/978b47a5/attachment-0001.html>



------------------------------



Message: 2

Date: Fri, 16 Sep 2016 12:16:42 +0300

From: ??????? ?????????             <michaelkatsoulis88 at gmail.com<mailto:michaelkatsoulis88 at gmail.com>>

To: Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>

Subject: [Project Clearwater]  Performance limit measurement

Message-ID:

                <CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA at mail.gmail.com<mailto:CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA at mail.gmail.com>>

Content-Type: text/plain; charset="utf-8"



Hi all,



we are running Stress Tests against our Clearwater Deployment using Sip Stress node.

We have noticed that the results are not consistent as the number of successfull calls changes during repetitions of the same test scenario.



We have tried to increase the values of max_tokens , init_token_rate, min_token_rate and target_latency_us but we did not observe any difference.



What is the proposed way to discover the deployment's limit on how many requests per second can be served?



Thanks in advance,

Michael Katsoulis

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/27a77df2/attachment-0001.html>



------------------------------



Message: 3

Date: Fri, 16 Sep 2016 12:20:26 +0000

From: "Graeme Robertson (projectclearwater.org)"

                <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>

To: "clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>"

                <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>

Subject: Re: [Project Clearwater] Mininet

Message-ID:

                <CY4PR02MB2616A144FD60BFC5F7796B1CE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com<mailto:CY4PR02MB2616A144FD60BFC5F7796B1CE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>>



Content-Type: text/plain; charset="utf-8"



Hello,



There is no reason why it shouldn?t just work, but as far as I know nobody has ever tried to install Project Clearwater on Mininet. If our AIO OVA doesn?t just work then you may have to follow our manual install instructions<http://clearwater.readthedocs.io/en/stable/Manual_Install.html>.



It would be interesting to hear how you get on!



Thanks,

Graeme



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of chess man

Sent: 16 September 2016 09:02

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: [Project Clearwater] Mininet



Hi everybody,

I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/782ad823/attachment-0001.html>



------------------------------



Message: 4

Date: Fri, 16 Sep 2016 12:33:07 +0000

From: "Graeme Robertson (projectclearwater.org)"

                <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>

To: "clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>"

                <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>

Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio

Message-ID:

                <CY4PR02MB2616B3B7962F78B040BAAD8BE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com<mailto:CY4PR02MB2616B3B7962F78B040BAAD8BE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>>



Content-Type: text/plain; charset="us-ascii"



Hi Surender,



I'm not sure what you mean about not being able to reply through the mailing list - I think this email went to the mailing list :).



That is odd. What is log_level set to if you run ' log_level="";. /etc/clearwater/config; echo $log_level'? And what is it set to if you run 'log_level="";. /etc/clearwater/user_settings; echo $log_level'?



The /etc/clearwater/config file is the config file that all of our processes actually read from. As you can see, this file pulls in all of our other config files (shared_config, local_config and user_settings).



Thanks,

Graeme



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh

Sent: 16 September 2016 07:18

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio



Hi Graeme,





I am not able to reply through mailing list as I already created the account.



I requesting you for quick response...





ims.hcl.com is another deployment.





I putted the said line in user_setting file and  restart the sprout but showing Log level set to 2 ,fine below logs and also find the attached sprout logs .





[cw-aio]root at cw-aio:~# sudo service sprout restart

* Restarting Sprout SIP Router sprout                                          Log directory set to /var/log/sprout

Log level set to 2

16-09-2016 11:10:01.436 UTC Status utils.cpp:494: Switching to daemon mode

                                                                         [ OK ]



-----user_settings file is under the path etc/Clearwater is----



trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>"

enum_file="enum.json"

log_level=5









I have one more query, what is meaning of config file under the path /etc/Clearwater



-------config  file------------------

if [ -f /etc/clearwater/shared_config ]

then

  . /etc/clearwater/shared_config

fi



. /etc/clearwater/local_config



if [ -f /etc/clearwater/user_settings ]

then

  . /etc/clearwater/user_settings

fi





Regards

Surender Singh



-------------------------------------------------------------



Hi Surender,



I noticed there are lots of instances of "Invalid ENUM response:

sip:@example.com" so it looks as though something is misconfigured, but it's not immediately obvious what. I think it would be useful to turn on debug logging for Sprout in order to dig into this issue further. In order to do this, add the line 'log_level=5' to /etc/clearwater/user_settings (creating it if it doesn't exist) and restart Sprout (by running sudo service sprout restart). This will cause far more detailed logs to be written to the Sprout log files.



Incidentally, I thought (from an earlier thread) that you'd changed your home domain from example.com to ims.hcl.com - is that on a different deployment? I just wanted to make sure there was no confusion here.



Thanks,

Graeme





From: Surender Singh

Sent: 15 September 2016 17:54

To: 'clearwater at lists.projectclearwater.org' <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org%3cmailto:clearwater at lists.projectclearwater.org>>>

Subject: RE: SIP Trunking With PBX_aio





Hi Team,



Kindly help



Please share the procedure to connect the SIP trunk with PBX.



I already tried many combination but  in/out calls failing in IMS.



When I also checking IBCF functionality are enabled or not then showing below o/p



[cw-aio]ubuntu at cw-aio:~$ sudo /usr/share/clearwater/bin/bono

/usr/share/clearwater/bin/bono: error while loading shared libraries: libmemcached.so.11: cannot open shared object file: No such file or directory [cw-aio]ubuntu at cw-aio:~$



Regards

Surender Singh



From: Surender Singh

Sent: 14 September 2016 17:26

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org%3cmailto:clearwater at lists.projectclearwater.org>>

Subject: SIP Trunking With PBX_aio



Hi team,



I deployed the aio solution in my Vmware Exsi  and made the connectivity with Mitel IP-PBX .



As per doc and mailing list  I made the Following entry .





1)      As per mailing IBCF and BGCF functionality are provided by BONO



2)      I create the one user-settings file (already not created ) under the path =/etc/clearwater/user_settings  with entry like trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>" . Here 10.112.87.177 is my PBX ip.



3)      After that I make the entry for bfcg.json and enum.json under the path =/etc/clearwater/sample. And then run the command sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_bgcf_json and enum_json. Post this command both files created the path under /etc/Clearwater. Files contents are





-----------BGCF.JSON------------------





   {

      "routes" : [

        {   "name" : "Clearwater AIO_TO_PBX",

          "domain" : "10.112.87.177",

          "route" : ["sip:example.com:5058","sip:10.112.87.177:5060"]

        }

    ]

  }



------ENUM.JSON---------------------



{

    "number_blocks" : [

        {

           "name" : "Internal numbers",

           "prefix" : "650555",

           "regex" : "!(^.*$)!sip:\\1 at 10.112.87.250!"

        },



        {

            "name" : "External numbers",

            "prefix" : "",

            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"

        }

    ]

}







My IP Scheme are:



10.112.87.177 : PBX IP address

10.112.87.250: IMS IP address and Domain is example.com



10.112.123.187 : my System ip where Zoper client insall





My call Flow is:-



SIP Client A(Registered in PBX) calls to -> SIP Client B (Registered in ims with example.com domain )



When I make the call from A to B then call coming in IMS network but getting error 'Not found 404' because B number registered with xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com%3cmailto:xxxxxx at exmaple.com>> .





Please find the attached wireshark traces and BONO, Sprout logs







Note : I also not able to call from from B to A Also



Regards

Surender Singh













::DISCLAIMER::

----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.

E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents (with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.

Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification, distribution and / or publication of this message without the prior written consent of authorized representative of HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.

Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/12d206af/attachment.html>



------------------------------



Subject: Digest Footer



_______________________________________________

Clearwater mailing list

Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>

http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org





------------------------------



End of Clearwater Digest, Vol 41, Issue 28

******************************************
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/c0205317/attachment.html>

From surender.s at hcl.com  Mon Sep 19 02:01:25 2016
From: surender.s at hcl.com (Surender Singh)
Date: Mon, 19 Sep 2016 06:01:25 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E16869@NDA-HCLT-MBS05.hclt.corp.hcl.in>

Hi Graeme,

I just make the entry in shared _config file of enum.json file. after that  enum look up started.

enum_file=enum.json

But Invite is are still not reaching to PBX (Captured the Wireshark at pbx ).Might be calls are not going outside the IMS .

Please go through my configuration, and help rectify the problem.


I have bit confusion regarding call flow (incoming and outgoing calls) in All in one node solution .

Please help what will be call flow for in/out calls.

Regards
Surender Singh








-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 17 September 2016 00:09
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 41, Issue 33

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. Re: SIP Trunking With PBX_aio
      (Graeme Robertson (projectclearwater.org))


----------------------------------------------------------------------

Message: 1
Date: Fri, 16 Sep 2016 18:38:41 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID:
	<CY4PR02MB26169F0D2D19946AC837C595E3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="us-ascii"

Hi Surender,



It's great to hear that you're making progress.



It sounds like there's a problem with your user_settings file - is it definitely in the /etc/clearwater directory? You suggested earlier it might be in /etc/Clearwater but I assumed that was a typo at the time. Anyway, it sounds like we're past the problems with Sprout, which is good. Unfortunately, I'm afraid I don't completely understand what your current issue is - please can you be more specific? It sounds like the INVITE is successfully being routed out to your PBX via the IBCF and your PBX is rejecting the INVITE with a 404. The INVITE that's being sent to the PBX looks sensible to me, so I think you need to work out why the PBX is rejecting it.



Thanks,

Graeme



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 16 September 2016 14:28
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP Trunking With PBX_aio



Hi Graeme,



Sorry I am not  able to understand regarding logs, I just changes the value in user_settings file and restart the sprout like below.



log_level="5"



But now after adding the below line in shared_config file , enum query is happing and able rewrite URI . IBFG/BGCF also able forward the packet at my pbx ip 10.112.87.177 but getting release from PBX 404.



I also disabled the lr at pbx.



Find below sip logs





Frame 7: 1665 bytes on wire (13320 bits), 1665 bytes captured (13320 bits) Linux cooked capture Internet Protocol Version 4, Src: 10.112.87.250, Dst: 10.112.87.177 Transmission Control Protocol, Src Port: 59955, Dst Port: 5060, Seq: 1, Ack: 1, Len: 1597 Session Initiation Protocol (INVITE)

    Request-Line: INVITE sip:1234 at 10.112.87.177;transport=TCP SIP/2.0

    Message Header

        Via: SIP/2.0/TCP 10.112.87.250:59955;rport;branch=z9hG4bKPja-Y65hsmJxWUbzhJeod0RuunKpnx1Gu6

        Via: SIP/2.0/TCP 10.112.87.250:42127;rport=42127;received=10.112.87.250;branch=z9hG4bKPj6ze-NaIX4.JPNaszyxOgTN3gp5j7Ngwp

        Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

        Via: SIP/2.0/TCP 10.112.87.250:42466;rport=42466;received=10.112.87.250;branch=z9hG4bKPj5xhgQqIs5lRKtimI4E9pvUXW2EhTqghw

        Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

        Record-Route: <sip:LKP68ldZs3 at cw-aio:5060;transport=TCP;lr>

        Via: SIP/2.0/TCP 10.112.123.187:43259;received=10.112.123.187;branch=z9hG4bK-524287-1---c83e12d46aead571

        Max-Forwards: 66

        Contact: <sip:6505550962 at 10.112.123.187:43259;transport=tcp>

        To: <sip:1234 at example.com>

        From: <sip:6505550962 at example.com>;tag=eb40917f

        Call-ID: VAescKO7V6NoEuXLTrcYVg..

        CSeq: 1 INVITE

        Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE

        Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri

        User-Agent: Z 3.9.32144 r32121

        Allow-Events: presence, kpml

        P-Asserted-Identity: <sip:6505550962 at example.com>

        Session-Expires: 600

        P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg

        Content-Type: application/sdp

        Content-Length:   243

    Message Body

        Session Description Protocol



Regards

Surender Singh











-----------------Write in Shared_config file------------------ enum_file=enum.json







{

    "routes" : [

        {   "name" : "Clearwater AIO_TO_PBX",

          "domain" : "10.112.87.177",

          "route" : ["sip:cw-aio:5058"]

        }

    ]



}







{

    "number_blocks" : [

        {

           "name" : "Internal numbers",

           "prefix" : "650555",

           "regex" : "!(^.*$)!sip:\\1 at example.com!"

        },

        {

            "name" : "External numbers",

            "prefix" : "",

            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177;transport=TCP!"

        }

    ]

}





-----Original Message-----

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org<mailto:clearwater-request at lists.projectclearwater.org>

Sent: 16 September 2016 18:04

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: Clearwater Digest, Vol 41, Issue 28



Send Clearwater mailing list submissions to

                clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>



To subscribe or unsubscribe via the World Wide Web, visit

                http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



or, via email, send a message with subject or body 'help' to

                clearwater-request at lists.projectclearwater.org<mailto:clearwater-request at lists.projectclearwater.org>



You can reach the person managing the list at

                clearwater-owner at lists.projectclearwater.org<mailto:clearwater-owner at lists.projectclearwater.org>



When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."





Today's Topics:



   1. Mininet (chess man)

   2.  Performance limit measurement (??????? ?????????)

   3. Re: Mininet (Graeme Robertson (projectclearwater.org))

   4. Re: SIP Trunking With PBX_aio

      (Graeme Robertson (projectclearwater.org))





----------------------------------------------------------------------



Message: 1

Date: Fri, 16 Sep 2016 09:02:07 +0100

From: chess man <chessmancaryl at gmail.com<mailto:chessmancaryl at gmail.com>>

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: [Project Clearwater] Mininet

Message-ID:

                <CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw at mail.gmail.com<mailto:CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw at mail.gmail.com>>

Content-Type: text/plain; charset="utf-8"



Hi everybody,



I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/978b47a5/attachment-0001.html>



------------------------------



Message: 2

Date: Fri, 16 Sep 2016 12:16:42 +0300

From: ??????? ?????????             <michaelkatsoulis88 at gmail.com<mailto:michaelkatsoulis88 at gmail.com>>

To: Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>

Subject: [Project Clearwater]  Performance limit measurement

Message-ID:

                <CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA at mail.gmail.com<mailto:CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA at mail.gmail.com>>

Content-Type: text/plain; charset="utf-8"



Hi all,



we are running Stress Tests against our Clearwater Deployment using Sip Stress node.

We have noticed that the results are not consistent as the number of successfull calls changes during repetitions of the same test scenario.



We have tried to increase the values of max_tokens , init_token_rate, min_token_rate and target_latency_us but we did not observe any difference.



What is the proposed way to discover the deployment's limit on how many requests per second can be served?



Thanks in advance,

Michael Katsoulis

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/27a77df2/attachment-0001.html>



------------------------------



Message: 3

Date: Fri, 16 Sep 2016 12:20:26 +0000

From: "Graeme Robertson (projectclearwater.org)"

                <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>

To: "clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>"

                <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>

Subject: Re: [Project Clearwater] Mininet

Message-ID:

                <CY4PR02MB2616A144FD60BFC5F7796B1CE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com<mailto:CY4PR02MB2616A144FD60BFC5F7796B1CE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>>



Content-Type: text/plain; charset="utf-8"



Hello,



There is no reason why it shouldn?t just work, but as far as I know nobody has ever tried to install Project Clearwater on Mininet. If our AIO OVA doesn?t just work then you may have to follow our manual install instructions<http://clearwater.readthedocs.io/en/stable/Manual_Install.html>.



It would be interesting to hear how you get on!



Thanks,

Graeme



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of chess man

Sent: 16 September 2016 09:02

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: [Project Clearwater] Mininet



Hi everybody,

I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/782ad823/attachment-0001.html>



------------------------------



Message: 4

Date: Fri, 16 Sep 2016 12:33:07 +0000

From: "Graeme Robertson (projectclearwater.org)"

                <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>

To: "clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>"

                <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>

Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio

Message-ID:

                <CY4PR02MB2616B3B7962F78B040BAAD8BE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com<mailto:CY4PR02MB2616B3B7962F78B040BAAD8BE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>>



Content-Type: text/plain; charset="us-ascii"



Hi Surender,



I'm not sure what you mean about not being able to reply through the mailing list - I think this email went to the mailing list :).



That is odd. What is log_level set to if you run ' log_level="";. /etc/clearwater/config; echo $log_level'? And what is it set to if you run 'log_level="";. /etc/clearwater/user_settings; echo $log_level'?



The /etc/clearwater/config file is the config file that all of our processes actually read from. As you can see, this file pulls in all of our other config files (shared_config, local_config and user_settings).



Thanks,

Graeme



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh

Sent: 16 September 2016 07:18

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio



Hi Graeme,





I am not able to reply through mailing list as I already created the account.



I requesting you for quick response...





ims.hcl.com is another deployment.





I putted the said line in user_setting file and  restart the sprout but showing Log level set to 2 ,fine below logs and also find the attached sprout logs .





[cw-aio]root at cw-aio:~# sudo service sprout restart

* Restarting Sprout SIP Router sprout                                          Log directory set to /var/log/sprout

Log level set to 2

16-09-2016 11:10:01.436 UTC Status utils.cpp:494: Switching to daemon mode

                                                                         [ OK ]



-----user_settings file is under the path etc/Clearwater is----



trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>"

enum_file="enum.json"

log_level=5









I have one more query, what is meaning of config file under the path /etc/Clearwater



-------config  file------------------

if [ -f /etc/clearwater/shared_config ]

then

  . /etc/clearwater/shared_config

fi



. /etc/clearwater/local_config



if [ -f /etc/clearwater/user_settings ]

then

  . /etc/clearwater/user_settings

fi





Regards

Surender Singh



-------------------------------------------------------------



Hi Surender,



I noticed there are lots of instances of "Invalid ENUM response:

sip:@example.com" so it looks as though something is misconfigured, but it's not immediately obvious what. I think it would be useful to turn on debug logging for Sprout in order to dig into this issue further. In order to do this, add the line 'log_level=5' to /etc/clearwater/user_settings (creating it if it doesn't exist) and restart Sprout (by running sudo service sprout restart). This will cause far more detailed logs to be written to the Sprout log files.



Incidentally, I thought (from an earlier thread) that you'd changed your home domain from example.com to ims.hcl.com - is that on a different deployment? I just wanted to make sure there was no confusion here.



Thanks,

Graeme





From: Surender Singh

Sent: 15 September 2016 17:54

To: 'clearwater at lists.projectclearwater.org' <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org%3cmailto:clearwater at lists.projectclearwater.org>>>

Subject: RE: SIP Trunking With PBX_aio





Hi Team,



Kindly help



Please share the procedure to connect the SIP trunk with PBX.



I already tried many combination but  in/out calls failing in IMS.



When I also checking IBCF functionality are enabled or not then showing below o/p



[cw-aio]ubuntu at cw-aio:~$ sudo /usr/share/clearwater/bin/bono

/usr/share/clearwater/bin/bono: error while loading shared libraries: libmemcached.so.11: cannot open shared object file: No such file or directory [cw-aio]ubuntu at cw-aio:~$



Regards

Surender Singh



From: Surender Singh

Sent: 14 September 2016 17:26

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org%3cmailto:clearwater at lists.projectclearwater.org>>

Subject: SIP Trunking With PBX_aio



Hi team,



I deployed the aio solution in my Vmware Exsi  and made the connectivity with Mitel IP-PBX .



As per doc and mailing list  I made the Following entry .





1)      As per mailing IBCF and BGCF functionality are provided by BONO



2)      I create the one user-settings file (already not created ) under the path =/etc/clearwater/user_settings  with entry like trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>" . Here 10.112.87.177 is my PBX ip.



3)      After that I make the entry for bfcg.json and enum.json under the path =/etc/clearwater/sample. And then run the command sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_bgcf_json and enum_json. Post this command both files created the path under /etc/Clearwater. Files contents are





-----------BGCF.JSON------------------





   {

      "routes" : [

        {   "name" : "Clearwater AIO_TO_PBX",

          "domain" : "10.112.87.177",

          "route" : ["sip:example.com:5058","sip:10.112.87.177:5060"]

        }

    ]

  }



------ENUM.JSON---------------------



{

    "number_blocks" : [

        {

           "name" : "Internal numbers",

           "prefix" : "650555",

           "regex" : "!(^.*$)!sip:\\1 at 10.112.87.250!"

        },



        {

            "name" : "External numbers",

            "prefix" : "",

            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"

        }

    ]

}







My IP Scheme are:



10.112.87.177 : PBX IP address

10.112.87.250: IMS IP address and Domain is example.com



10.112.123.187 : my System ip where Zoper client insall





My call Flow is:-



SIP Client A(Registered in PBX) calls to -> SIP Client B (Registered in ims with example.com domain )



When I make the call from A to B then call coming in IMS network but getting error 'Not found 404' because B number registered with xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com%3cmailto:xxxxxx at exmaple.com>> .





Please find the attached wireshark traces and BONO, Sprout logs







Note : I also not able to call from from B to A Also



Regards

Surender Singh













::DISCLAIMER::

----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.

E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents (with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.

Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification, distribution and / or publication of this message without the prior written consent of authorized representative of HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.

Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/12d206af/attachment.html>



------------------------------



Subject: Digest Footer



_______________________________________________

Clearwater mailing list

Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>

http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org





------------------------------



End of Clearwater Digest, Vol 41, Issue 28

******************************************
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/c0205317/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 41, Issue 33
******************************************



From michaelkatsoulis88 at gmail.com  Mon Sep 19 03:41:43 2016
From: michaelkatsoulis88 at gmail.com (=?UTF-8?B?zpzOuc+HzqzOu863z4IgzprOsc+Ez4POv8+NzrvOt8+C?=)
Date: Mon, 19 Sep 2016 10:41:43 +0300
Subject: [Project Clearwater] Performance limit measurement
In-Reply-To: <CY4PR02MB26163BAD3AB2F60F7D1A74BAE3F30@CY4PR02MB2616.namprd02.prod.outlook.com>
References: <CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA@mail.gmail.com>
	<CY4PR02MB26168AC7A5C61A9DD32EB0C2E3F30@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAJG3f9NLorKJEbEJbDh+gEKfbLfdqB0CQR1sv6J6pFtnbmy=+g@mail.gmail.com>
	<CY4PR02MB26163BAD3AB2F60F7D1A74BAE3F30@CY4PR02MB2616.namprd02.prod.outlook.com>
Message-ID: <CAJG3f9MrQq9Mr34hrpv1rgpR=6vpNP5g3tM6opTrM4BvnqwV5g@mail.gmail.com>

Hi Graeme,

i created a simpler scenario comparing to what the Sip Stress testing uses.
In each scenario two subscribers try just to register to IMS and do not
make any call to each other. I run this scenario for 15000 pairs of
subscribers (30000 subscribers). The register requests are distributed in 1
minute time. It seems tha Sprout node is the bottleneck. The return code of
most of the failed messages is  503 (Service Unavailable) and of some of
them 408 (Request Timeout). I have added resources in Sprout (4 CPUs and
8Gb memory) so i don't believe that resources is the issue.

Does Sprout somehow exposes the latency measurements that lead to the
throttling? We would like to take a look at them.



 *Here is the the xml file* .


<scenario name="Call Load Test">

  <User variables="my_dn,peer_dn,call_repeat" />
  <nop hide="true">
    <action>
      <!-- Get my and peer's DN -->
      <assignstr assign_to="my_dn" value="[field0]" />
      <!-- field1 is my_auth, but we can't store it in a variable -->
      <assignstr assign_to="peer_dn" value="[field2]" />
      <!-- field3 is peer_auth, but we can't store it in a variable -->
      <assign assign_to="reg_repeat" value="0"/>
      <assign assign_to="call_repeat" value="0"/>
    </action>
  </nop>

  <pause distribution="uniform" min="0" max="60000" />

  <send>
    <![CDATA[

      REGISTER sip:[$my_dn]@[service] SIP/2.0
      Via: SIP/2.0/[transport]
[local_ip]:[local_port];rport;branch=[branch]-[$my_dn]-[$reg_repeat]
      Route: <sip:[service];transport=[transport];lr>
      Max-Forwards: 70
      From: <sip:[$my_dn]@[service]>;tag=[pid]SIPpTag00[call_number]
      To: <sip:[$my_dn]@[service]>
      Call-ID: [$my_dn]///[call_id]
      CSeq: [cseq] REGISTER
      User-Agent: Accession 4.0.0.0
      Supported: outbound, path
      Contact:
<sip:[$my_dn]@[local_ip]:[local_port];transport=[transport];ob>;+sip.ice;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
      Expires: 3600
      Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, SUBSCRIBE, NOTIFY,
REFER, MESSAGE, OPTIONS
      Content-Length: 0

    ]]>
  </send>

  <recv response="401" auth="true">
    <action>
      <add assign_to="reg_repeat" value="1" />
    </action>
  </recv>

  <send>
    <![CDATA[

      REGISTER sip:[$my_dn]@[service] SIP/2.0
      Via: SIP/2.0/[transport]
[local_ip]:[local_port];rport;branch=[branch]-[$my_dn]-[$reg_repeat]
      Route: <sip:[service];transport=[transport];lr>
      Max-Forwards: 70
      From: <sip:[$my_dn]@[service]>;tag=[pid]SIPpTag00[call_number]
      To: <sip:[$my_dn]@[service]>
      Call-ID: [$my_dn]///[call_id]
      CSeq: [cseq] REGISTER
      User-Agent: Accession 4.0.0.0
      Supported: outbound, path
      Contact:
<sip:[$my_dn]@[local_ip]:[local_port];transport=[transport];ob>;+sip.ice;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
      Expires: 3600
      [field1]
      Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, SUBSCRIBE, NOTIFY,
REFER, MESSAGE, OPTIONS
      Content-Length: 0

    ]]>
  </send>

  <recv response="200">
    <action>
      <ereg regexp="rport=([^;]*);.*received=([^;]*);" search_in="hdr"
header="Via:" assign_to="dummy" />
      <add assign_to="reg_repeat" value="1" />
    </action>
  </recv>
  <Reference variables="dummy" />

  <send>
    <![CDATA[

      REGISTER sip:[$peer_dn]@[service] SIP/2.0
      Via: SIP/2.0/[transport]
[local_ip]:[local_port];rport;branch=[branch]-[$peer_dn]-[$reg_repeat]
      Route: <sip:[service];transport=[transport];lr>
      Max-Forwards: 70
      From: <sip:[$peer_dn]@[service]>;tag=[pid]SIPpTag00[call_number]
      To: <sip:[$peer_dn]@[service]>
      Call-ID: [$peer_dn]///[call_id]
      CSeq: [cseq] REGISTER
      User-Agent: Accession 4.0.0.0
      Supported: outbound, path
      Contact:
<sip:[$peer_dn]@[local_ip]:[local_port];transport=[transport];ob>;+sip.ice;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
      Expires: 3600
      Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, SUBSCRIBE, NOTIFY,
REFER, MESSAGE, OPTIONS
      Content-Length: 0

    ]]>
  </send>

  <recv response="401" auth="true">
    <action>
      <add assign_to="reg_repeat" value="1" />
    </action>
  </recv>

  <send>
    <![CDATA[

      REGISTER sip:[$peer_dn]@[service] SIP/2.0
      Via: SIP/2.0/[transport]
[local_ip]:[local_port];rport;branch=[branch]-[$peer_dn]-[$reg_repeat]
      Route: <sip:[service];transport=[transport];lr>
      Max-Forwards: 70
      From: <sip:[$peer_dn]@[service]>;tag=[pid]SIPpTag00[call_number]
      To: <sip:[$peer_dn]@[service]>
      Call-ID: [$peer_dn]///[call_id]
      CSeq: [cseq] REGISTER
      User-Agent: Accession 4.0.0.0
      Supported: outbound, path
      Contact:
<sip:[$peer_dn]@[local_ip]:[local_port];transport=[transport];ob>;+sip.ice;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
      Expires: 3600
      [field3]
      Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, SUBSCRIBE, NOTIFY,
REFER, MESSAGE, OPTIONS
      Content-Length: 0

    ]]>
  </send>

  <recv response="200">
    <action>
      <add assign_to="reg_repeat" value="1" />
    </action>
  </recv>

</scenario>


Best Regards,
Michael Katsoulis



2016-09-16 21:25 GMT+03:00 Graeme Robertson (projectclearwater.org) <
graeme at projectclearwater.org>:

> Hi Michael,
>
>
>
> Can you tell me more about your scenario? It sounds like you?re not using
> the clearwater-sip-stress package, or at least not in exactly the form we
> package up. If you?re not using the clearwater-sip-stress package then
> please can you send details of your stress scenario?
>
>
>
> Depending on how powerful your Sprout node is, I would expect 15000 calls
> per second to be towards the upper limit of its performance powers.
> However, if the CPU is not particularly high then that would suggest that
> Sprout?s throttling controls might require further tuning. Do you know what
> return code the ?unexpected messages? have? 503s indicate that there is
> overload somewhere. Sprout does adjust its throttling controls to match the
> load its able to process, but that process is not immediate, and we
> recommend building stress up gradually rather than immediately firing 15000
> calls per second into the system ? for more information on that, see
> http://www.projectclearwater.org/clearwater-performance-
> and-our-load-monitor/.
>
>
>
> One final thought I had was that the node you?re running stress on might
> be overloaded. If the stress node is not responding to messages in a timely
> fashion then that will generate time outs and unexpected messages.
>
>
>
> Thanks,
> Graeme
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *??????? ?ats?????
> *Sent:* 16 September 2016 15:16
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Performance limit measurement
>
>
>
> Hi Graeme,
>
>
>
> thanks a lot for your response.
>
>
>
> In our scenario we are using the Stress node to generate 15000 calls in 60
> seconds. The number of
>
> unsuccessful calls varies from ~500 to ~5000 even in subsequent
> repetitions of the same scenario.
>
> According to wireshark the failures happen because of Sprout that does not
> send the correct responses in time
>
> and so we get "time-outs" and "unexpected messages" in the Stress node.
>
> The Sprout node has sufficient CPU and memory resources.
>
> What could be the reason of this instability in our deployment?
>
>
>
> Thank you in advance,
>
> Michael Katsoulis
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> 2016-09-16 16:14 GMT+03:00 Graeme Robertson (projectclearwater.org) <
> graeme at projectclearwater.org>:
>
> Hi Michael,
>
>
>
> How many successes and failures are you seeing? We primarily use the
> clearwater-sip-stress package to check we haven?t introduced crashes under
> load, and to check we haven?t significantly regressed the performance of
> Project Clearwater. Unfortunately clearwater-sip-stress is not reliable
> enough to generate completely accurate performance numbers for Project
> Clearwater (and we don?t accurately measure Project Clearwater performance
> or provide numbers). We tend to see around 1% failures when running
> clearwater-sip-stress. If your failure numbers are fluctuating at around 1%
> then this is probably down to the test scripts not being completely
> reliable, and you won?t have actually hit the deployment?s limit until you
> start seeing more failures than this.
>
>
>
> Thanks,
>
> Graeme
>
>
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *??????? ?ats?????
> *Sent:* 16 September 2016 10:17
> *To:* Clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Performance limit measurement
>
>
>
> Hi all,
>
>
>
> we are running Stress Tests against our Clearwater Deployment using Sip
> Stress node.
>
> We have noticed that the results are not consistent as the number of
> successfull calls changes during repetitions of the same test scenario.
>
>
>
> We have tried to increase the values of max_tokens , init_token_rate,
> min_token_rate and
>
> target_latency_us but we did not observe any difference.
>
>
>
> What is the proposed way to discover the deployment's limit on how many
> requests per second can
>
> be served?
>
>
>
> Thanks in advance,
>
> Michael Katsoulis
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160919/60fac3ff/attachment.html>

From surender.s at hcl.com  Mon Sep 19 06:03:54 2016
From: surender.s at hcl.com (Surender Singh)
Date: Mon, 19 Sep 2016 10:03:54 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E188D8@NDA-HCLT-MBS05.hclt.corp.hcl.in>

Hi Graeme,

Sorry for inconvenience..

I am able to send invite to PBX .

But issue while PBX going to terminated the calls ,reason may be To header .

In  To header  IMS sending 1234 at example.com instead of 1234 at 10.112.87.177

Can you suggest me how to rewrite the To -header in IMS .

Same issue in incoming calls, where  not able to rewrite the domain ..from 6505550962 at 10.112.87.177 to 6505550962 at example.com.

My call Scenario are :

A Number = 1234 and B Number 6505550963;

A (registered at PBX) calls to B (registered at IMS) and vice versa.

Please also find the attached traces of Bono & Sprout nodes.

Regards
Surender Singh







-----Original Message-----
From: Surender Singh 
Sent: 19 September 2016 11:31
To: clearwater at lists.projectclearwater.org
Subject: SIP Trunking With PBX_aio

Hi Graeme,

I just make the entry in shared _config file of enum.json file. after that  enum look up started.

enum_file=enum.json

But Invite is are still not reaching to PBX (Captured the Wireshark at pbx ).Might be calls are not going outside the IMS .

Please go through my configuration, and help rectify the problem.


I have bit confusion regarding call flow (incoming and outgoing calls) in All in one node solution .

Please help what will be call flow for in/out calls.

Regards
Surender Singh








-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 17 September 2016 00:09
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 41, Issue 33

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. Re: SIP Trunking With PBX_aio
      (Graeme Robertson (projectclearwater.org))


----------------------------------------------------------------------

Message: 1
Date: Fri, 16 Sep 2016 18:38:41 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID:
	<CY4PR02MB26169F0D2D19946AC837C595E3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="us-ascii"

Hi Surender,



It's great to hear that you're making progress.



It sounds like there's a problem with your user_settings file - is it definitely in the /etc/clearwater directory? You suggested earlier it might be in /etc/Clearwater but I assumed that was a typo at the time. Anyway, it sounds like we're past the problems with Sprout, which is good. Unfortunately, I'm afraid I don't completely understand what your current issue is - please can you be more specific? It sounds like the INVITE is successfully being routed out to your PBX via the IBCF and your PBX is rejecting the INVITE with a 404. The INVITE that's being sent to the PBX looks sensible to me, so I think you need to work out why the PBX is rejecting it.



Thanks,

Graeme



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 16 September 2016 14:28
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP Trunking With PBX_aio



Hi Graeme,



Sorry I am not  able to understand regarding logs, I just changes the value in user_settings file and restart the sprout like below.



log_level="5"



But now after adding the below line in shared_config file , enum query is happing and able rewrite URI . IBFG/BGCF also able forward the packet at my pbx ip 10.112.87.177 but getting release from PBX 404.



I also disabled the lr at pbx.



Find below sip logs





Frame 7: 1665 bytes on wire (13320 bits), 1665 bytes captured (13320 bits) Linux cooked capture Internet Protocol Version 4, Src: 10.112.87.250, Dst: 10.112.87.177 Transmission Control Protocol, Src Port: 59955, Dst Port: 5060, Seq: 1, Ack: 1, Len: 1597 Session Initiation Protocol (INVITE)

    Request-Line: INVITE sip:1234 at 10.112.87.177;transport=TCP SIP/2.0

    Message Header

        Via: SIP/2.0/TCP 10.112.87.250:59955;rport;branch=z9hG4bKPja-Y65hsmJxWUbzhJeod0RuunKpnx1Gu6

        Via: SIP/2.0/TCP 10.112.87.250:42127;rport=42127;received=10.112.87.250;branch=z9hG4bKPj6ze-NaIX4.JPNaszyxOgTN3gp5j7Ngwp

        Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

        Via: SIP/2.0/TCP 10.112.87.250:42466;rport=42466;received=10.112.87.250;branch=z9hG4bKPj5xhgQqIs5lRKtimI4E9pvUXW2EhTqghw

        Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

        Record-Route: <sip:LKP68ldZs3 at cw-aio:5060;transport=TCP;lr>

        Via: SIP/2.0/TCP 10.112.123.187:43259;received=10.112.123.187;branch=z9hG4bK-524287-1---c83e12d46aead571

        Max-Forwards: 66

        Contact: <sip:6505550962 at 10.112.123.187:43259;transport=tcp>

        To: <sip:1234 at example.com>

        From: <sip:6505550962 at example.com>;tag=eb40917f

        Call-ID: VAescKO7V6NoEuXLTrcYVg..

        CSeq: 1 INVITE

        Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE

        Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri

        User-Agent: Z 3.9.32144 r32121

        Allow-Events: presence, kpml

        P-Asserted-Identity: <sip:6505550962 at example.com>

        Session-Expires: 600

        P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg

        Content-Type: application/sdp

        Content-Length:   243

    Message Body

        Session Description Protocol



Regards

Surender Singh











-----------------Write in Shared_config file------------------ enum_file=enum.json







{

    "routes" : [

        {   "name" : "Clearwater AIO_TO_PBX",

          "domain" : "10.112.87.177",

          "route" : ["sip:cw-aio:5058"]

        }

    ]



}







{

    "number_blocks" : [

        {

           "name" : "Internal numbers",

           "prefix" : "650555",

           "regex" : "!(^.*$)!sip:\\1 at example.com!"

        },

        {

            "name" : "External numbers",

            "prefix" : "",

            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177;transport=TCP!"

        }

    ]

}





-----Original Message-----

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org<mailto:clearwater-request at lists.projectclearwater.org>

Sent: 16 September 2016 18:04

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: Clearwater Digest, Vol 41, Issue 28



Send Clearwater mailing list submissions to

                clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>



To subscribe or unsubscribe via the World Wide Web, visit

                http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



or, via email, send a message with subject or body 'help' to

                clearwater-request at lists.projectclearwater.org<mailto:clearwater-request at lists.projectclearwater.org>



You can reach the person managing the list at

                clearwater-owner at lists.projectclearwater.org<mailto:clearwater-owner at lists.projectclearwater.org>



When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."





Today's Topics:



   1. Mininet (chess man)

   2.  Performance limit measurement (??????? ?????????)

   3. Re: Mininet (Graeme Robertson (projectclearwater.org))

   4. Re: SIP Trunking With PBX_aio

      (Graeme Robertson (projectclearwater.org))





----------------------------------------------------------------------



Message: 1

Date: Fri, 16 Sep 2016 09:02:07 +0100

From: chess man <chessmancaryl at gmail.com<mailto:chessmancaryl at gmail.com>>

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: [Project Clearwater] Mininet

Message-ID:

                <CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw at mail.gmail.com<mailto:CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw at mail.gmail.com>>

Content-Type: text/plain; charset="utf-8"



Hi everybody,



I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/978b47a5/attachment-0001.html>



------------------------------



Message: 2

Date: Fri, 16 Sep 2016 12:16:42 +0300

From: ??????? ?????????             <michaelkatsoulis88 at gmail.com<mailto:michaelkatsoulis88 at gmail.com>>

To: Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>

Subject: [Project Clearwater]  Performance limit measurement

Message-ID:

                <CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA at mail.gmail.com<mailto:CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA at mail.gmail.com>>

Content-Type: text/plain; charset="utf-8"



Hi all,



we are running Stress Tests against our Clearwater Deployment using Sip Stress node.

We have noticed that the results are not consistent as the number of successfull calls changes during repetitions of the same test scenario.



We have tried to increase the values of max_tokens , init_token_rate, min_token_rate and target_latency_us but we did not observe any difference.



What is the proposed way to discover the deployment's limit on how many requests per second can be served?



Thanks in advance,

Michael Katsoulis

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/27a77df2/attachment-0001.html>



------------------------------



Message: 3

Date: Fri, 16 Sep 2016 12:20:26 +0000

From: "Graeme Robertson (projectclearwater.org)"

                <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>

To: "clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>"

                <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>

Subject: Re: [Project Clearwater] Mininet

Message-ID:

                <CY4PR02MB2616A144FD60BFC5F7796B1CE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com<mailto:CY4PR02MB2616A144FD60BFC5F7796B1CE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>>



Content-Type: text/plain; charset="utf-8"



Hello,



There is no reason why it shouldn?t just work, but as far as I know nobody has ever tried to install Project Clearwater on Mininet. If our AIO OVA doesn?t just work then you may have to follow our manual install instructions<http://clearwater.readthedocs.io/en/stable/Manual_Install.html>.



It would be interesting to hear how you get on!



Thanks,

Graeme



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of chess man

Sent: 16 September 2016 09:02

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: [Project Clearwater] Mininet



Hi everybody,

I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/782ad823/attachment-0001.html>



------------------------------



Message: 4

Date: Fri, 16 Sep 2016 12:33:07 +0000

From: "Graeme Robertson (projectclearwater.org)"

                <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>

To: "clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>"

                <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>

Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio

Message-ID:

                <CY4PR02MB2616B3B7962F78B040BAAD8BE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com<mailto:CY4PR02MB2616B3B7962F78B040BAAD8BE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>>



Content-Type: text/plain; charset="us-ascii"



Hi Surender,



I'm not sure what you mean about not being able to reply through the mailing list - I think this email went to the mailing list :).



That is odd. What is log_level set to if you run ' log_level="";. /etc/clearwater/config; echo $log_level'? And what is it set to if you run 'log_level="";. /etc/clearwater/user_settings; echo $log_level'?



The /etc/clearwater/config file is the config file that all of our processes actually read from. As you can see, this file pulls in all of our other config files (shared_config, local_config and user_settings).



Thanks,

Graeme



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh

Sent: 16 September 2016 07:18

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio



Hi Graeme,





I am not able to reply through mailing list as I already created the account.



I requesting you for quick response...





ims.hcl.com is another deployment.





I putted the said line in user_setting file and  restart the sprout but showing Log level set to 2 ,fine below logs and also find the attached sprout logs .





[cw-aio]root at cw-aio:~# sudo service sprout restart

* Restarting Sprout SIP Router sprout                                          Log directory set to /var/log/sprout

Log level set to 2

16-09-2016 11:10:01.436 UTC Status utils.cpp:494: Switching to daemon mode

                                                                         [ OK ]



-----user_settings file is under the path etc/Clearwater is----



trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>"

enum_file="enum.json"

log_level=5









I have one more query, what is meaning of config file under the path /etc/Clearwater



-------config  file------------------

if [ -f /etc/clearwater/shared_config ]

then

  . /etc/clearwater/shared_config

fi



. /etc/clearwater/local_config



if [ -f /etc/clearwater/user_settings ]

then

  . /etc/clearwater/user_settings

fi





Regards

Surender Singh



-------------------------------------------------------------



Hi Surender,



I noticed there are lots of instances of "Invalid ENUM response:

sip:@example.com" so it looks as though something is misconfigured, but it's not immediately obvious what. I think it would be useful to turn on debug logging for Sprout in order to dig into this issue further. In order to do this, add the line 'log_level=5' to /etc/clearwater/user_settings (creating it if it doesn't exist) and restart Sprout (by running sudo service sprout restart). This will cause far more detailed logs to be written to the Sprout log files.



Incidentally, I thought (from an earlier thread) that you'd changed your home domain from example.com to ims.hcl.com - is that on a different deployment? I just wanted to make sure there was no confusion here.



Thanks,

Graeme





From: Surender Singh

Sent: 15 September 2016 17:54

To: 'clearwater at lists.projectclearwater.org' <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org%3cmailto:clearwater at lists.projectclearwater.org>>>

Subject: RE: SIP Trunking With PBX_aio





Hi Team,



Kindly help



Please share the procedure to connect the SIP trunk with PBX.



I already tried many combination but  in/out calls failing in IMS.



When I also checking IBCF functionality are enabled or not then showing below o/p



[cw-aio]ubuntu at cw-aio:~$ sudo /usr/share/clearwater/bin/bono

/usr/share/clearwater/bin/bono: error while loading shared libraries: libmemcached.so.11: cannot open shared object file: No such file or directory [cw-aio]ubuntu at cw-aio:~$



Regards

Surender Singh



From: Surender Singh

Sent: 14 September 2016 17:26

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org%3cmailto:clearwater at lists.projectclearwater.org>>

Subject: SIP Trunking With PBX_aio



Hi team,



I deployed the aio solution in my Vmware Exsi  and made the connectivity with Mitel IP-PBX .



As per doc and mailing list  I made the Following entry .





1)      As per mailing IBCF and BGCF functionality are provided by BONO



2)      I create the one user-settings file (already not created ) under the path =/etc/clearwater/user_settings  with entry like trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>" . Here 10.112.87.177 is my PBX ip.



3)      After that I make the entry for bfcg.json and enum.json under the path =/etc/clearwater/sample. And then run the command sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_bgcf_json and enum_json. Post this command both files created the path under /etc/Clearwater. Files contents are





-----------BGCF.JSON------------------





   {

      "routes" : [

        {   "name" : "Clearwater AIO_TO_PBX",

          "domain" : "10.112.87.177",

          "route" : ["sip:example.com:5058","sip:10.112.87.177:5060"]

        }

    ]

  }



------ENUM.JSON---------------------



{

    "number_blocks" : [

        {

           "name" : "Internal numbers",

           "prefix" : "650555",

           "regex" : "!(^.*$)!sip:\\1 at 10.112.87.250!"

        },



        {

            "name" : "External numbers",

            "prefix" : "",

            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"

        }

    ]

}







My IP Scheme are:



10.112.87.177 : PBX IP address

10.112.87.250: IMS IP address and Domain is example.com



10.112.123.187 : my System ip where Zoper client insall





My call Flow is:-



SIP Client A(Registered in PBX) calls to -> SIP Client B (Registered in ims with example.com domain )



When I make the call from A to B then call coming in IMS network but getting error 'Not found 404' because B number registered with xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com%3cmailto:xxxxxx at exmaple.com>> .





Please find the attached wireshark traces and BONO, Sprout logs







Note : I also not able to call from from B to A Also



Regards

Surender Singh













::DISCLAIMER::

----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.

E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents (with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.

Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification, distribution and / or publication of this message without the prior written consent of authorized representative of HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.

Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/12d206af/attachment.html>



------------------------------



Subject: Digest Footer



_______________________________________________

Clearwater mailing list

Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>

http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org





------------------------------



End of Clearwater Digest, Vol 41, Issue 28

******************************************
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/c0205317/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 41, Issue 33
******************************************
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sprout_current.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160919/45243003/attachment.txt>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: bono_current.txt
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160919/45243003/attachment-0001.txt>

From surender.s at hcl.com  Tue Sep 20 02:41:41 2016
From: surender.s at hcl.com (Surender Singh)
Date: Tue, 20 Sep 2016 06:41:41 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E18DE7@NDA-HCLT-MBS05.hclt.corp.hcl.in>

Hi Graeme/Team,

Updating...............

1) Now Incoming calls are working from PBX to IMS .

2) For Outgoing calls ,able to send request Invite to PBX IP 10.112.87.177 but getting 404 no route to destination/Not found.

3) Can you suggest me what parameter (To Header or Request URI) are checked by the Peer node (PBX) to terminate the calls.

4) I also not able to understand configuration/calls flow  in BGCF.JSON. How IMS route the calls using BGCF.JSON.

Please find some below  logs of Sprout.... 



-----------------------------------------------------------------------------------------------------------------------------------------------------


INVITE sip:1234 at example.com;transport=TCP SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250:43682;rport;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Max-Forwards: 70
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>
To: <sip:1234 at example.com>
From: <sip:6505550962 at example.com>;tag=2826a779
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Allow-Events: presence, kpml
P-Asserted-Identity: <sip:6505550962 at example.com>
Session-Expires: 600
Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;orig>
Content-Type: application/sdp
Content-Length:   241

v=0
o=Z 0 0 IN IP4 10.112.123.57
s=Z
c=IN IP4 10.112.123.57
t=0 0
m=audio 8000 RTP/AVP 3 110 8 0 97 101
a=rtpmap:110 speex/8000
a=rtpmap:97 iLBC/8000
a=fmtp:97 mode=30
a=rtpmap:101 telephone-event/8000
a=fmtp:101 0-16
a=sendrecv

--end msg--
20-09-2016 10:24:56.217 UTC Debug pjutils.cpp:1660: Logging SAS Call-ID marker, Call-ID YmAzwVukLM_49wFK3reZXQ..
20-09-2016 10:24:56.217 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f7444100698 for worker threads
20-09-2016 10:24:56.217 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f7444100698
20-09-2016 10:24:56.217 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg INVITE/cseq=1 (rdata0x7f7444100698)
20-09-2016 10:24:56.217 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:24:56.217 UTC Debug uri_classifier.cpp:199: Classified URI as 4
20-09-2016 10:24:56.217 UTC Debug authentication.cpp:804: Authentication module invoked
20-09-2016 10:24:56.217 UTC Debug authentication.cpp:814: Request does not need authentication
20-09-2016 10:24:56.217 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:24:56.217 UTC Debug uri_classifier.cpp:199: Classified URI as 4
20-09-2016 10:24:56.217 UTC Debug basicproxy.cpp:92: Process INVITE request
20-09-2016 10:24:56.217 UTC Verbose sproutletproxy.cpp:498: Sproutlet Proxy transaction (0x7f743c071ca0) created
20-09-2016 10:24:56.217 UTC Debug basicproxy.cpp:1271: Report SAS start marker - trail (1e6e)
20-09-2016 10:24:56.217 UTC Debug pjutils.cpp:674: Cloned Request msg INVITE/cseq=1 (rdata0x7f7444100698) to tdta0x7f743c0d8c20
20-09-2016 10:24:56.217 UTC Debug pjsip: tsx0x7f743c0db Transaction created for Request msg INVITE/cseq=1 (rdata0x7f7444100698)
20-09-2016 10:24:56.217 UTC Debug pjsip: tsx0x7f743c0db Incoming Request msg INVITE/cseq=1 (rdata0x7f7444100698) in state Null
20-09-2016 10:24:56.217 UTC Debug pjsip: tsx0x7f743c0db State changed from Null to Trying, event=RX_MSG
20-09-2016 10:24:56.217 UTC Debug basicproxy.cpp:213: tsx0x7f743c0dbc38 - tu_on_tsx_state UAS, TSX_STATE RX_MSG state=Trying
20-09-2016 10:24:56.217 UTC Debug pjsip:       endpoint Response msg 408/INVITE/cseq=1 (tdta0x7f743c0c3980) created
20-09-2016 10:24:56.217 UTC Debug basicproxy.cpp:586: Send immediate 100 Trying response
20-09-2016 10:24:56.217 UTC Debug pjsip: tsx0x7f743c0db Sending Response msg 100/INVITE/cseq=1 (tdta0x7f743c0c5940) in state Trying
20-09-2016 10:24:56.217 UTC Verbose common_sip_processing.cpp:136: TX 514 bytes Response msg 100/INVITE/cseq=1 (tdta0x7f743c0c5940) to TCP 10.112.87.250:43682:
--start msg--

SIP/2.0 100 Trying
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Call-ID: YmAzwVukLM_49wFK3reZXQ..
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>
CSeq: 1 INVITE
Content-Length:  0


--end msg--
20-09-2016 10:24:56.217 UTC Debug pjsip: tsx0x7f743c0db State changed from Trying to Proceeding, event=TX_MSG
20-09-2016 10:24:56.217 UTC Debug basicproxy.cpp:213: tsx0x7f743c0dbc38 - tu_on_tsx_state UAS, TSX_STATE TX_MSG state=Proceeding
20-09-2016 10:24:56.217 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request
20-09-2016 10:24:56.217 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:scscf.cw-aio:5054;transport=TCP;lr;orig
20-09-2016 10:24:56.217 UTC Debug sproutletproxy.cpp:329: Possible service name - scscf
20-09-2016 10:24:56.217 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio
20-09-2016 10:24:56.217 UTC Debug sproutletproxy.cpp:329: Possible service name - scscf
20-09-2016 10:24:56.217 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio
20-09-2016 10:24:56.217 UTC Debug sproutletproxy.cpp:329: Possible service name - scscf
20-09-2016 10:24:56.217 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio
20-09-2016 10:24:56.217 UTC Debug sproutletproxy.cpp:329: Possible service name - scscf
20-09-2016 10:24:56.217 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio
20-09-2016 10:24:56.217 UTC Debug scscfsproutlet.cpp:389: S-CSCF Transaction (0x7f743c098140) created
20-09-2016 10:24:56.217 UTC Verbose sproutletproxy.cpp:1154: Created Sproutlet scscf-0x7f743c098140 for Request msg INVITE/cseq=1 (tdta0x7f743c0d8c20)
20-09-2016 10:24:56.217 UTC Verbose sproutletproxy.cpp:2062: Routing Request msg INVITE/cseq=1 (tdta0x7f743c0d8c20) (1263 bytes) to downstream sproutlet scscf:
--start msg--

INVITE sip:1234 at example.com;transport=TCP SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Max-Forwards: 70
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>
To: <sip:1234 at example.com>
From: <sip:6505550962 at example.com>;tag=2826a779
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Allow-Events: presence, kpml
P-Asserted-Identity: <sip:6505550962 at example.com>
Session-Expires: 600
Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;orig>
Content-Type: application/sdp
Content-Length:   241

v=0
o=Z 0 0 IN IP4 10.112.123.57
s=Z
c=IN IP4 10.112.123.57
t=0 0
m=audio 8000 RTP/AVP 3 110 8 0 97 101
a=rtpmap:110 speex/8000
a=rtpmap:97 iLBC/8000
a=fmtp:97 mode=30
a=rtpmap:101 telephone-event/8000
a=fmtp:101 0-16
a=sendrecv

--end msg--
20-09-2016 10:24:56.217 UTC Debug pjutils.cpp:691: Cloned tdta0x7f743c0d8c20 to tdta0x7f743c0c9850
20-09-2016 10:24:56.217 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;orig>
20-09-2016 10:24:56.217 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f743c0c9e60 => txdata 0x7f743c0c98f8 mapping
20-09-2016 10:24:56.217 UTC Verbose sproutletproxy.cpp:1587: scscf-0x7f743c098140 pass initial request Request msg INVITE/cseq=1 (tdta0x7f743c0c9850) to Sproutlet
20-09-2016 10:24:56.217 UTC Info scscfsproutlet.cpp:431: S-CSCF received initial request
20-09-2016 10:24:56.217 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:24:56.217 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:24:56.217 UTC Debug scscfsproutlet.cpp:773: Route header references this system
20-09-2016 10:24:56.217 UTC Debug scscfsproutlet.cpp:826: No ODI token, or invalid ODI token, on request, and no P-Charging-Vector header (so can't log ICID for correlation)
20-09-2016 10:24:56.217 UTC Debug scscfsproutlet.cpp:832: Got our Route header, session case orig, OD=None
20-09-2016 10:24:56.217 UTC Debug pjutils.cpp:304: Served user from P-Asserted-Identity header
20-09-2016 10:24:56.217 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:24:56.218 UTC Debug uri_classifier.cpp:199: Classified URI as 4
20-09-2016 10:24:56.218 UTC Debug acr.cpp:49: Created ACR (0x7f743c015da0)
20-09-2016 10:24:56.218 UTC Debug scscfsproutlet.cpp:1015: Single Record-Route - initiation of originating handling
20-09-2016 10:24:56.218 UTC Debug session_expires_helper.cpp:124: Set session expires to 600
20-09-2016 10:24:56.218 UTC Debug sproutletproxy.cpp:387: Creating URI for scscf
20-09-2016 10:24:56.218 UTC Debug sproutletproxy.cpp:391: Add services parameter
20-09-2016 10:24:56.218 UTC Debug sproutletproxy.cpp:399: sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf
20-09-2016 10:24:56.218 UTC Debug scscfsproutlet.cpp:1021: Looking up iFCs for sip:6505550962 at example.com for new AS chain
20-09-2016 10:24:56.218 UTC Debug hssconnection.cpp:587: Making Homestead request for /impu/sip%3A6505550962%40example.com/reg-data
20-09-2016 10:24:56.218 UTC Debug httpresolver.cpp:72: HttpResolver::resolve for host 10.112.87.250, port 8888, family 2
20-09-2016 10:24:56.218 UTC Debug baseresolver.cpp:523: Attempt to parse 10.112.87.250 as IP address
20-09-2016 10:24:56.218 UTC Debug httpresolver.cpp:80: Target is an IP address
20-09-2016 10:24:56.218 UTC Debug connection_pool.h:225: Request for connection to IP: 10.112.87.250, port: 8888
20-09-2016 10:24:56.218 UTC Debug connection_pool.h:238: Found existing connection 0x7f745c08b760 in pool
20-09-2016 10:24:56.218 UTC Debug httpclient.cpp:466: Sending HTTP request : http://10.112.87.250:8888/impu/sip%3A6505550962%40example.com/reg-data (trying 10.112.87.250)
20-09-2016 10:24:56.220 UTC Debug httpclient.cpp:743: Received header http/1.1200ok with value 
20-09-2016 10:24:56.220 UTC Debug httpclient.cpp:743: Received header content-length with value 1366
20-09-2016 10:24:56.220 UTC Debug httpclient.cpp:743: Received header content-type with value text/plain
20-09-2016 10:24:56.220 UTC Debug httpclient.cpp:743: Received header  with value 
20-09-2016 10:24:56.220 UTC Debug httpclient.cpp:482: Received HTTP response: status=200, doc=<ClearwaterRegData>
	<RegistrationState>REGISTERED</RegistrationState>
	<IMSSubscription xsi="http://www.w3.org/2001/XMLSchema-instance" noNamespaceSchemaLocation="CxDataType.xsd">
		<PrivateID>Unspecified</PrivateID>
		<ServiceProfile>
			<InitialFilterCriteria>
				<TriggerPoint>
					<ConditionTypeCNF>0</ConditionTypeCNF>
					<SPT>
						<ConditionNegated>0</ConditionNegated>
						<Group>0</Group>
						<Method>INVITE</Method>
						<Extension/>
					</SPT>
				</TriggerPoint>
				<ApplicationServer>
					<ServerName>sip:mmtel.example.com</ServerName>
					<DefaultHandling>0</DefaultHandling>
				</ApplicationServer>
			</InitialFilterCriteria>
			<PublicIdentity>
				<Identity>sip:6505550962 at example.com</Identity>
			</PublicIdentity>
		</ServiceProfile>
		<ServiceProfile>
			<InitialFilterCriteria>
				<TriggerPoint>
					<ConditionTypeCNF>0</ConditionTypeCNF>
					<SPT>
						<ConditionNegated>0</ConditionNegated>
						<Group>0</Group>
						<Method>INVITE</Method>
						<Extension/>
					</SPT>
				</TriggerPoint>
				<ApplicationServer>
					<ServerName>sip:mmtel.example.com</ServerName>
					<DefaultHandling>0</DefaultHandling>
				</ApplicationServer>
			</InitialFilterCriteria>
			<PublicIdentity>
				<Identity>sip:6505550997 at example.com</Identity>
			</PublicIdentity>
		</ServiceProfile>
	</IMSSubscription>
</ClearwaterRegData>


20-09-2016 10:24:56.220 UTC Debug baseresolver.cpp:1004: Successful response from  10.112.87.250:8888 transport 6
20-09-2016 10:24:56.220 UTC Debug connection_pool.h:261: Release connection to IP: 10.112.87.250, port: 8888 to pool
20-09-2016 10:24:56.220 UTC Debug baseresolver.cpp:1034: 10.112.87.250:8888 transport 6 returned untested
20-09-2016 10:24:56.220 UTC Debug communicationmonitor.cpp:82: Checking communication changes - successful attempts 3, failures 0
20-09-2016 10:24:56.220 UTC Debug hssconnection.cpp:368: Processing Identity node from HSS XML - sip:6505550962 at example.com

20-09-2016 10:24:56.220 UTC Debug hssconnection.cpp:368: Processing Identity node from HSS XML - sip:6505550997 at example.com

20-09-2016 10:24:56.220 UTC Debug scscfsproutlet.cpp:1026: Successfully looked up iFCs
20-09-2016 10:24:56.220 UTC Debug aschain.cpp:71: Creating AsChain 0x7f743c071ad0 with 1 IFC and adding to map
20-09-2016 10:24:56.220 UTC Debug aschain.cpp:73: Attached ACR (0x7f743c015da0) to chain
20-09-2016 10:24:56.220 UTC Debug scscfsproutlet.cpp:1145: S-CSCF sproutlet transaction 0x7f743c098140 linked to AsChain AsChain-orig[0x7f743c071ad0]:1/1
20-09-2016 10:24:56.220 UTC Info scscfsproutlet.cpp:502: Found served user, so apply services
20-09-2016 10:24:56.220 UTC Debug scscfsproutlet.cpp:1153: Performing originating initiating request processing
20-09-2016 10:24:56.220 UTC Debug ifchandler.cpp:437: SPT class Method: result true
20-09-2016 10:24:56.220 UTC Debug ifchandler.cpp:541: Add to group 0 val true
20-09-2016 10:24:56.220 UTC Debug ifchandler.cpp:559: Result group 0 val true
20-09-2016 10:24:56.220 UTC Debug ifchandler.cpp:565: iFC matches
20-09-2016 10:24:56.220 UTC Debug aschain.cpp:203: Matched iFC AsChain-orig[0x7f743c071ad0]:1/1
20-09-2016 10:24:56.220 UTC Info ifchandler.cpp:684: Found (triggered) server sip:mmtel.example.com
20-09-2016 10:24:56.220 UTC Info scscfsproutlet.cpp:1308: Routing to Application Server sip:mmtel.example.com with ODI token odi_CpJtAOTjbG for AsChain-orig[0x7f743c071ad0]:1/1
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f743c0c9e60
20-09-2016 10:24:56.220 UTC Verbose sproutletproxy.cpp:1386: scscf-0x7f743c098140 sending Request msg INVITE/cseq=1 (tdta0x7f743c0c9850) on fork 0
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:459: Started Sproutlet timer, id = 140137199864608, duration = 2.000
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 1 timers
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f743c0c98f8, fork = 0
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:1914: scscf-0x7f743c098140 transmitting request on fork 0
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:1928: scscf-0x7f743c098140 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f743c0c9850) on fork 0
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f743c0c9e60 => txdata 0x7f743c0c98f8 mapping
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:mmtel.example.com;lr
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:329: Possible service name - mmtel
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:335: Hostname - example.com
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:329: Possible service name - mmtel
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:335: Hostname - example.com
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:329: Possible service name - mmtel
20-09-2016 10:24:56.220 UTC Debug sproutletproxy.cpp:335: Hostname - example.com
20-09-2016 10:24:56.220 UTC Debug mmtel.cpp:74: Found P-Served-User header: P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg
20-09-2016 10:24:56.220 UTC Debug mmtel.cpp:97: Fetching simservs configuration for sip:6505550962 at example.com
20-09-2016 10:24:56.221 UTC Debug httpresolver.cpp:72: HttpResolver::resolve for host 10.112.87.250, port 7888, family 2
20-09-2016 10:24:56.221 UTC Debug baseresolver.cpp:523: Attempt to parse 10.112.87.250 as IP address
20-09-2016 10:24:56.221 UTC Debug httpresolver.cpp:80: Target is an IP address
20-09-2016 10:24:56.221 UTC Debug connection_pool.h:225: Request for connection to IP: 10.112.87.250, port: 7888
20-09-2016 10:24:56.221 UTC Debug connection_pool.h:238: Found existing connection 0x7f744802b7e0 in pool
20-09-2016 10:24:56.221 UTC Debug httpclient.cpp:466: Sending HTTP request : http://10.112.87.250:7888/org.etsi.ngn.simservs/users/sip%3A6505550962%40example.com/simservs.xml (trying 10.112.87.250)
20-09-2016 10:24:56.225 UTC Debug httpclient.cpp:482: Received HTTP response: status=200, doc=<?xml version="1.0" encoding="UTF-8"?><simservs xmlns="http://uri.etsi.org/ngn/params/xml/simservs/xcap" xmlns:cp="urn:ietf:params:xml:ns:common-policy"><originating-identity-presentation active="true"/><originating-identity-presentation-restriction active="true"><default-behaviour>presentation-not-restricted</default-behaviour></originating-identity-presentation-restriction><communication-diversion active="true"><NoReplyTimer>30</NoReplyTimer><cp:ruleset/></communication-diversion><incoming-communication-barring active="true"><cp:ruleset><cp:rule id="rule0"><cp:conditions/><cp:actions><allow>true</allow></cp:actions></cp:rule></cp:ruleset></incoming-communication-barring><outgoing-communication-barring active="true"><cp:ruleset><cp:rule id="rule0"><cp:conditions/><cp:actions><allow>true</allow></cp:actions></cp:rule></cp:ruleset></outgoing-communication-barring></simservs>
20-09-2016 10:24:56.225 UTC Debug baseresolver.cpp:1004: Successful response from  10.112.87.250:7888 transport 6
20-09-2016 10:24:56.225 UTC Debug connection_pool.h:261: Release connection to IP: 10.112.87.250, port: 7888 to pool
20-09-2016 10:24:56.225 UTC Debug baseresolver.cpp:1034: 10.112.87.250:7888 transport 6 returned untested
20-09-2016 10:24:56.225 UTC Debug simservs.cpp:94: Processing simservs node: 'originating-identity-presentation'
20-09-2016 10:24:56.225 UTC Debug simservs.cpp:100: OIP enabled
20-09-2016 10:24:56.225 UTC Debug simservs.cpp:94: Processing simservs node: 'originating-identity-presentation-restriction'
20-09-2016 10:24:56.225 UTC Debug simservs.cpp:110: OIR enabled
20-09-2016 10:24:56.225 UTC Debug simservs.cpp:94: Processing simservs node: 'communication-diversion'
20-09-2016 10:24:56.225 UTC Debug simservs.cpp:127: CDIV enabled
20-09-2016 10:24:56.225 UTC Debug simservs.cpp:94: Processing simservs node: 'incoming-communication-barring'
20-09-2016 10:24:56.225 UTC Debug simservs.cpp:154: Inbound Call Barring enabled
20-09-2016 10:24:56.225 UTC Debug simservs.cpp:94: Processing simservs node: 'outgoing-communication-barring'
20-09-2016 10:24:56.225 UTC Debug simservs.cpp:178: Outbound Call Barring enabled
20-09-2016 10:24:56.225 UTC Debug mmtel.cpp:293: Found P-Served-User header: P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1154: Created Sproutlet mmtel-0x7f743c0dce40 for Request msg INVITE/cseq=1 (tdta0x7f743c0c9850)
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 100/INVITE/cseq=1 (tdta0x7f743c0ce700) (609 bytes) to upstream sproutlet scscf:
--start msg--

SIP/2.0 100 Trying
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Call-ID: YmAzwVukLM_49wFK3reZXQ..
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>
CSeq: 1 INVITE
Content-Length:  0


--end msg--
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f743c0ced10 => txdata 0x7f743c0ce7a8 mapping
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1623: scscf-0x7f743c098140 received provisional response Response msg 100/INVITE/cseq=1 (tdta0x7f743c0ce700) on fork 0, state = Proceeding
20-09-2016 10:24:56.225 UTC Info scscfsproutlet.cpp:561: S-CSCF received response
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:469: Cancelled Sproutlet timer, id = 140137199864608
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1413: scscf-0x7f743c098140 sending Response msg 100/INVITE/cseq=1 (tdta0x7f743c0ce700)
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 100
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1853: Discard 100/INVITE response (tdta0x7f743c0ce700)
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f743c0ced10 => txdata 0x7f743c0ce7a8 mapping
20-09-2016 10:24:56.225 UTC Debug pjsip: tdta0x7f743c0c Destroying txdata Response msg 100/INVITE/cseq=1 (tdta0x7f743c0ce700)
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:2062: Routing Request msg INVITE/cseq=1 (tdta0x7f743c0c9850) (1480 bytes) to downstream sproutlet mmtel:
--start msg--

INVITE sip:1234 at example.com;transport=TCP SIP/2.0
Route: <sip:mmtel.example.com;lr>
Route: <sip:odi_CpJtAOTjbG at 10.112.87.250:5054;lr;orig;service=scscf>
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Max-Forwards: 69
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>
To: <sip:1234 at example.com>
From: <sip:6505550962 at example.com>;tag=2826a779
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Allow-Events: presence, kpml
P-Asserted-Identity: <sip:6505550962 at example.com>
Session-Expires: 600
P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg
Content-Type: application/sdp
Content-Length:   241

v=0
o=Z 0 0 IN IP4 10.112.123.57
s=Z
c=IN IP4 10.112.123.57
t=0 0
m=audio 8000 RTP/AVP 3 110 8 0 97 101
a=rtpmap:110 speex/8000
a=rtpmap:97 iLBC/8000
a=fmtp:97 mode=30
a=rtpmap:101 telephone-event/8000
a=fmtp:101 0-16
a=sendrecv

--end msg--
20-09-2016 10:24:56.225 UTC Debug pjutils.cpp:691: Cloned tdta0x7f743c0c9850 to tdta0x7f743c0ce700
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:mmtel.example.com;lr>
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f743c0ced10 => txdata 0x7f743c0ce7a8 mapping
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1587: mmtel-0x7f743c0dce40 pass initial request Request msg INVITE/cseq=1 (tdta0x7f743c0ce700) to Sproutlet
20-09-2016 10:24:56.225 UTC Debug sproutletappserver.cpp:71: Store onward route-set for request
20-09-2016 10:24:56.225 UTC Debug sproutletappserver.cpp:77: Store header: Route: <sip:odi_CpJtAOTjbG at 10.112.87.250:5054;lr;orig;service=scscf>
20-09-2016 10:24:56.225 UTC Debug mmtel.cpp:796: Originating Identification Presentation Restriction enabled
20-09-2016 10:24:56.225 UTC Debug mmtel.cpp:831: Identity presentation is not restricted by default
20-09-2016 10:24:56.225 UTC Debug mmtel.cpp:713: Testing call against conditions (0x0)
20-09-2016 10:24:56.225 UTC Debug mmtel.cpp:680: Call barring rule allows call to continue
20-09-2016 10:24:56.225 UTC Debug sproutletappserver.cpp:213: Restore header: Route: <sip:odi_CpJtAOTjbG at 10.112.87.250:5054;lr;orig;service=scscf>
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f743c0ced10
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1386: mmtel-0x7f743c0dce40 sending Request msg INVITE/cseq=1 (tdta0x7f743c0ce700) on fork 0
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f743c0ce7a8, fork = 0
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1914: mmtel-0x7f743c0dce40 transmitting request on fork 0
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1928: mmtel-0x7f743c0dce40 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f743c0ce700) on fork 0
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f743c0ced10 => txdata 0x7f743c0ce7a8 mapping
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:odi_CpJtAOTjbG at 10.112.87.250:5054;lr;orig;service=scscf
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:284: Found services param - scscf
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:309: Found user - odi_CpJtAOTjbG
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:329: Possible service name - 10
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:335: Hostname - 112.87.250
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:284: Found services param - scscf
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:309: Found user - odi_CpJtAOTjbG
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:329: Possible service name - 10
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:335: Hostname - 112.87.250
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:284: Found services param - scscf
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:309: Found user - odi_CpJtAOTjbG
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:329: Possible service name - 10
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:335: Hostname - 112.87.250
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:284: Found services param - scscf
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:309: Found user - odi_CpJtAOTjbG
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:329: Possible service name - 10
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:335: Hostname - 112.87.250
20-09-2016 10:24:56.225 UTC Debug scscfsproutlet.cpp:389: S-CSCF Transaction (0x7f743c0cd8a0) created
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1154: Created Sproutlet scscf-0x7f743c0cd8a0 for Request msg INVITE/cseq=1 (tdta0x7f743c0ce700)
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 100/INVITE/cseq=1 (tdta0x7f743c0d1670) (609 bytes) to upstream sproutlet mmtel:
--start msg--

SIP/2.0 100 Trying
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Call-ID: YmAzwVukLM_49wFK3reZXQ..
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>
CSeq: 1 INVITE
Content-Length:  0


--end msg--
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f743c0d1c80 => txdata 0x7f743c0d1718 mapping
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1623: mmtel-0x7f743c0dce40 received provisional response Response msg 100/INVITE/cseq=1 (tdta0x7f743c0d1670) on fork 0, state = Proceeding
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1413: mmtel-0x7f743c0dce40 sending Response msg 100/INVITE/cseq=1 (tdta0x7f743c0d1670)
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 100
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1853: Discard 100/INVITE response (tdta0x7f743c0d1670)
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f743c0d1c80 => txdata 0x7f743c0d1718 mapping
20-09-2016 10:24:56.225 UTC Debug pjsip: tdta0x7f743c0d Destroying txdata Response msg 100/INVITE/cseq=1 (tdta0x7f743c0d1670)
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:2062: Routing Request msg INVITE/cseq=1 (tdta0x7f743c0ce700) (1445 bytes) to downstream sproutlet scscf:
--start msg--

INVITE sip:1234 at example.com;transport=TCP SIP/2.0
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Max-Forwards: 68
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>
To: <sip:1234 at example.com>
From: <sip:6505550962 at example.com>;tag=2826a779
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Allow-Events: presence, kpml
P-Asserted-Identity: <sip:6505550962 at example.com>
Session-Expires: 600
P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg
Route: <sip:odi_CpJtAOTjbG at 10.112.87.250:5054;lr;orig;service=scscf>
Content-Type: application/sdp
Content-Length:   241

v=0
o=Z 0 0 IN IP4 10.112.123.57
s=Z
c=IN IP4 10.112.123.57
t=0 0
m=audio 8000 RTP/AVP 3 110 8 0 97 101
a=rtpmap:110 speex/8000
a=rtpmap:97 iLBC/8000
a=fmtp:97 mode=30
a=rtpmap:101 telephone-event/8000
a=fmtp:101 0-16
a=sendrecv

--end msg--
20-09-2016 10:24:56.225 UTC Debug pjutils.cpp:691: Cloned tdta0x7f743c0ce700 to tdta0x7f743c0d1670
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:odi_CpJtAOTjbG at 10.112.87.250:5054;lr;orig;service=scscf>
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f743c0d1c80 => txdata 0x7f743c0d1718 mapping
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1587: scscf-0x7f743c0cd8a0 pass initial request Request msg INVITE/cseq=1 (tdta0x7f743c0d1670) to Sproutlet
20-09-2016 10:24:56.225 UTC Info scscfsproutlet.cpp:431: S-CSCF received initial request
20-09-2016 10:24:56.225 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:24:56.225 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:24:56.225 UTC Debug scscfsproutlet.cpp:773: Route header references this system
20-09-2016 10:24:56.225 UTC Debug scscfsproutlet.cpp:786: Found ODI token CpJtAOTjbG
20-09-2016 10:24:56.225 UTC Debug aschain.h:131: AsChain inc ref 0x7f743c071ad0 -> 2
20-09-2016 10:24:56.225 UTC Info scscfsproutlet.cpp:793: Original dialog for odi_CpJtAOTjbG found: AsChain-orig[0x7f743c071ad0]:2/1
20-09-2016 10:24:56.225 UTC Debug scscfsproutlet.cpp:832: Got our Route header, session case orig, OD=AsChain-orig[0x7f743c071ad0]:2/1
20-09-2016 10:24:56.225 UTC Debug pjutils.cpp:291: Served user from P-Served-User header
20-09-2016 10:24:56.225 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:24:56.225 UTC Debug uri_classifier.cpp:199: Classified URI as 4
20-09-2016 10:24:56.225 UTC Info scscfsproutlet.cpp:502: Found served user, so apply services
20-09-2016 10:24:56.225 UTC Debug scscfsproutlet.cpp:1153: Performing originating initiating request processing
20-09-2016 10:24:56.225 UTC Info scscfsproutlet.cpp:1178: Completed applying originating services
20-09-2016 10:24:56.225 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true
20-09-2016 10:24:56.225 UTC Debug uri_classifier.cpp:199: Classified URI as 2
20-09-2016 10:24:56.225 UTC Debug pjutils.cpp:2218: Translating URI
20-09-2016 10:24:56.225 UTC Debug pjutils.cpp:2189: Performing ENUM translation for user 1234
20-09-2016 10:24:56.225 UTC Debug enumservice.cpp:240: Translating URI via JSON ENUM lookup
20-09-2016 10:24:56.225 UTC Debug enumservice.cpp:305: Comparing first 4 numbers of 1234 against prefix 650555
20-09-2016 10:24:56.225 UTC Debug enumservice.cpp:305: Comparing first 0 numbers of 1234 against prefix 
20-09-2016 10:24:56.225 UTC Debug enumservice.cpp:312: Match found
20-09-2016 10:24:56.225 UTC Info enumservice.cpp:280: Number 1234 found, translated URI = sip:1234 at 10.112.87.177;transport=TCP
20-09-2016 10:24:56.225 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true
20-09-2016 10:24:56.225 UTC Debug uri_classifier.cpp:199: Classified URI as 5
20-09-2016 10:24:56.225 UTC Debug pjutils.cpp:2249: Translated URI sip:1234 at 10.112.87.177;transport=TCP is a real SIP URI - replacing Request-URI
20-09-2016 10:24:56.225 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:24:56.225 UTC Debug uri_classifier.cpp:199: Classified URI as 5
20-09-2016 10:24:56.225 UTC Info scscfsproutlet.cpp:1194: New URI string is sip:1234 at 10.112.87.177;transport=TCP
20-09-2016 10:24:56.225 UTC Debug scscfsproutlet.cpp:1210: Routing to BGCF
20-09-2016 10:24:56.225 UTC Info scscfsproutlet.cpp:1446: Routing to BGCF sip:bgcf.cw-aio:5053;transport=TCP
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f743c0d1c80
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1386: scscf-0x7f743c0cd8a0 sending Request msg INVITE/cseq=1 (tdta0x7f743c0d1670) on fork 0
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f743c0d1718, fork = 0
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1914: scscf-0x7f743c0cd8a0 transmitting request on fork 0
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1928: scscf-0x7f743c0cd8a0 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f743c0d1670) on fork 0
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f743c0d1c80 => txdata 0x7f743c0d1718 mapping
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:bgcf.cw-aio:5053;transport=TCP;lr
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:329: Possible service name - bgcf
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1154: Created Sproutlet bgcf-0x7f743c0cd4e0 for Request msg INVITE/cseq=1 (tdta0x7f743c0d1670)
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 100/INVITE/cseq=1 (tdta0x7f743c0f6210) (609 bytes) to upstream sproutlet scscf:
--start msg--

SIP/2.0 100 Trying
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Call-ID: YmAzwVukLM_49wFK3reZXQ..
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>
CSeq: 1 INVITE
Content-Length:  0


--end msg--
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f743c0f6820 => txdata 0x7f743c0f62b8 mapping
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1623: scscf-0x7f743c0cd8a0 received provisional response Response msg 100/INVITE/cseq=1 (tdta0x7f743c0f6210) on fork 0, state = Proceeding
20-09-2016 10:24:56.225 UTC Info scscfsproutlet.cpp:561: S-CSCF received response
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:1413: scscf-0x7f743c0cd8a0 sending Response msg 100/INVITE/cseq=1 (tdta0x7f743c0f6210)
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 100
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1853: Discard 100/INVITE response (tdta0x7f743c0f6210)
20-09-2016 10:24:56.225 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f743c0f6820 => txdata 0x7f743c0f62b8 mapping
20-09-2016 10:24:56.225 UTC Debug pjsip: tdta0x7f743c0f Destroying txdata Response msg 100/INVITE/cseq=1 (tdta0x7f743c0f6210)
20-09-2016 10:24:56.225 UTC Verbose sproutletproxy.cpp:2062: Routing Request msg INVITE/cseq=1 (tdta0x7f743c0d1670) (1425 bytes) to downstream sproutlet bgcf:
--start msg--

INVITE sip:1234 at 10.112.87.177;transport=TCP SIP/2.0
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Max-Forwards: 67
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>
To: <sip:1234 at example.com>
From: <sip:6505550962 at example.com>;tag=2826a779
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Allow-Events: presence, kpml
P-Asserted-Identity: <sip:6505550962 at example.com>
Session-Expires: 600
P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg
Route: <sip:bgcf.cw-aio:5053;transport=TCP;lr>
Content-Type: application/sdp
Content-Length:   241

v=0
o=Z 0 0 IN IP4 10.112.123.57
s=Z
c=IN IP4 10.112.123.57
t=0 0
m=audio 8000 RTP/AVP 3 110 8 0 97 101
a=rtpmap:110 speex/8000
a=rtpmap:97 iLBC/8000
a=fmtp:97 mode=30
a=rtpmap:101 telephone-event/8000
a=fmtp:101 0-16
a=sendrecv

--end msg--
20-09-2016 10:24:56.226 UTC Debug pjutils.cpp:691: Cloned tdta0x7f743c0d1670 to tdta0x7f743c0f6210
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:bgcf.cw-aio:5053;transport=TCP;lr>
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f743c0f6820 => txdata 0x7f743c0f62b8 mapping
20-09-2016 10:24:56.226 UTC Verbose sproutletproxy.cpp:1587: bgcf-0x7f743c0cd4e0 pass initial request Request msg INVITE/cseq=1 (tdta0x7f743c0f6210) to Sproutlet
20-09-2016 10:24:56.226 UTC Debug acr.cpp:49: Created ACR (0x7f743c045240)
20-09-2016 10:24:56.226 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:24:56.226 UTC Debug uri_classifier.cpp:199: Classified URI as 5
20-09-2016 10:24:56.226 UTC Debug pjutils.cpp:2328: Not translating URI
20-09-2016 10:24:56.226 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:24:56.226 UTC Debug uri_classifier.cpp:199: Classified URI as 5
20-09-2016 10:24:56.226 UTC Debug bgcfservice.cpp:188: Getting route for URI domain 10.112.87.177 via BGCF lookup
20-09-2016 10:24:56.226 UTC Debug bgcfsproutlet.cpp:210: No route configured for 10.112.87.177
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f743c0f6820
20-09-2016 10:24:56.226 UTC Verbose sproutletproxy.cpp:1386: bgcf-0x7f743c0cd4e0 sending Request msg INVITE/cseq=1 (tdta0x7f743c0f6210) on fork 0
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f743c0f62b8, fork = 0
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:1914: bgcf-0x7f743c0cd4e0 transmitting request on fork 0
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:1928: bgcf-0x7f743c0cd4e0 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f743c0f6210) on fork 0
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f743c0f6820 => txdata 0x7f743c0f62b8 mapping
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:1234 at 10.112.87.177;transport=TCP
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:309: Found user - 1234
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:329: Possible service name - 10
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:335: Hostname - 112.87.177
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:309: Found user - 1234
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:329: Possible service name - 10
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:335: Hostname - 112.87.177
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:309: Found user - 1234
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:329: Possible service name - 10
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:335: Hostname - 112.87.177
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:309: Found user - 1234
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:329: Possible service name - 10
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:335: Hostname - 112.87.177
20-09-2016 10:24:56.226 UTC Debug sproutletproxy.cpp:874: No local sproutlet matches request
20-09-2016 10:24:56.226 UTC Debug pjsip: tsx0x7f743c0f9 Transaction created for Request msg INVITE/cseq=1 (tdta0x7f743c0f6210)
20-09-2016 10:24:56.226 UTC Debug basicproxy.cpp:1618: Added trail identifier 7790 to UAC transaction
20-09-2016 10:24:56.226 UTC Debug pjutils.cpp:490: Next hop node is encoded in Request-URI
20-09-2016 10:24:56.226 UTC Debug sipresolver.cpp:86: SIPResolver::resolve for name 10.112.87.177, port 0, transport 6, family 2
20-09-2016 10:24:56.226 UTC Debug baseresolver.cpp:523: Attempt to parse 10.112.87.177 as IP address
20-09-2016 10:24:56.226 UTC Debug sipresolver.cpp:103: Target is an IP address - default port/transport if required
20-09-2016 10:24:56.226 UTC Info pjutils.cpp:938: Resolved destination URI sip:1234 at 10.112.87.177;transport=TCP to 1 servers
20-09-2016 10:24:56.226 UTC Debug pjutils.cpp:490: Next hop node is encoded in Request-URI
20-09-2016 10:24:56.226 UTC Debug basicproxy.cpp:1641: Next hop 10.112.87.177 is not a stateless proxy
20-09-2016 10:24:56.226 UTC Debug basicproxy.cpp:1655: Sending request for sip:1234 at 10.112.87.177;transport=TCP
20-09-2016 10:24:56.226 UTC Debug pjsip: tsx0x7f743c0f9 Sending Request msg INVITE/cseq=1 (tdta0x7f743c0f6210) in state Null
20-09-2016 10:24:56.226 UTC Debug pjsip:       endpoint Request msg INVITE/cseq=1 (tdta0x7f743c0f6210): skipping target resolution because address is already set
20-09-2016 10:24:56.226 UTC Verbose pjsip: tcpc0x7f743c0f TCP client transport created
20-09-2016 10:24:56.226 UTC Verbose pjsip: tcpc0x7f743c0f TCP transport 10.112.87.250:58993 is connecting to 10.112.87.177:5060...
20-09-2016 10:24:56.226 UTC Verbose common_sip_processing.cpp:136: TX 1470 bytes Request msg INVITE/cseq=1 (tdta0x7f743c0f6210) to TCP 10.112.87.177:5060:
--start msg--

INVITE sip:1234 at 10.112.87.177;transport=TCP SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250:58993;rport;branch=z9hG4bKPj9xOTMWXCJeG-Qz5wUzQzl8TBjOCfEVCS
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Max-Forwards: 66
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>
To: <sip:1234 at example.com>
From: <sip:6505550962 at example.com>;tag=2826a779
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Allow-Events: presence, kpml
P-Asserted-Identity: <sip:6505550962 at example.com>
Session-Expires: 600
P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg
Content-Type: application/sdp
Content-Length:   241

v=0
o=Z 0 0 IN IP4 10.112.123.57
s=Z
c=IN IP4 10.112.123.57
t=0 0
m=audio 8000 RTP/AVP 3 110 8 0 97 101
a=rtpmap:110 speex/8000
a=rtpmap:97 iLBC/8000
a=fmtp:97 mode=30
a=rtpmap:101 telephone-event/8000
a=fmtp:101 0-16
a=sendrecv

--end msg--
20-09-2016 10:24:56.226 UTC Debug pjsip: tsx0x7f743c0f9 State changed from Null to Calling, event=TX_MSG
20-09-2016 10:24:56.226 UTC Debug basicproxy.cpp:213: tsx0x7f743c0f9228 - tu_on_tsx_state UAC, TSX_STATE TX_MSG state=Calling
20-09-2016 10:24:56.226 UTC Debug basicproxy.cpp:1813: tsx0x7f743c0f9228 - uac_tsx = 0x7f743c0d3630, uas_tsx = 0x7f743c071ca0
20-09-2016 10:24:56.226 UTC Debug basicproxy.cpp:1821: TX_MSG event on current UAC transaction
20-09-2016 10:24:56.226 UTC Debug basicproxy.cpp:2134: Starting timer C
20-09-2016 10:24:56.226 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f7444100698
20-09-2016 10:24:56.226 UTC Debug thread_dispatcher.cpp:199: Request latency = 8797us
20-09-2016 10:24:56.236 UTC Verbose pjsip: tcpc0x7f743c0f TCP transport 10.112.87.250:58993 is connected to 10.112.87.177:5060
20-09-2016 10:24:56.239 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Response msg 100/INVITE/cseq=1 (rdata0x7f743c0f9cf0)
20-09-2016 10:24:56.239 UTC Verbose common_sip_processing.cpp:120: RX 686 bytes Response msg 100/INVITE/cseq=1 (rdata0x7f743c0f9cf0) from TCP 10.112.87.177:5060:
--start msg--

SIP/2.0 100 Trying
Via: SIP/2.0/TCP 10.112.87.250:58993;rport;branch=z9hG4bKPj9xOTMWXCJeG-Qz5wUzQzl8TBjOCfEVCS,SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x,SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>,<sip:10.112.87.250:5058;transport=TCP;lr>,<sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>;tag=0_3606741008-335395922
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Content-Length: 0


--end msg--
20-09-2016 10:24:56.239 UTC Debug pjutils.cpp:1660: Logging SAS Call-ID marker, Call-ID YmAzwVukLM_49wFK3reZXQ..
20-09-2016 10:24:56.239 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f7444100698 for worker threads
20-09-2016 10:24:56.239 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f7444100698
20-09-2016 10:24:56.240 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Response msg 100/INVITE/cseq=1 (rdata0x7f7444100698)
20-09-2016 10:24:56.240 UTC Debug pjsip: tsx0x7f743c0f9 Incoming Response msg 100/INVITE/cseq=1 (rdata0x7f7444100698) in state Calling
20-09-2016 10:24:56.240 UTC Debug pjsip: tsx0x7f743c0f9 State changed from Calling to Proceeding, event=RX_MSG
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:213: tsx0x7f743c0f9228 - tu_on_tsx_state UAC, TSX_STATE RX_MSG state=Proceeding
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:1813: tsx0x7f743c0f9228 - uac_tsx = 0x7f743c0d3630, uas_tsx = 0x7f743c071ca0
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:1821: RX_MSG event on current UAC transaction
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:1884: tsx0x7f743c0f9228 - RX_MSG on active UAC transaction
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 100/INVITE/cseq=1 (tdta0x7f74380af050) (636 bytes) to upstream sproutlet bgcf:
--start msg--

SIP/2.0 100 Trying
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>;tag=0_3606741008-335395922
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Content-Length:  0


--end msg--
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f74380af660 => txdata 0x7f74380af0f8 mapping
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1623: bgcf-0x7f743c0cd4e0 received provisional response Response msg 100/INVITE/cseq=1 (tdta0x7f74380af050) on fork 0, state = Proceeding
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1413: bgcf-0x7f743c0cd4e0 sending Response msg 100/INVITE/cseq=1 (tdta0x7f74380af050)
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 100
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1853: Discard 100/INVITE response (tdta0x7f74380af050)
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f74380af660 => txdata 0x7f74380af0f8 mapping
20-09-2016 10:24:56.240 UTC Debug pjsip: tdta0x7f74380a Destroying txdata Response msg 100/INVITE/cseq=1 (tdta0x7f74380af050)
20-09-2016 10:24:56.240 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f7444100698
20-09-2016 10:24:56.240 UTC Debug thread_dispatcher.cpp:199: Request latency = 189us
20-09-2016 10:24:56.240 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Response msg 404/INVITE/cseq=1 (rdata0x7f743c0f9cf0)
20-09-2016 10:24:56.240 UTC Verbose common_sip_processing.cpp:120: RX 719 bytes Response msg 404/INVITE/cseq=1 (rdata0x7f743c0f9cf0) from TCP 10.112.87.177:5060:
--start msg--

SIP/2.0 404 Not Found
Via: SIP/2.0/TCP 10.112.87.250:58993;rport;branch=z9hG4bKPj9xOTMWXCJeG-Qz5wUzQzl8TBjOCfEVCS,SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x,SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>,<sip:10.112.87.250:5058;transport=TCP;lr>,<sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>;tag=0_3606741008-335395922
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Contact: <sip:10.112.87.177>
Content-Length: 0


--end msg--
20-09-2016 10:24:56.240 UTC Debug pjutils.cpp:1660: Logging SAS Call-ID marker, Call-ID YmAzwVukLM_49wFK3reZXQ..
20-09-2016 10:24:56.240 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f7444101648 for worker threads
20-09-2016 10:24:56.240 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f7444101648
20-09-2016 10:24:56.240 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Response msg 404/INVITE/cseq=1 (rdata0x7f7444101648)
20-09-2016 10:24:56.240 UTC Debug pjsip: tsx0x7f743c0f9 Incoming Response msg 404/INVITE/cseq=1 (rdata0x7f7444101648) in state Proceeding
20-09-2016 10:24:56.240 UTC Debug pjsip:       endpoint Request msg ACK/cseq=1 (tdta0x7f744409cbf0) created.
20-09-2016 10:24:56.240 UTC Verbose common_sip_processing.cpp:136: TX 335 bytes Request msg ACK/cseq=1 (tdta0x7f744409cbf0) to TCP 10.112.87.177:5060:
--start msg--

ACK sip:1234 at 10.112.87.177;transport=TCP SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250:58993;rport;branch=z9hG4bKPj9xOTMWXCJeG-Qz5wUzQzl8TBjOCfEVCS
Max-Forwards: 70
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>;tag=0_3606741008-335395922
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 ACK
Content-Length:  0


--end msg--
20-09-2016 10:24:56.240 UTC Debug pjsip: tsx0x7f743c0f9 State changed from Proceeding to Completed, event=RX_MSG
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:213: tsx0x7f743c0f9228 - tu_on_tsx_state UAC, TSX_STATE RX_MSG state=Completed
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:1813: tsx0x7f743c0f9228 - uac_tsx = 0x7f743c0d3630, uas_tsx = 0x7f743c071ca0
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:1821: RX_MSG event on current UAC transaction
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:2146: Stopping timer C
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:1884: tsx0x7f743c0f9228 - RX_MSG on active UAC transaction
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:1408: Dissociate UAC transaction 0x7f743c0d3630 for target 0
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50) (669 bytes) to upstream sproutlet bgcf:
--start msg--

SIP/2.0 404 Not Found
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>;tag=0_3606741008-335395922
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Contact: <sip:10.112.87.177>
Content-Length:  0


--end msg--
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f74440a0160 => txdata 0x7f744409fbf8 mapping
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1634: bgcf-0x7f743c0cd4e0 received final response Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50) on fork 0, state = Terminated
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1413: bgcf-0x7f743c0cd4e0 sending Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50)
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 404
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1886: 3xx/4xx/5xx/6xx response
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1890: Best 3xx/4xx/5xx/6xx response so far
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1777: All UAC responded
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f74440a0160 => txdata 0x7f744409fbf8 mapping
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50) (669 bytes) to upstream sproutlet scscf:
--start msg--

SIP/2.0 404 Not Found
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>;tag=0_3606741008-335395922
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Contact: <sip:10.112.87.177>
Content-Length:  0


--end msg--
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f74440a0160 => txdata 0x7f744409fbf8 mapping
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1634: scscf-0x7f743c0cd8a0 received final response Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50) on fork 0, state = Terminated
20-09-2016 10:24:56.240 UTC Info scscfsproutlet.cpp:561: S-CSCF received response
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1413: scscf-0x7f743c0cd8a0 sending Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50)
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 404
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1886: 3xx/4xx/5xx/6xx response
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1890: Best 3xx/4xx/5xx/6xx response so far
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1777: All UAC responded
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f74440a0160 => txdata 0x7f744409fbf8 mapping
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50) (669 bytes) to upstream sproutlet mmtel:
--start msg--

SIP/2.0 404 Not Found
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>;tag=0_3606741008-335395922
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Contact: <sip:10.112.87.177>
Content-Length:  0


--end msg--
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f74440a0160 => txdata 0x7f744409fbf8 mapping
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1634: mmtel-0x7f743c0dce40 received final response Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50) on fork 0, state = Terminated
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1413: mmtel-0x7f743c0dce40 sending Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50)
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 404
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1886: 3xx/4xx/5xx/6xx response
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1890: Best 3xx/4xx/5xx/6xx response so far
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1777: All UAC responded
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f74440a0160 => txdata 0x7f744409fbf8 mapping
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50) (669 bytes) to upstream sproutlet scscf:
--start msg--

SIP/2.0 404 Not Found
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>;tag=0_3606741008-335395922
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Contact: <sip:10.112.87.177>
Content-Length:  0


--end msg--
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f74440a0160 => txdata 0x7f744409fbf8 mapping
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1634: scscf-0x7f743c098140 received final response Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50) on fork 0, state = Terminated
20-09-2016 10:24:56.240 UTC Info scscfsproutlet.cpp:561: S-CSCF received response
20-09-2016 10:24:56.240 UTC Debug as_communication_tracker.cpp:59: Communication with AS sip:mmtel.example.com successful
20-09-2016 10:24:56.240 UTC Debug as_communication_tracker.cpp:107: Current time is 430697236, next AS check at 372304881
20-09-2016 10:24:56.240 UTC Debug as_communication_tracker.cpp:115: Check for ASs that have become healthy again
20-09-2016 10:24:56.240 UTC Debug as_communication_tracker.cpp:149: All ASs OK - clear the alarm
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1413: scscf-0x7f743c098140 sending Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50)
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 404
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1886: 3xx/4xx/5xx/6xx response
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1890: Best 3xx/4xx/5xx/6xx response so far
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1777: All UAC responded
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f74440a0160 => txdata 0x7f744409fbf8 mapping
20-09-2016 10:24:56.240 UTC Debug pjsip: tsx0x7f743c0db Sending Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50) in state Proceeding
20-09-2016 10:24:56.240 UTC Debug pjsip: tdta0x7f743c0c Destroying txdata Response msg 100/INVITE/cseq=1 (tdta0x7f743c0c5940)
20-09-2016 10:24:56.240 UTC Verbose common_sip_processing.cpp:136: TX 669 bytes Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50) to TCP 10.112.87.250:43682:
--start msg--

SIP/2.0 404 Not Found
Via: SIP/2.0/TCP 10.112.87.250:43682;rport=43682;received=10.112.87.250;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fc4374959259fa24
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>;tag=0_3606741008-335395922
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 INVITE
Contact: <sip:10.112.87.177>
Content-Length:  0


--end msg--
20-09-2016 10:24:56.240 UTC Debug pjsip: tsx0x7f743c0db State changed from Proceeding to Completed, event=TX_MSG
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:213: tsx0x7f743c0dbc38 - tu_on_tsx_state UAS, TSX_STATE TX_MSG state=Completed
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1828: scscf-0x7f743c098140 suiciding
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1160: Destroying SproutletWrapper 0x7f743c072ca0
20-09-2016 10:24:56.240 UTC Debug scscfsproutlet.cpp:395: S-CSCF Transaction (0x7f743c098140) destroyed
20-09-2016 10:24:56.240 UTC Debug aschain.h:139: AsChain dec ref 0x7f743c071ad0 -> 1
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1169: Free original request Request msg INVITE/cseq=1 (tdta0x7f743c0d8c20) (tdta0x7f743c0d8c20)
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1828: mmtel-0x7f743c0dce40 suiciding
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1160: Destroying SproutletWrapper 0x7f743c097f40
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1169: Free original request Request msg INVITE/cseq=1 (tdta0x7f743c0c9850) (tdta0x7f743c0c9850)
20-09-2016 10:24:56.240 UTC Debug pjsip: tdta0x7f743c0c Destroying txdata Request msg INVITE/cseq=1 (tdta0x7f743c0c9850)
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1828: scscf-0x7f743c0cd8a0 suiciding
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1160: Destroying SproutletWrapper 0x7f743c0cc870
20-09-2016 10:24:56.240 UTC Debug scscfsproutlet.cpp:395: S-CSCF Transaction (0x7f743c0cd8a0) destroyed
20-09-2016 10:24:56.240 UTC Debug aschain.h:139: AsChain dec ref 0x7f743c071ad0 -> 0
20-09-2016 10:24:56.240 UTC Debug aschain.cpp:88: Destroying AsChain 0x7f743c071ad0
20-09-2016 10:24:56.240 UTC Debug aschain.cpp:106: Sending ACR (0x7f743c015da0) from AS chain
20-09-2016 10:24:56.240 UTC Debug acr.cpp:83: Sending Null ACR (0x7f743c015da0)
20-09-2016 10:24:56.240 UTC Debug acr.cpp:54: Destroyed ACR (0x7f743c015da0)
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1169: Free original request Request msg INVITE/cseq=1 (tdta0x7f743c0ce700) (tdta0x7f743c0ce700)
20-09-2016 10:24:56.240 UTC Debug pjsip: tdta0x7f743c0c Destroying txdata Request msg INVITE/cseq=1 (tdta0x7f743c0ce700)
20-09-2016 10:24:56.240 UTC Verbose sproutletproxy.cpp:1828: bgcf-0x7f743c0cd4e0 suiciding
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1160: Destroying SproutletWrapper 0x7f743c0d3760
20-09-2016 10:24:56.240 UTC Debug acr.cpp:83: Sending Null ACR (0x7f743c045240)
20-09-2016 10:24:56.240 UTC Debug acr.cpp:54: Destroyed ACR (0x7f743c045240)
20-09-2016 10:24:56.240 UTC Debug sproutletproxy.cpp:1169: Free original request Request msg INVITE/cseq=1 (tdta0x7f743c0d1670) (tdta0x7f743c0d1670)
20-09-2016 10:24:56.240 UTC Debug pjsip: tdta0x7f743c0d Destroying txdata Request msg INVITE/cseq=1 (tdta0x7f743c0d1670)
20-09-2016 10:24:56.240 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f7444101648
20-09-2016 10:24:56.240 UTC Debug thread_dispatcher.cpp:199: Request latency = 391us
20-09-2016 10:24:56.240 UTC Debug pjsip: tsx0x7f743c0f9 Timeout timer event
20-09-2016 10:24:56.240 UTC Debug pjsip: tsx0x7f743c0f9 State changed from Completed to Terminated, event=TIMER
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:213: tsx0x7f743c0f9228 - tu_on_tsx_state UAC, TSX_STATE TIMER state=Terminated
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:1813: tsx0x7f743c0f9228 - uac_tsx = 0x7f743c0d3630, uas_tsx = (nil)
20-09-2016 10:24:56.240 UTC Debug pjsip: tsx0x7f743c0f9 Timeout timer event
20-09-2016 10:24:56.240 UTC Debug pjsip: tsx0x7f743c0f9 State changed from Terminated to Destroyed, event=TIMER
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:213: tsx0x7f743c0f9228 - tu_on_tsx_state UAC, TSX_STATE TIMER state=Destroyed
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:1813: tsx0x7f743c0f9228 - uac_tsx = 0x7f743c0d3630, uas_tsx = (nil)
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:1985: tsx0x7f743c0f9228 - UAC tsx destroyed
20-09-2016 10:24:56.240 UTC Debug basicproxy.cpp:1535: BasicProxy::UACTsx destructor (0x7f743c0d3630)
20-09-2016 10:24:56.240 UTC Debug pjsip: tdta0x7f743c0f Destroying txdata Request msg INVITE/cseq=1 (tdta0x7f743c0f6210)
20-09-2016 10:24:56.240 UTC Debug pjsip: tdta0x7f744409 Destroying txdata Request msg ACK/cseq=1 (tdta0x7f744409cbf0)
20-09-2016 10:24:56.241 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg ACK/cseq=1 (rdata0x7f744406ba40)
20-09-2016 10:24:56.241 UTC Verbose common_sip_processing.cpp:120: RX 387 bytes Request msg ACK/cseq=1 (rdata0x7f744406ba40) from TCP 10.112.87.250:43682:
--start msg--

ACK sip:1234 at example.com;transport=TCP SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250:43682;rport;branch=z9hG4bKPjODKseeiz-TNqEvuGjGeIqLwkpYdKVA1x
Max-Forwards: 70
From: <sip:6505550962 at example.com>;tag=2826a779
To: <sip:1234 at example.com>;tag=0_3606741008-335395922
Call-ID: YmAzwVukLM_49wFK3reZXQ..
CSeq: 1 ACK
Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
20-09-2016 10:24:56.241 UTC Debug pjutils.cpp:1660: Logging SAS Call-ID marker, Call-ID YmAzwVukLM_49wFK3reZXQ..
20-09-2016 10:24:56.241 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f7444100698 for worker threads
20-09-2016 10:24:56.241 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f7444100698
20-09-2016 10:24:56.241 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg ACK/cseq=1 (rdata0x7f7444100698)
20-09-2016 10:24:56.241 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:24:56.241 UTC Debug uri_classifier.cpp:199: Classified URI as 4
20-09-2016 10:24:56.241 UTC Debug authentication.cpp:804: Authentication module invoked
20-09-2016 10:24:56.241 UTC Debug authentication.cpp:814: Request does not need authentication
20-09-2016 10:24:56.241 UTC Debug pjsip: tsx0x7f743c0db Incoming Request msg ACK/cseq=1 (rdata0x7f7444100698) in state Completed
20-09-2016 10:24:56.241 UTC Debug pjsip: tsx0x7f743c0db State changed from Completed to Confirmed, event=RX_MSG
20-09-2016 10:24:56.241 UTC Debug basicproxy.cpp:213: tsx0x7f743c0dbc38 - tu_on_tsx_state UAS, TSX_STATE RX_MSG state=Confirmed
20-09-2016 10:24:56.241 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f7444100698
20-09-2016 10:24:56.241 UTC Debug thread_dispatcher.cpp:199: Request latency = 75us
20-09-2016 10:24:56.679 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:24:56.679 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:24:56.680 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:42930, sock=1556
20-09-2016 10:24:56.680 UTC Verbose pjsip: tcps0x7f744410 TCP server transport created
20-09-2016 10:24:56.682 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430697 (rdata0x7f7444100950)
20-09-2016 10:24:56.682 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430697 (rdata0x7f7444100950) from TCP 10.112.87.250:42930:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430697
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430697
Call-ID: poll-sip-430697
CSeq: 430697 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:24:56.682 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:24:56.682 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:24:56.682 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:24:56.682 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f744409bce8 for worker threads
20-09-2016 10:24:56.683 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f744409bce8
20-09-2016 10:24:56.683 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430697 (rdata0x7f744409bce8)
20-09-2016 10:24:56.683 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:24:56.683 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:24:56.683 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430697 (tdta0x7f744813bdd0) created
20-09-2016 10:24:56.683 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430697 (tdta0x7f744813bdd0) to TCP 10.112.87.250:42930:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=42930;received=10.112.87.250;branch=z9hG4bK-430697
Call-ID: poll-sip-430697
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430697
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430697
CSeq: 430697 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:24:56.683 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:24:56.683 UTC Debug pjsip: tdta0x7f744813 Destroying txdata Response msg 200/OPTIONS/cseq=430697 (tdta0x7f744813bdd0)
20-09-2016 10:24:56.683 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f744409bce8
20-09-2016 10:24:56.683 UTC Debug thread_dispatcher.cpp:199: Request latency = 1193us
20-09-2016 10:24:56.683 UTC Info load_monitor.cpp:232: Accepted 100.000000% of requests, latency error = -0.988670, overload responses = 0
20-09-2016 10:24:56.683 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 20 requests in last 130532ms, minimum threshold for a change is 65266.000000
20-09-2016 10:24:56.683 UTC Debug snmp_continuous_accumulator_table.cpp:108: Accumulating sample 1000ui into continuous accumulator statistic
20-09-2016 10:24:56.683 UTC Debug snmp_continuous_accumulator_table.cpp:108: Accumulating sample 1000ui into continuous accumulator statistic
20-09-2016 10:24:57.680 UTC Verbose pjsip: tcps0x7f744410 TCP connection closed
20-09-2016 10:24:57.680 UTC Debug connection_tracker.cpp:92: Connection 0x7f7444100618 has been destroyed
20-09-2016 10:24:57.680 UTC Verbose pjsip: tcps0x7f744410 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:24:58.731 UTC Debug alarm.cpp:253: Reraising all alarms with a known state
20-09-2016 10:24:58.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:24:58.731 UTC Status alarm.cpp:62: sprout issued 1004.1 alarm
20-09-2016 10:24:58.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:24:58.731 UTC Status alarm.cpp:62: sprout issued 1001.1 alarm
20-09-2016 10:24:58.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:24:58.731 UTC Status alarm.cpp:62: sprout issued 1002.1 alarm
20-09-2016 10:24:58.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:24:58.731 UTC Status alarm.cpp:62: sprout issued 1010.1 alarm
20-09-2016 10:25:01.241 UTC Debug pjsip: tsx0x7f743c0db Timeout timer event
20-09-2016 10:25:01.241 UTC Debug pjsip: tsx0x7f743c0db State changed from Confirmed to Terminated, event=TIMER
20-09-2016 10:25:01.241 UTC Debug basicproxy.cpp:213: tsx0x7f743c0dbc38 - tu_on_tsx_state UAS, TSX_STATE TIMER state=Terminated
20-09-2016 10:25:01.241 UTC Debug basicproxy.cpp:1281: Report SAS end marker - trail (1e6e)
20-09-2016 10:25:01.241 UTC Debug pjsip: tsx0x7f743c0db Timeout timer event
20-09-2016 10:25:01.241 UTC Debug pjsip: tsx0x7f743c0db State changed from Terminated to Destroyed, event=TIMER
20-09-2016 10:25:01.241 UTC Debug basicproxy.cpp:213: tsx0x7f743c0dbc38 - tu_on_tsx_state UAS, TSX_STATE TIMER state=Destroyed
20-09-2016 10:25:01.241 UTC Debug sproutletproxy.cpp:741: tsx0x7f743c0dbc38 - UAS tsx destroyed
20-09-2016 10:25:01.241 UTC Debug sproutletproxy.cpp:1081: Safe for UASTsx to suicide
20-09-2016 10:25:01.241 UTC Debug basicproxy.cpp:1456: Transaction ((nil)) suiciding
20-09-2016 10:25:01.241 UTC Verbose sproutletproxy.cpp:529: Sproutlet Proxy transaction (0x7f743c071ca0) destroyed
20-09-2016 10:25:01.241 UTC Debug basicproxy.cpp:467: BasicProxy::UASTsx destructor (0x7f743c071ca0)
20-09-2016 10:25:01.241 UTC Debug basicproxy.cpp:484: Disconnect UAC transactions from UAS transaction
20-09-2016 10:25:01.241 UTC Debug basicproxy.cpp:498: Free original request
20-09-2016 10:25:01.241 UTC Debug pjsip: tdta0x7f743c0d Destroying txdata Request msg INVITE/cseq=1 (tdta0x7f743c0d8c20)
20-09-2016 10:25:01.241 UTC Debug basicproxy.cpp:507: Free un-used best response
20-09-2016 10:25:01.241 UTC Debug pjsip: tdta0x7f743c0c Destroying txdata Response msg 408/INVITE/cseq=1 (tdta0x7f743c0c3980)
20-09-2016 10:25:01.241 UTC Debug basicproxy.cpp:528: BasicProxy::UASTsx destructor completed
20-09-2016 10:25:01.241 UTC Debug pjsip: tdta0x7f744409 Destroying txdata Response msg 404/INVITE/cseq=1 (tdta0x7f744409fb50)
20-09-2016 10:25:01.241 UTC Debug pjsip: tsx0x7f743c0db Transaction destroyed!
20-09-2016 10:25:01.241 UTC Debug pjsip: tsx0x7f743c0f9 Transaction destroyed!
20-09-2016 10:25:03.799 UTC Verbose pjsip:    tcplis:5052 TCP listener 10.112.87.250:5052: got incoming TCP connection from 10.112.87.250:58383, sock=1556
20-09-2016 10:25:03.799 UTC Verbose pjsip: tcps0x7f744410 TCP server transport created
20-09-2016 10:25:03.799 UTC Verbose pjsip: tcps0x7f744402 TCP connection closed
20-09-2016 10:25:03.799 UTC Verbose pjsip: tcps0x7f744402 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:25:06.705 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:42955, sock=1432
20-09-2016 10:25:06.705 UTC Verbose pjsip: tcps0x7f744402 TCP server transport created
20-09-2016 10:25:06.705 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430707 (rdata0x7f744402d7f0)
20-09-2016 10:25:06.705 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430707 (rdata0x7f744402d7f0) from TCP 10.112.87.250:42955:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430707
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430707
Call-ID: poll-sip-430707
CSeq: 430707 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:25:06.705 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:25:06.705 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:25:06.705 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:25:06.705 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f744409bce8 for worker threads
20-09-2016 10:25:06.705 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f744409bce8
20-09-2016 10:25:06.708 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430707 (rdata0x7f744409bce8)
20-09-2016 10:25:06.709 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:25:06.709 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:25:06.709 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430707 (tdta0x7f74280a69a0) created
20-09-2016 10:25:06.709 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430707 (tdta0x7f74280a69a0) to TCP 10.112.87.250:42955:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=42955;received=10.112.87.250;branch=z9hG4bK-430707
Call-ID: poll-sip-430707
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430707
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430707
CSeq: 430707 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:25:06.709 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:25:06.709 UTC Debug pjsip: tdta0x7f74280a Destroying txdata Response msg 200/OPTIONS/cseq=430707 (tdta0x7f74280a69a0)
20-09-2016 10:25:06.709 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f744409bce8
20-09-2016 10:25:06.709 UTC Debug thread_dispatcher.cpp:199: Request latency = 3553us
20-09-2016 10:25:06.716 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:25:06.716 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:25:07.705 UTC Verbose pjsip: tcps0x7f744402 TCP connection closed
20-09-2016 10:25:07.705 UTC Debug connection_tracker.cpp:92: Connection 0x7f744402d4b8 has been destroyed
20-09-2016 10:25:07.705 UTC Verbose pjsip: tcps0x7f744402 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:25:15.746 UTC Verbose pjsip: tcps0x7f744405 TCP transport destroyed normally
20-09-2016 10:25:15.800 UTC Verbose pjsip:    tcplis:5052 TCP listener 10.112.87.250:5052: got incoming TCP connection from 10.112.87.250:37357, sock=1432
20-09-2016 10:25:15.800 UTC Verbose pjsip: tcps0x7f744402 TCP server transport created
20-09-2016 10:25:16.743 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:25:16.743 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:25:16.743 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:42988, sock=1563
20-09-2016 10:25:16.743 UTC Verbose pjsip: tcps0x7f744405 TCP server transport created
20-09-2016 10:25:16.743 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430717 (rdata0x7f744405ff20)
20-09-2016 10:25:16.743 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430717 (rdata0x7f744405ff20) from TCP 10.112.87.250:42988:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430717
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430717
Call-ID: poll-sip-430717
CSeq: 430717 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:25:16.743 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:25:16.743 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:25:16.743 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:25:16.743 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f744409bce8 for worker threads
20-09-2016 10:25:16.743 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f744409bce8
20-09-2016 10:25:16.743 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430717 (rdata0x7f744409bce8)
20-09-2016 10:25:16.743 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:25:16.743 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:25:16.743 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430717 (tdta0x7f74300aa5e0) created
20-09-2016 10:25:16.743 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430717 (tdta0x7f74300aa5e0) to TCP 10.112.87.250:42988:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=42988;received=10.112.87.250;branch=z9hG4bK-430717
Call-ID: poll-sip-430717
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430717
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430717
CSeq: 430717 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:25:16.743 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:25:16.743 UTC Debug pjsip: tdta0x7f74300a Destroying txdata Response msg 200/OPTIONS/cseq=430717 (tdta0x7f74300aa5e0)
20-09-2016 10:25:16.743 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f744409bce8
20-09-2016 10:25:16.743 UTC Debug thread_dispatcher.cpp:199: Request latency = 179us
20-09-2016 10:25:17.743 UTC Verbose pjsip: tcps0x7f744405 TCP connection closed
20-09-2016 10:25:17.743 UTC Debug connection_tracker.cpp:92: Connection 0x7f744405fbe8 has been destroyed
20-09-2016 10:25:17.743 UTC Verbose pjsip: tcps0x7f744405 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:25:26.791 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43020, sock=1466
20-09-2016 10:25:26.791 UTC Verbose pjsip: tcps0x7f744405 TCP server transport created
20-09-2016 10:25:26.791 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430727 (rdata0x7f744405ff20)
20-09-2016 10:25:26.791 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430727 (rdata0x7f744405ff20) from TCP 10.112.87.250:43020:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430727
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430727
Call-ID: poll-sip-430727
CSeq: 430727 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:25:26.791 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:25:26.791 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:25:26.791 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:25:26.791 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f744409bce8 for worker threads
20-09-2016 10:25:26.791 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f744409bce8
20-09-2016 10:25:26.791 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430727 (rdata0x7f744409bce8)
20-09-2016 10:25:26.791 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:25:26.791 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:25:26.791 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430727 (tdta0x7f743c0d8c20) created
20-09-2016 10:25:26.791 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430727 (tdta0x7f743c0d8c20) to TCP 10.112.87.250:43020:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43020;received=10.112.87.250;branch=z9hG4bK-430727
Call-ID: poll-sip-430727
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430727
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430727
CSeq: 430727 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:25:26.791 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:25:26.791 UTC Debug pjsip: tdta0x7f743c0d Destroying txdata Response msg 200/OPTIONS/cseq=430727 (tdta0x7f743c0d8c20)
20-09-2016 10:25:26.791 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f744409bce8
20-09-2016 10:25:26.791 UTC Debug thread_dispatcher.cpp:199: Request latency = 101us
20-09-2016 10:25:26.795 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:25:26.795 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:25:27.791 UTC Verbose pjsip: tcps0x7f744405 TCP connection closed
20-09-2016 10:25:27.791 UTC Debug connection_tracker.cpp:92: Connection 0x7f744405fbe8 has been destroyed
20-09-2016 10:25:27.791 UTC Verbose pjsip: tcps0x7f744405 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:25:28.731 UTC Debug alarm.cpp:253: Reraising all alarms with a known state
20-09-2016 10:25:28.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:25:28.731 UTC Status alarm.cpp:62: sprout issued 1004.1 alarm
20-09-2016 10:25:28.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:25:28.731 UTC Status alarm.cpp:62: sprout issued 1001.1 alarm
20-09-2016 10:25:28.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:25:28.731 UTC Status alarm.cpp:62: sprout issued 1002.1 alarm
20-09-2016 10:25:28.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:25:28.731 UTC Status alarm.cpp:62: sprout issued 1010.1 alarm
20-09-2016 10:25:29.240 UTC Debug connection_tracker.cpp:92: Connection 0x7f743c0f99b8 has been destroyed
20-09-2016 10:25:29.240 UTC Verbose pjsip: tcpc0x7f743c0f TCP transport destroyed normally
20-09-2016 10:25:29.250 UTC Verbose pjsip: tcps0x7f744406 TCP connection closed
20-09-2016 10:25:29.250 UTC Debug connection_tracker.cpp:92: Connection 0x7f744406b708 has been destroyed
20-09-2016 10:25:29.250 UTC Verbose pjsip: tcps0x7f744406 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:25:36.825 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43051, sock=1301
20-09-2016 10:25:36.825 UTC Verbose pjsip: tcps0x7f744405 TCP server transport created
20-09-2016 10:25:36.825 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430737 (rdata0x7f744405ff20)
20-09-2016 10:25:36.825 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430737 (rdata0x7f744405ff20) from TCP 10.112.87.250:43051:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430737
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430737
Call-ID: poll-sip-430737
CSeq: 430737 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:25:36.825 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:25:36.825 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:25:36.825 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:25:36.825 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f7444024448 for worker threads
20-09-2016 10:25:36.826 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f7444024448
20-09-2016 10:25:36.826 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430737 (rdata0x7f7444024448)
20-09-2016 10:25:36.826 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:25:36.826 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:25:36.826 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430737 (tdta0x7f74380af050) created
20-09-2016 10:25:36.826 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430737 (tdta0x7f74380af050) to TCP 10.112.87.250:43051:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43051;received=10.112.87.250;branch=z9hG4bK-430737
Call-ID: poll-sip-430737
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430737
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430737
CSeq: 430737 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:25:36.826 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:25:36.826 UTC Debug pjsip: tdta0x7f74380a Destroying txdata Response msg 200/OPTIONS/cseq=430737 (tdta0x7f74380af050)
20-09-2016 10:25:36.826 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f7444024448
20-09-2016 10:25:36.826 UTC Debug thread_dispatcher.cpp:199: Request latency = 143us
20-09-2016 10:25:36.826 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:25:36.826 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:25:37.825 UTC Verbose pjsip: tcps0x7f744405 TCP connection closed
20-09-2016 10:25:37.825 UTC Debug connection_tracker.cpp:92: Connection 0x7f744405fbe8 has been destroyed
20-09-2016 10:25:37.826 UTC Verbose pjsip: tcps0x7f744405 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:25:46.865 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43081, sock=1301
20-09-2016 10:25:46.865 UTC Verbose pjsip: tcps0x7f744405 TCP server transport created
20-09-2016 10:25:46.865 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430747 (rdata0x7f744405ff20)
20-09-2016 10:25:46.865 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430747 (rdata0x7f744405ff20) from TCP 10.112.87.250:43081:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430747
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430747
Call-ID: poll-sip-430747
CSeq: 430747 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:25:46.865 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:25:46.865 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:25:46.865 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:25:46.865 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f7444024448 for worker threads
20-09-2016 10:25:46.865 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f7444024448
20-09-2016 10:25:46.865 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430747 (rdata0x7f7444024448)
20-09-2016 10:25:46.865 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:25:46.865 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:25:46.865 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430747 (tdta0x7f744406b6e0) created
20-09-2016 10:25:46.865 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430747 (tdta0x7f744406b6e0) to TCP 10.112.87.250:43081:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43081;received=10.112.87.250;branch=z9hG4bK-430747
Call-ID: poll-sip-430747
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430747
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430747
CSeq: 430747 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:25:46.865 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:25:46.865 UTC Debug pjsip: tdta0x7f744406 Destroying txdata Response msg 200/OPTIONS/cseq=430747 (tdta0x7f744406b6e0)
20-09-2016 10:25:46.865 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f7444024448
20-09-2016 10:25:46.865 UTC Debug thread_dispatcher.cpp:199: Request latency = 106us
20-09-2016 10:25:46.871 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:25:46.871 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:25:47.865 UTC Verbose pjsip: tcps0x7f744405 TCP connection closed
20-09-2016 10:25:47.865 UTC Debug connection_tracker.cpp:92: Connection 0x7f744405fbe8 has been destroyed
20-09-2016 10:25:47.865 UTC Verbose pjsip: tcps0x7f744405 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:25:56.919 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43113, sock=1301
20-09-2016 10:25:56.919 UTC Verbose pjsip: tcps0x7f744406 TCP server transport created
20-09-2016 10:25:56.919 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430757 (rdata0x7f744406ba40)
20-09-2016 10:25:56.919 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430757 (rdata0x7f744406ba40) from TCP 10.112.87.250:43113:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430757
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430757
Call-ID: poll-sip-430757
CSeq: 430757 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:25:56.919 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:25:56.919 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:25:56.919 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:25:56.919 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f74440779c8 for worker threads
20-09-2016 10:25:56.919 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f74440779c8
20-09-2016 10:25:56.919 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430757 (rdata0x7f74440779c8)
20-09-2016 10:25:56.919 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:25:56.919 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:25:56.919 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430757 (tdta0x7f74400bca20) created
20-09-2016 10:25:56.919 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430757 (tdta0x7f74400bca20) to TCP 10.112.87.250:43113:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43113;received=10.112.87.250;branch=z9hG4bK-430757
Call-ID: poll-sip-430757
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430757
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430757
CSeq: 430757 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:25:56.919 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:25:56.919 UTC Debug pjsip: tdta0x7f74400b Destroying txdata Response msg 200/OPTIONS/cseq=430757 (tdta0x7f74400bca20)
20-09-2016 10:25:56.919 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f74440779c8
20-09-2016 10:25:56.919 UTC Debug thread_dispatcher.cpp:199: Request latency = 91us
20-09-2016 10:25:56.919 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:25:56.919 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:25:57.916 UTC Verbose pjsip: tcps0x7f744406 TCP connection closed
20-09-2016 10:25:57.916 UTC Debug connection_tracker.cpp:92: Connection 0x7f744406b708 has been destroyed
20-09-2016 10:25:57.916 UTC Verbose pjsip: tcps0x7f744406 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:25:58.731 UTC Debug alarm.cpp:253: Reraising all alarms with a known state
20-09-2016 10:25:58.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:25:58.731 UTC Status alarm.cpp:62: sprout issued 1004.1 alarm
20-09-2016 10:25:58.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:25:58.731 UTC Status alarm.cpp:62: sprout issued 1001.1 alarm
20-09-2016 10:25:58.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:25:58.731 UTC Status alarm.cpp:62: sprout issued 1002.1 alarm
20-09-2016 10:25:58.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:25:58.731 UTC Status alarm.cpp:62: sprout issued 1010.1 alarm
20-09-2016 10:26:06.949 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43144, sock=1301
20-09-2016 10:26:06.949 UTC Verbose pjsip: tcps0x7f744406 TCP server transport created
20-09-2016 10:26:06.949 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430767 (rdata0x7f744406ba40)
20-09-2016 10:26:06.949 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430767 (rdata0x7f744406ba40) from TCP 10.112.87.250:43144:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430767
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430767
Call-ID: poll-sip-430767
CSeq: 430767 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:26:06.949 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:26:06.949 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:26:06.949 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:26:06.949 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f74440779c8 for worker threads
20-09-2016 10:26:06.949 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f74440779c8
20-09-2016 10:26:06.949 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430767 (rdata0x7f74440779c8)
20-09-2016 10:26:06.949 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:26:06.949 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:26:06.949 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430767 (tdta0x7f744813bdd0) created
20-09-2016 10:26:06.949 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430767 (tdta0x7f744813bdd0) to TCP 10.112.87.250:43144:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43144;received=10.112.87.250;branch=z9hG4bK-430767
Call-ID: poll-sip-430767
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430767
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430767
CSeq: 430767 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:26:06.949 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:26:06.949 UTC Debug pjsip: tdta0x7f744813 Destroying txdata Response msg 200/OPTIONS/cseq=430767 (tdta0x7f744813bdd0)
20-09-2016 10:26:06.949 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f74440779c8
20-09-2016 10:26:06.949 UTC Debug thread_dispatcher.cpp:199: Request latency = 87us
20-09-2016 10:26:06.951 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:26:06.951 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:26:07.948 UTC Verbose pjsip: tcps0x7f744406 TCP connection closed
20-09-2016 10:26:07.948 UTC Debug connection_tracker.cpp:92: Connection 0x7f744406b708 has been destroyed
20-09-2016 10:26:07.948 UTC Verbose pjsip: tcps0x7f744406 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:26:16.981 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43174, sock=1301
20-09-2016 10:26:16.981 UTC Verbose pjsip: tcps0x7f744406 TCP server transport created
20-09-2016 10:26:16.981 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430777 (rdata0x7f744406ba40)
20-09-2016 10:26:16.981 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430777 (rdata0x7f744406ba40) from TCP 10.112.87.250:43174:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430777
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430777
Call-ID: poll-sip-430777
CSeq: 430777 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:26:16.981 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:26:16.981 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:26:16.981 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:26:16.981 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f74440779c8 for worker threads
20-09-2016 10:26:16.983 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:26:16.983 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:26:16.984 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f74440779c8
20-09-2016 10:26:16.984 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430777 (rdata0x7f74440779c8)
20-09-2016 10:26:16.984 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:26:16.984 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:26:16.984 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430777 (tdta0x7f744c093160) created
20-09-2016 10:26:16.984 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430777 (tdta0x7f744c093160) to TCP 10.112.87.250:43174:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43174;received=10.112.87.250;branch=z9hG4bK-430777
Call-ID: poll-sip-430777
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430777
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430777
CSeq: 430777 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:26:16.984 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:26:16.984 UTC Debug pjsip: tdta0x7f744c09 Destroying txdata Response msg 200/OPTIONS/cseq=430777 (tdta0x7f744c093160)
20-09-2016 10:26:16.984 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f74440779c8
20-09-2016 10:26:16.984 UTC Debug thread_dispatcher.cpp:199: Request latency = 2832us
20-09-2016 10:26:17.981 UTC Verbose pjsip: tcps0x7f744406 TCP connection closed
20-09-2016 10:26:17.981 UTC Debug connection_tracker.cpp:92: Connection 0x7f744406b708 has been destroyed
20-09-2016 10:26:17.981 UTC Verbose pjsip: tcps0x7f744406 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:26:27.038 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43208, sock=1301
20-09-2016 10:26:27.038 UTC Verbose pjsip: tcps0x7f744406 TCP server transport created
20-09-2016 10:26:27.038 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430788 (rdata0x7f744406ba40)
20-09-2016 10:26:27.038 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430788 (rdata0x7f744406ba40) from TCP 10.112.87.250:43208:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430788
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430788
Call-ID: poll-sip-430788
CSeq: 430788 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:26:27.038 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:26:27.038 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:26:27.038 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:26:27.038 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f74440779c8 for worker threads
20-09-2016 10:26:27.038 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f74440779c8
20-09-2016 10:26:27.038 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430788 (rdata0x7f74440779c8)
20-09-2016 10:26:27.038 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:26:27.038 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:26:27.038 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430788 (tdta0x7f74580ba700) created
20-09-2016 10:26:27.038 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430788 (tdta0x7f74580ba700) to TCP 10.112.87.250:43208:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43208;received=10.112.87.250;branch=z9hG4bK-430788
Call-ID: poll-sip-430788
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430788
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430788
CSeq: 430788 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:26:27.038 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:26:27.038 UTC Debug pjsip: tdta0x7f74580b Destroying txdata Response msg 200/OPTIONS/cseq=430788 (tdta0x7f74580ba700)
20-09-2016 10:26:27.038 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f74440779c8
20-09-2016 10:26:27.038 UTC Debug thread_dispatcher.cpp:199: Request latency = 102us
20-09-2016 10:26:27.042 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:26:27.042 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:26:28.038 UTC Verbose pjsip: tcps0x7f744406 TCP connection closed
20-09-2016 10:26:28.038 UTC Debug connection_tracker.cpp:92: Connection 0x7f744406b708 has been destroyed
20-09-2016 10:26:28.038 UTC Verbose pjsip: tcps0x7f744406 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:26:28.731 UTC Debug alarm.cpp:253: Reraising all alarms with a known state
20-09-2016 10:26:28.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:26:28.731 UTC Status alarm.cpp:62: sprout issued 1004.1 alarm
20-09-2016 10:26:28.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:26:28.731 UTC Status alarm.cpp:62: sprout issued 1001.1 alarm
20-09-2016 10:26:28.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:26:28.731 UTC Status alarm.cpp:62: sprout issued 1002.1 alarm
20-09-2016 10:26:28.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:26:28.731 UTC Status alarm.cpp:62: sprout issued 1010.1 alarm
20-09-2016 10:26:29.806 UTC Verbose pjsip:    tcplis:5052 TCP listener 10.112.87.250:5052: got incoming TCP connection from 10.112.87.250:51493, sock=1301
20-09-2016 10:26:29.806 UTC Verbose pjsip: tcps0x7f744406 TCP server transport created
20-09-2016 10:26:29.816 UTC Verbose pjsip: tcps0x7f744400 TCP connection closed
20-09-2016 10:26:29.816 UTC Verbose pjsip: tcps0x7f744400 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:26:37.074 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:26:37.074 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:26:37.079 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43242, sock=1466
20-09-2016 10:26:37.079 UTC Verbose pjsip: tcps0x7f744400 TCP server transport created
20-09-2016 10:26:37.079 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430798 (rdata0x7f7444001120)
20-09-2016 10:26:37.079 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430798 (rdata0x7f7444001120) from TCP 10.112.87.250:43242:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430798
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430798
Call-ID: poll-sip-430798
CSeq: 430798 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:26:37.079 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:26:37.079 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:26:37.079 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:26:37.079 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f74440779c8 for worker threads
20-09-2016 10:26:37.079 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f74440779c8
20-09-2016 10:26:37.079 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430798 (rdata0x7f74440779c8)
20-09-2016 10:26:37.079 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:26:37.079 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:26:37.079 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430798 (tdta0x7f7454114e60) created
20-09-2016 10:26:37.079 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430798 (tdta0x7f7454114e60) to TCP 10.112.87.250:43242:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43242;received=10.112.87.250;branch=z9hG4bK-430798
Call-ID: poll-sip-430798
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430798
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430798
CSeq: 430798 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:26:37.079 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:26:37.079 UTC Debug pjsip: tdta0x7f745411 Destroying txdata Response msg 200/OPTIONS/cseq=430798 (tdta0x7f7454114e60)
20-09-2016 10:26:37.079 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f74440779c8
20-09-2016 10:26:37.079 UTC Debug thread_dispatcher.cpp:199: Request latency = 106us
20-09-2016 10:26:38.078 UTC Verbose pjsip: tcps0x7f744400 TCP connection closed
20-09-2016 10:26:38.078 UTC Debug connection_tracker.cpp:92: Connection 0x7f7444000de8 has been destroyed
20-09-2016 10:26:38.078 UTC Verbose pjsip: tcps0x7f744400 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:26:47.108 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43270, sock=1466
20-09-2016 10:26:47.108 UTC Verbose pjsip: tcps0x7f744400 TCP server transport created
20-09-2016 10:26:47.108 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430808 (rdata0x7f7444001120)
20-09-2016 10:26:47.108 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430808 (rdata0x7f7444001120) from TCP 10.112.87.250:43270:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430808
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430808
Call-ID: poll-sip-430808
CSeq: 430808 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:26:47.108 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:26:47.108 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:26:47.108 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:26:47.108 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f74440779c8 for worker threads
20-09-2016 10:26:47.108 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f74440779c8
20-09-2016 10:26:47.108 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430808 (rdata0x7f74440779c8)
20-09-2016 10:26:47.108 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:26:47.108 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:26:47.108 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430808 (tdta0x7f745c119d40) created
20-09-2016 10:26:47.108 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430808 (tdta0x7f745c119d40) to TCP 10.112.87.250:43270:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43270;received=10.112.87.250;branch=z9hG4bK-430808
Call-ID: poll-sip-430808
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430808
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430808
CSeq: 430808 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:26:47.108 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:26:47.108 UTC Debug pjsip: tdta0x7f745c11 Destroying txdata Response msg 200/OPTIONS/cseq=430808 (tdta0x7f745c119d40)
20-09-2016 10:26:47.108 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f74440779c8
20-09-2016 10:26:47.108 UTC Debug thread_dispatcher.cpp:199: Request latency = 101us
20-09-2016 10:26:47.116 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:26:47.116 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:26:48.108 UTC Verbose pjsip: tcps0x7f744400 TCP connection closed
20-09-2016 10:26:48.108 UTC Debug connection_tracker.cpp:92: Connection 0x7f7444000de8 has been destroyed
20-09-2016 10:26:48.108 UTC Verbose pjsip: tcps0x7f744400 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:26:49.753 UTC Verbose pjsip: tcps0x7f744402 TCP transport destroyed normally
20-09-2016 10:26:49.808 UTC Verbose pjsip:    tcplis:5052 TCP listener 10.112.87.250:5052: got incoming TCP connection from 10.112.87.250:45802, sock=1437
20-09-2016 10:26:49.808 UTC Verbose pjsip: tcps0x7f744400 TCP server transport created
20-09-2016 10:26:50.754 UTC Verbose pjsip: tcps0x7f744403 TCP transport destroyed normally
20-09-2016 10:26:50.808 UTC Verbose pjsip:    tcplis:5052 TCP listener 10.112.87.250:5052: got incoming TCP connection from 10.112.87.250:53684, sock=1466
20-09-2016 10:26:50.808 UTC Verbose pjsip: tcps0x7f744402 TCP server transport created
20-09-2016 10:26:57.151 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43302, sock=1481
20-09-2016 10:26:57.151 UTC Verbose pjsip: tcps0x7f744403 TCP server transport created
20-09-2016 10:26:57.151 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430818 (rdata0x7f74440382f0)
20-09-2016 10:26:57.151 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430818 (rdata0x7f74440382f0) from TCP 10.112.87.250:43302:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430818
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430818
Call-ID: poll-sip-430818
CSeq: 430818 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:26:57.151 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:26:57.152 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:26:57.152 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:26:57.152 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f74440779c8 for worker threads
20-09-2016 10:26:57.152 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f74440779c8
20-09-2016 10:26:57.152 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430818 (rdata0x7f74440779c8)
20-09-2016 10:26:57.152 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:26:57.152 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:26:57.152 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430818 (tdta0x7f74c40c0e90) created
20-09-2016 10:26:57.152 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430818 (tdta0x7f74c40c0e90) to TCP 10.112.87.250:43302:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43302;received=10.112.87.250;branch=z9hG4bK-430818
Call-ID: poll-sip-430818
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430818
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430818
CSeq: 430818 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:26:57.152 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:26:57.152 UTC Debug pjsip: tdta0x7f74c40c Destroying txdata Response msg 200/OPTIONS/cseq=430818 (tdta0x7f74c40c0e90)
20-09-2016 10:26:57.152 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f74440779c8
20-09-2016 10:26:57.152 UTC Debug thread_dispatcher.cpp:199: Request latency = 114us
20-09-2016 10:26:57.154 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:26:57.154 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:26:58.152 UTC Verbose pjsip: tcps0x7f744403 TCP connection closed
20-09-2016 10:26:58.152 UTC Debug connection_tracker.cpp:92: Connection 0x7f7444037fb8 has been destroyed
20-09-2016 10:26:58.152 UTC Verbose pjsip: tcps0x7f744403 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:26:58.731 UTC Debug alarm.cpp:253: Reraising all alarms with a known state
20-09-2016 10:26:58.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:26:58.731 UTC Status alarm.cpp:62: sprout issued 1004.1 alarm
20-09-2016 10:26:58.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:26:58.731 UTC Status alarm.cpp:62: sprout issued 1001.1 alarm
20-09-2016 10:26:58.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:26:58.731 UTC Status alarm.cpp:62: sprout issued 1002.1 alarm
20-09-2016 10:26:58.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:26:58.731 UTC Status alarm.cpp:62: sprout issued 1010.1 alarm
20-09-2016 10:27:03.809 UTC Verbose pjsip:    tcplis:5052 TCP listener 10.112.87.250:5052: got incoming TCP connection from 10.112.87.250:39508, sock=1481
20-09-2016 10:27:03.809 UTC Verbose pjsip: tcps0x7f744403 TCP server transport created
20-09-2016 10:27:03.819 UTC Verbose pjsip: tcps0x7f74440d TCP connection closed
20-09-2016 10:27:03.819 UTC Verbose pjsip: tcps0x7f74440d TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:27:07.189 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:27:07.189 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:27:07.190 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43335, sock=1486
20-09-2016 10:27:07.190 UTC Verbose pjsip: tcps0x7f74440d TCP server transport created
20-09-2016 10:27:07.190 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430828 (rdata0x7f74440d7440)
20-09-2016 10:27:07.190 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430828 (rdata0x7f74440d7440) from TCP 10.112.87.250:43335:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430828
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430828
Call-ID: poll-sip-430828
CSeq: 430828 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:27:07.190 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:07.190 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:27:07.190 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:27:07.190 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f74440779c8 for worker threads
20-09-2016 10:27:07.190 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f74440779c8
20-09-2016 10:27:07.190 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430828 (rdata0x7f74440779c8)
20-09-2016 10:27:07.190 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:07.190 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:27:07.190 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430828 (tdta0x7f74cc0f6d70) created
20-09-2016 10:27:07.190 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430828 (tdta0x7f74cc0f6d70) to TCP 10.112.87.250:43335:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43335;received=10.112.87.250;branch=z9hG4bK-430828
Call-ID: poll-sip-430828
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430828
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430828
CSeq: 430828 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:27:07.190 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:27:07.190 UTC Debug pjsip: tdta0x7f74cc0f Destroying txdata Response msg 200/OPTIONS/cseq=430828 (tdta0x7f74cc0f6d70)
20-09-2016 10:27:07.190 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f74440779c8
20-09-2016 10:27:07.190 UTC Debug thread_dispatcher.cpp:199: Request latency = 96us
20-09-2016 10:27:08.188 UTC Verbose pjsip: tcps0x7f74440d TCP connection closed
20-09-2016 10:27:08.188 UTC Debug connection_tracker.cpp:92: Connection 0x7f74440d7108 has been destroyed
20-09-2016 10:27:08.188 UTC Verbose pjsip: tcps0x7f74440d TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:27:11.810 UTC Verbose pjsip:    tcplis:5052 TCP listener 10.112.87.250:5052: got incoming TCP connection from 10.112.87.250:38033, sock=1486
20-09-2016 10:27:11.810 UTC Verbose pjsip: tcps0x7f74440d TCP server transport created
20-09-2016 10:27:11.820 UTC Verbose pjsip: tcps0x7f744402 TCP connection closed
20-09-2016 10:27:11.820 UTC Verbose pjsip: tcps0x7f744402 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:27:15.757 UTC Verbose pjsip: tcps0x7f74440d TCP transport destroyed normally
20-09-2016 10:27:15.810 UTC Verbose pjsip:    tcplis:5052 TCP listener 10.112.87.250:5052: got incoming TCP connection from 10.112.87.250:57692, sock=1500
20-09-2016 10:27:15.810 UTC Verbose pjsip: tcps0x7f74440d TCP server transport created
20-09-2016 10:27:17.224 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43366, sock=1529
20-09-2016 10:27:17.224 UTC Verbose pjsip: tcps0x7f744402 TCP server transport created
20-09-2016 10:27:17.224 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430838 (rdata0x7f744402f210)
20-09-2016 10:27:17.224 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430838 (rdata0x7f744402f210) from TCP 10.112.87.250:43366:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430838
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430838
Call-ID: poll-sip-430838
CSeq: 430838 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:27:17.224 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:17.224 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:27:17.224 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:27:17.224 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f74440779c8 for worker threads
20-09-2016 10:27:17.224 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f74440779c8
20-09-2016 10:27:17.224 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430838 (rdata0x7f74440779c8)
20-09-2016 10:27:17.224 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:17.224 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:27:17.224 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430838 (tdta0x7f74dc079950) created
20-09-2016 10:27:17.224 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430838 (tdta0x7f74dc079950) to TCP 10.112.87.250:43366:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43366;received=10.112.87.250;branch=z9hG4bK-430838
Call-ID: poll-sip-430838
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430838
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430838
CSeq: 430838 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:27:17.224 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:27:17.224 UTC Debug pjsip: tdta0x7f74dc07 Destroying txdata Response msg 200/OPTIONS/cseq=430838 (tdta0x7f74dc079950)
20-09-2016 10:27:17.224 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f74440779c8
20-09-2016 10:27:17.224 UTC Debug thread_dispatcher.cpp:199: Request latency = 91us
20-09-2016 10:27:17.229 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:27:17.229 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:27:18.224 UTC Verbose pjsip: tcps0x7f744402 TCP connection closed
20-09-2016 10:27:18.224 UTC Debug connection_tracker.cpp:92: Connection 0x7f744402eed8 has been destroyed
20-09-2016 10:27:18.224 UTC Verbose pjsip: tcps0x7f744402 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:27:19.756 UTC Verbose pjsip: tcps0x7f744408 TCP transport destroyed normally
20-09-2016 10:27:19.811 UTC Verbose pjsip:    tcplis:5052 TCP listener 10.112.87.250:5052: got incoming TCP connection from 10.112.87.250:41286, sock=1307
20-09-2016 10:27:19.811 UTC Verbose pjsip: tcps0x7f744408 TCP server transport created
20-09-2016 10:27:21.757 UTC Verbose pjsip: tcps0x7f744416 TCP transport destroyed normally
20-09-2016 10:27:21.811 UTC Verbose pjsip:    tcplis:5052 TCP listener 10.112.87.250:5052: got incoming TCP connection from 10.112.87.250:49730, sock=1329
20-09-2016 10:27:21.811 UTC Verbose pjsip: tcps0x7f744416 TCP server transport created
20-09-2016 10:27:27.275 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:27:27.275 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:27:27.278 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43400, sock=1529
20-09-2016 10:27:27.278 UTC Verbose pjsip: tcps0x7f744402 TCP server transport created
20-09-2016 10:27:27.278 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430848 (rdata0x7f744402f210)
20-09-2016 10:27:27.278 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430848 (rdata0x7f744402f210) from TCP 10.112.87.250:43400:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430848
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430848
Call-ID: poll-sip-430848
CSeq: 430848 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:27:27.278 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:27.278 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:27:27.278 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:27:27.278 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f74440779c8 for worker threads
20-09-2016 10:27:27.279 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f74440779c8
20-09-2016 10:27:27.279 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430848 (rdata0x7f74440779c8)
20-09-2016 10:27:27.279 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:27.279 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:27:27.279 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430848 (tdta0x7f74d40d2ea0) created
20-09-2016 10:27:27.279 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430848 (tdta0x7f74d40d2ea0) to TCP 10.112.87.250:43400:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43400;received=10.112.87.250;branch=z9hG4bK-430848
Call-ID: poll-sip-430848
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430848
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430848
CSeq: 430848 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:27:27.279 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:27:27.279 UTC Debug pjsip: tdta0x7f74d40d Destroying txdata Response msg 200/OPTIONS/cseq=430848 (tdta0x7f74d40d2ea0)
20-09-2016 10:27:27.279 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f74440779c8
20-09-2016 10:27:27.279 UTC Debug thread_dispatcher.cpp:199: Request latency = 1780us
20-09-2016 10:27:28.275 UTC Verbose pjsip: tcps0x7f744402 TCP connection closed
20-09-2016 10:27:28.276 UTC Debug connection_tracker.cpp:92: Connection 0x7f744402eed8 has been destroyed
20-09-2016 10:27:28.276 UTC Verbose pjsip: tcps0x7f744402 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:27:28.731 UTC Debug alarm.cpp:253: Reraising all alarms with a known state
20-09-2016 10:27:28.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:27:28.731 UTC Status alarm.cpp:62: sprout issued 1004.1 alarm
20-09-2016 10:27:28.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:27:28.731 UTC Status alarm.cpp:62: sprout issued 1001.1 alarm
20-09-2016 10:27:28.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:27:28.731 UTC Status alarm.cpp:62: sprout issued 1002.1 alarm
20-09-2016 10:27:28.731 UTC Debug alarm.cpp:312: AlarmReqAgent: queue overflowed
20-09-2016 10:27:28.731 UTC Status alarm.cpp:62: sprout issued 1010.1 alarm
20-09-2016 10:27:28.856 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg REGISTER/cseq=147 (rdata0x7f744405cb70)
20-09-2016 10:27:28.856 UTC Verbose common_sip_processing.cpp:120: RX 1242 bytes Request msg REGISTER/cseq=147 (rdata0x7f744405cb70) from TCP 10.112.87.250:56834:
--start msg--

REGISTER sip:example.com;transport=TCP SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250:56834;rport;branch=z9hG4bKPjZJrgNA3v0ZhvLg7MFYya6GmcF1l.10Sm
Path: <sip:paRGfRUGv7 at 10.112.87.250:5058;transport=TCP;lr;ob>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---c6b2fe2e8d687e30
Max-Forwards: 70
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e>
To: <sip:6505550962 at example.com>
From: <sip:6505550962 at example.com>;tag=c523ac41
Call-ID: Dxjgy0nc4Yr5QvvP7aRrqA..
CSeq: 147 REGISTER
Expires: 3600
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Authorization: Digest response="58c71fbab2b57fda1783fb96911fe3ed", username="6505550962 at example.com", realm="example.com", nonce="6a308edb6cb998ed", uri="sip:example.com;transport=TCP", algorithm=MD5, cnonce="1e4f3c75e95d0cac747959ea92f07d88", opaque="51de7ee852267446", qop=auth, nc=0000002e,integrity-protected=ip-assoc-yes
Allow-Events: presence, kpml
P-Visited-Network-ID: example.com
Route: <sip:scscf.cw-aio:5052;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
20-09-2016 10:27:28.856 UTC Debug pjutils.cpp:1660: Logging SAS Call-ID marker, Call-ID Dxjgy0nc4Yr5QvvP7aRrqA..
20-09-2016 10:27:28.856 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f74440779c8 for worker threads
20-09-2016 10:27:28.856 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f74440779c8
20-09-2016 10:27:28.856 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg REGISTER/cseq=147 (rdata0x7f74440779c8)
20-09-2016 10:27:28.856 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:28.856 UTC Debug uri_classifier.cpp:199: Classified URI as 4
20-09-2016 10:27:28.856 UTC Debug authentication.cpp:804: Authentication module invoked
20-09-2016 10:27:28.856 UTC Debug authentication.cpp:656: Request does not need authentication - not on S-CSCF port
20-09-2016 10:27:28.856 UTC Debug authentication.cpp:814: Request does not need authentication
20-09-2016 10:27:28.856 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:28.856 UTC Debug uri_classifier.cpp:199: Classified URI as 4
20-09-2016 10:27:28.856 UTC Debug basicproxy.cpp:92: Process REGISTER request
20-09-2016 10:27:28.856 UTC Verbose sproutletproxy.cpp:498: Sproutlet Proxy transaction (0x15809e0) created
20-09-2016 10:27:28.856 UTC Debug basicproxy.cpp:1271: Report SAS start marker - trail (1e7f)
20-09-2016 10:27:28.856 UTC Debug pjutils.cpp:674: Cloned Request msg REGISTER/cseq=147 (rdata0x7f74440779c8) to tdta0x25a7200
20-09-2016 10:27:28.856 UTC Debug pjsip:   tsx0x25a9268 Transaction created for Request msg REGISTER/cseq=147 (rdata0x7f74440779c8)
20-09-2016 10:27:28.856 UTC Debug pjsip:   tsx0x25a9268 Incoming Request msg REGISTER/cseq=147 (rdata0x7f74440779c8) in state Null
20-09-2016 10:27:28.856 UTC Debug pjsip:   tsx0x25a9268 State changed from Null to Trying, event=RX_MSG
20-09-2016 10:27:28.856 UTC Debug basicproxy.cpp:213: tsx0x25a9268 - tu_on_tsx_state UAS, TSX_STATE RX_MSG state=Trying
20-09-2016 10:27:28.856 UTC Debug pjsip:       endpoint Response msg 408/REGISTER/cseq=147 (tdta0x25a99d0) created
20-09-2016 10:27:28.856 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request
20-09-2016 10:27:28.856 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:scscf.cw-aio:5052;transport=TCP;lr;orig
20-09-2016 10:27:28.856 UTC Debug sproutletproxy.cpp:329: Possible service name - scscf
20-09-2016 10:27:28.856 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio
20-09-2016 10:27:28.856 UTC Debug sproutletproxy.cpp:329: Possible service name - scscf
20-09-2016 10:27:28.856 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio
20-09-2016 10:27:28.856 UTC Debug sproutletproxy.cpp:329: Possible service name - scscf
20-09-2016 10:27:28.856 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio
20-09-2016 10:27:28.856 UTC Debug sproutletproxy.cpp:329: Possible service name - scscf
20-09-2016 10:27:28.856 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio
20-09-2016 10:27:28.856 UTC Debug scscfsproutlet.cpp:389: S-CSCF Transaction (0x25a5c20) created
20-09-2016 10:27:28.856 UTC Verbose sproutletproxy.cpp:1154: Created Sproutlet scscf-0x25a5c20 for Request msg REGISTER/cseq=147 (tdta0x25a7200)
20-09-2016 10:27:28.857 UTC Verbose sproutletproxy.cpp:2062: Routing Request msg REGISTER/cseq=147 (tdta0x25a7200) (1271 bytes) to downstream sproutlet scscf:
--start msg--

REGISTER sip:example.com;transport=TCP SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250:56834;rport=56834;received=10.112.87.250;branch=z9hG4bKPjZJrgNA3v0ZhvLg7MFYya6GmcF1l.10Sm
Path: <sip:paRGfRUGv7 at 10.112.87.250:5058;transport=TCP;lr;ob>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---c6b2fe2e8d687e30
Max-Forwards: 70
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e>
To: <sip:6505550962 at example.com>
From: <sip:6505550962 at example.com>;tag=c523ac41
Call-ID: Dxjgy0nc4Yr5QvvP7aRrqA..
CSeq: 147 REGISTER
Expires: 3600
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Authorization: Digest response="58c71fbab2b57fda1783fb96911fe3ed", username="6505550962 at example.com", realm="example.com", nonce="6a308edb6cb998ed", uri="sip:example.com;transport=TCP", algorithm=MD5, cnonce="1e4f3c75e95d0cac747959ea92f07d88", opaque="51de7ee852267446", qop=auth, nc=0000002e,integrity-protected=ip-assoc-yes
Allow-Events: presence, kpml
P-Visited-Network-ID: example.com
Route: <sip:scscf.cw-aio:5052;transport=TCP;lr;orig>
Content-Length:  0


--end msg--
20-09-2016 10:27:28.857 UTC Debug pjutils.cpp:691: Cloned tdta0x25a7200 to tdta0x25aa9e0
20-09-2016 10:27:28.857 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:scscf.cw-aio:5052;transport=TCP;lr;orig>
20-09-2016 10:27:28.857 UTC Debug sproutletproxy.cpp:1735: Adding message 0x25aaff0 => txdata 0x25aaa88 mapping
20-09-2016 10:27:28.857 UTC Verbose sproutletproxy.cpp:1587: scscf-0x25a5c20 pass initial request Request msg REGISTER/cseq=147 (tdta0x25aa9e0) to Sproutlet
20-09-2016 10:27:28.857 UTC Info scscfsproutlet.cpp:431: S-CSCF received initial request
20-09-2016 10:27:28.857 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:28.857 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:27:28.857 UTC Debug scscfsproutlet.cpp:773: Route header references this system
20-09-2016 10:27:28.857 UTC Debug scscfsproutlet.cpp:826: No ODI token, or invalid ODI token, on request, and no P-Charging-Vector header (so can't log ICID for correlation)
20-09-2016 10:27:28.857 UTC Debug scscfsproutlet.cpp:832: Got our Route header, session case orig, OD=None
20-09-2016 10:27:28.857 UTC Debug pjutils.cpp:312: From header 0x25cb090
20-09-2016 10:27:28.857 UTC Debug pjutils.cpp:314: Served user from From header (0x25cb138)
20-09-2016 10:27:28.857 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:28.857 UTC Debug uri_classifier.cpp:199: Classified URI as 4
20-09-2016 10:27:28.857 UTC Debug acr.cpp:49: Created ACR (0x23c8d30)
20-09-2016 10:27:28.857 UTC Debug scscfsproutlet.cpp:1015: Single Record-Route - initiation of originating handling
20-09-2016 10:27:28.857 UTC Debug sproutletproxy.cpp:387: Creating URI for scscf
20-09-2016 10:27:28.857 UTC Debug sproutletproxy.cpp:391: Add services parameter
20-09-2016 10:27:28.857 UTC Debug sproutletproxy.cpp:399: sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf
20-09-2016 10:27:28.857 UTC Debug scscfsproutlet.cpp:1021: Looking up iFCs for sip:6505550962 at example.com for new AS chain
20-09-2016 10:27:28.857 UTC Debug hssconnection.cpp:587: Making Homestead request for /impu/sip%3A6505550962%40example.com/reg-data
20-09-2016 10:27:28.857 UTC Debug httpresolver.cpp:72: HttpResolver::resolve for host 10.112.87.250, port 8888, family 2
20-09-2016 10:27:28.857 UTC Debug baseresolver.cpp:523: Attempt to parse 10.112.87.250 as IP address
20-09-2016 10:27:28.857 UTC Debug httpresolver.cpp:80: Target is an IP address
20-09-2016 10:27:28.857 UTC Debug connection_pool.h:225: Request for connection to IP: 10.112.87.250, port: 8888
20-09-2016 10:27:28.857 UTC Debug connection_pool.h:238: Found existing connection 0x7f745c08b760 in pool
20-09-2016 10:27:28.857 UTC Debug httpclient.cpp:466: Sending HTTP request : http://10.112.87.250:8888/impu/sip%3A6505550962%40example.com/reg-data (trying 10.112.87.250)
20-09-2016 10:27:28.859 UTC Debug httpclient.cpp:743: Received header http/1.1200ok with value 
20-09-2016 10:27:28.859 UTC Debug httpclient.cpp:743: Received header content-length with value 1366
20-09-2016 10:27:28.859 UTC Debug httpclient.cpp:743: Received header content-type with value text/plain
20-09-2016 10:27:28.859 UTC Debug httpclient.cpp:743: Received header  with value 
20-09-2016 10:27:28.859 UTC Debug httpclient.cpp:482: Received HTTP response: status=200, doc=<ClearwaterRegData>
	<RegistrationState>REGISTERED</RegistrationState>
	<IMSSubscription xsi="http://www.w3.org/2001/XMLSchema-instance" noNamespaceSchemaLocation="CxDataType.xsd">
		<PrivateID>Unspecified</PrivateID>
		<ServiceProfile>
			<InitialFilterCriteria>
				<TriggerPoint>
					<ConditionTypeCNF>0</ConditionTypeCNF>
					<SPT>
						<ConditionNegated>0</ConditionNegated>
						<Group>0</Group>
						<Method>INVITE</Method>
						<Extension/>
					</SPT>
				</TriggerPoint>
				<ApplicationServer>
					<ServerName>sip:mmtel.example.com</ServerName>
					<DefaultHandling>0</DefaultHandling>
				</ApplicationServer>
			</InitialFilterCriteria>
			<PublicIdentity>
				<Identity>sip:6505550962 at example.com</Identity>
			</PublicIdentity>
		</ServiceProfile>
		<ServiceProfile>
			<InitialFilterCriteria>
				<TriggerPoint>
					<ConditionTypeCNF>0</ConditionTypeCNF>
					<SPT>
						<ConditionNegated>0</ConditionNegated>
						<Group>0</Group>
						<Method>INVITE</Method>
						<Extension/>
					</SPT>
				</TriggerPoint>
				<ApplicationServer>
					<ServerName>sip:mmtel.example.com</ServerName>
					<DefaultHandling>0</DefaultHandling>
				</ApplicationServer>
			</InitialFilterCriteria>
			<PublicIdentity>
				<Identity>sip:6505550997 at example.com</Identity>
			</PublicIdentity>
		</ServiceProfile>
	</IMSSubscription>
</ClearwaterRegData>


20-09-2016 10:27:28.859 UTC Debug baseresolver.cpp:1004: Successful response from  10.112.87.250:8888 transport 6
20-09-2016 10:27:28.859 UTC Debug connection_pool.h:261: Release connection to IP: 10.112.87.250, port: 8888 to pool
20-09-2016 10:27:28.859 UTC Debug baseresolver.cpp:1034: 10.112.87.250:8888 transport 6 returned untested
20-09-2016 10:27:28.859 UTC Debug communicationmonitor.cpp:82: Checking communication changes - successful attempts 1, failures 0
20-09-2016 10:27:28.859 UTC Debug hssconnection.cpp:368: Processing Identity node from HSS XML - sip:6505550962 at example.com

20-09-2016 10:27:28.859 UTC Debug hssconnection.cpp:368: Processing Identity node from HSS XML - sip:6505550997 at example.com

20-09-2016 10:27:28.859 UTC Debug scscfsproutlet.cpp:1026: Successfully looked up iFCs
20-09-2016 10:27:28.859 UTC Debug aschain.cpp:71: Creating AsChain 0x258edc0 with 1 IFC and adding to map
20-09-2016 10:27:28.859 UTC Debug aschain.cpp:73: Attached ACR (0x23c8d30) to chain
20-09-2016 10:27:28.859 UTC Debug scscfsproutlet.cpp:1145: S-CSCF sproutlet transaction 0x25a5c20 linked to AsChain AsChain-orig[0x258edc0]:1/1
20-09-2016 10:27:28.859 UTC Info scscfsproutlet.cpp:502: Found served user, so apply services
20-09-2016 10:27:28.859 UTC Debug scscfsproutlet.cpp:1153: Performing originating initiating request processing
20-09-2016 10:27:28.859 UTC Debug ifchandler.cpp:437: SPT class Method: result false
20-09-2016 10:27:28.859 UTC Debug ifchandler.cpp:541: Add to group 0 val false
20-09-2016 10:27:28.859 UTC Debug ifchandler.cpp:559: Result group 0 val false
20-09-2016 10:27:28.859 UTC Debug ifchandler.cpp:572: iFC does not match
20-09-2016 10:27:28.859 UTC Info scscfsproutlet.cpp:1178: Completed applying originating services
20-09-2016 10:27:28.859 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true
20-09-2016 10:27:28.859 UTC Debug uri_classifier.cpp:199: Classified URI as 2
20-09-2016 10:27:28.859 UTC Debug pjutils.cpp:2218: Translating URI
20-09-2016 10:27:28.859 UTC Debug pjutils.cpp:2189: Performing ENUM translation for user 
20-09-2016 10:27:28.859 UTC Debug enumservice.cpp:240: Translating URI via JSON ENUM lookup
20-09-2016 10:27:28.859 UTC Info enumservice.cpp:244: No dial string supplied, so don't do ENUM lookup
20-09-2016 10:27:28.859 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:28.859 UTC Debug uri_classifier.cpp:199: Classified URI as 4
20-09-2016 10:27:28.860 UTC Info scscfsproutlet.cpp:1194: New URI string is sip:example.com;transport=TCP
20-09-2016 10:27:28.860 UTC Info scscfsproutlet.cpp:1422: Routing to I-CSCF sip:icscf.cw-aio:5052;transport=TCP
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x25aaff0
20-09-2016 10:27:28.860 UTC Verbose sproutletproxy.cpp:1386: scscf-0x25a5c20 sending Request msg REGISTER/cseq=147 (tdta0x25aa9e0) on fork 0
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1790: Processing request 0x25aaa88, fork = 0
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1914: scscf-0x25a5c20 transmitting request on fork 0
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1928: scscf-0x25a5c20 store reference to non-ACK request Request msg REGISTER/cseq=147 (tdta0x25aa9e0) on fork 0
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1742: Removing message 0x25aaff0 => txdata 0x25aaa88 mapping
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:icscf.cw-aio:5052;transport=TCP;lr
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:329: Possible service name - icscf
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:329: Possible service name - icscf
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio
20-09-2016 10:27:28.860 UTC Verbose sproutletproxy.cpp:1154: Created Sproutlet icscf-0x25a7020 for Request msg REGISTER/cseq=147 (tdta0x25aa9e0)
20-09-2016 10:27:28.860 UTC Verbose sproutletproxy.cpp:2062: Routing Request msg REGISTER/cseq=147 (tdta0x25aa9e0) (1361 bytes) to downstream sproutlet icscf:
--start msg--

REGISTER sip:example.com;transport=TCP SIP/2.0
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Via: SIP/2.0/TCP 10.112.87.250:56834;rport=56834;received=10.112.87.250;branch=z9hG4bKPjZJrgNA3v0ZhvLg7MFYya6GmcF1l.10Sm
Path: <sip:paRGfRUGv7 at 10.112.87.250:5058;transport=TCP;lr;ob>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---c6b2fe2e8d687e30
Max-Forwards: 69
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e>
To: <sip:6505550962 at example.com>
From: <sip:6505550962 at example.com>;tag=c523ac41
Call-ID: Dxjgy0nc4Yr5QvvP7aRrqA..
CSeq: 147 REGISTER
Expires: 3600
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Authorization: Digest response="58c71fbab2b57fda1783fb96911fe3ed", username="6505550962 at example.com", realm="example.com", nonce="6a308edb6cb998ed", uri="sip:example.com;transport=TCP", algorithm=MD5, cnonce="1e4f3c75e95d0cac747959ea92f07d88", opaque="51de7ee852267446", qop=auth, nc=0000002e,integrity-protected=ip-assoc-yes
Allow-Events: presence, kpml
P-Visited-Network-ID: example.com
Route: <sip:icscf.cw-aio:5052;transport=TCP;lr>
Content-Length:  0


--end msg--
20-09-2016 10:27:28.860 UTC Debug pjutils.cpp:691: Cloned tdta0x25aa9e0 to tdta0x25d7fb0
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:icscf.cw-aio:5052;transport=TCP;lr>
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1735: Adding message 0x25d85c0 => txdata 0x25d8058 mapping
20-09-2016 10:27:28.860 UTC Verbose sproutletproxy.cpp:1587: icscf-0x25a7020 pass initial request Request msg REGISTER/cseq=147 (tdta0x25d7fb0) to Sproutlet
20-09-2016 10:27:28.860 UTC Debug acr.cpp:49: Created ACR (0x25984a0)
20-09-2016 10:27:28.860 UTC Debug icscfsproutlet.cpp:185: I-CSCF initialize transaction for REGISTER request
20-09-2016 10:27:28.860 UTC Debug icscfrouter.cpp:345: Perform UAR - impi 6505550962 at example.com, impu sip:6505550962 at example.com, vn example.com, auth_type REG
20-09-2016 10:27:28.860 UTC Debug httpresolver.cpp:72: HttpResolver::resolve for host 10.112.87.250, port 8888, family 2
20-09-2016 10:27:28.860 UTC Debug baseresolver.cpp:523: Attempt to parse 10.112.87.250 as IP address
20-09-2016 10:27:28.860 UTC Debug httpresolver.cpp:80: Target is an IP address
20-09-2016 10:27:28.860 UTC Debug connection_pool.h:225: Request for connection to IP: 10.112.87.250, port: 8888
20-09-2016 10:27:28.860 UTC Debug connection_pool.h:238: Found existing connection 0x7f745c08b760 in pool
20-09-2016 10:27:28.860 UTC Debug httpclient.cpp:466: Sending HTTP request : http://10.112.87.250:8888/impi/6505550962%40example.com/registration-status?impu=sip%3A6505550962%40example.com&visited-network=example.com&auth-type=REG (trying 10.112.87.250)
20-09-2016 10:27:28.860 UTC Debug httpclient.cpp:482: Received HTTP response: status=200, doc={"result-code":2001,"scscf":"sip:scscf.cw-aio:5054;transport=TCP"}
20-09-2016 10:27:28.860 UTC Debug baseresolver.cpp:1004: Successful response from  10.112.87.250:8888 transport 6
20-09-2016 10:27:28.860 UTC Debug connection_pool.h:261: Release connection to IP: 10.112.87.250, port: 8888 to pool
20-09-2016 10:27:28.860 UTC Debug baseresolver.cpp:1034: 10.112.87.250:8888 transport 6 returned untested
20-09-2016 10:27:28.860 UTC Debug icscfrouter.cpp:237: HSS returned S-CSCF sip:scscf.cw-aio:5054;transport=TCP as target
20-09-2016 10:27:28.860 UTC Debug icscfrouter.cpp:113: SCSCF specified by HSS: sip:scscf.cw-aio:5054;transport=TCP
20-09-2016 10:27:28.860 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:28.860 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:27:28.860 UTC Debug icscfsproutlet.cpp:271: Found SCSCF for REGISTER
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x25d85c0
20-09-2016 10:27:28.860 UTC Verbose sproutletproxy.cpp:1386: icscf-0x25a7020 sending Request msg REGISTER/cseq=147 (tdta0x25d7fb0) on fork 0
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1790: Processing request 0x25d8058, fork = 0
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1914: icscf-0x25a7020 transmitting request on fork 0
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1928: icscf-0x25a7020 store reference to non-ACK request Request msg REGISTER/cseq=147 (tdta0x25d7fb0) on fork 0
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:1742: Removing message 0x25d85c0 => txdata 0x25d8058 mapping
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:181: Can't determine whether the next route is a sproutlet based on the URI
20-09-2016 10:27:28.860 UTC Debug sproutletproxy.cpp:874: No local sproutlet matches request
20-09-2016 10:27:28.860 UTC Debug pjsip:   tsx0x25f3158 Transaction created for Request msg REGISTER/cseq=147 (tdta0x25d7fb0)
20-09-2016 10:27:28.860 UTC Debug basicproxy.cpp:1618: Added trail identifier 7807 to UAC transaction
20-09-2016 10:27:28.860 UTC Debug pjutils.cpp:490: Next hop node is encoded in Request-URI
20-09-2016 10:27:28.860 UTC Debug sipresolver.cpp:86: SIPResolver::resolve for name scscf.cw-aio, port 5054, transport 6, family 2
20-09-2016 10:27:28.860 UTC Debug baseresolver.cpp:523: Attempt to parse scscf.cw-aio as IP address
20-09-2016 10:27:28.860 UTC Debug sipresolver.cpp:128: Port is specified
20-09-2016 10:27:28.860 UTC Debug sipresolver.cpp:296: Perform A/AAAA record lookup only, name = scscf.cw-aio
20-09-2016 10:27:28.860 UTC Debug dnscachedresolver.cpp:724: Removing record for scscf.cw-aio (type 1, expiry time 1474366983) from the expiry list
20-09-2016 10:27:28.860 UTC Verbose dnscachedresolver.cpp:245: Check cache for scscf.cw-aio type 1
20-09-2016 10:27:28.860 UTC Debug dnscachedresolver.cpp:278: Expired entry found in cache - starting asynchronous query to update it
20-09-2016 10:27:28.860 UTC Debug dnscachedresolver.cpp:302: Create and execute DNS query transaction
20-09-2016 10:27:28.860 UTC Debug dnscachedresolver.cpp:315: Wait for query responses
20-09-2016 10:27:28.860 UTC Debug dnscachedresolver.cpp:465: Received DNS response for scscf.cw-aio type A
20-09-2016 10:27:28.860 UTC Debug dnsparser.cpp:90: Parsing DNS message
000000: ab3d8580 00010001 00000000 05736373 63660663 772d6169 6f000001 0001c00c    .=.. .... .... .scs cf.c w-ai o... .... 
000020: 00010001 00000000 00040a70 57fa                                            .... .... ...p W.                       

20-09-2016 10:27:28.860 UTC Debug dnsparser.cpp:95: Parsing header at offset 0x0
20-09-2016 10:27:28.860 UTC Debug dnsparser.cpp:98: 1 questions, 1 answers, 0 authorities, 0 additional records
20-09-2016 10:27:28.860 UTC Debug dnsparser.cpp:103: Parsing question 1 at offset 0xc
20-09-2016 10:27:28.860 UTC Debug dnsparser.cpp:229: Parsed domain name = scscf.cw-aio, encoded length = 14
20-09-2016 10:27:28.860 UTC Debug dnsparser.cpp:112: Parsing answer 1 at offset 0x1e
20-09-2016 10:27:28.860 UTC Debug dnsparser.cpp:229: Parsed domain name = scscf.cw-aio, encoded length = 2
20-09-2016 10:27:28.860 UTC Debug dnsparser.cpp:282: Resource Record NAME=scscf.cw-aio TYPE=A CLASS=IN TTL=0 RDLENGTH=4
20-09-2016 10:27:28.860 UTC Debug dnsparser.cpp:287: Parse A record RDATA
20-09-2016 10:27:28.860 UTC Debug dnsparser.cpp:142: Answer records
scscf.cw-aio            0       IN      A       10.112.87.250

20-09-2016 10:27:28.860 UTC Debug dnsparser.cpp:143: Authority records

20-09-2016 10:27:28.860 UTC Debug dnsparser.cpp:144: Additional records

20-09-2016 10:27:28.860 UTC Debug dnscachedresolver.cpp:761: Adding record to cache entry, TTL=0, expiry=1474367248
20-09-2016 10:27:28.860 UTC Debug dnscachedresolver.cpp:765: Update cache entry expiry to 1474367248
20-09-2016 10:27:28.860 UTC Debug dnscachedresolver.cpp:707: Adding scscf.cw-aio to cache expiry list with deletion time of 1474367548
20-09-2016 10:27:28.860 UTC Debug dnscachedresolver.cpp:319: Received all query responses
20-09-2016 10:27:28.860 UTC Debug dnscachedresolver.cpp:347: Pulling 1 records from cache for scscf.cw-aio A
20-09-2016 10:27:28.860 UTC Debug baseresolver.cpp:362: Found 1 A/AAAA records, randomizing
20-09-2016 10:27:28.860 UTC Debug baseresolver.cpp:993: 10.112.87.250:5054 transport 6 has state: WHITE
20-09-2016 10:27:28.860 UTC Debug baseresolver.cpp:993: 10.112.87.250:5054 transport 6 has state: WHITE
20-09-2016 10:27:28.861 UTC Debug baseresolver.cpp:406: Added a server, now have 1 of 5
20-09-2016 10:27:28.861 UTC Debug baseresolver.cpp:447: Adding 0 servers from blacklist
20-09-2016 10:27:28.861 UTC Info pjutils.cpp:938: Resolved destination URI sip:scscf.cw-aio:5054;transport=TCP to 1 servers
20-09-2016 10:27:28.861 UTC Debug pjutils.cpp:490: Next hop node is encoded in Request-URI
20-09-2016 10:27:28.861 UTC Debug basicproxy.cpp:1641: Next hop scscf.cw-aio is not a stateless proxy
20-09-2016 10:27:28.861 UTC Debug basicproxy.cpp:1655: Sending request for sip:scscf.cw-aio:5054;transport=TCP
20-09-2016 10:27:28.861 UTC Debug pjsip:   tsx0x25f3158 Sending Request msg REGISTER/cseq=147 (tdta0x25d7fb0) in state Null
20-09-2016 10:27:28.861 UTC Debug pjsip:       endpoint Request msg REGISTER/cseq=147 (tdta0x25d7fb0): skipping target resolution because address is already set
20-09-2016 10:27:28.861 UTC Verbose pjsip:  tcpc0x2605ae8 TCP client transport created
20-09-2016 10:27:28.861 UTC Verbose pjsip:  tcpc0x2605ae8 TCP transport 10.112.87.250:47664 is connecting to 10.112.87.250:5054...
20-09-2016 10:27:28.861 UTC Verbose common_sip_processing.cpp:136: TX 1411 bytes Request msg REGISTER/cseq=147 (tdta0x25d7fb0) to TCP 10.112.87.250:5054:
--start msg--

REGISTER sip:scscf.cw-aio:5054;transport=TCP SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250:47664;rport;branch=z9hG4bKPj1qdcVyJP09qYQ1yEjWI1uoO9PZ7l2Ezt
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Via: SIP/2.0/TCP 10.112.87.250:56834;rport=56834;received=10.112.87.250;branch=z9hG4bKPjZJrgNA3v0ZhvLg7MFYya6GmcF1l.10Sm
Path: <sip:paRGfRUGv7 at 10.112.87.250:5058;transport=TCP;lr;ob>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---c6b2fe2e8d687e30
Max-Forwards: 68
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e>
To: <sip:6505550962 at example.com>
From: <sip:6505550962 at example.com>;tag=c523ac41
Call-ID: Dxjgy0nc4Yr5QvvP7aRrqA..
CSeq: 147 REGISTER
Expires: 3600
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Authorization: Digest response="58c71fbab2b57fda1783fb96911fe3ed", username="6505550962 at example.com", realm="example.com", nonce="6a308edb6cb998ed", uri="sip:example.com;transport=TCP", algorithm=MD5, cnonce="1e4f3c75e95d0cac747959ea92f07d88", opaque="51de7ee852267446", qop=auth, nc=0000002e,integrity-protected=ip-assoc-yes
Allow-Events: presence, kpml
P-Visited-Network-ID: example.com
Content-Length:  0


--end msg--
20-09-2016 10:27:28.861 UTC Debug pjsip:   tsx0x25f3158 State changed from Null to Calling, event=TX_MSG
20-09-2016 10:27:28.861 UTC Debug basicproxy.cpp:213: tsx0x25f3158 - tu_on_tsx_state UAC, TSX_STATE TX_MSG state=Calling
20-09-2016 10:27:28.861 UTC Debug basicproxy.cpp:1813: tsx0x25f3158 - uac_tsx = 0x25cd1f0, uas_tsx = 0x15809e0
20-09-2016 10:27:28.861 UTC Debug basicproxy.cpp:1821: TX_MSG event on current UAC transaction
20-09-2016 10:27:28.861 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f74440779c8
20-09-2016 10:27:28.861 UTC Debug thread_dispatcher.cpp:199: Request latency = 4369us
20-09-2016 10:27:28.861 UTC Verbose pjsip:  tcpc0x2605ae8 TCP transport 10.112.87.250:47664 is connected to 10.112.87.250:5054
20-09-2016 10:27:28.861 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:47664, sock=1550
20-09-2016 10:27:28.861 UTC Verbose pjsip: tcps0x7f744402 TCP server transport created
20-09-2016 10:27:28.861 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg REGISTER/cseq=147 (rdata0x7f744402f210)
20-09-2016 10:27:28.861 UTC Verbose common_sip_processing.cpp:120: RX 1411 bytes Request msg REGISTER/cseq=147 (rdata0x7f744402f210) from TCP 10.112.87.250:47664:
--start msg--

REGISTER sip:scscf.cw-aio:5054;transport=TCP SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250:47664;rport;branch=z9hG4bKPj1qdcVyJP09qYQ1yEjWI1uoO9PZ7l2Ezt
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Via: SIP/2.0/TCP 10.112.87.250:56834;rport=56834;received=10.112.87.250;branch=z9hG4bKPjZJrgNA3v0ZhvLg7MFYya6GmcF1l.10Sm
Path: <sip:paRGfRUGv7 at 10.112.87.250:5058;transport=TCP;lr;ob>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---c6b2fe2e8d687e30
Max-Forwards: 68
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e>
To: <sip:6505550962 at example.com>
From: <sip:6505550962 at example.com>;tag=c523ac41
Call-ID: Dxjgy0nc4Yr5QvvP7aRrqA..
CSeq: 147 REGISTER
Expires: 3600
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Authorization: Digest response="58c71fbab2b57fda1783fb96911fe3ed", username="6505550962 at example.com", realm="example.com", nonce="6a308edb6cb998ed", uri="sip:example.com;transport=TCP", algorithm=MD5, cnonce="1e4f3c75e95d0cac747959ea92f07d88", opaque="51de7ee852267446", qop=auth, nc=0000002e,integrity-protected=ip-assoc-yes
Allow-Events: presence, kpml
P-Visited-Network-ID: example.com
Content-Length:  0


--end msg--
20-09-2016 10:27:28.861 UTC Debug pjutils.cpp:1660: Logging SAS Call-ID marker, Call-ID Dxjgy0nc4Yr5QvvP7aRrqA..
20-09-2016 10:27:28.861 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f744405fc68 for worker threads
20-09-2016 10:27:28.861 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f744405fc68
20-09-2016 10:27:28.861 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg REGISTER/cseq=147 (rdata0x7f744405fc68)
20-09-2016 10:27:28.861 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:28.861 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:27:28.861 UTC Debug authentication.cpp:804: Authentication module invoked
20-09-2016 10:27:28.861 UTC Debug authentication.cpp:696: Authorization header in request
20-09-2016 10:27:28.861 UTC Info authentication.cpp:717: SIP Digest authenticated request integrity protected by edge proxy
20-09-2016 10:27:28.861 UTC Debug authentication.cpp:814: Request does not need authentication
20-09-2016 10:27:28.861 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:28.861 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:27:28.861 UTC Debug acr.cpp:49: Created ACR (0x7f73e0031330)
20-09-2016 10:27:28.861 UTC Debug registrar.cpp:572: Process REGISTER for public ID sip:6505550962 at example.com
20-09-2016 10:27:28.861 UTC Debug registrar.cpp:580: Report SAS start marker - trail (1e80)
20-09-2016 10:27:28.861 UTC Debug hssconnection.cpp:587: Making Homestead request for /impu/sip%3A6505550962%40example.com/reg-data?private_id=6505550962%40example.com
20-09-2016 10:27:28.862 UTC Debug httpresolver.cpp:72: HttpResolver::resolve for host 10.112.87.250, port 8888, family 2
20-09-2016 10:27:28.862 UTC Debug baseresolver.cpp:523: Attempt to parse 10.112.87.250 as IP address
20-09-2016 10:27:28.862 UTC Debug httpresolver.cpp:80: Target is an IP address
20-09-2016 10:27:28.862 UTC Debug connection_pool.h:225: Request for connection to IP: 10.112.87.250, port: 8888
20-09-2016 10:27:28.862 UTC Debug connection_pool.h:238: Found existing connection 0x7f745c08b760 in pool
20-09-2016 10:27:28.862 UTC Debug httpclient.cpp:466: Sending HTTP request : http://10.112.87.250:8888/impu/sip%3A6505550962%40example.com/reg-data?private_id=6505550962%40example.com (trying 10.112.87.250)
20-09-2016 10:27:28.863 UTC Debug httpclient.cpp:743: Received header http/1.1200ok with value 
20-09-2016 10:27:28.863 UTC Debug httpclient.cpp:743: Received header content-length with value 1366
20-09-2016 10:27:28.863 UTC Debug httpclient.cpp:743: Received header content-type with value text/plain
20-09-2016 10:27:28.863 UTC Debug httpclient.cpp:743: Received header  with value 
20-09-2016 10:27:28.863 UTC Debug httpclient.cpp:482: Received HTTP response: status=200, doc=<ClearwaterRegData>
	<RegistrationState>REGISTERED</RegistrationState>
	<IMSSubscription xsi="http://www.w3.org/2001/XMLSchema-instance" noNamespaceSchemaLocation="CxDataType.xsd">
		<PrivateID>Unspecified</PrivateID>
		<ServiceProfile>
			<InitialFilterCriteria>
				<TriggerPoint>
					<ConditionTypeCNF>0</ConditionTypeCNF>
					<SPT>
						<ConditionNegated>0</ConditionNegated>
						<Group>0</Group>
						<Method>INVITE</Method>
						<Extension/>
					</SPT>
				</TriggerPoint>
				<ApplicationServer>
					<ServerName>sip:mmtel.example.com</ServerName>
					<DefaultHandling>0</DefaultHandling>
				</ApplicationServer>
			</InitialFilterCriteria>
			<PublicIdentity>
				<Identity>sip:6505550962 at example.com</Identity>
			</PublicIdentity>
		</ServiceProfile>
		<ServiceProfile>
			<InitialFilterCriteria>
				<TriggerPoint>
					<ConditionTypeCNF>0</ConditionTypeCNF>
					<SPT>
						<ConditionNegated>0</ConditionNegated>
						<Group>0</Group>
						<Method>INVITE</Method>
						<Extension/>
					</SPT>
				</TriggerPoint>
				<ApplicationServer>
					<ServerName>sip:mmtel.example.com</ServerName>
					<DefaultHandling>0</DefaultHandling>
				</ApplicationServer>
			</InitialFilterCriteria>
			<PublicIdentity>
				<Identity>sip:6505550997 at example.com</Identity>
			</PublicIdentity>
		</ServiceProfile>
	</IMSSubscription>
</ClearwaterRegData>


20-09-2016 10:27:28.863 UTC Debug baseresolver.cpp:1004: Successful response from  10.112.87.250:8888 transport 6
20-09-2016 10:27:28.863 UTC Debug connection_pool.h:261: Release connection to IP: 10.112.87.250, port: 8888 to pool
20-09-2016 10:27:28.863 UTC Debug baseresolver.cpp:1034: 10.112.87.250:8888 transport 6 returned untested
20-09-2016 10:27:28.863 UTC Debug hssconnection.cpp:368: Processing Identity node from HSS XML - sip:6505550962 at example.com

20-09-2016 10:27:28.863 UTC Debug hssconnection.cpp:368: Processing Identity node from HSS XML - sip:6505550997 at example.com

20-09-2016 10:27:28.863 UTC Debug registrar.cpp:664: REGISTER for public ID sip:6505550962 at example.com uses AOR sip:6505550962 at example.com
20-09-2016 10:27:28.863 UTC Debug subscriber_data_manager.cpp:362: Get AoR data for sip:6505550962 at example.com
20-09-2016 10:27:28.863 UTC Debug memcachedstore.cpp:326: Key reg\\sip:6505550962 at example.com hashes to vbucket 54 via hash 0x81019fb6
20-09-2016 10:27:28.863 UTC Debug memcachedstore.cpp:369: Set up new view 4 for thread
20-09-2016 10:27:28.863 UTC Debug memcachedstore.cpp:376: Setting up server 0 for connection 0x7f73e004cbe0 (--CONNECT-TIMEOUT=10 --SUPPORT-CAS --POLL-TIMEOUT=250 --BINARY-PROTOCOL)
20-09-2016 10:27:28.863 UTC Debug memcachedstore.cpp:378: Set up connection 0x7f73e0050eb0 to server 10.112.87.250:11211
20-09-2016 10:27:28.863 UTC Debug memcachedstore.cpp:389: Setting server to IP address 10.112.87.250 port 11211
20-09-2016 10:27:28.863 UTC Debug memcachedstore.cpp:555: 1 read replicas for key reg\\sip:6505550962 at example.com
20-09-2016 10:27:28.863 UTC Debug memcachedstore.cpp:590: Attempt to read from replica 0 (connection 0x7f73e0050eb0)
20-09-2016 10:27:28.863 UTC Debug memcachedstore.cpp:100: Fetch result
20-09-2016 10:27:28.863 UTC Debug memcachedstore.cpp:108: Found record on replica
20-09-2016 10:27:28.863 UTC Debug memcachedstore.cpp:598: Read for reg\\sip:6505550962 at example.com on replica 0 returned SUCCESS
20-09-2016 10:27:28.863 UTC Debug memcachedstore.cpp:641: Read 499 bytes from table reg key sip:6505550962 at example.com, CAS = 1014
20-09-2016 10:27:28.863 UTC Debug communicationmonitor.cpp:82: Checking communication changes - successful attempts 4, failures 0
20-09-2016 10:27:28.863 UTC Debug subscriber_data_manager.cpp:372: Data store returned a record, CAS = 1014
20-09-2016 10:27:28.863 UTC Debug subscriber_data_manager.cpp:473: Try to deserialize record for sip:6505550962 at example.com with 'JSON' deserializer
20-09-2016 10:27:28.863 UTC Debug subscriber_data_manager.cpp:1009: Deserialize JSON document: {"bindings":{"sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e":{"uri":"sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e","cid":"Dxjgy0nc4Yr5QvvP7aRrqA..","cseq":146,"expires":1474367278,"priority":0,"params":{},"paths":["sip:paRGfRUGv7 at 10.112.87.250:5058;transport=TCP;lr;ob"],"timer_id":"Deprecated","private_id":"6505550962 at example.com","emergency_reg":false}},"subscriptions":{},"notify_cseq":5,"timer_id":"066967ab0930003f2040001012000100"}
20-09-2016 10:27:28.863 UTC Debug subscriber_data_manager.cpp:1034:   Binding: sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e
20-09-2016 10:27:28.863 UTC Debug subscriber_data_manager.cpp:478: Deserialization suceeded
20-09-2016 10:27:28.863 UTC Debug registrar.cpp:254: Retrieved AoR data 0x7f73e0051990
20-09-2016 10:27:28.863 UTC Debug registrar.cpp:372: Binding identifier for contact = sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e
20-09-2016 10:27:28.863 UTC Debug registrar.cpp:399: Path header sip:paRGfRUGv7 at 10.112.87.250:5058;transport=TCP;lr;ob
20-09-2016 10:27:28.863 UTC Debug subscriber_data_manager.cpp:192: Set AoR data for sip:6505550962 at example.com, CAS=1014, expiry = 1474367558
20-09-2016 10:27:28.864 UTC Debug httpresolver.cpp:72: HttpResolver::resolve for host 127.0.0.1, port 7253, family 2
20-09-2016 10:27:28.864 UTC Debug baseresolver.cpp:523: Attempt to parse 127.0.0.1 as IP address
20-09-2016 10:27:28.864 UTC Debug httpresolver.cpp:80: Target is an IP address
20-09-2016 10:27:28.864 UTC Debug connection_pool.h:225: Request for connection to IP: 127.0.0.1, port: 7253
20-09-2016 10:27:28.864 UTC Debug connection_pool.h:238: Found existing connection 0x7f745c04ccb0 in pool
20-09-2016 10:27:28.864 UTC Debug httpclient.cpp:466: Sending HTTP request : http://127.0.0.1:7253/timers/066967ab0930003f2040001012000100 (trying 127.0.0.1)
20-09-2016 10:27:28.864 UTC Debug httpclient.cpp:743: Received header http/1.1200ok with value 
20-09-2016 10:27:28.864 UTC Debug httpclient.cpp:743: Received header location with value /timers/066967ab0930003f2040001012000100
20-09-2016 10:27:28.864 UTC Debug httpclient.cpp:743: Received header content-length with value 0
20-09-2016 10:27:28.864 UTC Debug httpclient.cpp:743: Received header  with value 
20-09-2016 10:27:28.864 UTC Debug httpclient.cpp:482: Received HTTP response: status=200, doc=
20-09-2016 10:27:28.864 UTC Debug baseresolver.cpp:1004: Successful response from  127.0.0.1:7253 transport 6
20-09-2016 10:27:28.864 UTC Debug connection_pool.h:261: Release connection to IP: 127.0.0.1, port: 7253 to pool
20-09-2016 10:27:28.864 UTC Debug baseresolver.cpp:1034: 127.0.0.1:7253 transport 6 returned untested
20-09-2016 10:27:28.864 UTC Debug communicationmonitor.cpp:82: Checking communication changes - successful attempts 1, failures 0
20-09-2016 10:27:28.864 UTC Debug memcachedstore.cpp:730: Writing 499 bytes to table reg key sip:6505550962 at example.com, CAS = 1014, expiry = 310
20-09-2016 10:27:28.864 UTC Debug memcachedstore.cpp:326: Key reg\\sip:6505550962 at example.com hashes to vbucket 54 via hash 0x81019fb6
20-09-2016 10:27:28.864 UTC Debug memcachedstore.cpp:750: 1 write replicas for key reg\\sip:6505550962 at example.com
20-09-2016 10:27:28.864 UTC Debug memcachedstore.cpp:804: Attempt conditional write to vbucket 54 on replica 0 (connection 0x7f73e0050eb0), CAS = 1014, expiry = 310
20-09-2016 10:27:28.864 UTC Debug memcachedstore.cpp:845: Conditional write succeeded to replica 0
20-09-2016 10:27:28.864 UTC Debug subscriber_data_manager.cpp:434: Data store set_data returned 1
20-09-2016 10:27:28.864 UTC Debug registrar.cpp:117: Bindings for sip:6505550962 at example.com, timer ID 066967ab0930003f2040001012000100
20-09-2016 10:27:28.864 UTC Debug registrar.cpp:131:   sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e URI=sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e expires=1474367548 q=0 from=Dxjgy0nc4Yr5QvvP7aRrqA.. cseq=147 timer=Deprecated private_id=6505550962 at example.com emergency_registration=false
20-09-2016 10:27:28.864 UTC Debug pjsip:       endpoint Response msg 200/REGISTER/cseq=147 (tdta0x7f73e004a8f0) created
20-09-2016 10:27:28.864 UTC Verbose common_sip_processing.cpp:136: TX 1026 bytes Response msg 200/REGISTER/cseq=147 (tdta0x7f73e004a8f0) to TCP 10.112.87.250:47664:
--start msg--

SIP/2.0 200 OK
Service-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;orig>
Via: SIP/2.0/TCP 10.112.87.250:47664;rport=47664;received=10.112.87.250;branch=z9hG4bKPj1qdcVyJP09qYQ1yEjWI1uoO9PZ7l2Ezt
Via: SIP/2.0/TCP 10.112.87.250:56834;rport=56834;received=10.112.87.250;branch=z9hG4bKPjZJrgNA3v0ZhvLg7MFYya6GmcF1l.10Sm
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---c6b2fe2e8d687e30
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Call-ID: Dxjgy0nc4Yr5QvvP7aRrqA..
From: <sip:6505550962 at example.com>;tag=c523ac41
To: <sip:6505550962 at example.com>;tag=z9hG4bKPj1qdcVyJP09qYQ1yEjWI1uoO9PZ7l2Ezt
CSeq: 147 REGISTER
Supported: outbound
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e>;expires=300
Require: outbound
Path: <sip:paRGfRUGv7 at 10.112.87.250:5058;transport=TCP;lr;ob>
P-Associated-URI: <sip:6505550962 at example.com>
P-Associated-URI: <sip:6505550997 at example.com>
Content-Length:  0


--end msg--
20-09-2016 10:27:28.864 UTC Debug acr.cpp:83: Sending Null ACR (0x7f73e0031330)
20-09-2016 10:27:28.864 UTC Debug acr.cpp:54: Destroyed ACR (0x7f73e0031330)
20-09-2016 10:27:28.864 UTC Debug ifchandler.cpp:763: Interpreting orig IFC information
20-09-2016 10:27:28.864 UTC Debug ifchandler.cpp:437: SPT class Method: result false
20-09-2016 10:27:28.864 UTC Debug ifchandler.cpp:541: Add to group 0 val false
20-09-2016 10:27:28.864 UTC Debug ifchandler.cpp:559: Result group 0 val false
20-09-2016 10:27:28.864 UTC Debug ifchandler.cpp:572: iFC does not match
20-09-2016 10:27:28.864 UTC Info registration_utils.cpp:187: Found 0 Application Servers
20-09-2016 10:27:28.864 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Response msg 200/REGISTER/cseq=147 (rdata0x2605e20)
20-09-2016 10:27:28.864 UTC Verbose common_sip_processing.cpp:120: RX 1026 bytes Response msg 200/REGISTER/cseq=147 (rdata0x2605e20) from TCP 10.112.87.250:5054:
--start msg--

SIP/2.0 200 OK
Service-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;orig>
Via: SIP/2.0/TCP 10.112.87.250:47664;rport=47664;received=10.112.87.250;branch=z9hG4bKPj1qdcVyJP09qYQ1yEjWI1uoO9PZ7l2Ezt
Via: SIP/2.0/TCP 10.112.87.250:56834;rport=56834;received=10.112.87.250;branch=z9hG4bKPjZJrgNA3v0ZhvLg7MFYya6GmcF1l.10Sm
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---c6b2fe2e8d687e30
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Call-ID: Dxjgy0nc4Yr5QvvP7aRrqA..
From: <sip:6505550962 at example.com>;tag=c523ac41
To: <sip:6505550962 at example.com>;tag=z9hG4bKPj1qdcVyJP09qYQ1yEjWI1uoO9PZ7l2Ezt
CSeq: 147 REGISTER
Supported: outbound
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e>;expires=300
Require: outbound
Path: <sip:paRGfRUGv7 at 10.112.87.250:5058;transport=TCP;lr;ob>
P-Associated-URI: <sip:6505550962 at example.com>
P-Associated-URI: <sip:6505550997 at example.com>
Content-Length:  0


--end msg--
20-09-2016 10:27:28.864 UTC Debug pjsip: tdta0x7f73e004 Destroying txdata Response msg 200/REGISTER/cseq=147 (tdta0x7f73e004a8f0)
20-09-2016 10:27:28.864 UTC Debug registrar.cpp:1067: Report SAS end marker - trail (1e80)
20-09-2016 10:27:28.864 UTC Debug pjutils.cpp:1660: Logging SAS Call-ID marker, Call-ID Dxjgy0nc4Yr5QvvP7aRrqA..
20-09-2016 10:27:28.864 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f744405fc68
20-09-2016 10:27:28.864 UTC Debug thread_dispatcher.cpp:199: Request latency = 3456us
20-09-2016 10:27:28.864 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f744405fc68 for worker threads
20-09-2016 10:27:28.864 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f744405fc68
20-09-2016 10:27:28.864 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Response msg 200/REGISTER/cseq=147 (rdata0x7f744405fc68)
20-09-2016 10:27:28.864 UTC Debug pjsip:   tsx0x25f3158 Incoming Response msg 200/REGISTER/cseq=147 (rdata0x7f744405fc68) in state Calling
20-09-2016 10:27:28.864 UTC Debug pjsip:   tsx0x25f3158 State changed from Calling to Completed, event=RX_MSG
20-09-2016 10:27:28.864 UTC Debug basicproxy.cpp:213: tsx0x25f3158 - tu_on_tsx_state UAC, TSX_STATE RX_MSG state=Completed
20-09-2016 10:27:28.864 UTC Debug basicproxy.cpp:1813: tsx0x25f3158 - uac_tsx = 0x25cd1f0, uas_tsx = 0x15809e0
20-09-2016 10:27:28.864 UTC Debug basicproxy.cpp:1821: RX_MSG event on current UAC transaction
20-09-2016 10:27:28.864 UTC Debug basicproxy.cpp:1884: tsx0x25f3158 - RX_MSG on active UAC transaction
20-09-2016 10:27:28.864 UTC Debug basicproxy.cpp:1408: Dissociate UAC transaction 0x25cd1f0 for target 0
20-09-2016 10:27:28.864 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 200/REGISTER/cseq=147 (tdta0x7f73e8051700) (904 bytes) to upstream sproutlet icscf:
--start msg--

SIP/2.0 200 OK
Service-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;orig>
Via: SIP/2.0/TCP 10.112.87.250:56834;rport=56834;received=10.112.87.250;branch=z9hG4bKPjZJrgNA3v0ZhvLg7MFYya6GmcF1l.10Sm
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---c6b2fe2e8d687e30
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Call-ID: Dxjgy0nc4Yr5QvvP7aRrqA..
From: <sip:6505550962 at example.com>;tag=c523ac41
To: <sip:6505550962 at example.com>;tag=z9hG4bKPj1qdcVyJP09qYQ1yEjWI1uoO9PZ7l2Ezt
CSeq: 147 REGISTER
Supported: outbound
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e>;expires=300
Require: outbound
Path: <sip:paRGfRUGv7 at 10.112.87.250:5058;transport=TCP;lr;ob>
P-Associated-URI: <sip:6505550962 at example.com>
P-Associated-URI: <sip:6505550997 at example.com>
Content-Length:  0


--end msg--
20-09-2016 10:27:28.864 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f73e8051d10 => txdata 0x7f73e80517a8 mapping
20-09-2016 10:27:28.864 UTC Verbose sproutletproxy.cpp:1634: icscf-0x25a7020 received final response Response msg 200/REGISTER/cseq=147 (tdta0x7f73e8051700) on fork 0, state = Terminated
20-09-2016 10:27:28.864 UTC Debug icscfsproutlet.cpp:321: Check retry conditions for REGISTER, status = 200, S-CSCF responsive
20-09-2016 10:27:28.864 UTC Verbose sproutletproxy.cpp:1413: icscf-0x25a7020 sending Response msg 200/REGISTER/cseq=147 (tdta0x7f73e8051700)
20-09-2016 10:27:28.864 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
20-09-2016 10:27:28.864 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 200
20-09-2016 10:27:28.864 UTC Debug sproutletproxy.cpp:1869: Forward 2xx response
20-09-2016 10:27:28.864 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f73e8051d10 => txdata 0x7f73e80517a8 mapping
20-09-2016 10:27:28.864 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 200/REGISTER/cseq=147 (tdta0x7f73e8051700) (904 bytes) to upstream sproutlet scscf:
--start msg--

SIP/2.0 200 OK
Service-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;orig>
Via: SIP/2.0/TCP 10.112.87.250:56834;rport=56834;received=10.112.87.250;branch=z9hG4bKPjZJrgNA3v0ZhvLg7MFYya6GmcF1l.10Sm
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---c6b2fe2e8d687e30
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Call-ID: Dxjgy0nc4Yr5QvvP7aRrqA..
From: <sip:6505550962 at example.com>;tag=c523ac41
To: <sip:6505550962 at example.com>;tag=z9hG4bKPj1qdcVyJP09qYQ1yEjWI1uoO9PZ7l2Ezt
CSeq: 147 REGISTER
Supported: outbound
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e>;expires=300
Require: outbound
Path: <sip:paRGfRUGv7 at 10.112.87.250:5058;transport=TCP;lr;ob>
P-Associated-URI: <sip:6505550962 at example.com>
P-Associated-URI: <sip:6505550997 at example.com>
Content-Length:  0


--end msg--
20-09-2016 10:27:28.864 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f73e8051d10 => txdata 0x7f73e80517a8 mapping
20-09-2016 10:27:28.864 UTC Verbose sproutletproxy.cpp:1634: scscf-0x25a5c20 received final response Response msg 200/REGISTER/cseq=147 (tdta0x7f73e8051700) on fork 0, state = Terminated
20-09-2016 10:27:28.864 UTC Info scscfsproutlet.cpp:561: S-CSCF received response
20-09-2016 10:27:28.864 UTC Verbose sproutletproxy.cpp:1413: scscf-0x25a5c20 sending Response msg 200/REGISTER/cseq=147 (tdta0x7f73e8051700)
20-09-2016 10:27:28.864 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
20-09-2016 10:27:28.864 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 200
20-09-2016 10:27:28.864 UTC Debug sproutletproxy.cpp:1869: Forward 2xx response
20-09-2016 10:27:28.864 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f73e8051d10 => txdata 0x7f73e80517a8 mapping
20-09-2016 10:27:28.864 UTC Debug pjsip:   tsx0x25a9268 Sending Response msg 200/REGISTER/cseq=147 (tdta0x7f73e8051700) in state Trying
20-09-2016 10:27:28.864 UTC Verbose common_sip_processing.cpp:136: TX 904 bytes Response msg 200/REGISTER/cseq=147 (tdta0x7f73e8051700) to TCP 10.112.87.250:56834:
--start msg--

SIP/2.0 200 OK
Service-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;orig>
Via: SIP/2.0/TCP 10.112.87.250:56834;rport=56834;received=10.112.87.250;branch=z9hG4bKPjZJrgNA3v0ZhvLg7MFYya6GmcF1l.10Sm
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---c6b2fe2e8d687e30
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Call-ID: Dxjgy0nc4Yr5QvvP7aRrqA..
From: <sip:6505550962 at example.com>;tag=c523ac41
To: <sip:6505550962 at example.com>;tag=z9hG4bKPj1qdcVyJP09qYQ1yEjWI1uoO9PZ7l2Ezt
CSeq: 147 REGISTER
Supported: outbound
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp;rinstance=37ea47d1183ebd0e>;expires=300
Require: outbound
Path: <sip:paRGfRUGv7 at 10.112.87.250:5058;transport=TCP;lr;ob>
P-Associated-URI: <sip:6505550962 at example.com>
P-Associated-URI: <sip:6505550997 at example.com>
Content-Length:  0


--end msg--
20-09-2016 10:27:28.864 UTC Debug pjsip:   tsx0x25a9268 State changed from Trying to Completed, event=TX_MSG
20-09-2016 10:27:28.864 UTC Debug basicproxy.cpp:213: tsx0x25a9268 - tu_on_tsx_state UAS, TSX_STATE TX_MSG state=Completed
20-09-2016 10:27:28.864 UTC Verbose sproutletproxy.cpp:1828: scscf-0x25a5c20 suiciding
20-09-2016 10:27:28.864 UTC Debug sproutletproxy.cpp:1160: Destroying SproutletWrapper 0x25a5740
20-09-2016 10:27:28.864 UTC Debug scscfsproutlet.cpp:395: S-CSCF Transaction (0x25a5c20) destroyed
20-09-2016 10:27:28.864 UTC Debug aschain.h:139: AsChain dec ref 0x258edc0 -> 0
20-09-2016 10:27:28.864 UTC Debug aschain.cpp:88: Destroying AsChain 0x258edc0
20-09-2016 10:27:28.865 UTC Debug aschain.cpp:106: Sending ACR (0x23c8d30) from AS chain
20-09-2016 10:27:28.865 UTC Debug acr.cpp:83: Sending Null ACR (0x23c8d30)
20-09-2016 10:27:28.865 UTC Debug acr.cpp:54: Destroyed ACR (0x23c8d30)
20-09-2016 10:27:28.865 UTC Debug sproutletproxy.cpp:1169: Free original request Request msg REGISTER/cseq=147 (tdta0x25a7200) (tdta0x25a7200)
20-09-2016 10:27:28.865 UTC Verbose sproutletproxy.cpp:1828: icscf-0x25a7020 suiciding
20-09-2016 10:27:28.865 UTC Debug sproutletproxy.cpp:1160: Destroying SproutletWrapper 0x25a6ef0
20-09-2016 10:27:28.865 UTC Debug acr.cpp:83: Sending Null ACR (0x25984a0)
20-09-2016 10:27:28.865 UTC Debug acr.cpp:54: Destroyed ACR (0x25984a0)
20-09-2016 10:27:28.865 UTC Debug sproutletproxy.cpp:1169: Free original request Request msg REGISTER/cseq=147 (tdta0x25aa9e0) (tdta0x25aa9e0)
20-09-2016 10:27:28.865 UTC Debug pjsip:  tdta0x25aa9e0 Destroying txdata Request msg REGISTER/cseq=147 (tdta0x25aa9e0)
20-09-2016 10:27:28.865 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f744405fc68
20-09-2016 10:27:28.865 UTC Debug thread_dispatcher.cpp:199: Request latency = 280us
20-09-2016 10:27:28.874 UTC Debug pjsip:   tsx0x25f3158 Timeout timer event
20-09-2016 10:27:28.874 UTC Debug pjsip:   tsx0x25f3158 State changed from Completed to Terminated, event=TIMER
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:213: tsx0x25f3158 - tu_on_tsx_state UAC, TSX_STATE TIMER state=Terminated
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:1813: tsx0x25f3158 - uac_tsx = 0x25cd1f0, uas_tsx = (nil)
20-09-2016 10:27:28.874 UTC Debug pjsip:   tsx0x25a9268 Timeout timer event
20-09-2016 10:27:28.874 UTC Debug pjsip:   tsx0x25a9268 State changed from Completed to Terminated, event=TIMER
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:213: tsx0x25a9268 - tu_on_tsx_state UAS, TSX_STATE TIMER state=Terminated
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:1281: Report SAS end marker - trail (1e7f)
20-09-2016 10:27:28.874 UTC Debug pjsip:   tsx0x25f3158 Timeout timer event
20-09-2016 10:27:28.874 UTC Debug pjsip:   tsx0x25f3158 State changed from Terminated to Destroyed, event=TIMER
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:213: tsx0x25f3158 - tu_on_tsx_state UAC, TSX_STATE TIMER state=Destroyed
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:1813: tsx0x25f3158 - uac_tsx = 0x25cd1f0, uas_tsx = (nil)
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:1985: tsx0x25f3158 - UAC tsx destroyed
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:1535: BasicProxy::UACTsx destructor (0x25cd1f0)
20-09-2016 10:27:28.874 UTC Debug pjsip:  tdta0x25d7fb0 Destroying txdata Request msg REGISTER/cseq=147 (tdta0x25d7fb0)
20-09-2016 10:27:28.874 UTC Debug pjsip:   tsx0x25a9268 Timeout timer event
20-09-2016 10:27:28.874 UTC Debug pjsip:   tsx0x25a9268 State changed from Terminated to Destroyed, event=TIMER
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:213: tsx0x25a9268 - tu_on_tsx_state UAS, TSX_STATE TIMER state=Destroyed
20-09-2016 10:27:28.874 UTC Debug sproutletproxy.cpp:741: tsx0x25a9268 - UAS tsx destroyed
20-09-2016 10:27:28.874 UTC Debug sproutletproxy.cpp:1081: Safe for UASTsx to suicide
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:1456: Transaction ((nil)) suiciding
20-09-2016 10:27:28.874 UTC Verbose sproutletproxy.cpp:529: Sproutlet Proxy transaction (0x15809e0) destroyed
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:467: BasicProxy::UASTsx destructor (0x15809e0)
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:484: Disconnect UAC transactions from UAS transaction
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:498: Free original request
20-09-2016 10:27:28.874 UTC Debug pjsip:  tdta0x25a7200 Destroying txdata Request msg REGISTER/cseq=147 (tdta0x25a7200)
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:507: Free un-used best response
20-09-2016 10:27:28.874 UTC Debug pjsip:  tdta0x25a99d0 Destroying txdata Response msg 408/REGISTER/cseq=147 (tdta0x25a99d0)
20-09-2016 10:27:28.874 UTC Debug basicproxy.cpp:528: BasicProxy::UASTsx destructor completed
20-09-2016 10:27:28.874 UTC Debug pjsip: tdta0x7f73e805 Destroying txdata Response msg 200/REGISTER/cseq=147 (tdta0x7f73e8051700)
20-09-2016 10:27:28.874 UTC Debug pjsip:   tsx0x25a9268 Transaction destroyed!
20-09-2016 10:27:28.874 UTC Debug pjsip:   tsx0x25f3158 Transaction destroyed!
20-09-2016 10:27:37.306 UTC Verbose pjsip:    tcplis:5054 TCP listener 10.112.87.250:5054: got incoming TCP connection from 10.112.87.250:43431, sock=1564
20-09-2016 10:27:37.306 UTC Verbose pjsip: tcps0x7f744405 TCP server transport created
20-09-2016 10:27:37.306 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Request msg OPTIONS/cseq=430858 (rdata0x7f744405ff20)
20-09-2016 10:27:37.306 UTC Verbose common_sip_processing.cpp:120: RX 356 bytes Request msg OPTIONS/cseq=430858 (rdata0x7f744405ff20) from TCP 10.112.87.250:43431:
--start msg--

OPTIONS sip:poll-sip at 10.112.87.250:5054 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250;rport;branch=z9hG4bK-430858
Max-Forwards: 2
To: <sip:poll-sip at 10.112.87.250:5054>
From: poll-sip <sip:poll-sip at 10.112.87.250>;tag=430858
Call-ID: poll-sip-430858
CSeq: 430858 OPTIONS
Contact: <sip:10.112.87.250>
Accept: application/sdp
Content-Length: 0
User-Agent: poll-sip


--end msg--
20-09-2016 10:27:37.306 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:37.306 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:27:37.306 UTC Debug common_sip_processing.cpp:212: Skipping SAS logging for OPTIONS request
20-09-2016 10:27:37.306 UTC Debug thread_dispatcher.cpp:253: Queuing cloned received message 0x7f744409bce8 for worker threads
20-09-2016 10:27:37.306 UTC Debug thread_dispatcher.cpp:149: Worker thread dequeue message 0x7f744409bce8
20-09-2016 10:27:37.307 UTC Debug pjsip: sip_endpoint.c Distributing rdata to modules: Request msg OPTIONS/cseq=430858 (rdata0x7f744409bce8)
20-09-2016 10:27:37.307 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 10:27:37.307 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 10:27:37.307 UTC Debug pjsip:       endpoint Response msg 200/OPTIONS/cseq=430858 (tdta0x7f73f40ed4e0) created
20-09-2016 10:27:37.307 UTC Verbose common_sip_processing.cpp:136: TX 286 bytes Response msg 200/OPTIONS/cseq=430858 (tdta0x7f73f40ed4e0) to TCP 10.112.87.250:43431:
--start msg--

SIP/2.0 200 OK
Via: SIP/2.0/TCP 10.112.87.250;rport=43431;received=10.112.87.250;branch=z9hG4bK-430858
Call-ID: poll-sip-430858
From: "poll-sip" <sip:poll-sip at 10.112.87.250>;tag=430858
To: <sip:poll-sip at 10.112.87.250>;tag=z9hG4bK-430858
CSeq: 430858 OPTIONS
Content-Length:  0


--end msg--
20-09-2016 10:27:37.307 UTC Debug common_sip_processing.cpp:254: Skipping SAS logging for OPTIONS response
20-09-2016 10:27:37.307 UTC Debug pjsip: tdta0x7f73f40e Destroying txdata Response msg 200/OPTIONS/cseq=430858 (tdta0x7f73f40ed4e0)
20-09-2016 10:27:37.307 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f744409bce8
20-09-2016 10:27:37.307 UTC Debug thread_dispatcher.cpp:199: Request latency = 121us
20-09-2016 10:27:37.312 UTC Verbose httpstack.cpp:293: Process request for URL /ping, args (null)
20-09-2016 10:27:37.312 UTC Verbose httpstack.cpp:69: Sending response 200 to request for URL /ping, args (null)
20-09-2016 10:27:38.306 UTC Verbose pjsip: tcps0x7f744405 TCP connection closed
20-09-2016 10:27:38.306 UTC Debug connection_tracker.cpp:92: Connection 0x7f744405fbe8 has been destroyed
20-09-2016 10:27:38.306 UTC Verbose pjsip: tcps0x7f744405 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:27:39.813 UTC Verbose pjsip:    tcplis:5052 TCP listener 10.112.87.250:5052: got incoming TCP connection from 10.112.87.250:57822, sock=1564
20-09-2016 10:27:39.813 UTC Verbose pjsip: tcps0x7f744405 TCP server transport created
20-09-2016 10:27:39.823 UTC Verbose pjsip: tcps0x7f744409 TCP connection closed
20-09-2016 10:27:39.823 UTC Verbose pjsip: tcps0x7f744409 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)
20-09-2016 10:27:42.813 UTC Verbose pjsip:    tcplis:5052 TCP listener 10.112.87.250:5052: got incoming TCP connection from 10.112.87.250:53808, sock=1314
20-09-2016 10:27:42.813 UTC Verbose pjsip: tcps0x7f744409 TCP server transport created
20-09-2016 10:27:42.823 UTC Verbose pjsip: tcps0x7f744405 TCP connection closed
20-09-2016 10:27:42.823 UTC Verbose pjsip: tcps0x7f744405 TCP transport destroyed with reason 70016: End of file (PJ_EEOF)

 


	








-----Original Message-----
From: Surender Singh 
Sent: 19 September 2016 11:31
To: clearwater at lists.projectclearwater.org
Subject: SIP Trunking With PBX_aio

Hi Graeme,

I just make the entry in shared _config file of enum.json file. after that  enum look up started.

enum_file=enum.json

But Invite is are still not reaching to PBX (Captured the Wireshark at pbx ).Might be calls are not going outside the IMS .

Please go through my configuration, and help rectify the problem.


I have bit confusion regarding call flow (incoming and outgoing calls) in All in one node solution .

Please help what will be call flow for in/out calls.

Regards
Surender Singh








-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 17 September 2016 00:09
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 41, Issue 33

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. Re: SIP Trunking With PBX_aio
      (Graeme Robertson (projectclearwater.org))


----------------------------------------------------------------------

Message: 1
Date: Fri, 16 Sep 2016 18:38:41 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID:
	<CY4PR02MB26169F0D2D19946AC837C595E3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="us-ascii"

Hi Surender,



It's great to hear that you're making progress.



It sounds like there's a problem with your user_settings file - is it definitely in the /etc/clearwater directory? You suggested earlier it might be in /etc/Clearwater but I assumed that was a typo at the time. Anyway, it sounds like we're past the problems with Sprout, which is good. Unfortunately, I'm afraid I don't completely understand what your current issue is - please can you be more specific? It sounds like the INVITE is successfully being routed out to your PBX via the IBCF and your PBX is rejecting the INVITE with a 404. The INVITE that's being sent to the PBX looks sensible to me, so I think you need to work out why the PBX is rejecting it.



Thanks,

Graeme



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 16 September 2016 14:28
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP Trunking With PBX_aio



Hi Graeme,



Sorry I am not  able to understand regarding logs, I just changes the value in user_settings file and restart the sprout like below.



log_level="5"



But now after adding the below line in shared_config file , enum query is happing and able rewrite URI . IBFG/BGCF also able forward the packet at my pbx ip 10.112.87.177 but getting release from PBX 404.



I also disabled the lr at pbx.



Find below sip logs





Frame 7: 1665 bytes on wire (13320 bits), 1665 bytes captured (13320 bits) Linux cooked capture Internet Protocol Version 4, Src: 10.112.87.250, Dst: 10.112.87.177 Transmission Control Protocol, Src Port: 59955, Dst Port: 5060, Seq: 1, Ack: 1, Len: 1597 Session Initiation Protocol (INVITE)

    Request-Line: INVITE sip:1234 at 10.112.87.177;transport=TCP SIP/2.0

    Message Header

        Via: SIP/2.0/TCP 10.112.87.250:59955;rport;branch=z9hG4bKPja-Y65hsmJxWUbzhJeod0RuunKpnx1Gu6

        Via: SIP/2.0/TCP 10.112.87.250:42127;rport=42127;received=10.112.87.250;branch=z9hG4bKPj6ze-NaIX4.JPNaszyxOgTN3gp5j7Ngwp

        Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

        Via: SIP/2.0/TCP 10.112.87.250:42466;rport=42466;received=10.112.87.250;branch=z9hG4bKPj5xhgQqIs5lRKtimI4E9pvUXW2EhTqghw

        Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

        Record-Route: <sip:LKP68ldZs3 at cw-aio:5060;transport=TCP;lr>

        Via: SIP/2.0/TCP 10.112.123.187:43259;received=10.112.123.187;branch=z9hG4bK-524287-1---c83e12d46aead571

        Max-Forwards: 66

        Contact: <sip:6505550962 at 10.112.123.187:43259;transport=tcp>

        To: <sip:1234 at example.com>

        From: <sip:6505550962 at example.com>;tag=eb40917f

        Call-ID: VAescKO7V6NoEuXLTrcYVg..

        CSeq: 1 INVITE

        Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE

        Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri

        User-Agent: Z 3.9.32144 r32121

        Allow-Events: presence, kpml

        P-Asserted-Identity: <sip:6505550962 at example.com>

        Session-Expires: 600

        P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg

        Content-Type: application/sdp

        Content-Length:   243

    Message Body

        Session Description Protocol



Regards

Surender Singh











-----------------Write in Shared_config file------------------ enum_file=enum.json







{

    "routes" : [

        {   "name" : "Clearwater AIO_TO_PBX",

          "domain" : "10.112.87.177",

          "route" : ["sip:cw-aio:5058"]

        }

    ]



}







{

    "number_blocks" : [

        {

           "name" : "Internal numbers",

           "prefix" : "650555",

           "regex" : "!(^.*$)!sip:\\1 at example.com!"

        },

        {

            "name" : "External numbers",

            "prefix" : "",

            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177;transport=TCP!"

        }

    ]

}





-----Original Message-----

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org<mailto:clearwater-request at lists.projectclearwater.org>

Sent: 16 September 2016 18:04

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: Clearwater Digest, Vol 41, Issue 28



Send Clearwater mailing list submissions to

                clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>



To subscribe or unsubscribe via the World Wide Web, visit

                http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



or, via email, send a message with subject or body 'help' to

                clearwater-request at lists.projectclearwater.org<mailto:clearwater-request at lists.projectclearwater.org>



You can reach the person managing the list at

                clearwater-owner at lists.projectclearwater.org<mailto:clearwater-owner at lists.projectclearwater.org>



When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."





Today's Topics:



   1. Mininet (chess man)

   2.  Performance limit measurement (??????? ?????????)

   3. Re: Mininet (Graeme Robertson (projectclearwater.org))

   4. Re: SIP Trunking With PBX_aio

      (Graeme Robertson (projectclearwater.org))





----------------------------------------------------------------------



Message: 1

Date: Fri, 16 Sep 2016 09:02:07 +0100

From: chess man <chessmancaryl at gmail.com<mailto:chessmancaryl at gmail.com>>

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: [Project Clearwater] Mininet

Message-ID:

                <CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw at mail.gmail.com<mailto:CAB9Pg_o3RCs=As0L=1tOOuwp-U2wB4kghkX9rcVSZe_L-6Ywxw at mail.gmail.com>>

Content-Type: text/plain; charset="utf-8"



Hi everybody,



I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/978b47a5/attachment-0001.html>



------------------------------



Message: 2

Date: Fri, 16 Sep 2016 12:16:42 +0300

From: ??????? ?????????             <michaelkatsoulis88 at gmail.com<mailto:michaelkatsoulis88 at gmail.com>>

To: Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>

Subject: [Project Clearwater]  Performance limit measurement

Message-ID:

                <CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA at mail.gmail.com<mailto:CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA at mail.gmail.com>>

Content-Type: text/plain; charset="utf-8"



Hi all,



we are running Stress Tests against our Clearwater Deployment using Sip Stress node.

We have noticed that the results are not consistent as the number of successfull calls changes during repetitions of the same test scenario.



We have tried to increase the values of max_tokens , init_token_rate, min_token_rate and target_latency_us but we did not observe any difference.



What is the proposed way to discover the deployment's limit on how many requests per second can be served?



Thanks in advance,

Michael Katsoulis

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/27a77df2/attachment-0001.html>



------------------------------



Message: 3

Date: Fri, 16 Sep 2016 12:20:26 +0000

From: "Graeme Robertson (projectclearwater.org)"

                <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>

To: "clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>"

                <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>

Subject: Re: [Project Clearwater] Mininet

Message-ID:

                <CY4PR02MB2616A144FD60BFC5F7796B1CE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com<mailto:CY4PR02MB2616A144FD60BFC5F7796B1CE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>>



Content-Type: text/plain; charset="utf-8"



Hello,



There is no reason why it shouldn?t just work, but as far as I know nobody has ever tried to install Project Clearwater on Mininet. If our AIO OVA doesn?t just work then you may have to follow our manual install instructions<http://clearwater.readthedocs.io/en/stable/Manual_Install.html>.



It would be interesting to hear how you get on!



Thanks,

Graeme



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of chess man

Sent: 16 September 2016 09:02

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: [Project Clearwater] Mininet



Hi everybody,

I would like to know if it is possible to install clearwater images on the Virtual machines of mininet ?

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/782ad823/attachment-0001.html>



------------------------------



Message: 4

Date: Fri, 16 Sep 2016 12:33:07 +0000

From: "Graeme Robertson (projectclearwater.org)"

                <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>

To: "clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>"

                <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>

Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio

Message-ID:

                <CY4PR02MB2616B3B7962F78B040BAAD8BE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com<mailto:CY4PR02MB2616B3B7962F78B040BAAD8BE3F30 at CY4PR02MB2616.namprd02.prod.outlook.com>>



Content-Type: text/plain; charset="us-ascii"



Hi Surender,



I'm not sure what you mean about not being able to reply through the mailing list - I think this email went to the mailing list :).



That is odd. What is log_level set to if you run ' log_level="";. /etc/clearwater/config; echo $log_level'? And what is it set to if you run 'log_level="";. /etc/clearwater/user_settings; echo $log_level'?



The /etc/clearwater/config file is the config file that all of our processes actually read from. As you can see, this file pulls in all of our other config files (shared_config, local_config and user_settings).



Thanks,

Graeme



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh

Sent: 16 September 2016 07:18

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio



Hi Graeme,





I am not able to reply through mailing list as I already created the account.



I requesting you for quick response...





ims.hcl.com is another deployment.





I putted the said line in user_setting file and  restart the sprout but showing Log level set to 2 ,fine below logs and also find the attached sprout logs .





[cw-aio]root at cw-aio:~# sudo service sprout restart

* Restarting Sprout SIP Router sprout                                          Log directory set to /var/log/sprout

Log level set to 2

16-09-2016 11:10:01.436 UTC Status utils.cpp:494: Switching to daemon mode

                                                                         [ OK ]



-----user_settings file is under the path etc/Clearwater is----



trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>"

enum_file="enum.json"

log_level=5









I have one more query, what is meaning of config file under the path /etc/Clearwater



-------config  file------------------

if [ -f /etc/clearwater/shared_config ]

then

  . /etc/clearwater/shared_config

fi



. /etc/clearwater/local_config



if [ -f /etc/clearwater/user_settings ]

then

  . /etc/clearwater/user_settings

fi





Regards

Surender Singh



-------------------------------------------------------------



Hi Surender,



I noticed there are lots of instances of "Invalid ENUM response:

sip:@example.com" so it looks as though something is misconfigured, but it's not immediately obvious what. I think it would be useful to turn on debug logging for Sprout in order to dig into this issue further. In order to do this, add the line 'log_level=5' to /etc/clearwater/user_settings (creating it if it doesn't exist) and restart Sprout (by running sudo service sprout restart). This will cause far more detailed logs to be written to the Sprout log files.



Incidentally, I thought (from an earlier thread) that you'd changed your home domain from example.com to ims.hcl.com - is that on a different deployment? I just wanted to make sure there was no confusion here.



Thanks,

Graeme





From: Surender Singh

Sent: 15 September 2016 17:54

To: 'clearwater at lists.projectclearwater.org' <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org%3cmailto:clearwater at lists.projectclearwater.org>>>

Subject: RE: SIP Trunking With PBX_aio





Hi Team,



Kindly help



Please share the procedure to connect the SIP trunk with PBX.



I already tried many combination but  in/out calls failing in IMS.



When I also checking IBCF functionality are enabled or not then showing below o/p



[cw-aio]ubuntu at cw-aio:~$ sudo /usr/share/clearwater/bin/bono

/usr/share/clearwater/bin/bono: error while loading shared libraries: libmemcached.so.11: cannot open shared object file: No such file or directory [cw-aio]ubuntu at cw-aio:~$



Regards

Surender Singh



From: Surender Singh

Sent: 14 September 2016 17:26

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org%3cmailto:clearwater at lists.projectclearwater.org>>

Subject: SIP Trunking With PBX_aio



Hi team,



I deployed the aio solution in my Vmware Exsi  and made the connectivity with Mitel IP-PBX .



As per doc and mailing list  I made the Following entry .





1)      As per mailing IBCF and BGCF functionality are provided by BONO



2)      I create the one user-settings file (already not created ) under the path =/etc/clearwater/user_settings  with entry like trusted_peers="<10.112.86.87>,<10.112.87.177>,<10.112.123.187>" . Here 10.112.87.177 is my PBX ip.



3)      After that I make the entry for bfcg.json and enum.json under the path =/etc/clearwater/sample. And then run the command sudo /usr/share/clearwater/clearwater-config-manager/scripts/upload_bgcf_json and enum_json. Post this command both files created the path under /etc/Clearwater. Files contents are





-----------BGCF.JSON------------------





   {

      "routes" : [

        {   "name" : "Clearwater AIO_TO_PBX",

          "domain" : "10.112.87.177",

          "route" : ["sip:example.com:5058","sip:10.112.87.177:5060"]

        }

    ]

  }



------ENUM.JSON---------------------



{

    "number_blocks" : [

        {

           "name" : "Internal numbers",

           "prefix" : "650555",

           "regex" : "!(^.*$)!sip:\\1 at 10.112.87.250!"

        },



        {

            "name" : "External numbers",

            "prefix" : "",

            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"

        }

    ]

}







My IP Scheme are:



10.112.87.177 : PBX IP address

10.112.87.250: IMS IP address and Domain is example.com



10.112.123.187 : my System ip where Zoper client insall





My call Flow is:-



SIP Client A(Registered in PBX) calls to -> SIP Client B (Registered in ims with example.com domain )



When I make the call from A to B then call coming in IMS network but getting error 'Not found 404' because B number registered with xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com<mailto:xxxxxx at exmaple.com%3cmailto:xxxxxx at exmaple.com>> .





Please find the attached wireshark traces and BONO, Sprout logs







Note : I also not able to call from from B to A Also



Regards

Surender Singh













::DISCLAIMER::

----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.

E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents (with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.

Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification, distribution and / or publication of this message without the prior written consent of authorized representative of HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.

Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------

-------------- next part --------------

An HTML attachment was scrubbed...

URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/12d206af/attachment.html>



------------------------------



Subject: Digest Footer



_______________________________________________

Clearwater mailing list

Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>

http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org





------------------------------



End of Clearwater Digest, Vol 41, Issue 28

******************************************
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160916/c0205317/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 41, Issue 33
******************************************



From graeme at projectclearwater.org  Tue Sep 20 07:06:39 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Tue, 20 Sep 2016 11:06:39 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
References: <16AE9ED83DBA5B4D85B058EAF39C918107E18DE7@NDA-HCLT-MBS05.hclt.corp.hcl.in>
Message-ID: <CY4PR02MB2616BA771B427321072C871CE3F70@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Surender,



At the end of originating processing at the S-CSCF, we do an ENUM lookup and find a match and translate the Request URI to sip:1234 at 10.112.87.177;transport=TCP. We then identify that this is not a local user, so we send it to the BGCF to route off-net. The BGCF thinks there are no routes configured for 10.112.87.177, and so it just attempts normal SIP proxying - i.e. it sends the INVITE to 10.112.87.177:5060, which is your PBX. It looks as though (from an earlier email) that you were hoping to route it via Bono (as an IBCF) but for some reason this configuration hasn't been picked up. However, I'm not sure there's actually any value to including the IBCF in this flow - the same INVITE would still end up being routed to 10.112.87.177:5060.



Project Clearwater is designed as a spec-compliant IMS core. In the IMS world, the user in the To header is never really used, and we never touch it. This is not very consistent with the original standards for SIP detailed in RFC3261<https://www.ietf.org/rfc/rfc3261.txt>, so we don't always interoperate perfectly with non-IMS devices. I'm afraid I don't know anything about your PBX, but if it is non-IMS and is expecting the To header to contain a certain value, then this would explain why it is rejecting the call. If you really need to re-write the To header on your request, and you are not averse to writing a bit of code, Sprout provides a really easy-to-use API for developing your own app server. https://github.com/Metaswitch/greeter is a sample application server that uses this API and simply adds the header "Subject: Hello world!" to a SIP message - you could write something similar that changes to the To header on your SIP messages. Alternatively, Metaswitch has a commercial product called Perimeta<http://www.metaswitch.com/perimeta-session-border-controller-sbc> which can be deployed as an IBCF, and which has a SIP Message Manipulation Framework which you would be able to use to alter your To header.



Thanks,

Graeme



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 20 September 2016 07:42
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio



Hi Graeme/Team,



Updating...............



1) Now Incoming calls are working from PBX to IMS .



2) For Outgoing calls ,able to send request Invite to PBX IP 10.112.87.177 but getting 404 no route to destination/Not found.



3) Can you suggest me what parameter (To Header or Request URI) are checked by the Peer node (PBX) to terminate the calls.



4) I also not able to understand configuration/calls flow  in BGCF.JSON. How IMS route the calls using BGCF.JSON.



Please find some below  logs of Sprout....






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160920/e5c1d912/attachment.html>

From graeme at projectclearwater.org  Tue Sep 20 07:47:06 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Tue, 20 Sep 2016 11:47:06 +0000
Subject: [Project Clearwater] Performance limit measurement
In-Reply-To: <CAJG3f9MrQq9Mr34hrpv1rgpR=6vpNP5g3tM6opTrM4BvnqwV5g@mail.gmail.com>
References: <CAJG3f9OpGwghf=JBa_7KY5hAmucvkBK0nQBvfwYB6LcvsPpyPA@mail.gmail.com>
	<CY4PR02MB26168AC7A5C61A9DD32EB0C2E3F30@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAJG3f9NLorKJEbEJbDh+gEKfbLfdqB0CQR1sv6J6pFtnbmy=+g@mail.gmail.com>
	<CY4PR02MB26163BAD3AB2F60F7D1A74BAE3F30@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAJG3f9MrQq9Mr34hrpv1rgpR=6vpNP5g3tM6opTrM4BvnqwV5g@mail.gmail.com>
Message-ID: <MWHPR02MB2621F5D2D0FB35D6FFA34F10E3F70@MWHPR02MB2621.namprd02.prod.outlook.com>

Hi Michael,

It?s worth noting that Project Clearwater is designed to scale horizontally rather than vertically, so we would expect multiple less powerful Sprout nodes to out-perform a single powerful Sprout node. However, that doesn?t mean that your Sprout node isn?t capable of handling the load you?re hitting it with.

We do expose latency measurements over SNMP ? see http://clearwater.readthedocs.io/en/stable/Clearwater_SNMP_Statistics.html for more details. In particular, under Sprout statistics we have various latency statistics including latency for SIP requests and latency for requests to Homestead. There are a couple of other statistics that might be useful for determining when exactly our requests are failing ? if the number of initial registration failures and/or the number of authentication failures are non-zero this would indicate that the bottleneck is actually at Homestead.

It does sound as though Sprout is reporting itself as overloaded even though it could handle more requests. As I mentioned previously, Sprout will tweak it?s overload controls to rectify this, but it won?t be immediate, which might explain the failures. I know you?ve already tried tweaking the token controls, but it might be worth looking at them again. Over the course of the minute of your test, I think we expect to receive 60,000 REGISTERs (2 per subscriber), and they should be even distributed, so we?re expecting 1,000 requests per second. Have you tried setting init_token_rate to 1,000? You?ll want to make sure this change is picked up on both Sprout and Homestead ? you can do this by editing /etc/clearwater/shared_config on a single node and running /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config. After a few minutes the change will have propagated around the deployment.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of ??????? ?ats?????
Sent: 19 September 2016 08:42
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Performance limit measurement

Hi Graeme,

i created a simpler scenario comparing to what the Sip Stress testing uses. In each scenario two subscribers try just to register to IMS and do not make any call to each other. I run this scenario for 15000 pairs of subscribers (30000 subscribers). The register requests are distributed in 1 minute time. It seems tha Sprout node is the bottleneck. The return code of most of the failed messages is  503 (Service Unavailable) and of some of them 408 (Request Timeout). I have added resources in Sprout (4 CPUs and 8Gb memory) so i don't believe that resources is the issue.

Does Sprout somehow exposes the latency measurements that lead to the throttling? We would like to take a look at them.



 Here is the the xml file .


<scenario name="Call Load Test">

  <User variables="my_dn,peer_dn,call_repeat" />
  <nop hide="true">
    <action>
      <!-- Get my and peer's DN -->
      <assignstr assign_to="my_dn" value="[field0]" />
      <!-- field1 is my_auth, but we can't store it in a variable -->
      <assignstr assign_to="peer_dn" value="[field2]" />
      <!-- field3 is peer_auth, but we can't store it in a variable -->
      <assign assign_to="reg_repeat" value="0"/>
      <assign assign_to="call_repeat" value="0"/>
    </action>
  </nop>

  <pause distribution="uniform" min="0" max="60000" />

  <send>
    <![CDATA[

      REGISTER sip:[$my_dn]@[service] SIP/2.0
      Via: SIP/2.0/[transport] [local_ip]:[local_port];rport;branch=[branch]-[$my_dn]-[$reg_repeat]
      Route: <sip:[service];transport=[transport];lr>
      Max-Forwards: 70
      From: <sip:[$my_dn]@[service]>;tag=[pid]SIPpTag00[call_number]
      To: <sip:[$my_dn]@[service]>
      Call-ID: [$my_dn]///[call_id]
      CSeq: [cseq] REGISTER
      User-Agent: Accession 4.0.0.0
      Supported: outbound, path
      Contact: <sip:[$my_dn]@[local_ip]:[local_port];transport=[transport];ob>;+sip.ice;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
      Expires: 3600
      Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
      Content-Length: 0

    ]]>
  </send>

  <recv response="401" auth="true">
    <action>
      <add assign_to="reg_repeat" value="1" />
    </action>
  </recv>

  <send>
    <![CDATA[

      REGISTER sip:[$my_dn]@[service] SIP/2.0
      Via: SIP/2.0/[transport] [local_ip]:[local_port];rport;branch=[branch]-[$my_dn]-[$reg_repeat]
      Route: <sip:[service];transport=[transport];lr>
      Max-Forwards: 70
      From: <sip:[$my_dn]@[service]>;tag=[pid]SIPpTag00[call_number]
      To: <sip:[$my_dn]@[service]>
      Call-ID: [$my_dn]///[call_id]
      CSeq: [cseq] REGISTER
      User-Agent: Accession 4.0.0.0
      Supported: outbound, path
      Contact: <sip:[$my_dn]@[local_ip]:[local_port];transport=[transport];ob>;+sip.ice;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
      Expires: 3600
      [field1]
      Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
      Content-Length: 0

    ]]>
  </send>

  <recv response="200">
    <action>
      <ereg regexp="rport=([^;]*);.*received=([^;]*);" search_in="hdr" header="Via:" assign_to="dummy" />
      <add assign_to="reg_repeat" value="1" />
    </action>
  </recv>
  <Reference variables="dummy" />

  <send>
    <![CDATA[

      REGISTER sip:[$peer_dn]@[service] SIP/2.0
      Via: SIP/2.0/[transport] [local_ip]:[local_port];rport;branch=[branch]-[$peer_dn]-[$reg_repeat]
      Route: <sip:[service];transport=[transport];lr>
      Max-Forwards: 70
      From: <sip:[$peer_dn]@[service]>;tag=[pid]SIPpTag00[call_number]
      To: <sip:[$peer_dn]@[service]>
      Call-ID: [$peer_dn]///[call_id]
      CSeq: [cseq] REGISTER
      User-Agent: Accession 4.0.0.0
      Supported: outbound, path
      Contact: <sip:[$peer_dn]@[local_ip]:[local_port];transport=[transport];ob>;+sip.ice;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
      Expires: 3600
      Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
      Content-Length: 0

    ]]>
  </send>

  <recv response="401" auth="true">
    <action>
      <add assign_to="reg_repeat" value="1" />
    </action>
  </recv>

  <send>
    <![CDATA[

      REGISTER sip:[$peer_dn]@[service] SIP/2.0
      Via: SIP/2.0/[transport] [local_ip]:[local_port];rport;branch=[branch]-[$peer_dn]-[$reg_repeat]
      Route: <sip:[service];transport=[transport];lr>
      Max-Forwards: 70
      From: <sip:[$peer_dn]@[service]>;tag=[pid]SIPpTag00[call_number]
      To: <sip:[$peer_dn]@[service]>
      Call-ID: [$peer_dn]///[call_id]
      CSeq: [cseq] REGISTER
      User-Agent: Accession 4.0.0.0
      Supported: outbound, path
      Contact: <sip:[$peer_dn]@[local_ip]:[local_port];transport=[transport];ob>;+sip.ice;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
      Expires: 3600
      [field3]
      Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
      Content-Length: 0

    ]]>
  </send>

  <recv response="200">
    <action>
      <add assign_to="reg_repeat" value="1" />
    </action>
  </recv>

</scenario>


Best Regards,
Michael Katsoulis



2016-09-16 21:25 GMT+03:00 Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>:
Hi Michael,

Can you tell me more about your scenario? It sounds like you?re not using the clearwater-sip-stress package, or at least not in exactly the form we package up. If you?re not using the clearwater-sip-stress package then please can you send details of your stress scenario?

Depending on how powerful your Sprout node is, I would expect 15000 calls per second to be towards the upper limit of its performance powers. However, if the CPU is not particularly high then that would suggest that Sprout?s throttling controls might require further tuning. Do you know what return code the ?unexpected messages? have? 503s indicate that there is overload somewhere. Sprout does adjust its throttling controls to match the load its able to process, but that process is not immediate, and we recommend building stress up gradually rather than immediately firing 15000 calls per second into the system ? for more information on that, see http://www.projectclearwater.org/clearwater-performance-and-our-load-monitor/.

One final thought I had was that the node you?re running stress on might be overloaded. If the stress node is not responding to messages in a timely fashion then that will generate time outs and unexpected messages.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of ??????? ?ats?????
Sent: 16 September 2016 15:16
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Performance limit measurement

Hi Graeme,

thanks a lot for your response.

In our scenario we are using the Stress node to generate 15000 calls in 60 seconds. The number of
unsuccessful calls varies from ~500 to ~5000 even in subsequent repetitions of the same scenario.
According to wireshark the failures happen because of Sprout that does not send the correct responses in time
and so we get "time-outs" and "unexpected messages" in the Stress node.
The Sprout node has sufficient CPU and memory resources.
What could be the reason of this instability in our deployment?

Thank you in advance,
Michael Katsoulis














2016-09-16 16:14 GMT+03:00 Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>:
Hi Michael,

How many successes and failures are you seeing? We primarily use the clearwater-sip-stress package to check we haven?t introduced crashes under load, and to check we haven?t significantly regressed the performance of Project Clearwater. Unfortunately clearwater-sip-stress is not reliable enough to generate completely accurate performance numbers for Project Clearwater (and we don?t accurately measure Project Clearwater performance or provide numbers). We tend to see around 1% failures when running clearwater-sip-stress. If your failure numbers are fluctuating at around 1% then this is probably down to the test scripts not being completely reliable, and you won?t have actually hit the deployment?s limit until you start seeing more failures than this.

Thanks,
Graeme


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of ??????? ?ats?????
Sent: 16 September 2016 10:17
To: Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Performance limit measurement

Hi all,

we are running Stress Tests against our Clearwater Deployment using Sip Stress node.
We have noticed that the results are not consistent as the number of successfull calls changes during repetitions of the same test scenario.

We have tried to increase the values of max_tokens , init_token_rate, min_token_rate and
target_latency_us but we did not observe any difference.

What is the proposed way to discover the deployment's limit on how many requests per second can
be served?

Thanks in advance,
Michael Katsoulis

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160920/5dbe2e56/attachment.html>

From surender.s at hcl.com  Tue Sep 20 08:44:37 2016
From: surender.s at hcl.com (Surender Singh)
Date: Tue, 20 Sep 2016 12:44:37 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E18EB7@NDA-HCLT-MBS05.hclt.corp.hcl.in>

Hi Greame,

Thanks for support .

But my issue is ,when I calling from A to B (Out call) its showing example.com in To_header and From_header . and same value going at my PBX ,due this might be  pbx unable to identify the value like 1234 at example.com  instead of 1234 at 10.112.87.177 . Can have any possibility to change it at IMS end.


I also change the configuration in Bgcf.json but I unable to check whether calls are forwarding through BGCF route or not .I found configuration like below... in one thread.


{
    "number_blocks" : [
        {
           "name" : "Internal numbers",
           "prefix" : "650555",
           "regex" : "!(^.*$)!sip:\\1 at example.com!"
        },
        {
            "name" : "External numbers",
            "prefix" : "",
            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"
        }
        

    ]
}



{
    "routes" : [
        {   "name" : "TO_PBX",
          "domain" : "10.112.87.177",
          "route" : ["<sip:cw-aio:5058;transport=UDP;lr;orig>"]
        }
	 
    ]
  
}




20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:691: Cloned tdta0x7f744c14aea0 to tdta0x7f744c14e530
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:odi_fWh8yrqIzM at 10.112.87.250:5054;lr;orig;service=scscf>
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c14eb40 => txdata 0x7f744c14e5d8 mapping
20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1587: scscf-0x7f744c14df40 pass initial request Request msg INVITE/cseq=1 (tdta0x7f744c14e530) to Sproutlet
20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:431: S-CSCF received initial request
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 3
20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:773: Route header references this system
20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:786: Found ODI token fWh8yrqIzM
20-09-2016 16:37:12.508 UTC Debug aschain.h:131: AsChain inc ref 0x7f744c1364b0 -> 2
20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:793: Original dialog for odi_fWh8yrqIzM found: AsChain-orig[0x7f744c1364b0]:2/1
20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:832: Got our Route header, session case orig, OD=AsChain-orig[0x7f744c1364b0]:2/1
20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:291: Served user from P-Served-User header
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 4
20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:502: Found served user, so apply services
20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:1153: Performing originating initiating request processing
20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1178: Completed applying originating services
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 2
20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2218: Translating URI
20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2189: Performing ENUM translation for user 1234
20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:240: Translating URI via JSON ENUM lookup
20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:305: Comparing first 4 numbers of 1234 against prefix 650555
20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:305: Comparing first 0 numbers of 1234 against prefix 
20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:312: Match found
20-09-2016 16:37:12.508 UTC Info enumservice.cpp:280: Number 1234 found, translated URI = sip:1234 at 10.112.87.177
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5
20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2249: Translated URI sip:1234 at 10.112.87.177 is a real SIP URI - replacing Request-URI
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5
20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1194: New URI string is sip:1234 at 10.112.87.177
20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:1210: Routing to BGCF
20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1446: Routing to BGCF sip:bgcf.cw-aio:5053;transport=TCP
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f744c14eb40
20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1386: scscf-0x7f744c14df40 sending Request msg INVITE/cseq=1 (tdta0x7f744c14e530) on fork 0
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f744c14e5d8, fork = 0
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1914: scscf-0x7f744c14df40 transmitting request on fork 0
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1928: scscf-0x7f744c14df40 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f744c14e530) on fork 0
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c14eb40 => txdata 0x7f744c14e5d8 mapping
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:bgcf.cw-aio:5053;transport=TCP;lr
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:329: Possible service name - bgcf
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio
20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1154: Created Sproutlet bgcf-0x7f744c05f0b0 for Request msg INVITE/cseq=1 (tdta0x7f744c14e530)
20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0) (609 bytes) to upstream sproutlet scscf:
--start msg--

SIP/2.0 100 Trying
Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Call-ID: OV0U_26XyoJOUXCQ0B_6FA..
From: <sip:6505550962 at example.com>;tag=9b0c5d3e
To: <sip:1234 at example.com>
CSeq: 1 INVITE
Content-Length:  0


--end msg--
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c151be0 => txdata 0x7f744c151678 mapping
20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1623: scscf-0x7f744c14df40 received provisional response Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0, state = Proceeding
20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:561: S-CSCF received response
20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1413: scscf-0x7f744c14df40 sending Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0)
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 100
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1853: Discard 100/INVITE response (tdta0x7f744c1515d0)
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c151be0 => txdata 0x7f744c151678 mapping
20-09-2016 16:37:12.508 UTC Debug pjsip: tdta0x7f744c15 Destroying txdata Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0)
20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:2062: Routing Request msg INVITE/cseq=1 (tdta0x7f744c14e530) (1411 bytes) to downstream sproutlet bgcf:
--start msg--

INVITE sip:1234 at 10.112.87.177 SIP/2.0
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be
Max-Forwards: 67
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>
To: <sip:1234 at example.com>
From: <sip:6505550962 at example.com>;tag=9b0c5d3e
Call-ID: OV0U_26XyoJOUXCQ0B_6FA..
CSeq: 1 INVITE
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Allow-Events: presence, kpml
P-Asserted-Identity: <sip:6505550962 at example.com>
Session-Expires: 600
P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg
Route: <sip:bgcf.cw-aio:5053;transport=TCP;lr>
Content-Type: application/sdp
Content-Length:   241

v=0
o=Z 0 0 IN IP4 10.112.123.57
s=Z
c=IN IP4 10.112.123.57
t=0 0
m=audio 8000 RTP/AVP 3 110 8 0 97 101
a=rtpmap:110 speex/8000
a=rtpmap:97 iLBC/8000
a=fmtp:97 mode=30
a=rtpmap:101 telephone-event/8000
a=fmtp:101 0-16
a=sendrecv

--end msg--
20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:691: Cloned tdta0x7f744c14e530 to tdta0x7f744c1515d0
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:bgcf.cw-aio:5053;transport=TCP;lr>
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c151be0 => txdata 0x7f744c151678 mapping
20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1587: bgcf-0x7f744c05f0b0 pass initial request Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) to Sproutlet
20-09-2016 16:37:12.508 UTC Debug acr.cpp:49: Created ACR (0x7f744c05f030)
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5
20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2328: Not translating URI
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5
20-09-2016 16:37:12.508 UTC Debug bgcfservice.cpp:188: Getting route for URI domain 10.112.87.177 via BGCF lookup
20-09-2016 16:37:12.508 UTC Info bgcfservice.cpp:198: Found route to domain 10.112.87.177
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f744c151be0
20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1386: bgcf-0x7f744c05f0b0 sending Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f744c151678, fork = 0
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1914: bgcf-0x7f744c05f0b0 transmitting request on fork 0
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1928: bgcf-0x7f744c05f0b0 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c151be0 => txdata 0x7f744c151678 mapping
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:cw-aio:5058;transport=UDP;lr;orig
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:190: No Sproutlet found using service name or host
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:196: Find default service for port 5058
20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:874: No local sproutlet matches request
20-09-2016 16:37:12.508 UTC Debug pjsip: tsx0x7f744c154 Transaction created for Request msg INVITE/cseq=1 (tdta0x7f744c1515d0)
20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1618: Added trail identifier 10262 to UAC transaction
20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:490: Next hop node is encoded in top route header
20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:86: SIPResolver::resolve for name cw-aio, port 5058, transport 17, family 2
20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:523: Attempt to parse cw-aio as IP address
20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:128: Port is specified
20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:296: Perform A/AAAA record lookup only, name = cw-aio
20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:724: Removing record for scscf.cw-aio (type 1, expiry time 1474389247) from the expiry list
20-09-2016 16:37:12.508 UTC Verbose dnscachedresolver.cpp:245: Check cache for cw-aio type 1
20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:251: No entry found in cache
20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:254: Create cache entry pending query
20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:302: Create and execute DNS query transaction
20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:315: Wait for query responses
20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:465: Received DNS response for cw-aio type A
20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:90: Parsing DNS message
000000: 19378580 00010001 00000000 0663772d 61696f00 00010001 c00c0001 00010000    .7.. .... .... .cw- aio. .... .... .... 
000020: 00000004 0a7057fa                                                          .... .pW.                               

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:95: Parsing header at offset 0x0
20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:98: 1 questions, 1 answers, 0 authorities, 0 additional records
20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:103: Parsing question 1 at offset 0xc
20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:229: Parsed domain name = cw-aio, encoded length = 8
20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:112: Parsing answer 1 at offset 0x18
20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:229: Parsed domain name = cw-aio, encoded length = 2
20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:282: Resource Record NAME=cw-aio TYPE=A CLASS=IN TTL=0 RDLENGTH=4
20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:287: Parse A record RDATA
20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:142: Answer records
cw-aio                  0       IN      A       10.112.87.250

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:143: Authority records

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:144: Additional records

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:761: Adding record to cache entry, TTL=0, expiry=1474389432
20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:765: Update cache entry expiry to 1474389432
20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:707: Adding cw-aio to cache expiry list with deletion time of 1474389732
20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:319: Received all query responses
20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:347: Pulling 1 records from cache for cw-aio A
20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:362: Found 1 A/AAAA records, randomizing
20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:993: 10.112.87.250:5058 transport 17 has state: WHITE
20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:993: 10.112.87.250:5058 transport 17 has state: WHITE
20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:406: Added a server, now have 1 of 5
20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:447: Adding 0 servers from blacklist
20-09-2016 16:37:12.508 UTC Info pjutils.cpp:938: Resolved destination URI sip:cw-aio:5058;transport=UDP;lr;orig to 1 servers
20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:490: Next hop node is encoded in top route header
20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1641: Next hop cw-aio is not a stateless proxy
20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1655: Sending request for sip:1234 at 10.112.87.177
20-09-2016 16:37:12.508 UTC Debug pjsip: tsx0x7f744c154 Sending Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) in state Null
20-09-2016 16:37:12.508 UTC Debug pjsip:       endpoint Request msg INVITE/cseq=1 (tdta0x7f744c1515d0): skipping target resolution because address is already set
20-09-2016 16:37:12.508 UTC Debug pjsip:       endpoint Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) exceeds UDP size threshold (1300), sending with TCP
20-09-2016 16:37:12.508 UTC Verbose pjsip: tcpc0x7f744c15 TCP client transport created
20-09-2016 16:37:12.508 UTC Verbose pjsip: tcpc0x7f744c15 TCP transport 10.112.87.250:57743 is connecting to 10.112.87.250:5058...
20-09-2016 16:37:12.509 UTC Verbose common_sip_processing.cpp:136: TX 1504 bytes Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) to TCP 10.112.87.250:5058:
--start msg--

INVITE sip:1234 at 10.112.87.177 SIP/2.0
Via: SIP/2.0/TCP 10.112.87.250:57743;rport;branch=z9hG4bKPjeUUjYygIls2DRu2LAAHqVGU27AHZ.-S3
Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>
Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh
Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>
Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>
Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be
Max-Forwards: 66
Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>
To: <sip:1234 at example.com>
From: <sip:6505550962 at example.com>;tag=9b0c5d3e
Call-ID: OV0U_26XyoJOUXCQ0B_6FA..
CSeq: 1 INVITE
Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE
Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri
User-Agent: Z 3.9.32144 r32121
Allow-Events: presence, kpml
P-Asserted-Identity: <sip:6505550962 at example.com>
Session-Expires: 600
P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg
Route: <sip:cw-aio:5058;transport=UDP;lr;orig>
Content-Type: application/sdp
Content-Length:   241

v=0
o=Z 0 0 IN IP4 10.112.123.57
s=Z
c=IN IP4 10.112.123.57
t=0 0
m=audio 8000 RTP/AVP 3 110 8 0 97 101
a=rtpmap:110 speex/8000
a=rtpmap:97 iLBC/8000
a=fmtp:97 mode=30
a=rtpmap:101 telephone-event/8000
a=fmtp:101 0-16
a=sendrecv

--end msg--
20-09-2016 16:37:12.509 UTC Debug pjsip: tsx0x7f744c154 State changed from Null to Calling, event=TX_MSG
20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:213: tsx0x7f744c1546a8 - tu_on_tsx_state UAC, TSX_STATE TX_MSG state=Calling
20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:1813: tsx0x7f744c1546a8 - uac_tsx = 0x7f744c154540, uas_tsx = 0x7f744c001b20
20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:1821: TX_MSG event on current UAC transaction
20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:2134: Starting timer C
20-09-2016 16:37:12.509 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f744409acd8
20-09-2016 16:37:12.509 UTC Debug thread_dispatcher.cpp:199: Request latency = 8327us
20-09-2016 16:37:12.519 UTC Verbose pjsip: tcpc0x7f744c15 TCP transport 10.112.87.250:57743 is connected to 10.112.87.250:5058
20-09-2016 16:37:12.524 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Response msg 100/INVITE/cseq=1 (rdata0x7f744c157410)
20-09-2016 16:37:12.524 UTC Verbose common_sip_processing.cpp:120: RX 864 bytes Response msg 100/INVITE/cseq=1 (rdata0x7f744c157410) from TCP 10.112.87.250:5058:
--start msg--







-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 20 September 2016 17:18
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 41, Issue 37

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. Re: SIP Trunking With PBX_aio
      (Graeme Robertson (projectclearwater.org))
   2. Re: Performance limit measurement
      (Graeme Robertson (projectclearwater.org))


----------------------------------------------------------------------

Message: 1
Date: Tue, 20 Sep 2016 11:06:39 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID:
	<CY4PR02MB2616BA771B427321072C871CE3F70 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="us-ascii"

Hi Surender,



At the end of originating processing at the S-CSCF, we do an ENUM lookup and find a match and translate the Request URI to sip:1234 at 10.112.87.177;transport=TCP. We then identify that this is not a local user, so we send it to the BGCF to route off-net. The BGCF thinks there are no routes configured for 10.112.87.177, and so it just attempts normal SIP proxying - i.e. it sends the INVITE to 10.112.87.177:5060, which is your PBX. It looks as though (from an earlier email) that you were hoping to route it via Bono (as an IBCF) but for some reason this configuration hasn't been picked up. However, I'm not sure there's actually any value to including the IBCF in this flow - the same INVITE would still end up being routed to 10.112.87.177:5060.



Project Clearwater is designed as a spec-compliant IMS core. In the IMS world, the user in the To header is never really used, and we never touch it. This is not very consistent with the original standards for SIP detailed in RFC3261<https://www.ietf.org/rfc/rfc3261.txt>, so we don't always interoperate perfectly with non-IMS devices. I'm afraid I don't know anything about your PBX, but if it is non-IMS and is expecting the To header to contain a certain value, then this would explain why it is rejecting the call. If you really need to re-write the To header on your request, and you are not averse to writing a bit of code, Sprout provides a really easy-to-use API for developing your own app server. https://github.com/Metaswitch/greeter is a sample application server that uses this API and simply adds the header "Subject: Hello world!" to a SIP message - you could write something similar that changes to the To header on your SIP messages. Alternatively, Metaswitch has a commercial product called Perimeta<http://www.metaswitch.com/perimeta-session-border-controller-sbc> which can be deployed as an IBCF, and which has a SIP Message Manipulation Framework which you would be able to use to alter your To header.



Thanks,

Graeme



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 20 September 2016 07:42
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio



Hi Graeme/Team,



Updating...............



1) Now Incoming calls are working from PBX to IMS .



2) For Outgoing calls ,able to send request Invite to PBX IP 10.112.87.177 but getting 404 no route to destination/Not found.



3) Can you suggest me what parameter (To Header or Request URI) are checked by the Peer node (PBX) to terminate the calls.



4) I also not able to understand configuration/calls flow  in BGCF.JSON. How IMS route the calls using BGCF.JSON.



Please find some below  logs of Sprout....






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160920/e5c1d912/attachment-0001.html>

------------------------------

Message: 2
Date: Tue, 20 Sep 2016 11:47:06 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Performance limit measurement
Message-ID:
	<MWHPR02MB2621F5D2D0FB35D6FFA34F10E3F70 at MWHPR02MB2621.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="utf-8"

Hi Michael,

It?s worth noting that Project Clearwater is designed to scale horizontally rather than vertically, so we would expect multiple less powerful Sprout nodes to out-perform a single powerful Sprout node. However, that doesn?t mean that your Sprout node isn?t capable of handling the load you?re hitting it with.

We do expose latency measurements over SNMP ? see http://clearwater.readthedocs.io/en/stable/Clearwater_SNMP_Statistics.html for more details. In particular, under Sprout statistics we have various latency statistics including latency for SIP requests and latency for requests to Homestead. There are a couple of other statistics that might be useful for determining when exactly our requests are failing ? if the number of initial registration failures and/or the number of authentication failures are non-zero this would indicate that the bottleneck is actually at Homestead.

It does sound as though Sprout is reporting itself as overloaded even though it could handle more requests. As I mentioned previously, Sprout will tweak it?s overload controls to rectify this, but it won?t be immediate, which might explain the failures. I know you?ve already tried tweaking the token controls, but it might be worth looking at them again. Over the course of the minute of your test, I think we expect to receive 60,000 REGISTERs (2 per subscriber), and they should be even distributed, so we?re expecting 1,000 requests per second. Have you tried setting init_token_rate to 1,000? You?ll want to make sure this change is picked up on both Sprout and Homestead ? you can do this by editing /etc/clearwater/shared_config on a single node and running /usr/share/clearwater/clearwater-config-manager/scripts/upload_shared_config. After a few minutes the change will have propagated around the deployment.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of ??????? ?ats?????
Sent: 19 September 2016 08:42
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Performance limit measurement

Hi Graeme,

i created a simpler scenario comparing to what the Sip Stress testing uses. In each scenario two subscribers try just to register to IMS and do not make any call to each other. I run this scenario for 15000 pairs of subscribers (30000 subscribers). The register requests are distributed in 1 minute time. It seems tha Sprout node is the bottleneck. The return code of most of the failed messages is  503 (Service Unavailable) and of some of them 408 (Request Timeout). I have added resources in Sprout (4 CPUs and 8Gb memory) so i don't believe that resources is the issue.

Does Sprout somehow exposes the latency measurements that lead to the throttling? We would like to take a look at them.



 Here is the the xml file .


<scenario name="Call Load Test">

  <User variables="my_dn,peer_dn,call_repeat" />
  <nop hide="true">
    <action>
      <!-- Get my and peer's DN -->
      <assignstr assign_to="my_dn" value="[field0]" />
      <!-- field1 is my_auth, but we can't store it in a variable -->
      <assignstr assign_to="peer_dn" value="[field2]" />
      <!-- field3 is peer_auth, but we can't store it in a variable -->
      <assign assign_to="reg_repeat" value="0"/>
      <assign assign_to="call_repeat" value="0"/>
    </action>
  </nop>

  <pause distribution="uniform" min="0" max="60000" />

  <send>
    <![CDATA[

      REGISTER sip:[$my_dn]@[service] SIP/2.0
      Via: SIP/2.0/[transport] [local_ip]:[local_port];rport;branch=[branch]-[$my_dn]-[$reg_repeat]
      Route: <sip:[service];transport=[transport];lr>
      Max-Forwards: 70
      From: <sip:[$my_dn]@[service]>;tag=[pid]SIPpTag00[call_number]
      To: <sip:[$my_dn]@[service]>
      Call-ID: [$my_dn]///[call_id]
      CSeq: [cseq] REGISTER
      User-Agent: Accession 4.0.0.0
      Supported: outbound, path
      Contact: <sip:[$my_dn]@[local_ip]:[local_port];transport=[transport];ob>;+sip.ice;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
      Expires: 3600
      Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
      Content-Length: 0

    ]]>
  </send>

  <recv response="401" auth="true">
    <action>
      <add assign_to="reg_repeat" value="1" />
    </action>
  </recv>

  <send>
    <![CDATA[

      REGISTER sip:[$my_dn]@[service] SIP/2.0
      Via: SIP/2.0/[transport] [local_ip]:[local_port];rport;branch=[branch]-[$my_dn]-[$reg_repeat]
      Route: <sip:[service];transport=[transport];lr>
      Max-Forwards: 70
      From: <sip:[$my_dn]@[service]>;tag=[pid]SIPpTag00[call_number]
      To: <sip:[$my_dn]@[service]>
      Call-ID: [$my_dn]///[call_id]
      CSeq: [cseq] REGISTER
      User-Agent: Accession 4.0.0.0
      Supported: outbound, path
      Contact: <sip:[$my_dn]@[local_ip]:[local_port];transport=[transport];ob>;+sip.ice;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
      Expires: 3600
      [field1]
      Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
      Content-Length: 0

    ]]>
  </send>

  <recv response="200">
    <action>
      <ereg regexp="rport=([^;]*);.*received=([^;]*);" search_in="hdr" header="Via:" assign_to="dummy" />
      <add assign_to="reg_repeat" value="1" />
    </action>
  </recv>
  <Reference variables="dummy" />

  <send>
    <![CDATA[

      REGISTER sip:[$peer_dn]@[service] SIP/2.0
      Via: SIP/2.0/[transport] [local_ip]:[local_port];rport;branch=[branch]-[$peer_dn]-[$reg_repeat]
      Route: <sip:[service];transport=[transport];lr>
      Max-Forwards: 70
      From: <sip:[$peer_dn]@[service]>;tag=[pid]SIPpTag00[call_number]
      To: <sip:[$peer_dn]@[service]>
      Call-ID: [$peer_dn]///[call_id]
      CSeq: [cseq] REGISTER
      User-Agent: Accession 4.0.0.0
      Supported: outbound, path
      Contact: <sip:[$peer_dn]@[local_ip]:[local_port];transport=[transport];ob>;+sip.ice;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
      Expires: 3600
      Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
      Content-Length: 0

    ]]>
  </send>

  <recv response="401" auth="true">
    <action>
      <add assign_to="reg_repeat" value="1" />
    </action>
  </recv>

  <send>
    <![CDATA[

      REGISTER sip:[$peer_dn]@[service] SIP/2.0
      Via: SIP/2.0/[transport] [local_ip]:[local_port];rport;branch=[branch]-[$peer_dn]-[$reg_repeat]
      Route: <sip:[service];transport=[transport];lr>
      Max-Forwards: 70
      From: <sip:[$peer_dn]@[service]>;tag=[pid]SIPpTag00[call_number]
      To: <sip:[$peer_dn]@[service]>
      Call-ID: [$peer_dn]///[call_id]
      CSeq: [cseq] REGISTER
      User-Agent: Accession 4.0.0.0
      Supported: outbound, path
      Contact: <sip:[$peer_dn]@[local_ip]:[local_port];transport=[transport];ob>;+sip.ice;reg-id=1;+sip.instance="<urn:uuid:00000000-0000-0000-0000-000000000001>"
      Expires: 3600
      [field3]
      Allow: PRACK, INVITE, ACK, BYE, CANCEL, UPDATE, SUBSCRIBE, NOTIFY, REFER, MESSAGE, OPTIONS
      Content-Length: 0

    ]]>
  </send>

  <recv response="200">
    <action>
      <add assign_to="reg_repeat" value="1" />
    </action>
  </recv>

</scenario>


Best Regards,
Michael Katsoulis



2016-09-16 21:25 GMT+03:00 Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>:
Hi Michael,

Can you tell me more about your scenario? It sounds like you?re not using the clearwater-sip-stress package, or at least not in exactly the form we package up. If you?re not using the clearwater-sip-stress package then please can you send details of your stress scenario?

Depending on how powerful your Sprout node is, I would expect 15000 calls per second to be towards the upper limit of its performance powers. However, if the CPU is not particularly high then that would suggest that Sprout?s throttling controls might require further tuning. Do you know what return code the ?unexpected messages? have? 503s indicate that there is overload somewhere. Sprout does adjust its throttling controls to match the load its able to process, but that process is not immediate, and we recommend building stress up gradually rather than immediately firing 15000 calls per second into the system ? for more information on that, see http://www.projectclearwater.org/clearwater-performance-and-our-load-monitor/.

One final thought I had was that the node you?re running stress on might be overloaded. If the stress node is not responding to messages in a timely fashion then that will generate time outs and unexpected messages.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of ??????? ?ats?????
Sent: 16 September 2016 15:16
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Performance limit measurement

Hi Graeme,

thanks a lot for your response.

In our scenario we are using the Stress node to generate 15000 calls in 60 seconds. The number of unsuccessful calls varies from ~500 to ~5000 even in subsequent repetitions of the same scenario.
According to wireshark the failures happen because of Sprout that does not send the correct responses in time and so we get "time-outs" and "unexpected messages" in the Stress node.
The Sprout node has sufficient CPU and memory resources.
What could be the reason of this instability in our deployment?

Thank you in advance,
Michael Katsoulis














2016-09-16 16:14 GMT+03:00 Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>>:
Hi Michael,

How many successes and failures are you seeing? We primarily use the clearwater-sip-stress package to check we haven?t introduced crashes under load, and to check we haven?t significantly regressed the performance of Project Clearwater. Unfortunately clearwater-sip-stress is not reliable enough to generate completely accurate performance numbers for Project Clearwater (and we don?t accurately measure Project Clearwater performance or provide numbers). We tend to see around 1% failures when running clearwater-sip-stress. If your failure numbers are fluctuating at around 1% then this is probably down to the test scripts not being completely reliable, and you won?t have actually hit the deployment?s limit until you start seeing more failures than this.

Thanks,
Graeme


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of ??????? ?ats?????
Sent: 16 September 2016 10:17
To: Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Performance limit measurement

Hi all,

we are running Stress Tests against our Clearwater Deployment using Sip Stress node.
We have noticed that the results are not consistent as the number of successfull calls changes during repetitions of the same test scenario.

We have tried to increase the values of max_tokens , init_token_rate, min_token_rate and target_latency_us but we did not observe any difference.

What is the proposed way to discover the deployment's limit on how many requests per second can be served?

Thanks in advance,
Michael Katsoulis

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160920/5dbe2e56/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 41, Issue 37
******************************************


::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------




From jaafar.bendriss at orange.com  Tue Sep 20 08:59:06 2016
From: jaafar.bendriss at orange.com (jaafar.bendriss at orange.com)
Date: Tue, 20 Sep 2016 12:59:06 +0000
Subject: [Project Clearwater] Clearwater Metrics
Message-ID: <4711_1474376347_57E1329B_4711_4263_1_22D8017AEC6D3B4793872DB3413DC7BF01B99B37@OPEXCNORM52.corporate.adroot.infra.ftgroup>

Hello , I would like to thank you for your vIMS product.

I would like to know if you had already defined some set of metrics that are specific to your VNFs beyond the standard system and network metrics (such as sip client etc ...)

Thank you

[Logo Orange]<http://www.orange.com/>

Jaafar Bendriss
Ph.D. Candidate on autonomic and cognitive management
ORANGE/IMT/OLN/CNC/R3S/SPO
Orange Gardens
1D-2-12-C
Fixe : +33 1 57 39 67 28 <https://monsi.sso.francetelecom.fr/index.asp?target=http%3A%2F%2Fclicvoice.sso.francetelecom.fr%2FClicvoiceV2%2FToolBar.do%3Faction%3Ddefault%26rootservice%3DSIGNATURE%26to%3D+33%201%2057%2039%2067%2028>
jaafar.bendriss at orange.com<mailto:jaafar.bendriss at orange.com>


_________________________________________________________________________________________________________________________

Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.

This message and its attachments may contain confidential or privileged information that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and delete this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
Thank you.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160920/d90b0511/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.gif
Type: image/gif
Size: 1450 bytes
Desc: image001.gif
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160920/d90b0511/attachment.gif>

From sarbajitc at gmail.com  Tue Sep 20 10:05:06 2016
From: sarbajitc at gmail.com (Sarbajit Chatterjee)
Date: Tue, 20 Sep 2016 19:35:06 +0530
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster using
	docker-compose
Message-ID: <CAN20VnjYp3aGAnrkMUn-WYEck6+uTi+HwB6PbbZUC9wuPB1W1w@mail.gmail.com>

Hello,

I am following the instructions from
https://github.com/Metaswitch/clearwater-docker. I can successfully deploy
it on a single Docker node but, the compose file does not work with Swarm
cluster.

I did try to modify the compose file like this -


version: '2'
services:
  etcd:
    image: quay.io/coreos/etcd:v2.2.5
    command: >
      -name etcd0
      -advertise-client-urls http://etcd:2379,http://etcd:4001
      -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001
      -initial-advertise-peer-urls http://etcd:2380
      -listen-peer-urls http://0.0.0.0:2380
      -initial-cluster etcd0=http://etcd:2380
      -initial-cluster-state new
  bono:
    image: swarm-node:5000/clearwaterdocker_bono
    ports:
      - 22
      - "3478:3478"
      - "3478:3478/udp"
      - "5060:5060"
      - "5060:5060/udp"
      - "5062:5062"
  sprout:
    image: swarm-node:5000/clearwaterdocker_sprout
    networks:
      default:
        aliases:
          - scscf.sprout
          - icscf.sprout
    ports:
      - 22
  homestead:
    image: swarm-node:5000/clearwaterdocker_homestead
    ports:
      - 22
  homer:
    image: swarm-node:5000/clearwaterdocker_homer
    ports:
      - 22
  ralf:
    image: swarm-node:5000/clearwaterdocker_ralf
    ports:
      - 22
  ellis:
    image: swarm-node:5000/clearwaterdocker_ellis
    ports:
      - 22
      - "80:80"


where swarm-node:5000 is the local docker registry and it hosts the
pre-built images of Clearwater containers. Even though the deployment
succeeded, clearwater-livetests are failing with following error -


Basic Registration (TCP) - Failed
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     -
/usr/local/rvm/gems/ruby-1.9.3-p551/gems/quaff-0.7.3/lib/sources.rb:41:in
`initialize'


Any suggestions on how I can deploy Clearwater on a Swarm cluster?

Thanks,
Sarbajit
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160920/1647ff73/attachment.html>

From graeme at projectclearwater.org  Wed Sep 21 07:24:02 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Wed, 21 Sep 2016 11:24:02 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
In-Reply-To: <16AE9ED83DBA5B4D85B058EAF39C918107E18EB7@NDA-HCLT-MBS05.hclt.corp.hcl.in>
References: <16AE9ED83DBA5B4D85B058EAF39C918107E18EB7@NDA-HCLT-MBS05.hclt.corp.hcl.in>
Message-ID: <CY4PR02MB2616A2D97C1BC38B1E842CC5E3F60@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Surender,



Project Clearwater is IMS compliant, and so I'm afraid it doesn't alter either the To header or the From header, and there is no trivial way to configure Project Clearwater to do this manipulation, or any plans to add this ability to Project Clearwater. It sounds as though your PBX is non-IMS compliant. As I mentioned before, if you really need to re-write the To header on your request, and you are not averse to writing a bit of code, Sprout provides a really easy-to-use API for developing your own app server. https://github.com/Metaswitch/greeter is a sample application server that uses this API and simply adds the header "Subject: Hello world!" to a SIP message - you could write something similar that changes to the To header on your SIP messages. Alternatively, Metaswitch has a commercial product called Perimeta<http://www.metaswitch.com/perimeta-session-border-controller-sbc> which can be deployed as an IBCF, and which has a SIP Message Manipulation Framework which you would be able to use to alter your To header.



>From the logs below, it looks as though the BGCF is now applying your route and the INVITE will presumably be going out through Bono, but unfortunately I don't think that will solve your problem.



Thanks,

Graeme



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 20 September 2016 13:45
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP Trunking With PBX_aio



Hi Greame,



Thanks for support .



But my issue is ,when I calling from A to B (Out call) its showing example.com in To_header and From_header . and same value going at my PBX ,due this might be  pbx unable to identify the value like 1234 at example.com<mailto:1234 at example.com>  instead of 1234 at 10.112.87.177<mailto:1234 at 10.112.87.177> . Can have any possibility to change it at IMS end.





I also change the configuration in Bgcf.json but I unable to check whether calls are forwarding through BGCF route or not .I found configuration like below... in one thread.





{

    "number_blocks" : [

        {

           "name" : "Internal numbers",

           "prefix" : "650555",

           "regex" : "!(^.*$)!sip:\\1 at example.com!"

        },

        {

            "name" : "External numbers",

            "prefix" : "",

            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"

        }





    ]

}







{

    "routes" : [

        {   "name" : "TO_PBX",

          "domain" : "10.112.87.177",

          "route" : ["<sip:cw-aio:5058;transport=UDP;lr;orig>"]

        }



    ]



}









20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:691: Cloned tdta0x7f744c14aea0 to tdta0x7f744c14e530

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:odi_fWh8yrqIzM at 10.112.87.250:5054;lr;orig;service=scscf>

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c14eb40 => txdata 0x7f744c14e5d8 mapping

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1587: scscf-0x7f744c14df40 pass initial request Request msg INVITE/cseq=1 (tdta0x7f744c14e530) to Sproutlet

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:431: S-CSCF received initial request

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 3

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:773: Route header references this system

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:786: Found ODI token fWh8yrqIzM

20-09-2016 16:37:12.508 UTC Debug aschain.h:131: AsChain inc ref 0x7f744c1364b0 -> 2

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:793: Original dialog for odi_fWh8yrqIzM found: AsChain-orig[0x7f744c1364b0]:2/1

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:832: Got our Route header, session case orig, OD=AsChain-orig[0x7f744c1364b0]:2/1

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:291: Served user from P-Served-User header

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 4

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:502: Found served user, so apply services

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:1153: Performing originating initiating request processing

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1178: Completed applying originating services

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 2

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2218: Translating URI

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2189: Performing ENUM translation for user 1234

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:240: Translating URI via JSON ENUM lookup

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:305: Comparing first 4 numbers of 1234 against prefix 650555

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:305: Comparing first 0 numbers of 1234 against prefix

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:312: Match found

20-09-2016 16:37:12.508 UTC Info enumservice.cpp:280: Number 1234 found, translated URI = sip:1234 at 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2249: Translated URI sip:1234 at 10.112.87.177 is a real SIP URI - replacing Request-URI

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1194: New URI string is sip:1234 at 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:1210: Routing to BGCF

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1446: Routing to BGCF sip:bgcf.cw-aio:5053;transport=TCP

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f744c14eb40

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1386: scscf-0x7f744c14df40 sending Request msg INVITE/cseq=1 (tdta0x7f744c14e530) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f744c14e5d8, fork = 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1914: scscf-0x7f744c14df40 transmitting request on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1928: scscf-0x7f744c14df40 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f744c14e530) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c14eb40 => txdata 0x7f744c14e5d8 mapping

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:bgcf.cw-aio:5053;transport=TCP;lr

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:329: Possible service name - bgcf

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1154: Created Sproutlet bgcf-0x7f744c05f0b0 for Request msg INVITE/cseq=1 (tdta0x7f744c14e530)

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0) (609 bytes) to upstream sproutlet scscf:

--start msg--



SIP/2.0 100 Trying

Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh

Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be

Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>

Call-ID: OV0U_26XyoJOUXCQ0B_6FA..

From: <sip:6505550962 at example.com>;tag=9b0c5d3e

To: <sip:1234 at example.com>

CSeq: 1 INVITE

Content-Length:  0





--end msg--

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1623: scscf-0x7f744c14df40 received provisional response Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0, state = Proceeding

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:561: S-CSCF received response

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1413: scscf-0x7f744c14df40 sending Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 100

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1853: Discard 100/INVITE response (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Debug pjsip: tdta0x7f744c15 Destroying txdata Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:2062: Routing Request msg INVITE/cseq=1 (tdta0x7f744c14e530) (1411 bytes) to downstream sproutlet bgcf:

--start msg--



INVITE sip:1234 at 10.112.87.177 SIP/2.0

Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh

Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>

Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be

Max-Forwards: 67

Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>

To: <sip:1234 at example.com>

From: <sip:6505550962 at example.com>;tag=9b0c5d3e

Call-ID: OV0U_26XyoJOUXCQ0B_6FA..

CSeq: 1 INVITE

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE

Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri

User-Agent: Z 3.9.32144 r32121

Allow-Events: presence, kpml

P-Asserted-Identity: <sip:6505550962 at example.com>

Session-Expires: 600

P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg

Route: <sip:bgcf.cw-aio:5053;transport=TCP;lr>

Content-Type: application/sdp

Content-Length:   241



v=0

o=Z 0 0 IN IP4 10.112.123.57

s=Z

c=IN IP4 10.112.123.57

t=0 0

m=audio 8000 RTP/AVP 3 110 8 0 97 101

a=rtpmap:110 speex/8000

a=rtpmap:97 iLBC/8000

a=fmtp:97 mode=30

a=rtpmap:101 telephone-event/8000

a=fmtp:101 0-16

a=sendrecv



--end msg--

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:691: Cloned tdta0x7f744c14e530 to tdta0x7f744c1515d0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:bgcf.cw-aio:5053;transport=TCP;lr>

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1587: bgcf-0x7f744c05f0b0 pass initial request Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) to Sproutlet

20-09-2016 16:37:12.508 UTC Debug acr.cpp:49: Created ACR (0x7f744c05f030)

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2328: Not translating URI

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Debug bgcfservice.cpp:188: Getting route for URI domain 10.112.87.177 via BGCF lookup

20-09-2016 16:37:12.508 UTC Info bgcfservice.cpp:198: Found route to domain 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f744c151be0

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1386: bgcf-0x7f744c05f0b0 sending Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f744c151678, fork = 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1914: bgcf-0x7f744c05f0b0 transmitting request on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1928: bgcf-0x7f744c05f0b0 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:cw-aio:5058;transport=UDP;lr;orig

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:190: No Sproutlet found using service name or host

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:196: Find default service for port 5058

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:874: No local sproutlet matches request

20-09-2016 16:37:12.508 UTC Debug pjsip: tsx0x7f744c154 Transaction created for Request msg INVITE/cseq=1 (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1618: Added trail identifier 10262 to UAC transaction

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:490: Next hop node is encoded in top route header

20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:86: SIPResolver::resolve for name cw-aio, port 5058, transport 17, family 2

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:523: Attempt to parse cw-aio as IP address

20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:128: Port is specified

20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:296: Perform A/AAAA record lookup only, name = cw-aio

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:724: Removing record for scscf.cw-aio (type 1, expiry time 1474389247) from the expiry list

20-09-2016 16:37:12.508 UTC Verbose dnscachedresolver.cpp:245: Check cache for cw-aio type 1

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:251: No entry found in cache

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:254: Create cache entry pending query

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:302: Create and execute DNS query transaction

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:315: Wait for query responses

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:465: Received DNS response for cw-aio type A

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:90: Parsing DNS message

000000: 19378580 00010001 00000000 0663772d 61696f00 00010001 c00c0001 00010000    .7.. .... .... .cw- aio. .... .... ....

000020: 00000004 0a7057fa                                                          .... .pW.



20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:95: Parsing header at offset 0x0

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:98: 1 questions, 1 answers, 0 authorities, 0 additional records

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:103: Parsing question 1 at offset 0xc

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:229: Parsed domain name = cw-aio, encoded length = 8

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:112: Parsing answer 1 at offset 0x18

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:229: Parsed domain name = cw-aio, encoded length = 2

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:282: Resource Record NAME=cw-aio TYPE=A CLASS=IN TTL=0 RDLENGTH=4

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:287: Parse A record RDATA

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:142: Answer records

cw-aio                  0       IN      A       10.112.87.250



20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:143: Authority records



20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:144: Additional records



20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:761: Adding record to cache entry, TTL=0, expiry=1474389432

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:765: Update cache entry expiry to 1474389432

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:707: Adding cw-aio to cache expiry list with deletion time of 1474389732

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:319: Received all query responses

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:347: Pulling 1 records from cache for cw-aio A

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:362: Found 1 A/AAAA records, randomizing

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:993: 10.112.87.250:5058 transport 17 has state: WHITE

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:993: 10.112.87.250:5058 transport 17 has state: WHITE

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:406: Added a server, now have 1 of 5

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:447: Adding 0 servers from blacklist

20-09-2016 16:37:12.508 UTC Info pjutils.cpp:938: Resolved destination URI sip:cw-aio:5058;transport=UDP;lr;orig to 1 servers

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:490: Next hop node is encoded in top route header

20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1641: Next hop cw-aio is not a stateless proxy

20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1655: Sending request for sip:1234 at 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug pjsip: tsx0x7f744c154 Sending Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) in state Null

20-09-2016 16:37:12.508 UTC Debug pjsip:       endpoint Request msg INVITE/cseq=1 (tdta0x7f744c1515d0): skipping target resolution because address is already set

20-09-2016 16:37:12.508 UTC Debug pjsip:       endpoint Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) exceeds UDP size threshold (1300), sending with TCP

20-09-2016 16:37:12.508 UTC Verbose pjsip: tcpc0x7f744c15 TCP client transport created

20-09-2016 16:37:12.508 UTC Verbose pjsip: tcpc0x7f744c15 TCP transport 10.112.87.250:57743 is connecting to 10.112.87.250:5058...

20-09-2016 16:37:12.509 UTC Verbose common_sip_processing.cpp:136: TX 1504 bytes Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) to TCP 10.112.87.250:5058:

--start msg--



INVITE sip:1234 at 10.112.87.177 SIP/2.0

Via: SIP/2.0/TCP 10.112.87.250:57743;rport;branch=z9hG4bKPjeUUjYygIls2DRu2LAAHqVGU27AHZ.-S3

Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh

Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>

Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be

Max-Forwards: 66

Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>

To: <sip:1234 at example.com>

From: <sip:6505550962 at example.com>;tag=9b0c5d3e

Call-ID: OV0U_26XyoJOUXCQ0B_6FA..

CSeq: 1 INVITE

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE

Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri

User-Agent: Z 3.9.32144 r32121

Allow-Events: presence, kpml

P-Asserted-Identity: <sip:6505550962 at example.com>

Session-Expires: 600

P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg

Route: <sip:cw-aio:5058;transport=UDP;lr;orig>

Content-Type: application/sdp

Content-Length:   241



v=0

o=Z 0 0 IN IP4 10.112.123.57

s=Z

c=IN IP4 10.112.123.57

t=0 0

m=audio 8000 RTP/AVP 3 110 8 0 97 101

a=rtpmap:110 speex/8000

a=rtpmap:97 iLBC/8000

a=fmtp:97 mode=30

a=rtpmap:101 telephone-event/8000

a=fmtp:101 0-16

a=sendrecv



--end msg--

20-09-2016 16:37:12.509 UTC Debug pjsip: tsx0x7f744c154 State changed from Null to Calling, event=TX_MSG

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:213: tsx0x7f744c1546a8 - tu_on_tsx_state UAC, TSX_STATE TX_MSG state=Calling

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:1813: tsx0x7f744c1546a8 - uac_tsx = 0x7f744c154540, uas_tsx = 0x7f744c001b20

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:1821: TX_MSG event on current UAC transaction

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:2134: Starting timer C

20-09-2016 16:37:12.509 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f744409acd8

20-09-2016 16:37:12.509 UTC Debug thread_dispatcher.cpp:199: Request latency = 8327us

20-09-2016 16:37:12.519 UTC Verbose pjsip: tcpc0x7f744c15 TCP transport 10.112.87.250:57743 is connected to 10.112.87.250:5058

20-09-2016 16:37:12.524 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Response msg 100/INVITE/cseq=1 (rdata0x7f744c157410)

20-09-2016 16:37:12.524 UTC Verbose common_sip_processing.cpp:120: RX 864 bytes Response msg 100/INVITE/cseq=1 (rdata0x7f744c157410) from TCP 10.112.87.250:5058:

--start msg--






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160921/b9498df0/attachment.html>

From graeme at projectclearwater.org  Wed Sep 21 07:26:00 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Wed, 21 Sep 2016 11:26:00 +0000
Subject: [Project Clearwater] Clearwater Metrics
In-Reply-To: <4711_1474376347_57E1329B_4711_4263_1_22D8017AEC6D3B4793872DB3413DC7BF01B99B37@OPEXCNORM52.corporate.adroot.infra.ftgroup>
References: <4711_1474376347_57E1329B_4711_4263_1_22D8017AEC6D3B4793872DB3413DC7BF01B99B37@OPEXCNORM52.corporate.adroot.infra.ftgroup>
Message-ID: <CY4PR02MB261655B535257DF5A72A9EADE3F60@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Jaafar,

I'm glad you like Project Clearwater! Yes, we provide a range of statistics which are exposed over SNMP and which are all documented here<http://clearwater.readthedocs.io/en/stable/Clearwater_SNMP_Statistics.html>. Let me know if you've got any questions about them.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of jaafar.bendriss at orange.com
Sent: 20 September 2016 13:59
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Clearwater Metrics

Hello , I would like to thank you for your vIMS product.

I would like to know if you had already defined some set of metrics that are specific to your VNFs beyond the standard system and network metrics (such as sip client etc ...)

Thank you

[Logo Orange]<http://www.orange.com/>

Jaafar Bendriss
Ph.D. Candidate on autonomic and cognitive management
ORANGE/IMT/OLN/CNC/R3S/SPO
Orange Gardens
1D-2-12-C
Fixe : +33 1 57 39 67 28 <https://monsi.sso.francetelecom.fr/index.asp?target=http%3A%2F%2Fclicvoice.sso.francetelecom.fr%2FClicvoiceV2%2FToolBar.do%3Faction%3Ddefault%26rootservice%3DSIGNATURE%26to%3D+33%201%2057%2039%2067%2028>
jaafar.bendriss at orange.com<mailto:jaafar.bendriss at orange.com>


_________________________________________________________________________________________________________________________



Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc

pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler

a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,

Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.



This message and its attachments may contain confidential or privileged information that may be protected by law;

they should not be distributed, used or copied without authorisation.

If you have received this email in error, please notify the sender and delete this message and its attachments.

As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.

Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160921/0e0509fd/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.gif
Type: image/gif
Size: 1450 bytes
Desc: image001.gif
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160921/0e0509fd/attachment.gif>

From graeme at projectclearwater.org  Wed Sep 21 08:39:36 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Wed, 21 Sep 2016 12:39:36 +0000
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster
	using	docker-compose
References: <CAN20VnjYp3aGAnrkMUn-WYEck6+uTi+HwB6PbbZUC9wuPB1W1w@mail.gmail.com>
Message-ID: <CY4PR02MB261618E6A2A54183C25FAC46E3F60@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Sarbajit,

I don?t think we?ve never tried deploying Project Clearwater in a Docker Swarm cluster, but I don?t see any reason why it couldn?t work. The tests are failing very early ? they?re not able to connect to Ellis on port 80. I can think of a couple of reasons for this ? either Ellis isn?t running or the Ellis port mapping hasn?t worked for some reason.

Can you connect to the Ellis container and run ps ?eaf | grep ellis and ps ?eaf | grep nginx to confirm that NGINX and Ellis are running? Can you also run sudo netstat -planut | grep nginx or something equivalent to check that NGINX is listening on port 80? If there?s a problem with either NGINX or Ellis we probably need to look in the logs at /var/log/nginx/ or /var/log/ellis/ on the Ellis container.

If however that all looks fine, then it sounds like the port mapping has failed for some reason. Can you run nc -z <ip> 80 from the box you?re running the live tests on? This will scan for anything listening at <ip>:80 and will return successfully if it finds anything.

Thanks,
Graeme

________________________________
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sarbajit Chatterjee
Sent: 20 September 2016 15:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Hello,

I am following the instructions from https://github.com/Metaswitch/clearwater-docker. I can successfully deploy it on a single Docker node but, the compose file does not work with Swarm cluster.

I did try to modify the compose file like this -


version: '2'
services:
  etcd:
    image: quay.io/coreos/etcd:v2.2.5<http://quay.io/coreos/etcd:v2.2.5>
    command: >
      -name etcd0
      -advertise-client-urls http://etcd:2379,http://etcd:4001
      -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001
      -initial-advertise-peer-urls http://etcd:2380
      -listen-peer-urls http://0.0.0.0:2380
      -initial-cluster etcd0=http://etcd:2380
      -initial-cluster-state new
  bono:
    image: swarm-node:5000/clearwaterdocker_bono
    ports:
      - 22
      - "3478:3478"
      - "3478:3478/udp"
      - "5060:5060"
      - "5060:5060/udp"
      - "5062:5062"
  sprout:
    image: swarm-node:5000/clearwaterdocker_sprout
    networks:
      default:
        aliases:
          - scscf.sprout
          - icscf.sprout
    ports:
      - 22
  homestead:
    image: swarm-node:5000/clearwaterdocker_homestead
    ports:
      - 22
  homer:
    image: swarm-node:5000/clearwaterdocker_homer
    ports:
      - 22
  ralf:
    image: swarm-node:5000/clearwaterdocker_ralf
    ports:
      - 22
  ellis:
    image: swarm-node:5000/clearwaterdocker_ellis
    ports:
      - 22
      - "80:80"


where swarm-node:5000 is the local docker registry and it hosts the pre-built images of Clearwater containers. Even though the deployment succeeded, clearwater-livetests are failing with following error -


Basic Registration (TCP) - Failed
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/local/rvm/gems/ruby-1.9.3-p551/gems/quaff-0.7.3/lib/sources.rb:41:in `initialize'


Any suggestions on how I can deploy Clearwater on a Swarm cluster?

Thanks,
Sarbajit

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160921/cb26f861/attachment.html>

From sarbajitc at gmail.com  Wed Sep 21 11:41:22 2016
From: sarbajitc at gmail.com (Sarbajit Chatterjee)
Date: Wed, 21 Sep 2016 21:11:22 +0530
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster using
	docker-compose
In-Reply-To: <CY4PR02MB261618E6A2A54183C25FAC46E3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
References: <CAN20VnjYp3aGAnrkMUn-WYEck6+uTi+HwB6PbbZUC9wuPB1W1w@mail.gmail.com>
	<CY4PR02MB261618E6A2A54183C25FAC46E3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
Message-ID: <CAN20VnhU6-_S71FK_F0QateTrNxG2j_WN2BPEyx3PdhxOhGhmQ@mail.gmail.com>

Thanks Graeme for your reply. Here are the command outputs that you had
asked -

root at e994b17b4563:/# ps -eaf | grep ellis
root       177     1  0 Sep19 ?        00:00:10
/usr/share/clearwater/clearwater-cluster-manager/env/bin/python
/usr/share/clearwater/bin/clearwater-cluster-manager
--mgmt-local-ip=10.0.1.7 --sig-local-ip=10.0.1.7 --local-site=site1
--remote-site= --remote-cassandra-seeds= --signaling-namespace=
--uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater
--etcd-cluster-key=ellis --log-level=3
--log-directory=/var/log/clearwater-cluster-manager
--pidfile=/var/run/clearwater-cluster-manager.pid
root       180     1  0 Sep19 ?        00:00:00 /bin/sh /etc/init.d/ellis
run
ellis      185   180  0 Sep19 ?        00:00:05
/usr/share/clearwater/ellis/env/bin/python -m metaswitch.ellis.main
root       287   253  0 15:18 ?        00:00:00 grep --color=auto ellis
root at e994b17b4563:/#
root at e994b17b4563:/# ps -eaf | grep nginx
root       179     1  0 Sep19 ?        00:00:00 nginx: master process
/usr/sbin/nginx -g daemon off;
www-data   186   179  0 Sep19 ?        00:00:16 nginx: worker process
www-data   187   179  0 Sep19 ?        00:00:00 nginx: worker process
www-data   188   179  0 Sep19 ?        00:00:16 nginx: worker process
www-data   189   179  0 Sep19 ?        00:00:16 nginx: worker process
root       289   253  0 15:19 ?        00:00:00 grep --color=auto nginx
root at e994b17b4563:/#
root at e994b17b4563:/# netstat -planut | grep nginx
tcp6       0      0 :::80                   :::*                    LISTEN
     179/nginx -g daemon
root at e994b17b4563:/#


I think both ellis and nginx are running fine inside the container. I can
also open the ellis login page from a web browser. I also checked the MySQL
DB in ellis container. I can see livetest user entry in users table and
1000 rows in numbers table.

I can also connect to ellis (host IP 10.109.190.10) from my livetest
container -

root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.10 80
Connection to 10.109.190.10 80 port [tcp/http] succeeded!
root at 40efba73deb5:~/clearwater-live-test#


Is this happening because Clearwater containers are spread across multiple
hosts? What other areas I should check?


Thanks,
Sarbajit


On Wed, Sep 21, 2016 at 6:09 PM, Graeme Robertson (projectclearwater.org) <
graeme at projectclearwater.org> wrote:

> Hi Sarbajit,
>
>
>
> I don?t think we?ve never tried deploying Project Clearwater in a Docker
> Swarm cluster, but I don?t see any reason why it couldn?t work. The tests
> are failing very early ? they?re not able to connect to Ellis on port 80. I
> can think of a couple of reasons for this ? either Ellis isn?t running or
> the Ellis port mapping hasn?t worked for some reason.
>
>
>
> Can you connect to the Ellis container and run ps ?eaf | grep ellis and ps
> ?eaf | grep nginx to confirm that NGINX and Ellis are running? Can you
> also run sudo netstat -planut | grep nginx or something equivalent to
> check that NGINX is listening on port 80? If there?s a problem with either
> NGINX or Ellis we probably need to look in the logs at /var/log/nginx/ or
> /var/log/ellis/ on the Ellis container.
>
>
>
> If however that all looks fine, then it sounds like the port mapping has
> failed for some reason. Can you run nc -z <ip> 80 from the box you?re
> running the live tests on? This will scan for anything listening at
> <ip>:80 and will return successfully if it finds anything.
>
>
>
> Thanks,
>
> Graeme
>
>
> ------------------------------
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org
> <clearwater-bounces at lists.projectclearwater.org>] *On Behalf Of *Sarbajit
> Chatterjee
> *Sent:* 20 September 2016 15:05
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Hello,
>
>
>
> I am following the instructions from https://github.com/
> Metaswitch/clearwater-docker. I can successfully deploy it on a single
> Docker node but, the compose file does not work with Swarm cluster.
>
>
>
> I did try to modify the compose file like this -
>
>
>
>
>
> version: '2'
>
> services:
>
>   etcd:
>
>     image: quay.io/coreos/etcd:v2.2.5
>
>     command: >
>
>       -name etcd0
>
>       -advertise-client-urls http://etcd:2379,http://etcd:4001
>
>       -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001
>
>       -initial-advertise-peer-urls http://etcd:2380
>
>       -listen-peer-urls http://0.0.0.0:2380
>
>       -initial-cluster etcd0=http://etcd:2380
>
>       -initial-cluster-state new
>
>   bono:
>
>     image: swarm-node:5000/clearwaterdocker_bono
>
>     ports:
>
>       - 22
>
>       - "3478:3478"
>
>       - "3478:3478/udp"
>
>       - "5060:5060"
>
>       - "5060:5060/udp"
>
>       - "5062:5062"
>
>   sprout:
>
>     image: swarm-node:5000/clearwaterdocker_sprout
>
>     networks:
>
>       default:
>
>         aliases:
>
>           - scscf.sprout
>
>           - icscf.sprout
>
>     ports:
>
>       - 22
>
>   homestead:
>
>     image: swarm-node:5000/clearwaterdocker_homestead
>
>     ports:
>
>       - 22
>
>   homer:
>
>     image: swarm-node:5000/clearwaterdocker_homer
>
>     ports:
>
>       - 22
>
>   ralf:
>
>     image: swarm-node:5000/clearwaterdocker_ralf
>
>     ports:
>
>       - 22
>
>   ellis:
>
>     image: swarm-node:5000/clearwaterdocker_ellis
>
>     ports:
>
>       - 22
>
>       - "80:80"
>
>
>
>
>
> where swarm-node:5000 is the local docker registry and it hosts the
> pre-built images of Clearwater containers. Even though the deployment
> succeeded, clearwater-livetests are failing with following error -
>
>
>
>
>
> Basic Registration (TCP) - Failed
>
>   Errno::ECONNREFUSED thrown:
>
>    - Connection refused - connect(2)
>
>      - /usr/local/rvm/gems/ruby-1.9.3-p551/gems/quaff-0.7.3/lib/sources.rb:41:in
> `initialize'
>
>
>
>
>
> Any suggestions on how I can deploy Clearwater on a Swarm cluster?
>
>
>
> Thanks,
>
> Sarbajit
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160921/181101ee/attachment.html>

From graeme at projectclearwater.org  Wed Sep 21 12:31:46 2016
From: graeme at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Wed, 21 Sep 2016 16:31:46 +0000
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster
	using	docker-compose
In-Reply-To: <CAN20VnhU6-_S71FK_F0QateTrNxG2j_WN2BPEyx3PdhxOhGhmQ@mail.gmail.com>
References: <CAN20VnjYp3aGAnrkMUn-WYEck6+uTi+HwB6PbbZUC9wuPB1W1w@mail.gmail.com>
	<CY4PR02MB261618E6A2A54183C25FAC46E3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VnhU6-_S71FK_F0QateTrNxG2j_WN2BPEyx3PdhxOhGhmQ@mail.gmail.com>
Message-ID: <CY4PR02MB2616E42F5584556E6172483FE3F60@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Sarbajit,

I?ve had another look at this, and actually I think clearwater-live-test checks it can connect to Bono before it tries to provision numbers from Ellis, and it?s actually that connection that?s failing ? apologies!

Can you do similar checks for the Bono container, i.e. connect to the Bono container and run ps -eaf | grep bono and run nc -z -v <ip> 5060 from your live test container (where <ip> is the IP of your Bono)?

One other thought ? what command are you using to run the tests? You?ll need to set the PROXY option to your Bono IP and the ELLIS option to your Ellis IP.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sarbajit Chatterjee
Sent: 21 September 2016 16:41
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Thanks Graeme for your reply. Here are the command outputs that you had asked -

root at e994b17b4563:/# ps -eaf | grep ellis
root       177     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager --mgmt-local-ip=10.0.1.7 --sig-local-ip=10.0.1.7 --local-site=site1 --remote-site= --remote-cassandra-seeds= --signaling-namespace= --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater --etcd-cluster-key=ellis --log-level=3 --log-directory=/var/log/clearwater-cluster-manager --pidfile=/var/run/clearwater-cluster-manager.pid
root       180     1  0 Sep19 ?        00:00:00 /bin/sh /etc/init.d/ellis run
ellis      185   180  0 Sep19 ?        00:00:05 /usr/share/clearwater/ellis/env/bin/python -m metaswitch.ellis.main
root       287   253  0 15:18 ?        00:00:00 grep --color=auto ellis
root at e994b17b4563:/#
root at e994b17b4563:/# ps -eaf | grep nginx
root       179     1  0 Sep19 ?        00:00:00 nginx: master process /usr/sbin/nginx -g daemon off;
www-data   186   179  0 Sep19 ?        00:00:16 nginx: worker process
www-data   187   179  0 Sep19 ?        00:00:00 nginx: worker process
www-data   188   179  0 Sep19 ?        00:00:16 nginx: worker process
www-data   189   179  0 Sep19 ?        00:00:16 nginx: worker process
root       289   253  0 15:19 ?        00:00:00 grep --color=auto nginx
root at e994b17b4563:/#
root at e994b17b4563:/# netstat -planut | grep nginx
tcp6       0      0 :::80                   :::*                    LISTEN      179/nginx -g daemon
root at e994b17b4563:/#


I think both ellis and nginx are running fine inside the container. I can also open the ellis login page from a web browser. I also checked the MySQL DB in ellis container. I can see livetest user entry in users table and 1000 rows in numbers table.

I can also connect to ellis (host IP 10.109.190.10) from my livetest container -

root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.10 80
Connection to 10.109.190.10 80 port [tcp/http] succeeded!
root at 40efba73deb5:~/clearwater-live-test#


Is this happening because Clearwater containers are spread across multiple hosts? What other areas I should check?


Thanks,
Sarbajit


On Wed, Sep 21, 2016 at 6:09 PM, Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>> wrote:
Hi Sarbajit,

I don?t think we?ve never tried deploying Project Clearwater in a Docker Swarm cluster, but I don?t see any reason why it couldn?t work. The tests are failing very early ? they?re not able to connect to Ellis on port 80. I can think of a couple of reasons for this ? either Ellis isn?t running or the Ellis port mapping hasn?t worked for some reason.

Can you connect to the Ellis container and run ps ?eaf | grep ellis and ps ?eaf | grep nginx to confirm that NGINX and Ellis are running? Can you also run sudo netstat -planut | grep nginx or something equivalent to check that NGINX is listening on port 80? If there?s a problem with either NGINX or Ellis we probably need to look in the logs at /var/log/nginx/ or /var/log/ellis/ on the Ellis container.

If however that all looks fine, then it sounds like the port mapping has failed for some reason. Can you run nc -z <ip> 80 from the box you?re running the live tests on? This will scan for anything listening at <ip>:80 and will return successfully if it finds anything.

Thanks,
Graeme

________________________________
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sarbajit Chatterjee
Sent: 20 September 2016 15:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Hello,

I am following the instructions from https://github.com/Metaswitch/clearwater-docker. I can successfully deploy it on a single Docker node but, the compose file does not work with Swarm cluster.

I did try to modify the compose file like this -


version: '2'
services:
  etcd:
    image: quay.io/coreos/etcd:v2.2.5<http://quay.io/coreos/etcd:v2.2.5>
    command: >
      -name etcd0
      -advertise-client-urls http://etcd:2379,http://etcd:4001
      -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001
      -initial-advertise-peer-urls http://etcd:2380
      -listen-peer-urls http://0.0.0.0:2380
      -initial-cluster etcd0=http://etcd:2380
      -initial-cluster-state new
  bono:
    image: swarm-node:5000/clearwaterdocker_bono
    ports:
      - 22
      - "3478:3478"
      - "3478:3478/udp"
      - "5060:5060"
      - "5060:5060/udp"
      - "5062:5062"
  sprout:
    image: swarm-node:5000/clearwaterdocker_sprout
    networks:
      default:
        aliases:
          - scscf.sprout
          - icscf.sprout
    ports:
      - 22
  homestead:
    image: swarm-node:5000/clearwaterdocker_homestead
    ports:
      - 22
  homer:
    image: swarm-node:5000/clearwaterdocker_homer
    ports:
      - 22
  ralf:
    image: swarm-node:5000/clearwaterdocker_ralf
    ports:
      - 22
  ellis:
    image: swarm-node:5000/clearwaterdocker_ellis
    ports:
      - 22
      - "80:80"


where swarm-node:5000 is the local docker registry and it hosts the pre-built images of Clearwater containers. Even though the deployment succeeded, clearwater-livetests are failing with following error -


Basic Registration (TCP) - Failed
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/local/rvm/gems/ruby-1.9.3-p551/gems/quaff-0.7.3/lib/sources.rb:41:in `initialize'


Any suggestions on how I can deploy Clearwater on a Swarm cluster?

Thanks,
Sarbajit


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160921/2a6c8a37/attachment.html>

From sarbajitc at gmail.com  Wed Sep 21 14:39:20 2016
From: sarbajitc at gmail.com (Sarbajit Chatterjee)
Date: Thu, 22 Sep 2016 00:09:20 +0530
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster using
	docker-compose
In-Reply-To: <CY4PR02MB2616E42F5584556E6172483FE3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
References: <CAN20VnjYp3aGAnrkMUn-WYEck6+uTi+HwB6PbbZUC9wuPB1W1w@mail.gmail.com>
	<CY4PR02MB261618E6A2A54183C25FAC46E3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VnhU6-_S71FK_F0QateTrNxG2j_WN2BPEyx3PdhxOhGhmQ@mail.gmail.com>
	<CY4PR02MB2616E42F5584556E6172483FE3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
Message-ID: <CAN20VngUB9yN6Td29LcV0p0Ths2eFON_fx+Go94uaevvykbRDA@mail.gmail.com>

Hi Graeme,

I'm using following command to run the livetest -

rake test[example.com] TESTS="Basic*" SIGNUP_CODE=secret PROXY=10.109.190.9
ELLIS=10.109.190.10

here PROXY ip is where the bono container is launched and ELLIS ip is where
the ellis container is launched.

The bono service seems to be running in the container -

root at 9837c4dab241:/# ps -eaf | grep bono
root       122     1  0 Sep19 ?        00:00:10
/usr/share/clearwater/clearwater-cluster-manager/env/bin/python
/usr/share/clearwater/bin/clearwater-cluster-manager
--mgmt-local-ip=10.0.1.2 --sig-local-ip=10.0.1.2 --local-site=site1
--remote-site= --remote-cassandra-seeds= --signaling-namespace=
--uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater
--etcd-cluster-key=bono --log-level=3
--log-directory=/var/log/clearwater-cluster-manager
--pidfile=/var/run/clearwater-cluster-manager.pid
root       124     1  0 Sep19 ?        00:00:00 /bin/bash /etc/init.d/bono
run
root       139   124  0 Sep19 ?        00:00:00 /bin/bash
/usr/share/clearwater/bin/run-in-signaling-namespace start-stop-daemon
--start --quiet --exec /usr/share/clearwater/bin/bono --chuid bono --chdir
/etc/clearwater -- --domain=example.com --localhost=10.0.1.2,10.0.1.2
--alias=10.0.1.2 --pcscf=5060,5058 --webrtc-port=5062
--routing-proxy=scscf.sprout,5052,50,600 --ralf=ralf:10888 --sas=0.0.0.0,
bono at 10.0.1.2 --dns-server=127.0.0.11 --worker-threads=4
--analytics=/var/log/bono --log-file=/var/log/bono --log-level=2
bono       140   139  0 Sep19 ?        00:11:15
/usr/share/clearwater/bin/bono --domain=example.com
--localhost=10.0.1.2,10.0.1.2 --alias=10.0.1.2 --pcscf=5060,5058
--webrtc-port=5062 --routing-proxy=scscf.sprout,5052,50,600
--ralf=ralf:10888 --sas=0.0.0.0,bono at 10.0.1.2 --dns-server=127.0.0.11
--worker-threads=4 --analytics=/var/log/bono --log-file=/var/log/bono
--log-level=2
root       322   293  0 17:48 ?        00:00:00 grep --color=auto bono
root at 9837c4dab241:/#
root at 9837c4dab241:/# netstat -planut | grep 5060
tcp        0      0 10.0.1.2:5060           0.0.0.0:*               LISTEN
     -
udp        0      0 10.0.1.2:5060           0.0.0.0:*
    -
root at 9837c4dab241:/#


But the connection to bono is failing from livetest container as you had
predicted.

root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.9 5060
nc: connect to 10.109.190.9 port 5060 (tcp) failed: Connection refused
root at 40efba73deb5:~/clearwater-live-test#


On checking the bono log, I see a series of errors like below
in beginning of the log -

19-09-2016 15:41:44.612 UTC Status utils.cpp:591: Log level set to 2
19-09-2016 15:41:44.612 UTC Status main.cpp:1388: Access logging enabled to
/var/log/bono
19-09-2016 15:41:44.613 UTC Warning main.cpp:1435: SAS server option was
invalid or not configured - SAS is disabled
19-09-2016 15:41:44.613 UTC Warning main.cpp:1511: A registration expiry
period should not be specified for P-CSCF
19-09-2016 15:41:44.613 UTC Status snmp_agent.cpp:117: AgentX agent
initialised
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:105: Constructing
LoadMonitor
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:106:    Target latency
(usecs)   : 100000
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:107:    Max bucket size
         : 1000
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:108:    Initial token
fill rate/s: 100.000000
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:109:    Min token fill
rate/s    : 10.000000
19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:144: Creating
Cached Resolver using servers:
19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:154:     127.0.0.11
19-09-2016 15:41:44.613 UTC Status sipresolver.cpp:60: Created SIP resolver
19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5058
19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5060
19-09-2016 15:41:44.638 UTC Status stack.cpp:855: Local host aliases:
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  172.18.0.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:
19-09-2016 15:41:44.639 UTC Status httpresolver.cpp:52: Created HTTP
resolver
19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:114: Configuring HTTP
Connection
19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:115:   Connection
created for server ralf:10888
19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:116:   Connection
will use a response timeout of 500ms
19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:72: Creating
connection pool to scscf.sprout:5052
19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:73:   connections =
50, recycle time = 600 +/- 120 seconds
19-09-2016 15:41:44.649 UTC Status bono.cpp:3314: Create list of PBXes
19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:63: Loading plug-ins
from /usr/share/clearwater/sprout/plugins
19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:158: Finished loading
plug-ins
19-09-2016 15:41:44.652 UTC Warning (Net-SNMP): Warning: Failed to connect
to the agentx master agent ([NIL]):
19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f3df8 TCP connect()
error: Connection refused [code=120111]
19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f5c38 TCP connect()
error: Connection refused [code=120111]


I have observed that in the bono container 5060 port is not listening in
all interfaces while 5062 port is listening in all interfaces.

root at 9837c4dab241:/var/log/bono# netstat -planut | grep LISTEN
tcp        0      0 10.0.1.2:4000           0.0.0.0:*               LISTEN
     -
tcp        0      0 10.0.1.2:5058           0.0.0.0:*               LISTEN
     -
tcp        0      0 127.0.0.11:43395        0.0.0.0:*               LISTEN
     -
tcp        0      0 10.0.1.2:5060           0.0.0.0:*               LISTEN
     -
tcp        0      0 0.0.0.0:5062            0.0.0.0:*               LISTEN
     -
tcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN
     -
tcp        0      0 10.0.1.2:3478           0.0.0.0:*               LISTEN
     -
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
     9/sshd
tcp6       0      0 :::22                   :::*                    LISTEN
     9/sshd
root at 9837c4dab241:/var/log/bono#


Is it right to have 5060 port listen on only local port? Please help me to
debug the issue.

Thanks,
Sarbajit


On Wed, Sep 21, 2016 at 10:01 PM, Graeme Robertson (projectclearwater.org) <
graeme at projectclearwater.org> wrote:

> Hi Sarbajit,
>
>
>
> I?ve had another look at this, and actually I think clearwater-live-test
> checks it can connect to Bono before it tries to provision numbers from
> Ellis, and it?s actually that connection that?s failing ? apologies!
>
>
>
> Can you do similar checks for the Bono container, i.e. connect to the Bono
> container and run ps -eaf | grep bono and run nc -z -v <ip> 5060 from
> your live test container (where <ip> is the IP of your Bono)?
>
>
>
> One other thought ? what command are you using to run the tests? You?ll
> need to set the PROXY option to your Bono IP and the ELLIS option to your
> Ellis IP.
>
>
>
> Thanks,
>
> Graeme
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sarbajit Chatterjee
> *Sent:* 21 September 2016 16:41
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Thanks Graeme for your reply. Here are the command outputs that you had
> asked -
>
>
>
> root at e994b17b4563:/# ps -eaf | grep ellis
>
> root       177     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/
> clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager
> --mgmt-local-ip=10.0.1.7 --sig-local-ip=10.0.1.7 --local-site=site1
> --remote-site= --remote-cassandra-seeds= --signaling-namespace=
> --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater
> --etcd-cluster-key=ellis --log-level=3 --log-directory=/var/log/clearwater-cluster-manager
> --pidfile=/var/run/clearwater-cluster-manager.pid
>
> root       180     1  0 Sep19 ?        00:00:00 /bin/sh /etc/init.d/ellis
> run
>
> ellis      185   180  0 Sep19 ?        00:00:05
> /usr/share/clearwater/ellis/env/bin/python -m metaswitch.ellis.main
>
> root       287   253  0 15:18 ?        00:00:00 grep --color=auto ellis
>
> root at e994b17b4563:/#
>
> root at e994b17b4563:/# ps -eaf | grep nginx
>
> root       179     1  0 Sep19 ?        00:00:00 nginx: master process
> /usr/sbin/nginx -g daemon off;
>
> www-data   186   179  0 Sep19 ?        00:00:16 nginx: worker process
>
> www-data   187   179  0 Sep19 ?        00:00:00 nginx: worker process
>
> www-data   188   179  0 Sep19 ?        00:00:16 nginx: worker process
>
> www-data   189   179  0 Sep19 ?        00:00:16 nginx: worker process
>
> root       289   253  0 15:19 ?        00:00:00 grep --color=auto nginx
>
> root at e994b17b4563:/#
>
> root at e994b17b4563:/# netstat -planut | grep nginx
>
> tcp6       0      0 :::80                   :::*                    LISTEN
>      179/nginx -g daemon
>
> root at e994b17b4563:/#
>
>
>
>
>
> I think both ellis and nginx are running fine inside the container. I can
> also open the ellis login page from a web browser. I also checked the MySQL
> DB in ellis container. I can see livetest user entry in users table and
> 1000 rows in numbers table.
>
>
>
> I can also connect to ellis (host IP 10.109.190.10) from my livetest
> container -
>
>
>
> root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.10 80
>
> Connection to 10.109.190.10 80 port [tcp/http] succeeded!
>
> root at 40efba73deb5:~/clearwater-live-test#
>
>
>
>
>
> Is this happening because Clearwater containers are spread across multiple
> hosts? What other areas I should check?
>
>
>
>
>
> Thanks,
>
> Sarbajit
>
>
>
>
>
> On Wed, Sep 21, 2016 at 6:09 PM, Graeme Robertson (projectclearwater.org)
> <graeme at projectclearwater.org> wrote:
>
> Hi Sarbajit,
>
>
>
> I don?t think we?ve never tried deploying Project Clearwater in a Docker
> Swarm cluster, but I don?t see any reason why it couldn?t work. The tests
> are failing very early ? they?re not able to connect to Ellis on port 80. I
> can think of a couple of reasons for this ? either Ellis isn?t running or
> the Ellis port mapping hasn?t worked for some reason.
>
>
>
> Can you connect to the Ellis container and run ps ?eaf | grep ellis and ps
> ?eaf | grep nginx to confirm that NGINX and Ellis are running? Can you
> also run sudo netstat -planut | grep nginx or something equivalent to
> check that NGINX is listening on port 80? If there?s a problem with either
> NGINX or Ellis we probably need to look in the logs at /var/log/nginx/ or
> /var/log/ellis/ on the Ellis container.
>
>
>
> If however that all looks fine, then it sounds like the port mapping has
> failed for some reason. Can you run nc -z <ip> 80 from the box you?re
> running the live tests on? This will scan for anything listening at
> <ip>:80 and will return successfully if it finds anything.
>
>
>
> Thanks,
>
> Graeme
>
>
> ------------------------------
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org
> <clearwater-bounces at lists.projectclearwater.org>] *On Behalf Of *Sarbajit
> Chatterjee
> *Sent:* 20 September 2016 15:05
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Hello,
>
>
>
> I am following the instructions from https://github.com/
> Metaswitch/clearwater-docker. I can successfully deploy it on a single
> Docker node but, the compose file does not work with Swarm cluster.
>
>
>
> I did try to modify the compose file like this -
>
>
>
>
>
> version: '2'
>
> services:
>
>   etcd:
>
>     image: quay.io/coreos/etcd:v2.2.5
>
>     command: >
>
>       -name etcd0
>
>       -advertise-client-urls http://etcd:2379,http://etcd:4001
>
>       -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001
>
>       -initial-advertise-peer-urls http://etcd:2380
>
>       -listen-peer-urls http://0.0.0.0:2380
>
>       -initial-cluster etcd0=http://etcd:2380
>
>       -initial-cluster-state new
>
>   bono:
>
>     image: swarm-node:5000/clearwaterdocker_bono
>
>     ports:
>
>       - 22
>
>       - "3478:3478"
>
>       - "3478:3478/udp"
>
>       - "5060:5060"
>
>       - "5060:5060/udp"
>
>       - "5062:5062"
>
>   sprout:
>
>     image: swarm-node:5000/clearwaterdocker_sprout
>
>     networks:
>
>       default:
>
>         aliases:
>
>           - scscf.sprout
>
>           - icscf.sprout
>
>     ports:
>
>       - 22
>
>   homestead:
>
>     image: swarm-node:5000/clearwaterdocker_homestead
>
>     ports:
>
>       - 22
>
>   homer:
>
>     image: swarm-node:5000/clearwaterdocker_homer
>
>     ports:
>
>       - 22
>
>   ralf:
>
>     image: swarm-node:5000/clearwaterdocker_ralf
>
>     ports:
>
>       - 22
>
>   ellis:
>
>     image: swarm-node:5000/clearwaterdocker_ellis
>
>     ports:
>
>       - 22
>
>       - "80:80"
>
>
>
>
>
> where swarm-node:5000 is the local docker registry and it hosts the
> pre-built images of Clearwater containers. Even though the deployment
> succeeded, clearwater-livetests are failing with following error -
>
>
>
>
>
> Basic Registration (TCP) - Failed
>
>   Errno::ECONNREFUSED thrown:
>
>    - Connection refused - connect(2)
>
>      - /usr/local/rvm/gems/ruby-1.9.3-p551/gems/quaff-0.7.3/lib/sources.rb:41:in
> `initialize'
>
>
>
>
>
> Any suggestions on how I can deploy Clearwater on a Swarm cluster?
>
>
>
> Thanks,
>
> Sarbajit
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160922/5f240265/attachment.html>

From surender.s at hcl.com  Thu Sep 22 01:48:58 2016
From: surender.s at hcl.com (Surender Singh)
Date: Thu, 22 Sep 2016 05:48:58 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E1B181@NDA-HCLT-MBS05.hclt.corp.hcl.in>

Hi Graeme,

Thanks for support..

But have seen one old  thread subject 'SIP Trunking  ' by Schahzad .His solution working fine without any changes at PBX end.

Please give comments on call scenarios for out calls to PBX

SIP Client ----> Sprout ---->ENUM Query(Find new request URI based on B number prefix) ------>BGCF(match the domain name with new request URI which is re-write by ENUM and find out the route value )

In my case route value is    ["<sip:cw-aio:5058;transport=UDP;lr;orig>"] and Domain is 10.112.87.177 .

But here I have confusion regarding route value '' sip:cw-aio:5058;transport=UDP;lr;orig '  what is mean of this.

Please give clarity  on route part ..


Regards
Surender Singh
8826292018

	

-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 21 September 2016 16:55
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 41, Issue 40

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. Re: SIP Trunking With PBX_aio
      (Graeme Robertson (projectclearwater.org))


----------------------------------------------------------------------

Message: 1
Date: Wed, 21 Sep 2016 11:24:02 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID:
	<CY4PR02MB2616A2D97C1BC38B1E842CC5E3F60 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="us-ascii"

Hi Surender,



Project Clearwater is IMS compliant, and so I'm afraid it doesn't alter either the To header or the From header, and there is no trivial way to configure Project Clearwater to do this manipulation, or any plans to add this ability to Project Clearwater. It sounds as though your PBX is non-IMS compliant. As I mentioned before, if you really need to re-write the To header on your request, and you are not averse to writing a bit of code, Sprout provides a really easy-to-use API for developing your own app server. https://github.com/Metaswitch/greeter is a sample application server that uses this API and simply adds the header "Subject: Hello world!" to a SIP message - you could write something similar that changes to the To header on your SIP messages. Alternatively, Metaswitch has a commercial product called Perimeta<http://www.metaswitch.com/perimeta-session-border-controller-sbc> which can be deployed as an IBCF, and which has a SIP Message Manipulation Framework which you would be able to use to alter your To header.



>From the logs below, it looks as though the BGCF is now applying your route and the INVITE will presumably be going out through Bono, but unfortunately I don't think that will solve your problem.



Thanks,

Graeme



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 20 September 2016 13:45
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP Trunking With PBX_aio



Hi Greame,



Thanks for support .



But my issue is ,when I calling from A to B (Out call) its showing example.com in To_header and From_header . and same value going at my PBX ,due this might be  pbx unable to identify the value like 1234 at example.com<mailto:1234 at example.com>  instead of 1234 at 10.112.87.177<mailto:1234 at 10.112.87.177> . Can have any possibility to change it at IMS end.





I also change the configuration in Bgcf.json but I unable to check whether calls are forwarding through BGCF route or not .I found configuration like below... in one thread.





{

    "number_blocks" : [

        {

           "name" : "Internal numbers",

           "prefix" : "650555",

           "regex" : "!(^.*$)!sip:\\1 at example.com!"

        },

        {

            "name" : "External numbers",

            "prefix" : "",

            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"

        }





    ]

}







{

    "routes" : [

        {   "name" : "TO_PBX",

          "domain" : "10.112.87.177",

          "route" : ["<sip:cw-aio:5058;transport=UDP;lr;orig>"]

        }



    ]



}









20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:691: Cloned tdta0x7f744c14aea0 to tdta0x7f744c14e530

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:odi_fWh8yrqIzM at 10.112.87.250:5054;lr;orig;service=scscf>

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c14eb40 => txdata 0x7f744c14e5d8 mapping

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1587: scscf-0x7f744c14df40 pass initial request Request msg INVITE/cseq=1 (tdta0x7f744c14e530) to Sproutlet

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:431: S-CSCF received initial request

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 3

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:773: Route header references this system

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:786: Found ODI token fWh8yrqIzM

20-09-2016 16:37:12.508 UTC Debug aschain.h:131: AsChain inc ref 0x7f744c1364b0 -> 2

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:793: Original dialog for odi_fWh8yrqIzM found: AsChain-orig[0x7f744c1364b0]:2/1

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:832: Got our Route header, session case orig, OD=AsChain-orig[0x7f744c1364b0]:2/1

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:291: Served user from P-Served-User header

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 4

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:502: Found served user, so apply services

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:1153: Performing originating initiating request processing

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1178: Completed applying originating services

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 2

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2218: Translating URI

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2189: Performing ENUM translation for user 1234

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:240: Translating URI via JSON ENUM lookup

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:305: Comparing first 4 numbers of 1234 against prefix 650555

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:305: Comparing first 0 numbers of 1234 against prefix

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:312: Match found

20-09-2016 16:37:12.508 UTC Info enumservice.cpp:280: Number 1234 found, translated URI = sip:1234 at 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2249: Translated URI sip:1234 at 10.112.87.177 is a real SIP URI - replacing Request-URI

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1194: New URI string is sip:1234 at 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:1210: Routing to BGCF

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1446: Routing to BGCF sip:bgcf.cw-aio:5053;transport=TCP

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f744c14eb40

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1386: scscf-0x7f744c14df40 sending Request msg INVITE/cseq=1 (tdta0x7f744c14e530) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f744c14e5d8, fork = 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1914: scscf-0x7f744c14df40 transmitting request on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1928: scscf-0x7f744c14df40 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f744c14e530) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c14eb40 => txdata 0x7f744c14e5d8 mapping

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:bgcf.cw-aio:5053;transport=TCP;lr

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:329: Possible service name - bgcf

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1154: Created Sproutlet bgcf-0x7f744c05f0b0 for Request msg INVITE/cseq=1 (tdta0x7f744c14e530)

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0) (609 bytes) to upstream sproutlet scscf:

--start msg--



SIP/2.0 100 Trying

Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh

Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be

Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>

Call-ID: OV0U_26XyoJOUXCQ0B_6FA..

From: <sip:6505550962 at example.com>;tag=9b0c5d3e

To: <sip:1234 at example.com>

CSeq: 1 INVITE

Content-Length:  0





--end msg--

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1623: scscf-0x7f744c14df40 received provisional response Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0, state = Proceeding

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:561: S-CSCF received response

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1413: scscf-0x7f744c14df40 sending Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 100

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1853: Discard 100/INVITE response (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Debug pjsip: tdta0x7f744c15 Destroying txdata Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:2062: Routing Request msg INVITE/cseq=1 (tdta0x7f744c14e530) (1411 bytes) to downstream sproutlet bgcf:

--start msg--



INVITE sip:1234 at 10.112.87.177 SIP/2.0

Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh

Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>

Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be

Max-Forwards: 67

Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>

To: <sip:1234 at example.com>

From: <sip:6505550962 at example.com>;tag=9b0c5d3e

Call-ID: OV0U_26XyoJOUXCQ0B_6FA..

CSeq: 1 INVITE

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE

Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri

User-Agent: Z 3.9.32144 r32121

Allow-Events: presence, kpml

P-Asserted-Identity: <sip:6505550962 at example.com>

Session-Expires: 600

P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg

Route: <sip:bgcf.cw-aio:5053;transport=TCP;lr>

Content-Type: application/sdp

Content-Length:   241



v=0

o=Z 0 0 IN IP4 10.112.123.57

s=Z

c=IN IP4 10.112.123.57

t=0 0

m=audio 8000 RTP/AVP 3 110 8 0 97 101

a=rtpmap:110 speex/8000

a=rtpmap:97 iLBC/8000

a=fmtp:97 mode=30

a=rtpmap:101 telephone-event/8000

a=fmtp:101 0-16

a=sendrecv



--end msg--

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:691: Cloned tdta0x7f744c14e530 to tdta0x7f744c1515d0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:bgcf.cw-aio:5053;transport=TCP;lr>

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1587: bgcf-0x7f744c05f0b0 pass initial request Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) to Sproutlet

20-09-2016 16:37:12.508 UTC Debug acr.cpp:49: Created ACR (0x7f744c05f030)

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2328: Not translating URI

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Debug bgcfservice.cpp:188: Getting route for URI domain 10.112.87.177 via BGCF lookup

20-09-2016 16:37:12.508 UTC Info bgcfservice.cpp:198: Found route to domain 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f744c151be0

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1386: bgcf-0x7f744c05f0b0 sending Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f744c151678, fork = 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1914: bgcf-0x7f744c05f0b0 transmitting request on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1928: bgcf-0x7f744c05f0b0 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:cw-aio:5058;transport=UDP;lr;orig

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:190: No Sproutlet found using service name or host

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:196: Find default service for port 5058

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:874: No local sproutlet matches request

20-09-2016 16:37:12.508 UTC Debug pjsip: tsx0x7f744c154 Transaction created for Request msg INVITE/cseq=1 (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1618: Added trail identifier 10262 to UAC transaction

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:490: Next hop node is encoded in top route header

20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:86: SIPResolver::resolve for name cw-aio, port 5058, transport 17, family 2

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:523: Attempt to parse cw-aio as IP address

20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:128: Port is specified

20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:296: Perform A/AAAA record lookup only, name = cw-aio

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:724: Removing record for scscf.cw-aio (type 1, expiry time 1474389247) from the expiry list

20-09-2016 16:37:12.508 UTC Verbose dnscachedresolver.cpp:245: Check cache for cw-aio type 1

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:251: No entry found in cache

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:254: Create cache entry pending query

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:302: Create and execute DNS query transaction

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:315: Wait for query responses

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:465: Received DNS response for cw-aio type A

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:90: Parsing DNS message

000000: 19378580 00010001 00000000 0663772d 61696f00 00010001 c00c0001 00010000    .7.. .... .... .cw- aio. .... .... ....

000020: 00000004 0a7057fa                                                          .... .pW.



20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:95: Parsing header at offset 0x0

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:98: 1 questions, 1 answers, 0 authorities, 0 additional records

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:103: Parsing question 1 at offset 0xc

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:229: Parsed domain name = cw-aio, encoded length = 8

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:112: Parsing answer 1 at offset 0x18

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:229: Parsed domain name = cw-aio, encoded length = 2

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:282: Resource Record NAME=cw-aio TYPE=A CLASS=IN TTL=0 RDLENGTH=4

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:287: Parse A record RDATA

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:142: Answer records

cw-aio                  0       IN      A       10.112.87.250



20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:143: Authority records



20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:144: Additional records



20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:761: Adding record to cache entry, TTL=0, expiry=1474389432

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:765: Update cache entry expiry to 1474389432

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:707: Adding cw-aio to cache expiry list with deletion time of 1474389732

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:319: Received all query responses

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:347: Pulling 1 records from cache for cw-aio A

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:362: Found 1 A/AAAA records, randomizing

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:993: 10.112.87.250:5058 transport 17 has state: WHITE

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:993: 10.112.87.250:5058 transport 17 has state: WHITE

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:406: Added a server, now have 1 of 5

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:447: Adding 0 servers from blacklist

20-09-2016 16:37:12.508 UTC Info pjutils.cpp:938: Resolved destination URI sip:cw-aio:5058;transport=UDP;lr;orig to 1 servers

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:490: Next hop node is encoded in top route header

20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1641: Next hop cw-aio is not a stateless proxy

20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1655: Sending request for sip:1234 at 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug pjsip: tsx0x7f744c154 Sending Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) in state Null

20-09-2016 16:37:12.508 UTC Debug pjsip:       endpoint Request msg INVITE/cseq=1 (tdta0x7f744c1515d0): skipping target resolution because address is already set

20-09-2016 16:37:12.508 UTC Debug pjsip:       endpoint Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) exceeds UDP size threshold (1300), sending with TCP

20-09-2016 16:37:12.508 UTC Verbose pjsip: tcpc0x7f744c15 TCP client transport created

20-09-2016 16:37:12.508 UTC Verbose pjsip: tcpc0x7f744c15 TCP transport 10.112.87.250:57743 is connecting to 10.112.87.250:5058...

20-09-2016 16:37:12.509 UTC Verbose common_sip_processing.cpp:136: TX 1504 bytes Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) to TCP 10.112.87.250:5058:

--start msg--



INVITE sip:1234 at 10.112.87.177 SIP/2.0

Via: SIP/2.0/TCP 10.112.87.250:57743;rport;branch=z9hG4bKPjeUUjYygIls2DRu2LAAHqVGU27AHZ.-S3

Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh

Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>

Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be

Max-Forwards: 66

Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>

To: <sip:1234 at example.com>

From: <sip:6505550962 at example.com>;tag=9b0c5d3e

Call-ID: OV0U_26XyoJOUXCQ0B_6FA..

CSeq: 1 INVITE

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE

Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri

User-Agent: Z 3.9.32144 r32121

Allow-Events: presence, kpml

P-Asserted-Identity: <sip:6505550962 at example.com>

Session-Expires: 600

P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg

Route: <sip:cw-aio:5058;transport=UDP;lr;orig>

Content-Type: application/sdp

Content-Length:   241



v=0

o=Z 0 0 IN IP4 10.112.123.57

s=Z

c=IN IP4 10.112.123.57

t=0 0

m=audio 8000 RTP/AVP 3 110 8 0 97 101

a=rtpmap:110 speex/8000

a=rtpmap:97 iLBC/8000

a=fmtp:97 mode=30

a=rtpmap:101 telephone-event/8000

a=fmtp:101 0-16

a=sendrecv



--end msg--

20-09-2016 16:37:12.509 UTC Debug pjsip: tsx0x7f744c154 State changed from Null to Calling, event=TX_MSG

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:213: tsx0x7f744c1546a8 - tu_on_tsx_state UAC, TSX_STATE TX_MSG state=Calling

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:1813: tsx0x7f744c1546a8 - uac_tsx = 0x7f744c154540, uas_tsx = 0x7f744c001b20

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:1821: TX_MSG event on current UAC transaction

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:2134: Starting timer C

20-09-2016 16:37:12.509 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f744409acd8

20-09-2016 16:37:12.509 UTC Debug thread_dispatcher.cpp:199: Request latency = 8327us

20-09-2016 16:37:12.519 UTC Verbose pjsip: tcpc0x7f744c15 TCP transport 10.112.87.250:57743 is connected to 10.112.87.250:5058

20-09-2016 16:37:12.524 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Response msg 100/INVITE/cseq=1 (rdata0x7f744c157410)

20-09-2016 16:37:12.524 UTC Verbose common_sip_processing.cpp:120: RX 864 bytes Response msg 100/INVITE/cseq=1 (rdata0x7f744c157410) from TCP 10.112.87.250:5058:

--start msg--






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160921/b9498df0/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 41, Issue 40
******************************************


::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted,
lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents
(with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the
views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification,
distribution and / or publication of this message without the prior written consent of authorized representative of
HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------




From g at projectclearwater.org  Thu Sep 22 06:38:27 2016
From: g at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Thu, 22 Sep 2016 10:38:27 +0000
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster
	using	docker-compose
In-Reply-To: <CAN20VngUB9yN6Td29LcV0p0Ths2eFON_fx+Go94uaevvykbRDA@mail.gmail.com>
References: <CAN20VnjYp3aGAnrkMUn-WYEck6+uTi+HwB6PbbZUC9wuPB1W1w@mail.gmail.com>
	<CY4PR02MB261618E6A2A54183C25FAC46E3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VnhU6-_S71FK_F0QateTrNxG2j_WN2BPEyx3PdhxOhGhmQ@mail.gmail.com>
	<CY4PR02MB2616E42F5584556E6172483FE3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VngUB9yN6Td29LcV0p0Ths2eFON_fx+Go94uaevvykbRDA@mail.gmail.com>
Message-ID: <CY4PR02MB2616F4AE6B78CC82364A1B10E3C90@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Sarbajit,

That output all looks fine, but it sounds as though the port mapping has failed ? i.e. port 5060 on the Bono container hasn?t been exposed as port 5060 on the host. I?m not sure why that would have failed. Can you run nc -z -v 10.0.1.2 5060 inside the Bono container (this should work, but it?s worth doing as a sanity check!). Then can you run docker ps in your clearwater-docker checkout? The output should include a line that looks something like 0b4058027844        clearwaterdocker_bono        "/usr/bin/supervisord"   About a minute ago   Up About a minute   0.0.0.0:3478->3478/tcp, 0.0.0.0:3478->3478/udp, 0.0.0.0:5060->5060/tcp, 0.0.0.0:5062->5062/tcp, 0.0.0.0:5060->5060/udp, 5058/tcp, 0.0.0.0:42513->22/tcp   clearwaterdocker_bono_1, which should indicate that the port mapping is active. If this all looks fine it might be worth also running nc -v -z 10.109.190.9 5060 on the host that?s running the Bono container.

Thanks,
Graeme

________________________________
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sarbajit Chatterjee
Sent: 21 September 2016 19:39
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Hi Graeme,

I'm using following command to run the livetest -

rake test[example.com<http://example.com>] TESTS="Basic*" SIGNUP_CODE=secret PROXY=10.109.190.9 ELLIS=10.109.190.10

here PROXY ip is where the bono container is launched and ELLIS ip is where the ellis container is launched.

The bono service seems to be running in the container -

root at 9837c4dab241:/# ps -eaf | grep bono
root       122     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager --mgmt-local-ip=10.0.1.2 --sig-local-ip=10.0.1.2 --local-site=site1 --remote-site= --remote-cassandra-seeds= --signaling-namespace= --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater --etcd-cluster-key=bono --log-level=3 --log-directory=/var/log/clearwater-cluster-manager --pidfile=/var/run/clearwater-cluster-manager.pid
root       124     1  0 Sep19 ?        00:00:00 /bin/bash /etc/init.d/bono run
root       139   124  0 Sep19 ?        00:00:00 /bin/bash /usr/share/clearwater/bin/run-in-signaling-namespace start-stop-daemon --start --quiet --exec /usr/share/clearwater/bin/bono --chuid bono --chdir /etc/clearwater -- --domain=example.com<http://example.com> --localhost=10.0.1.2,10.0.1.2 --alias=10.0.1.2 --pcscf=5060,5058 --webrtc-port=5062 --routing-proxy=scscf.sprout,5052,50,600 --ralf=ralf:10888 --sas=0.0.0.0,bono at 10.0.1.2<mailto:bono at 10.0.1.2> --dns-server=127.0.0.11 --worker-threads=4 --analytics=/var/log/bono --log-file=/var/log/bono --log-level=2
bono       140   139  0 Sep19 ?        00:11:15 /usr/share/clearwater/bin/bono --domain=example.com<http://example.com> --localhost=10.0.1.2,10.0.1.2 --alias=10.0.1.2 --pcscf=5060,5058 --webrtc-port=5062 --routing-proxy=scscf.sprout,5052,50,600 --ralf=ralf:10888 --sas=0.0.0.0,bono at 10.0.1.2<mailto:bono at 10.0.1.2> --dns-server=127.0.0.11 --worker-threads=4 --analytics=/var/log/bono --log-file=/var/log/bono --log-level=2
root       322   293  0 17:48 ?        00:00:00 grep --color=auto bono
root at 9837c4dab241:/#
root at 9837c4dab241:/# netstat -planut | grep 5060
tcp        0      0 10.0.1.2:5060<http://10.0.1.2:5060>           0.0.0.0:*               LISTEN      -
udp        0      0 10.0.1.2:5060<http://10.0.1.2:5060>           0.0.0.0:*                           -
root at 9837c4dab241:/#


But the connection to bono is failing from livetest container as you had predicted.

root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.9 5060
nc: connect to 10.109.190.9 port 5060 (tcp) failed: Connection refused
root at 40efba73deb5:~/clearwater-live-test#


On checking the bono log, I see a series of errors like below in beginning of the log -

19-09-2016 15:41:44.612 UTC Status utils.cpp:591: Log level set to 2
19-09-2016 15:41:44.612 UTC Status main.cpp:1388: Access logging enabled to /var/log/bono
19-09-2016 15:41:44.613 UTC Warning main.cpp:1435: SAS server option was invalid or not configured - SAS is disabled
19-09-2016 15:41:44.613 UTC Warning main.cpp:1511: A registration expiry period should not be specified for P-CSCF
19-09-2016 15:41:44.613 UTC Status snmp_agent.cpp:117: AgentX agent initialised
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:105: Constructing LoadMonitor
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:106:    Target latency (usecs)   : 100000
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:107:    Max bucket size          : 1000
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:108:    Initial token fill rate/s: 100.000000
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:109:    Min token fill rate/s    : 10.000000
19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:144: Creating Cached Resolver using servers:
19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:154:     127.0.0.11
19-09-2016 15:41:44.613 UTC Status sipresolver.cpp:60: Created SIP resolver
19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5058
19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5060
19-09-2016 15:41:44.638 UTC Status stack.cpp:855: Local host aliases:
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  172.18.0.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:
19-09-2016 15:41:44.639 UTC Status httpresolver.cpp:52: Created HTTP resolver
19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:114: Configuring HTTP Connection
19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:115:   Connection created for server ralf:10888
19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:116:   Connection will use a response timeout of 500ms
19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:72: Creating connection pool to scscf.sprout:5052
19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:73:   connections = 50, recycle time = 600 +/- 120 seconds
19-09-2016 15:41:44.649 UTC Status bono.cpp:3314: Create list of PBXes
19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins
19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:158: Finished loading plug-ins
19-09-2016 15:41:44.652 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f3df8 TCP connect() error: Connection refused [code=120111]
19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f5c38 TCP connect() error: Connection refused [code=120111]


I have observed that in the bono container 5060 port is not listening in all interfaces while 5062 port is listening in all interfaces.

root at 9837c4dab241:/var/log/bono# netstat -planut | grep LISTEN
tcp        0      0 10.0.1.2:4000<http://10.0.1.2:4000>           0.0.0.0:*               LISTEN      -
tcp        0      0 10.0.1.2:5058<http://10.0.1.2:5058>           0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.11:43395<http://127.0.0.11:43395>        0.0.0.0:*               LISTEN      -
tcp        0      0 10.0.1.2:5060<http://10.0.1.2:5060>           0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:5062<http://0.0.0.0:5062>            0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.1:8080<http://127.0.0.1:8080>          0.0.0.0:*               LISTEN      -
tcp        0      0 10.0.1.2:3478<http://10.0.1.2:3478>           0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:22<http://0.0.0.0:22>              0.0.0.0:*               LISTEN      9/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      9/sshd
root at 9837c4dab241:/var/log/bono#


Is it right to have 5060 port listen on only local port? Please help me to debug the issue.

Thanks,
Sarbajit


On Wed, Sep 21, 2016 at 10:01 PM, Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>> wrote:
Hi Sarbajit,

I?ve had another look at this, and actually I think clearwater-live-test checks it can connect to Bono before it tries to provision numbers from Ellis, and it?s actually that connection that?s failing ? apologies!

Can you do similar checks for the Bono container, i.e. connect to the Bono container and run ps -eaf | grep bono and run nc -z -v <ip> 5060 from your live test container (where <ip> is the IP of your Bono)?

One other thought ? what command are you using to run the tests? You?ll need to set the PROXY option to your Bono IP and the ELLIS option to your Ellis IP.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Sarbajit Chatterjee
Sent: 21 September 2016 16:41
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Thanks Graeme for your reply. Here are the command outputs that you had asked -

root at e994b17b4563:/# ps -eaf | grep ellis
root       177     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager --mgmt-local-ip=10.0.1.7 --sig-local-ip=10.0.1.7 --local-site=site1 --remote-site= --remote-cassandra-seeds= --signaling-namespace= --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater --etcd-cluster-key=ellis --log-level=3 --log-directory=/var/log/clearwater-cluster-manager --pidfile=/var/run/clearwater-cluster-manager.pid
root       180     1  0 Sep19 ?        00:00:00 /bin/sh /etc/init.d/ellis run
ellis      185   180  0 Sep19 ?        00:00:05 /usr/share/clearwater/ellis/env/bin/python -m metaswitch.ellis.main
root       287   253  0 15:18 ?        00:00:00 grep --color=auto ellis
root at e994b17b4563:/#
root at e994b17b4563:/# ps -eaf | grep nginx
root       179     1  0 Sep19 ?        00:00:00 nginx: master process /usr/sbin/nginx -g daemon off;
www-data   186   179  0 Sep19 ?        00:00:16 nginx: worker process
www-data   187   179  0 Sep19 ?        00:00:00 nginx: worker process
www-data   188   179  0 Sep19 ?        00:00:16 nginx: worker process
www-data   189   179  0 Sep19 ?        00:00:16 nginx: worker process
root       289   253  0 15:19 ?        00:00:00 grep --color=auto nginx
root at e994b17b4563:/#
root at e994b17b4563:/# netstat -planut | grep nginx
tcp6       0      0 :::80                   :::*                    LISTEN      179/nginx -g daemon
root at e994b17b4563:/#


I think both ellis and nginx are running fine inside the container. I can also open the ellis login page from a web browser. I also checked the MySQL DB in ellis container. I can see livetest user entry in users table and 1000 rows in numbers table.

I can also connect to ellis (host IP 10.109.190.10) from my livetest container -

root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.10 80
Connection to 10.109.190.10 80 port [tcp/http] succeeded!
root at 40efba73deb5:~/clearwater-live-test#


Is this happening because Clearwater containers are spread across multiple hosts? What other areas I should check?


Thanks,
Sarbajit


On Wed, Sep 21, 2016 at 6:09 PM, Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>> wrote:
Hi Sarbajit,

I don?t think we?ve never tried deploying Project Clearwater in a Docker Swarm cluster, but I don?t see any reason why it couldn?t work. The tests are failing very early ? they?re not able to connect to Ellis on port 80. I can think of a couple of reasons for this ? either Ellis isn?t running or the Ellis port mapping hasn?t worked for some reason.

Can you connect to the Ellis container and run ps ?eaf | grep ellis and ps ?eaf | grep nginx to confirm that NGINX and Ellis are running? Can you also run sudo netstat -planut | grep nginx or something equivalent to check that NGINX is listening on port 80? If there?s a problem with either NGINX or Ellis we probably need to look in the logs at /var/log/nginx/ or /var/log/ellis/ on the Ellis container.

If however that all looks fine, then it sounds like the port mapping has failed for some reason. Can you run nc -z <ip> 80 from the box you?re running the live tests on? This will scan for anything listening at <ip>:80 and will return successfully if it finds anything.

Thanks,
Graeme

________________________________
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sarbajit Chatterjee
Sent: 20 September 2016 15:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Hello,

I am following the instructions from https://github.com/Metaswitch/clearwater-docker. I can successfully deploy it on a single Docker node but, the compose file does not work with Swarm cluster.

I did try to modify the compose file like this -


version: '2'
services:
  etcd:
    image: quay.io/coreos/etcd:v2.2.5<http://quay.io/coreos/etcd:v2.2.5>
    command: >
      -name etcd0
      -advertise-client-urls http://etcd:2379,http://etcd:4001
      -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001
      -initial-advertise-peer-urls http://etcd:2380
      -listen-peer-urls http://0.0.0.0:2380
      -initial-cluster etcd0=http://etcd:2380
      -initial-cluster-state new
  bono:
    image: swarm-node:5000/clearwaterdocker_bono
    ports:
      - 22
      - "3478:3478"
      - "3478:3478/udp"
      - "5060:5060"
      - "5060:5060/udp"
      - "5062:5062"
  sprout:
    image: swarm-node:5000/clearwaterdocker_sprout
    networks:
      default:
        aliases:
          - scscf.sprout
          - icscf.sprout
    ports:
      - 22
  homestead:
    image: swarm-node:5000/clearwaterdocker_homestead
    ports:
      - 22
  homer:
    image: swarm-node:5000/clearwaterdocker_homer
    ports:
      - 22
  ralf:
    image: swarm-node:5000/clearwaterdocker_ralf
    ports:
      - 22
  ellis:
    image: swarm-node:5000/clearwaterdocker_ellis
    ports:
      - 22
      - "80:80"


where swarm-node:5000 is the local docker registry and it hosts the pre-built images of Clearwater containers. Even though the deployment succeeded, clearwater-livetests are failing with following error -


Basic Registration (TCP) - Failed
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/local/rvm/gems/ruby-1.9.3-p551/gems/quaff-0.7.3/lib/sources.rb:41:in `initialize'


Any suggestions on how I can deploy Clearwater on a Swarm cluster?

Thanks,
Sarbajit


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160922/c79962fd/attachment.html>

From g at projectclearwater.org  Thu Sep 22 06:38:37 2016
From: g at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Thu, 22 Sep 2016 10:38:37 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
In-Reply-To: <16AE9ED83DBA5B4D85B058EAF39C918107E1B181@NDA-HCLT-MBS05.hclt.corp.hcl.in>
References: <16AE9ED83DBA5B4D85B058EAF39C918107E1B181@NDA-HCLT-MBS05.hclt.corp.hcl.in>
Message-ID: <CY4PR02MB2616419418B169728B755550E3C90@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Surender,

I'm not sure Schahzad ever said he was routing to a PBX - it looked like he was just doing SIP trunking - but if he was routing to a PBX, presumably he was using a different PBX to you.

You have a BGCF rule which says that requests to 10.112.87.177 should be routed via sip:cw-aio:5058. It looks like cw-aio resolves to the IP address of your AIO node, and Bono listens on port 5058 for traffic from the core, so the message gets routed to Bono which acts as an IBCF and presumably routes the message out to your PBX. Does that answer your question?

Thanks,
Graeme

-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 22 September 2016 06:49
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP Trunking With PBX_aio

Hi Graeme,

Thanks for support..

But have seen one old  thread subject 'SIP Trunking  ' by Schahzad .His solution working fine without any changes at PBX end.

Please give comments on call scenarios for out calls to PBX

SIP Client ----> Sprout ---->ENUM Query(Find new request URI based on B number prefix) ------>BGCF(match the domain name with new request URI which is re-write by ENUM and find out the route value )

In my case route value is    ["<sip:cw-aio:5058;transport=UDP;lr;orig>"] and Domain is 10.112.87.177 .

But here I have confusion regarding route value '' sip:cw-aio:5058;transport=UDP;lr;orig '  what is mean of this.

Please give clarity  on route part ..


Regards
Surender Singh
8826292018

	

-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 21 September 2016 16:55
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 41, Issue 40

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. Re: SIP Trunking With PBX_aio
      (Graeme Robertson (projectclearwater.org))


----------------------------------------------------------------------

Message: 1
Date: Wed, 21 Sep 2016 11:24:02 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID:
	<CY4PR02MB2616A2D97C1BC38B1E842CC5E3F60 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="us-ascii"

Hi Surender,



Project Clearwater is IMS compliant, and so I'm afraid it doesn't alter either the To header or the From header, and there is no trivial way to configure Project Clearwater to do this manipulation, or any plans to add this ability to Project Clearwater. It sounds as though your PBX is non-IMS compliant. As I mentioned before, if you really need to re-write the To header on your request, and you are not averse to writing a bit of code, Sprout provides a really easy-to-use API for developing your own app server. https://github.com/Metaswitch/greeter is a sample application server that uses this API and simply adds the header "Subject: Hello world!" to a SIP message - you could write something similar that changes to the To header on your SIP messages. Alternatively, Metaswitch has a commercial product called Perimeta<http://www.metaswitch.com/perimeta-session-border-controller-sbc> which can be deployed as an IBCF, and which has a SIP Message Manipulation Framework which you would be able to use to alter your To header.



>From the logs below, it looks as though the BGCF is now applying your route and the INVITE will presumably be going out through Bono, but unfortunately I don't think that will solve your problem.



Thanks,

Graeme



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 20 September 2016 13:45
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP Trunking With PBX_aio



Hi Greame,



Thanks for support .



But my issue is ,when I calling from A to B (Out call) its showing example.com in To_header and From_header . and same value going at my PBX ,due this might be  pbx unable to identify the value like 1234 at example.com<mailto:1234 at example.com>  instead of 1234 at 10.112.87.177<mailto:1234 at 10.112.87.177> . Can have any possibility to change it at IMS end.





I also change the configuration in Bgcf.json but I unable to check whether calls are forwarding through BGCF route or not .I found configuration like below... in one thread.





{

    "number_blocks" : [

        {

           "name" : "Internal numbers",

           "prefix" : "650555",

           "regex" : "!(^.*$)!sip:\\1 at example.com!"

        },

        {

            "name" : "External numbers",

            "prefix" : "",

            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"

        }





    ]

}







{

    "routes" : [

        {   "name" : "TO_PBX",

          "domain" : "10.112.87.177",

          "route" : ["<sip:cw-aio:5058;transport=UDP;lr;orig>"]

        }



    ]



}









20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:691: Cloned tdta0x7f744c14aea0 to tdta0x7f744c14e530

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:odi_fWh8yrqIzM at 10.112.87.250:5054;lr;orig;service=scscf>

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c14eb40 => txdata 0x7f744c14e5d8 mapping

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1587: scscf-0x7f744c14df40 pass initial request Request msg INVITE/cseq=1 (tdta0x7f744c14e530) to Sproutlet

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:431: S-CSCF received initial request

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 3

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:773: Route header references this system

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:786: Found ODI token fWh8yrqIzM

20-09-2016 16:37:12.508 UTC Debug aschain.h:131: AsChain inc ref 0x7f744c1364b0 -> 2

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:793: Original dialog for odi_fWh8yrqIzM found: AsChain-orig[0x7f744c1364b0]:2/1

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:832: Got our Route header, session case orig, OD=AsChain-orig[0x7f744c1364b0]:2/1

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:291: Served user from P-Served-User header

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 4

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:502: Found served user, so apply services

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:1153: Performing originating initiating request processing

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1178: Completed applying originating services

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 2

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2218: Translating URI

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2189: Performing ENUM translation for user 1234

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:240: Translating URI via JSON ENUM lookup

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:305: Comparing first 4 numbers of 1234 against prefix 650555

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:305: Comparing first 0 numbers of 1234 against prefix

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:312: Match found

20-09-2016 16:37:12.508 UTC Info enumservice.cpp:280: Number 1234 found, translated URI = sip:1234 at 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2249: Translated URI sip:1234 at 10.112.87.177 is a real SIP URI - replacing Request-URI

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1194: New URI string is sip:1234 at 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:1210: Routing to BGCF

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1446: Routing to BGCF sip:bgcf.cw-aio:5053;transport=TCP

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f744c14eb40

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1386: scscf-0x7f744c14df40 sending Request msg INVITE/cseq=1 (tdta0x7f744c14e530) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f744c14e5d8, fork = 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1914: scscf-0x7f744c14df40 transmitting request on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1928: scscf-0x7f744c14df40 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f744c14e530) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c14eb40 => txdata 0x7f744c14e5d8 mapping

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:bgcf.cw-aio:5053;transport=TCP;lr

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:329: Possible service name - bgcf

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1154: Created Sproutlet bgcf-0x7f744c05f0b0 for Request msg INVITE/cseq=1 (tdta0x7f744c14e530)

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0) (609 bytes) to upstream sproutlet scscf:

--start msg--



SIP/2.0 100 Trying

Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh

Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be

Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>

Call-ID: OV0U_26XyoJOUXCQ0B_6FA..

From: <sip:6505550962 at example.com>;tag=9b0c5d3e

To: <sip:1234 at example.com>

CSeq: 1 INVITE

Content-Length:  0





--end msg--

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1623: scscf-0x7f744c14df40 received provisional response Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0, state = Proceeding

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:561: S-CSCF received response

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1413: scscf-0x7f744c14df40 sending Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 100

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1853: Discard 100/INVITE response (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Debug pjsip: tdta0x7f744c15 Destroying txdata Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:2062: Routing Request msg INVITE/cseq=1 (tdta0x7f744c14e530) (1411 bytes) to downstream sproutlet bgcf:

--start msg--



INVITE sip:1234 at 10.112.87.177 SIP/2.0

Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh

Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>

Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be

Max-Forwards: 67

Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>

To: <sip:1234 at example.com>

From: <sip:6505550962 at example.com>;tag=9b0c5d3e

Call-ID: OV0U_26XyoJOUXCQ0B_6FA..

CSeq: 1 INVITE

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE

Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri

User-Agent: Z 3.9.32144 r32121

Allow-Events: presence, kpml

P-Asserted-Identity: <sip:6505550962 at example.com>

Session-Expires: 600

P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg

Route: <sip:bgcf.cw-aio:5053;transport=TCP;lr>

Content-Type: application/sdp

Content-Length:   241



v=0

o=Z 0 0 IN IP4 10.112.123.57

s=Z

c=IN IP4 10.112.123.57

t=0 0

m=audio 8000 RTP/AVP 3 110 8 0 97 101

a=rtpmap:110 speex/8000

a=rtpmap:97 iLBC/8000

a=fmtp:97 mode=30

a=rtpmap:101 telephone-event/8000

a=fmtp:101 0-16

a=sendrecv



--end msg--

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:691: Cloned tdta0x7f744c14e530 to tdta0x7f744c1515d0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:bgcf.cw-aio:5053;transport=TCP;lr>

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1587: bgcf-0x7f744c05f0b0 pass initial request Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) to Sproutlet

20-09-2016 16:37:12.508 UTC Debug acr.cpp:49: Created ACR (0x7f744c05f030)

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2328: Not translating URI

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Debug bgcfservice.cpp:188: Getting route for URI domain 10.112.87.177 via BGCF lookup

20-09-2016 16:37:12.508 UTC Info bgcfservice.cpp:198: Found route to domain 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f744c151be0

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1386: bgcf-0x7f744c05f0b0 sending Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f744c151678, fork = 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1914: bgcf-0x7f744c05f0b0 transmitting request on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1928: bgcf-0x7f744c05f0b0 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:cw-aio:5058;transport=UDP;lr;orig

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:190: No Sproutlet found using service name or host

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:196: Find default service for port 5058

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:874: No local sproutlet matches request

20-09-2016 16:37:12.508 UTC Debug pjsip: tsx0x7f744c154 Transaction created for Request msg INVITE/cseq=1 (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1618: Added trail identifier 10262 to UAC transaction

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:490: Next hop node is encoded in top route header

20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:86: SIPResolver::resolve for name cw-aio, port 5058, transport 17, family 2

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:523: Attempt to parse cw-aio as IP address

20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:128: Port is specified

20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:296: Perform A/AAAA record lookup only, name = cw-aio

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:724: Removing record for scscf.cw-aio (type 1, expiry time 1474389247) from the expiry list

20-09-2016 16:37:12.508 UTC Verbose dnscachedresolver.cpp:245: Check cache for cw-aio type 1

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:251: No entry found in cache

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:254: Create cache entry pending query

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:302: Create and execute DNS query transaction

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:315: Wait for query responses

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:465: Received DNS response for cw-aio type A

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:90: Parsing DNS message

000000: 19378580 00010001 00000000 0663772d 61696f00 00010001 c00c0001 00010000    .7.. .... .... .cw- aio. .... .... ....

000020: 00000004 0a7057fa                                                          .... .pW.



20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:95: Parsing header at offset 0x0

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:98: 1 questions, 1 answers, 0 authorities, 0 additional records

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:103: Parsing question 1 at offset 0xc

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:229: Parsed domain name = cw-aio, encoded length = 8

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:112: Parsing answer 1 at offset 0x18

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:229: Parsed domain name = cw-aio, encoded length = 2

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:282: Resource Record NAME=cw-aio TYPE=A CLASS=IN TTL=0 RDLENGTH=4

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:287: Parse A record RDATA

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:142: Answer records

cw-aio                  0       IN      A       10.112.87.250



20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:143: Authority records



20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:144: Additional records



20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:761: Adding record to cache entry, TTL=0, expiry=1474389432

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:765: Update cache entry expiry to 1474389432

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:707: Adding cw-aio to cache expiry list with deletion time of 1474389732

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:319: Received all query responses

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:347: Pulling 1 records from cache for cw-aio A

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:362: Found 1 A/AAAA records, randomizing

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:993: 10.112.87.250:5058 transport 17 has state: WHITE

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:993: 10.112.87.250:5058 transport 17 has state: WHITE

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:406: Added a server, now have 1 of 5

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:447: Adding 0 servers from blacklist

20-09-2016 16:37:12.508 UTC Info pjutils.cpp:938: Resolved destination URI sip:cw-aio:5058;transport=UDP;lr;orig to 1 servers

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:490: Next hop node is encoded in top route header

20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1641: Next hop cw-aio is not a stateless proxy

20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1655: Sending request for sip:1234 at 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug pjsip: tsx0x7f744c154 Sending Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) in state Null

20-09-2016 16:37:12.508 UTC Debug pjsip:       endpoint Request msg INVITE/cseq=1 (tdta0x7f744c1515d0): skipping target resolution because address is already set

20-09-2016 16:37:12.508 UTC Debug pjsip:       endpoint Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) exceeds UDP size threshold (1300), sending with TCP

20-09-2016 16:37:12.508 UTC Verbose pjsip: tcpc0x7f744c15 TCP client transport created

20-09-2016 16:37:12.508 UTC Verbose pjsip: tcpc0x7f744c15 TCP transport 10.112.87.250:57743 is connecting to 10.112.87.250:5058...

20-09-2016 16:37:12.509 UTC Verbose common_sip_processing.cpp:136: TX 1504 bytes Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) to TCP 10.112.87.250:5058:

--start msg--



INVITE sip:1234 at 10.112.87.177 SIP/2.0

Via: SIP/2.0/TCP 10.112.87.250:57743;rport;branch=z9hG4bKPjeUUjYygIls2DRu2LAAHqVGU27AHZ.-S3

Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh

Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>

Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be

Max-Forwards: 66

Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>

To: <sip:1234 at example.com>

From: <sip:6505550962 at example.com>;tag=9b0c5d3e

Call-ID: OV0U_26XyoJOUXCQ0B_6FA..

CSeq: 1 INVITE

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE

Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri

User-Agent: Z 3.9.32144 r32121

Allow-Events: presence, kpml

P-Asserted-Identity: <sip:6505550962 at example.com>

Session-Expires: 600

P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg

Route: <sip:cw-aio:5058;transport=UDP;lr;orig>

Content-Type: application/sdp

Content-Length:   241



v=0

o=Z 0 0 IN IP4 10.112.123.57

s=Z

c=IN IP4 10.112.123.57

t=0 0

m=audio 8000 RTP/AVP 3 110 8 0 97 101

a=rtpmap:110 speex/8000

a=rtpmap:97 iLBC/8000

a=fmtp:97 mode=30

a=rtpmap:101 telephone-event/8000

a=fmtp:101 0-16

a=sendrecv



--end msg--

20-09-2016 16:37:12.509 UTC Debug pjsip: tsx0x7f744c154 State changed from Null to Calling, event=TX_MSG

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:213: tsx0x7f744c1546a8 - tu_on_tsx_state UAC, TSX_STATE TX_MSG state=Calling

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:1813: tsx0x7f744c1546a8 - uac_tsx = 0x7f744c154540, uas_tsx = 0x7f744c001b20

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:1821: TX_MSG event on current UAC transaction

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:2134: Starting timer C

20-09-2016 16:37:12.509 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f744409acd8

20-09-2016 16:37:12.509 UTC Debug thread_dispatcher.cpp:199: Request latency = 8327us

20-09-2016 16:37:12.519 UTC Verbose pjsip: tcpc0x7f744c15 TCP transport 10.112.87.250:57743 is connected to 10.112.87.250:5058

20-09-2016 16:37:12.524 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Response msg 100/INVITE/cseq=1 (rdata0x7f744c157410)

20-09-2016 16:37:12.524 UTC Verbose common_sip_processing.cpp:120: RX 864 bytes Response msg 100/INVITE/cseq=1 (rdata0x7f744c157410) from TCP 10.112.87.250:5058:

--start msg--






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160921/b9498df0/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 41, Issue 40
******************************************


::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents (with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification, distribution and / or publication of this message without the prior written consent of authorized representative of HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



From sarbajitc at gmail.com  Thu Sep 22 07:13:40 2016
From: sarbajitc at gmail.com (Sarbajit Chatterjee)
Date: Thu, 22 Sep 2016 16:43:40 +0530
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster using
	docker-compose
In-Reply-To: <CY4PR02MB2616F4AE6B78CC82364A1B10E3C90@CY4PR02MB2616.namprd02.prod.outlook.com>
References: <CAN20VnjYp3aGAnrkMUn-WYEck6+uTi+HwB6PbbZUC9wuPB1W1w@mail.gmail.com>
	<CY4PR02MB261618E6A2A54183C25FAC46E3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VnhU6-_S71FK_F0QateTrNxG2j_WN2BPEyx3PdhxOhGhmQ@mail.gmail.com>
	<CY4PR02MB2616E42F5584556E6172483FE3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VngUB9yN6Td29LcV0p0Ths2eFON_fx+Go94uaevvykbRDA@mail.gmail.com>
	<CY4PR02MB2616F4AE6B78CC82364A1B10E3C90@CY4PR02MB2616.namprd02.prod.outlook.com>
Message-ID: <CAN20Vni=Do=b6vEy6d3KMbb2HVvhp-jA7Uv07PKx8O0z=g3Kcg@mail.gmail.com>

Hi Graeme,

I can connect to 5060 port in 10.0.1.2 from sprout container.

root at bb818f0a5535:/# nc -v -z 10.0.1.2 5060
Connection to 10.0.1.2 5060 port [tcp/sip] succeeded!
root at bb818f0a5535:/#


Docker PS on the cluster shows bono container exposing port 5060


CONTAINER ID        IMAGE                                        COMMAND
               CREATED             STATUS              PORTS


            NAMES
bb818f0a5535        swarm-node:5000/clearwaterdocker_sprout
 "/usr/bin/supervisord"   47 hours ago        Up 47 hours         5052/tcp,
5054/tcp, 10.109.190.10:32822->22/tcp

                      swarm-node/clearwaterdocker_sprout_1
9ff210721451        swarm-node:5000/clearwaterdocker_homer
"/usr/bin/supervisord"   47 hours ago        Up 47 hours         7888/tcp,
10.109.190.9:32788->22/tcp

                       swarm-master/clearwaterdocker_homer_1
51653420c979        swarm-node:5000/clearwaterdocker_homestead
"/usr/bin/supervisord"   47 hours ago        Up 47 hours
8888-8889/tcp, 10.109.190.9:32787->22/tcp

                                swarm-master/clearwaterdocker_homestead_1
e994b17b4563        swarm-node:5000/clearwaterdocker_ellis
"/usr/bin/supervisord"   47 hours ago        Up 47 hours
10.109.190.10:80->80/tcp, 10.109.190.10:32821->22/tcp

                                swarm-node/clearwaterdocker_ellis_1
9837c4dab241        swarm-node:5000/clearwaterdocker_bono
 "/usr/bin/supervisord"   47 hours ago        Up 47 hours
10.109.190.9:3478->3478/tcp, 10.109.190.9:3478->3478/udp,
10.109.190.9:5060->5060/tcp,
10.109.190.9:5062->5062/tcp, 10.109.190.9:5060->5060/udp, 5058/tcp,
10.109.190.9:32786->22/tcp   swarm-master/clearwaterdocker_bono_1
3db967c58754        swarm-node:5000/clearwaterdocker_ralf
 "/usr/bin/supervisord"   47 hours ago        Up 47 hours
10888/tcp, 10.109.190.10:32820->22/tcp

                                 swarm-node/clearwaterdocker_ralf_1
c499d05af8e7        quay.io/coreos/etcd:v2.2.5                   "/etcd
-name etcd0 -a"   47 hours ago        Up 47 hours         2379-2380/tcp,
4001/tcp, 7001/tcp

                  swarm-node/clearwaterdocker_etcd_1
5fe0c51979e7        ubuntu:14.04.5
"/bin/bash"              8 days ago          Up 8 days



But I can't reach the 5060 port from the host machine where it is launched.
Though the port seems to be open.


root at swarm-master:~# nc -v -z 10.109.190.9 5060
nc: connect to 10.109.190.9 port 5060 (tcp) failed: Connection refused
root at swarm-master:~#
root at swarm-master:~# netstat -anp | grep 5060
tcp6       0      0 :::5060                 :::*                    LISTEN
     21818/docker-proxy
udp6       0      0 :::5060                 :::*
     21829/docker-proxy
root at swarm-master:~#


Any suggestion?

-Sarbajit


On Thu, Sep 22, 2016 at 4:08 PM, Graeme Robertson (projectclearwater.org) <
g at projectclearwater.org> wrote:

> Hi Sarbajit,
>
>
>
> That output all looks fine, but it sounds as though the port mapping has
> failed ? i.e. port 5060 on the Bono container hasn?t been exposed as port
> 5060 on the host. I?m not sure why that would have failed. Can you run nc
> -z -v 10.0.1.2 5060 inside the Bono container (this should work, but it?s
> worth doing as a sanity check!). Then can you run docker ps in your
> clearwater-docker checkout? The output should include a line that looks
> something like 0b4058027844        clearwaterdocker_bono
> "/usr/bin/supervisord"   About a minute ago   Up About a minute
> 0.0.0.0:3478->3478/tcp, 0.0.0.0:3478->3478/udp, *0.0.0.0:5060->5060/tcp*,
> 0.0.0.0:5062->5062/tcp, *0.0.0.0:5060->5060/udp*, 5058/tcp, 0.0.0.0:42513->22/tcp
> clearwaterdocker_bono_1, which should indicate that the port mapping is
> active. If this all looks fine it might be worth also running nc -v -z
> 10.109.190.9 5060 on the host that?s running the Bono container.
>
>
>
> Thanks,
>
> Graeme
>
>
> ------------------------------
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sarbajit Chatterjee
> *Sent:* 21 September 2016 19:39
>
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Hi Graeme,
>
>
>
> I'm using following command to run the livetest -
>
>
>
> rake test[example.com] TESTS="Basic*" SIGNUP_CODE=secret
> PROXY=10.109.190.9 ELLIS=10.109.190.10
>
>
>
> here PROXY ip is where the bono container is launched and ELLIS ip is
> where the ellis container is launched.
>
>
>
> The bono service seems to be running in the container -
>
>
>
> root at 9837c4dab241:/# ps -eaf | grep bono
>
> root       122     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/
> clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager
> --mgmt-local-ip=10.0.1.2 --sig-local-ip=10.0.1.2 --local-site=site1
> --remote-site= --remote-cassandra-seeds= --signaling-namespace=
> --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater
> --etcd-cluster-key=bono --log-level=3 --log-directory=/var/log/clearwater-cluster-manager
> --pidfile=/var/run/clearwater-cluster-manager.pid
>
> root       124     1  0 Sep19 ?        00:00:00 /bin/bash /etc/init.d/bono
> run
>
> root       139   124  0 Sep19 ?        00:00:00 /bin/bash
> /usr/share/clearwater/bin/run-in-signaling-namespace start-stop-daemon
> --start --quiet --exec /usr/share/clearwater/bin/bono --chuid bono --chdir
> /etc/clearwater -- --domain=example.com --localhost=10.0.1.2,10.0.1.2
> --alias=10.0.1.2 --pcscf=5060,5058 --webrtc-port=5062
> --routing-proxy=scscf.sprout,5052,50,600 --ralf=ralf:10888 --sas=0.0.0.0,
> bono at 10.0.1.2 --dns-server=127.0.0.11 --worker-threads=4
> --analytics=/var/log/bono --log-file=/var/log/bono --log-level=2
>
> bono       140   139  0 Sep19 ?        00:11:15
> /usr/share/clearwater/bin/bono --domain=example.com
> --localhost=10.0.1.2,10.0.1.2 --alias=10.0.1.2 --pcscf=5060,5058
> --webrtc-port=5062 --routing-proxy=scscf.sprout,5052,50,600
> --ralf=ralf:10888 --sas=0.0.0.0,bono at 10.0.1.2 --dns-server=127.0.0.11
> --worker-threads=4 --analytics=/var/log/bono --log-file=/var/log/bono
> --log-level=2
>
> root       322   293  0 17:48 ?        00:00:00 grep --color=auto bono
>
> root at 9837c4dab241:/#
>
> root at 9837c4dab241:/# netstat -planut | grep 5060
>
> tcp        0      0 10.0.1.2:5060           0.0.0.0:*
> LISTEN      -
>
> udp        0      0 10.0.1.2:5060           0.0.0.0:*
>       -
>
> root at 9837c4dab241:/#
>
>
>
>
>
> But the connection to bono is failing from livetest container as you had
> predicted.
>
>
>
> root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.9 5060
>
> nc: connect to 10.109.190.9 port 5060 (tcp) failed: Connection refused
>
> root at 40efba73deb5:~/clearwater-live-test#
>
>
>
>
>
> On checking the bono log, I see a series of errors like below
> in beginning of the log -
>
>
>
> 19-09-2016 15:41:44.612 UTC Status utils.cpp:591: Log level set to 2
>
> 19-09-2016 15:41:44.612 UTC Status main.cpp:1388: Access logging enabled
> to /var/log/bono
>
> 19-09-2016 15:41:44.613 UTC Warning main.cpp:1435: SAS server option was
> invalid or not configured - SAS is disabled
>
> 19-09-2016 15:41:44.613 UTC Warning main.cpp:1511: A registration expiry
> period should not be specified for P-CSCF
>
> 19-09-2016 15:41:44.613 UTC Status snmp_agent.cpp:117: AgentX agent
> initialised
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:105: Constructing
> LoadMonitor
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:106:    Target latency
> (usecs)   : 100000
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:107:    Max bucket
> size          : 1000
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:108:    Initial token
> fill rate/s: 100.000000
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:109:    Min token fill
> rate/s    : 10.000000
>
> 19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:144: Creating
> Cached Resolver using servers:
>
> 19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:154:
> 127.0.0.11
>
> 19-09-2016 15:41:44.613 UTC Status sipresolver.cpp:60: Created SIP resolver
>
> 19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5058
>
> 19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5060
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:855: Local host aliases:
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  172.18.0.2
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:
>
> 19-09-2016 15:41:44.639 UTC Status httpresolver.cpp:52: Created HTTP
> resolver
>
> 19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:114: Configuring
> HTTP Connection
>
> 19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:115:   Connection
> created for server ralf:10888
>
> 19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:116:   Connection
> will use a response timeout of 500ms
>
> 19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:72: Creating
> connection pool to scscf.sprout:5052
>
> 19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:73:   connections =
> 50, recycle time = 600 +/- 120 seconds
>
> 19-09-2016 15:41:44.649 UTC Status bono.cpp:3314: Create list of PBXes
>
> 19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:63: Loading plug-ins
> from /usr/share/clearwater/sprout/plugins
>
> 19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:158: Finished loading
> plug-ins
>
> 19-09-2016 15:41:44.652 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
> 19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f3df8 TCP connect()
> error: Connection refused [code=120111]
>
> 19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f5c38 TCP connect()
> error: Connection refused [code=120111]
>
>
>
>
>
> I have observed that in the bono container 5060 port is not listening in
> all interfaces while 5062 port is listening in all interfaces.
>
>
>
> root at 9837c4dab241:/var/log/bono# netstat -planut | grep LISTEN
>
> tcp        0      0 10.0.1.2:4000           0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 10.0.1.2:5058           0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 127.0.0.11:43395        0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 10.0.1.2:5060           0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 0.0.0.0:5062            0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 127.0.0.1:8080          0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 10.0.1.2:3478           0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 0.0.0.0:22              0.0.0.0:*
> LISTEN      9/sshd
>
> tcp6       0      0 :::22                   :::*                    LISTEN
>      9/sshd
>
> root at 9837c4dab241:/var/log/bono#
>
>
>
>
>
> Is it right to have 5060 port listen on only local port? Please help me to
> debug the issue.
>
>
>
> Thanks,
>
> Sarbajit
>
>
>
>
>
> On Wed, Sep 21, 2016 at 10:01 PM, Graeme Robertson (projectclearwater.org)
> <graeme at projectclearwater.org> wrote:
>
> Hi Sarbajit,
>
>
>
> I?ve had another look at this, and actually I think clearwater-live-test
> checks it can connect to Bono before it tries to provision numbers from
> Ellis, and it?s actually that connection that?s failing ? apologies!
>
>
>
> Can you do similar checks for the Bono container, i.e. connect to the Bono
> container and run ps -eaf | grep bono and run nc -z -v <ip> 5060 from
> your live test container (where <ip> is the IP of your Bono)?
>
>
>
> One other thought ? what command are you using to run the tests? You?ll
> need to set the PROXY option to your Bono IP and the ELLIS option to your
> Ellis IP.
>
>
>
> Thanks,
>
> Graeme
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sarbajit Chatterjee
> *Sent:* 21 September 2016 16:41
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Thanks Graeme for your reply. Here are the command outputs that you had
> asked -
>
>
>
> root at e994b17b4563:/# ps -eaf | grep ellis
>
> root       177     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/
> clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager
> --mgmt-local-ip=10.0.1.7 --sig-local-ip=10.0.1.7 --local-site=site1
> --remote-site= --remote-cassandra-seeds= --signaling-namespace=
> --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater
> --etcd-cluster-key=ellis --log-level=3 --log-directory=/var/log/clearwater-cluster-manager
> --pidfile=/var/run/clearwater-cluster-manager.pid
>
> root       180     1  0 Sep19 ?        00:00:00 /bin/sh /etc/init.d/ellis
> run
>
> ellis      185   180  0 Sep19 ?        00:00:05
> /usr/share/clearwater/ellis/env/bin/python -m metaswitch.ellis.main
>
> root       287   253  0 15:18 ?        00:00:00 grep --color=auto ellis
>
> root at e994b17b4563:/#
>
> root at e994b17b4563:/# ps -eaf | grep nginx
>
> root       179     1  0 Sep19 ?        00:00:00 nginx: master process
> /usr/sbin/nginx -g daemon off;
>
> www-data   186   179  0 Sep19 ?        00:00:16 nginx: worker process
>
> www-data   187   179  0 Sep19 ?        00:00:00 nginx: worker process
>
> www-data   188   179  0 Sep19 ?        00:00:16 nginx: worker process
>
> www-data   189   179  0 Sep19 ?        00:00:16 nginx: worker process
>
> root       289   253  0 15:19 ?        00:00:00 grep --color=auto nginx
>
> root at e994b17b4563:/#
>
> root at e994b17b4563:/# netstat -planut | grep nginx
>
> tcp6       0      0 :::80                   :::*                    LISTEN
>      179/nginx -g daemon
>
> root at e994b17b4563:/#
>
>
>
>
>
> I think both ellis and nginx are running fine inside the container. I can
> also open the ellis login page from a web browser. I also checked the MySQL
> DB in ellis container. I can see livetest user entry in users table and
> 1000 rows in numbers table.
>
>
>
> I can also connect to ellis (host IP 10.109.190.10) from my livetest
> container -
>
>
>
> root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.10 80
>
> Connection to 10.109.190.10 80 port [tcp/http] succeeded!
>
> root at 40efba73deb5:~/clearwater-live-test#
>
>
>
>
>
> Is this happening because Clearwater containers are spread across multiple
> hosts? What other areas I should check?
>
>
>
>
>
> Thanks,
>
> Sarbajit
>
>
>
>
>
> On Wed, Sep 21, 2016 at 6:09 PM, Graeme Robertson (projectclearwater.org)
> <graeme at projectclearwater.org> wrote:
>
> Hi Sarbajit,
>
>
>
> I don?t think we?ve never tried deploying Project Clearwater in a Docker
> Swarm cluster, but I don?t see any reason why it couldn?t work. The tests
> are failing very early ? they?re not able to connect to Ellis on port 80. I
> can think of a couple of reasons for this ? either Ellis isn?t running or
> the Ellis port mapping hasn?t worked for some reason.
>
>
>
> Can you connect to the Ellis container and run ps ?eaf | grep ellis and ps
> ?eaf | grep nginx to confirm that NGINX and Ellis are running? Can you
> also run sudo netstat -planut | grep nginx or something equivalent to
> check that NGINX is listening on port 80? If there?s a problem with either
> NGINX or Ellis we probably need to look in the logs at /var/log/nginx/ or
> /var/log/ellis/ on the Ellis container.
>
>
>
> If however that all looks fine, then it sounds like the port mapping has
> failed for some reason. Can you run nc -z <ip> 80 from the box you?re
> running the live tests on? This will scan for anything listening at
> <ip>:80 and will return successfully if it finds anything.
>
>
>
> Thanks,
>
> Graeme
>
>
> ------------------------------
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org
> <clearwater-bounces at lists.projectclearwater.org>] *On Behalf Of *Sarbajit
> Chatterjee
> *Sent:* 20 September 2016 15:05
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Hello,
>
>
>
> I am following the instructions from https://github.com/
> Metaswitch/clearwater-docker. I can successfully deploy it on a single
> Docker node but, the compose file does not work with Swarm cluster.
>
>
>
> I did try to modify the compose file like this -
>
>
>
>
>
> version: '2'
>
> services:
>
>   etcd:
>
>     image: quay.io/coreos/etcd:v2.2.5
>
>     command: >
>
>       -name etcd0
>
>       -advertise-client-urls http://etcd:2379,http://etcd:4001
>
>       -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001
>
>       -initial-advertise-peer-urls http://etcd:2380
>
>       -listen-peer-urls http://0.0.0.0:2380
>
>       -initial-cluster etcd0=http://etcd:2380
>
>       -initial-cluster-state new
>
>   bono:
>
>     image: swarm-node:5000/clearwaterdocker_bono
>
>     ports:
>
>       - 22
>
>       - "3478:3478"
>
>       - "3478:3478/udp"
>
>       - "5060:5060"
>
>       - "5060:5060/udp"
>
>       - "5062:5062"
>
>   sprout:
>
>     image: swarm-node:5000/clearwaterdocker_sprout
>
>     networks:
>
>       default:
>
>         aliases:
>
>           - scscf.sprout
>
>           - icscf.sprout
>
>     ports:
>
>       - 22
>
>   homestead:
>
>     image: swarm-node:5000/clearwaterdocker_homestead
>
>     ports:
>
>       - 22
>
>   homer:
>
>     image: swarm-node:5000/clearwaterdocker_homer
>
>     ports:
>
>       - 22
>
>   ralf:
>
>     image: swarm-node:5000/clearwaterdocker_ralf
>
>     ports:
>
>       - 22
>
>   ellis:
>
>     image: swarm-node:5000/clearwaterdocker_ellis
>
>     ports:
>
>       - 22
>
>       - "80:80"
>
>
>
>
>
> where swarm-node:5000 is the local docker registry and it hosts the
> pre-built images of Clearwater containers. Even though the deployment
> succeeded, clearwater-livetests are failing with following error -
>
>
>
>
>
> Basic Registration (TCP) - Failed
>
>   Errno::ECONNREFUSED thrown:
>
>    - Connection refused - connect(2)
>
>      - /usr/local/rvm/gems/ruby-1.9.3-p551/gems/quaff-0.7.3/lib/sources.rb:41:in
> `initialize'
>
>
>
>
>
> Any suggestions on how I can deploy Clearwater on a Swarm cluster?
>
>
>
> Thanks,
>
> Sarbajit
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160922/2aa9a679/attachment.html>

From g at projectclearwater.org  Fri Sep 23 06:52:42 2016
From: g at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Fri, 23 Sep 2016 10:52:42 +0000
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster
	using	docker-compose
In-Reply-To: <CAN20Vni=Do=b6vEy6d3KMbb2HVvhp-jA7Uv07PKx8O0z=g3Kcg@mail.gmail.com>
References: <CAN20VnjYp3aGAnrkMUn-WYEck6+uTi+HwB6PbbZUC9wuPB1W1w@mail.gmail.com>
	<CY4PR02MB261618E6A2A54183C25FAC46E3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VnhU6-_S71FK_F0QateTrNxG2j_WN2BPEyx3PdhxOhGhmQ@mail.gmail.com>
	<CY4PR02MB2616E42F5584556E6172483FE3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VngUB9yN6Td29LcV0p0Ths2eFON_fx+Go94uaevvykbRDA@mail.gmail.com>
	<CY4PR02MB2616F4AE6B78CC82364A1B10E3C90@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20Vni=Do=b6vEy6d3KMbb2HVvhp-jA7Uv07PKx8O0z=g3Kcg@mail.gmail.com>
Message-ID: <CY4PR02MB2616B8AEAC1514546FD6BD82E3C80@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Sarbajit,

That is very strange ? netstat thinks there is a process listening on port 5060 on the host, but netcat doesn?t. I?m not really sure what further to suggest. I?ve got a couple of ideas for things you could do to try and narrow down the problem, but I?m afraid there?s nothing particularly concrete.

You could try stopping the Bono container and running netcat as both a server and a client on the host to check that the host hasn?t got some weird iptables rule blocking the connection. It might also be worth taking packet captures to try and work out exactly what?s going on.

Also, have you tried just deploying on a single host using Docker (i.e. not using Docker Swarm)? It would be good to verify that that works.

I hope you manage to get to the bottom of the problem!

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sarbajit Chatterjee
Sent: 22 September 2016 12:14
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Hi Graeme,

I can connect to 5060 port in 10.0.1.2 from sprout container.

root at bb818f0a5535:/# nc -v -z 10.0.1.2 5060
Connection to 10.0.1.2 5060 port [tcp/sip] succeeded!
root at bb818f0a5535:/#


Docker PS on the cluster shows bono container exposing port 5060


CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS              PORTS                                                                                                                                                                                   NAMES
bb818f0a5535        swarm-node:5000/clearwaterdocker_sprout      "/usr/bin/supervisord"   47 hours ago        Up 47 hours         5052/tcp, 5054/tcp, 10.109.190.10:32822->22/tcp                                                                                                                                         swarm-node/clearwaterdocker_sprout_1
9ff210721451        swarm-node:5000/clearwaterdocker_homer       "/usr/bin/supervisord"   47 hours ago        Up 47 hours         7888/tcp, 10.109.190.9:32788->22/tcp                                                                                                                                                    swarm-master/clearwaterdocker_homer_1
51653420c979        swarm-node:5000/clearwaterdocker_homestead   "/usr/bin/supervisord"   47 hours ago        Up 47 hours         8888-8889/tcp, 10.109.190.9:32787->22/tcp                                                                                                                                               swarm-master/clearwaterdocker_homestead_1
e994b17b4563        swarm-node:5000/clearwaterdocker_ellis       "/usr/bin/supervisord"   47 hours ago        Up 47 hours         10.109.190.10:80->80/tcp, 10.109.190.10:32821->22/tcp                                                                                                                                   swarm-node/clearwaterdocker_ellis_1
9837c4dab241        swarm-node:5000/clearwaterdocker_bono        "/usr/bin/supervisord"   47 hours ago        Up 47 hours         10.109.190.9:3478->3478/tcp, 10.109.190.9:3478->3478/udp, 10.109.190.9:5060->5060/tcp, 10.109.190.9:5062->5062/tcp, 10.109.190.9:5060->5060/udp, 5058/tcp, 10.109.190.9:32786->22/tcp   swarm-master/clearwaterdocker_bono_1
3db967c58754        swarm-node:5000/clearwaterdocker_ralf        "/usr/bin/supervisord"   47 hours ago        Up 47 hours         10888/tcp, 10.109.190.10:32820->22/tcp                                                                                                                                                  swarm-node/clearwaterdocker_ralf_1
c499d05af8e7        quay.io/coreos/etcd:v2.2.5<http://quay.io/coreos/etcd:v2.2.5>                   "/etcd -name etcd0 -a"   47 hours ago        Up 47 hours         2379-2380/tcp, 4001/tcp, 7001/tcp                                                                                                                                                       swarm-node/clearwaterdocker_etcd_1
5fe0c51979e7        ubuntu:14.04.5                               "/bin/bash"              8 days ago          Up 8 days


But I can't reach the 5060 port from the host machine where it is launched. Though the port seems to be open.


root at swarm-master:~# nc -v -z 10.109.190.9 5060
nc: connect to 10.109.190.9 port 5060 (tcp) failed: Connection refused
root at swarm-master:~#
root at swarm-master:~# netstat -anp | grep 5060
tcp6       0      0 :::5060                 :::*                    LISTEN      21818/docker-proxy
udp6       0      0 :::5060                 :::*                                21829/docker-proxy
root at swarm-master:~#


Any suggestion?

-Sarbajit


On Thu, Sep 22, 2016 at 4:08 PM, Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <g at projectclearwater.org<mailto:g at projectclearwater.org>> wrote:
Hi Sarbajit,

That output all looks fine, but it sounds as though the port mapping has failed ? i.e. port 5060 on the Bono container hasn?t been exposed as port 5060 on the host. I?m not sure why that would have failed. Can you run nc -z -v 10.0.1.2 5060 inside the Bono container (this should work, but it?s worth doing as a sanity check!). Then can you run docker ps in your clearwater-docker checkout? The output should include a line that looks something like 0b4058027844        clearwaterdocker_bono        "/usr/bin/supervisord"   About a minute ago   Up About a minute   0.0.0.0:3478->3478/tcp, 0.0.0.0:3478->3478/udp, 0.0.0.0:5060->5060/tcp, 0.0.0.0:5062->5062/tcp, 0.0.0.0:5060->5060/udp, 5058/tcp, 0.0.0.0:42513->22/tcp   clearwaterdocker_bono_1, which should indicate that the port mapping is active. If this all looks fine it might be worth also running nc -v -z 10.109.190.9 5060 on the host that?s running the Bono container.

Thanks,
Graeme

________________________________
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Sarbajit Chatterjee
Sent: 21 September 2016 19:39

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Hi Graeme,

I'm using following command to run the livetest -

rake test[example.com<http://example.com>] TESTS="Basic*" SIGNUP_CODE=secret PROXY=10.109.190.9 ELLIS=10.109.190.10

here PROXY ip is where the bono container is launched and ELLIS ip is where the ellis container is launched.

The bono service seems to be running in the container -

root at 9837c4dab241:/# ps -eaf | grep bono
root       122     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager --mgmt-local-ip=10.0.1.2 --sig-local-ip=10.0.1.2 --local-site=site1 --remote-site= --remote-cassandra-seeds= --signaling-namespace= --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater --etcd-cluster-key=bono --log-level=3 --log-directory=/var/log/clearwater-cluster-manager --pidfile=/var/run/clearwater-cluster-manager.pid
root       124     1  0 Sep19 ?        00:00:00 /bin/bash /etc/init.d/bono run
root       139   124  0 Sep19 ?        00:00:00 /bin/bash /usr/share/clearwater/bin/run-in-signaling-namespace start-stop-daemon --start --quiet --exec /usr/share/clearwater/bin/bono --chuid bono --chdir /etc/clearwater -- --domain=example.com<http://example.com> --localhost=10.0.1.2,10.0.1.2 --alias=10.0.1.2 --pcscf=5060,5058 --webrtc-port=5062 --routing-proxy=scscf.sprout,5052,50,600 --ralf=ralf:10888 --sas=0.0.0.0,bono at 10.0.1.2<mailto:bono at 10.0.1.2> --dns-server=127.0.0.11 --worker-threads=4 --analytics=/var/log/bono --log-file=/var/log/bono --log-level=2
bono       140   139  0 Sep19 ?        00:11:15 /usr/share/clearwater/bin/bono --domain=example.com<http://example.com> --localhost=10.0.1.2,10.0.1.2 --alias=10.0.1.2 --pcscf=5060,5058 --webrtc-port=5062 --routing-proxy=scscf.sprout,5052,50,600 --ralf=ralf:10888 --sas=0.0.0.0,bono at 10.0.1.2<mailto:bono at 10.0.1.2> --dns-server=127.0.0.11 --worker-threads=4 --analytics=/var/log/bono --log-file=/var/log/bono --log-level=2
root       322   293  0 17:48 ?        00:00:00 grep --color=auto bono
root at 9837c4dab241:/#
root at 9837c4dab241:/# netstat -planut | grep 5060
tcp        0      0 10.0.1.2:5060<http://10.0.1.2:5060>           0.0.0.0:*               LISTEN      -
udp        0      0 10.0.1.2:5060<http://10.0.1.2:5060>           0.0.0.0:*                           -
root at 9837c4dab241:/#


But the connection to bono is failing from livetest container as you had predicted.

root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.9 5060
nc: connect to 10.109.190.9 port 5060 (tcp) failed: Connection refused
root at 40efba73deb5:~/clearwater-live-test#


On checking the bono log, I see a series of errors like below in beginning of the log -

19-09-2016 15:41:44.612 UTC Status utils.cpp:591: Log level set to 2
19-09-2016 15:41:44.612 UTC Status main.cpp:1388: Access logging enabled to /var/log/bono
19-09-2016 15:41:44.613 UTC Warning main.cpp:1435: SAS server option was invalid or not configured - SAS is disabled
19-09-2016 15:41:44.613 UTC Warning main.cpp:1511: A registration expiry period should not be specified for P-CSCF
19-09-2016 15:41:44.613 UTC Status snmp_agent.cpp:117: AgentX agent initialised
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:105: Constructing LoadMonitor
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:106:    Target latency (usecs)   : 100000
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:107:    Max bucket size          : 1000
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:108:    Initial token fill rate/s: 100.000000
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:109:    Min token fill rate/s    : 10.000000
19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:144: Creating Cached Resolver using servers:
19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:154:     127.0.0.11
19-09-2016 15:41:44.613 UTC Status sipresolver.cpp:60: Created SIP resolver
19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5058
19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5060
19-09-2016 15:41:44.638 UTC Status stack.cpp:855: Local host aliases:
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  172.18.0.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:
19-09-2016 15:41:44.639 UTC Status httpresolver.cpp:52: Created HTTP resolver
19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:114: Configuring HTTP Connection
19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:115:   Connection created for server ralf:10888
19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:116:   Connection will use a response timeout of 500ms
19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:72: Creating connection pool to scscf.sprout:5052
19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:73:   connections = 50, recycle time = 600 +/- 120 seconds
19-09-2016 15:41:44.649 UTC Status bono.cpp:3314: Create list of PBXes
19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins
19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:158: Finished loading plug-ins
19-09-2016 15:41:44.652 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f3df8 TCP connect() error: Connection refused [code=120111]
19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f5c38 TCP connect() error: Connection refused [code=120111]


I have observed that in the bono container 5060 port is not listening in all interfaces while 5062 port is listening in all interfaces.

root at 9837c4dab241:/var/log/bono# netstat -planut | grep LISTEN
tcp        0      0 10.0.1.2:4000<http://10.0.1.2:4000>           0.0.0.0:*               LISTEN      -
tcp        0      0 10.0.1.2:5058<http://10.0.1.2:5058>           0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.11:43395<http://127.0.0.11:43395>        0.0.0.0:*               LISTEN      -
tcp        0      0 10.0.1.2:5060<http://10.0.1.2:5060>           0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:5062<http://0.0.0.0:5062>            0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.1:8080<http://127.0.0.1:8080>          0.0.0.0:*               LISTEN      -
tcp        0      0 10.0.1.2:3478<http://10.0.1.2:3478>           0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:22<http://0.0.0.0:22>              0.0.0.0:*               LISTEN      9/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      9/sshd
root at 9837c4dab241:/var/log/bono#


Is it right to have 5060 port listen on only local port? Please help me to debug the issue.

Thanks,
Sarbajit


On Wed, Sep 21, 2016 at 10:01 PM, Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>> wrote:
Hi Sarbajit,

I?ve had another look at this, and actually I think clearwater-live-test checks it can connect to Bono before it tries to provision numbers from Ellis, and it?s actually that connection that?s failing ? apologies!

Can you do similar checks for the Bono container, i.e. connect to the Bono container and run ps -eaf | grep bono and run nc -z -v <ip> 5060 from your live test container (where <ip> is the IP of your Bono)?

One other thought ? what command are you using to run the tests? You?ll need to set the PROXY option to your Bono IP and the ELLIS option to your Ellis IP.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Sarbajit Chatterjee
Sent: 21 September 2016 16:41
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Thanks Graeme for your reply. Here are the command outputs that you had asked -

root at e994b17b4563:/# ps -eaf | grep ellis
root       177     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager --mgmt-local-ip=10.0.1.7 --sig-local-ip=10.0.1.7 --local-site=site1 --remote-site= --remote-cassandra-seeds= --signaling-namespace= --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater --etcd-cluster-key=ellis --log-level=3 --log-directory=/var/log/clearwater-cluster-manager --pidfile=/var/run/clearwater-cluster-manager.pid
root       180     1  0 Sep19 ?        00:00:00 /bin/sh /etc/init.d/ellis run
ellis      185   180  0 Sep19 ?        00:00:05 /usr/share/clearwater/ellis/env/bin/python -m metaswitch.ellis.main
root       287   253  0 15:18 ?        00:00:00 grep --color=auto ellis
root at e994b17b4563:/#
root at e994b17b4563:/# ps -eaf | grep nginx
root       179     1  0 Sep19 ?        00:00:00 nginx: master process /usr/sbin/nginx -g daemon off;
www-data   186   179  0 Sep19 ?        00:00:16 nginx: worker process
www-data   187   179  0 Sep19 ?        00:00:00 nginx: worker process
www-data   188   179  0 Sep19 ?        00:00:16 nginx: worker process
www-data   189   179  0 Sep19 ?        00:00:16 nginx: worker process
root       289   253  0 15:19 ?        00:00:00 grep --color=auto nginx
root at e994b17b4563:/#
root at e994b17b4563:/# netstat -planut | grep nginx
tcp6       0      0 :::80                   :::*                    LISTEN      179/nginx -g daemon
root at e994b17b4563:/#


I think both ellis and nginx are running fine inside the container. I can also open the ellis login page from a web browser. I also checked the MySQL DB in ellis container. I can see livetest user entry in users table and 1000 rows in numbers table.

I can also connect to ellis (host IP 10.109.190.10) from my livetest container -

root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.10 80
Connection to 10.109.190.10 80 port [tcp/http] succeeded!
root at 40efba73deb5:~/clearwater-live-test#


Is this happening because Clearwater containers are spread across multiple hosts? What other areas I should check?


Thanks,
Sarbajit


On Wed, Sep 21, 2016 at 6:09 PM, Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>> wrote:
Hi Sarbajit,

I don?t think we?ve never tried deploying Project Clearwater in a Docker Swarm cluster, but I don?t see any reason why it couldn?t work. The tests are failing very early ? they?re not able to connect to Ellis on port 80. I can think of a couple of reasons for this ? either Ellis isn?t running or the Ellis port mapping hasn?t worked for some reason.

Can you connect to the Ellis container and run ps ?eaf | grep ellis and ps ?eaf | grep nginx to confirm that NGINX and Ellis are running? Can you also run sudo netstat -planut | grep nginx or something equivalent to check that NGINX is listening on port 80? If there?s a problem with either NGINX or Ellis we probably need to look in the logs at /var/log/nginx/ or /var/log/ellis/ on the Ellis container.

If however that all looks fine, then it sounds like the port mapping has failed for some reason. Can you run nc -z <ip> 80 from the box you?re running the live tests on? This will scan for anything listening at <ip>:80 and will return successfully if it finds anything.

Thanks,
Graeme

________________________________
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sarbajit Chatterjee
Sent: 20 September 2016 15:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Hello,

I am following the instructions from https://github.com/Metaswitch/clearwater-docker. I can successfully deploy it on a single Docker node but, the compose file does not work with Swarm cluster.

I did try to modify the compose file like this -


version: '2'
services:
  etcd:
    image: quay.io/coreos/etcd:v2.2.5<http://quay.io/coreos/etcd:v2.2.5>
    command: >
      -name etcd0
      -advertise-client-urls http://etcd:2379,http://etcd:4001
      -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001
      -initial-advertise-peer-urls http://etcd:2380
      -listen-peer-urls http://0.0.0.0:2380
      -initial-cluster etcd0=http://etcd:2380
      -initial-cluster-state new
  bono:
    image: swarm-node:5000/clearwaterdocker_bono
    ports:
      - 22
      - "3478:3478"
      - "3478:3478/udp"
      - "5060:5060"
      - "5060:5060/udp"
      - "5062:5062"
  sprout:
    image: swarm-node:5000/clearwaterdocker_sprout
    networks:
      default:
        aliases:
          - scscf.sprout
          - icscf.sprout
    ports:
      - 22
  homestead:
    image: swarm-node:5000/clearwaterdocker_homestead
    ports:
      - 22
  homer:
    image: swarm-node:5000/clearwaterdocker_homer
    ports:
      - 22
  ralf:
    image: swarm-node:5000/clearwaterdocker_ralf
    ports:
      - 22
  ellis:
    image: swarm-node:5000/clearwaterdocker_ellis
    ports:
      - 22
      - "80:80"


where swarm-node:5000 is the local docker registry and it hosts the pre-built images of Clearwater containers. Even though the deployment succeeded, clearwater-livetests are failing with following error -


Basic Registration (TCP) - Failed
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/local/rvm/gems/ruby-1.9.3-p551/gems/quaff-0.7.3/lib/sources.rb:41:in `initialize'


Any suggestions on how I can deploy Clearwater on a Swarm cluster?

Thanks,
Sarbajit


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160923/b32a9c24/attachment.html>

From sarbajitc at gmail.com  Fri Sep 23 07:07:07 2016
From: sarbajitc at gmail.com (Sarbajit Chatterjee)
Date: Fri, 23 Sep 2016 16:37:07 +0530
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster using
	docker-compose
In-Reply-To: <CY4PR02MB2616B8AEAC1514546FD6BD82E3C80@CY4PR02MB2616.namprd02.prod.outlook.com>
References: <CAN20VnjYp3aGAnrkMUn-WYEck6+uTi+HwB6PbbZUC9wuPB1W1w@mail.gmail.com>
	<CY4PR02MB261618E6A2A54183C25FAC46E3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VnhU6-_S71FK_F0QateTrNxG2j_WN2BPEyx3PdhxOhGhmQ@mail.gmail.com>
	<CY4PR02MB2616E42F5584556E6172483FE3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VngUB9yN6Td29LcV0p0Ths2eFON_fx+Go94uaevvykbRDA@mail.gmail.com>
	<CY4PR02MB2616F4AE6B78CC82364A1B10E3C90@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20Vni=Do=b6vEy6d3KMbb2HVvhp-jA7Uv07PKx8O0z=g3Kcg@mail.gmail.com>
	<CY4PR02MB2616B8AEAC1514546FD6BD82E3C80@CY4PR02MB2616.namprd02.prod.outlook.com>
Message-ID: <CAN20VniUfLiESAxYFka=Pe=+T+MhzBP+HbxF5L+rCCSFtCHmoA@mail.gmail.com>

Hi Graeme,

I see the iptables nat rules that docker has created on the host machine,
contains 172.18.0.2 IP of bono, not the other interface 10.0.1.2 IP.


root at swarm-master:~# iptables -t nat -L
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
DOCKER     all  --  anywhere             anywhere             ADDRTYPE
match dst-type LOCAL

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
DOCKER     all  --  anywhere            !loopback/8           ADDRTYPE
match dst-type LOCAL

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  172.18.0.0/16        anywhere
MASQUERADE  all  --  172.17.0.0/16        anywhere
MASQUERADE  tcp  --  172.17.0.2           172.17.0.2           tcp dpt:3376
MASQUERADE  tcp  --  172.17.0.5           172.17.0.5           tcp
dpt:http-alt
MASQUERADE  tcp  --  172.18.0.2           172.18.0.2           tcp dpt:5062
MASQUERADE  tcp  --  172.18.0.2           172.18.0.2           tcp dpt:sip
MASQUERADE  udp  --  172.18.0.2           172.18.0.2           udp dpt:sip
MASQUERADE  tcp  --  172.18.0.2           172.18.0.2           tcp dpt:3478
MASQUERADE  udp  --  172.18.0.2           172.18.0.2           udp dpt:3478
MASQUERADE  tcp  --  172.18.0.2           172.18.0.2           tcp dpt:ssh
MASQUERADE  tcp  --  172.18.0.3           172.18.0.3           tcp dpt:ssh
MASQUERADE  tcp  --  172.18.0.4           172.18.0.4           tcp dpt:ssh

Chain DOCKER (2 references)
target     prot opt source               destination
RETURN     all  --  anywhere             anywhere
RETURN     all  --  anywhere             anywhere
DNAT       tcp  --  anywhere             anywhere             tcp dpt:3376
to:172.17.0.2:3376
DNAT       tcp  --  anywhere             anywhere             tcp dpt:32774
to:172.17.0.5:8080
DNAT       tcp  --  anywhere             anywhere             tcp dpt:5062
to:172.18.0.2:5062
DNAT       tcp  --  anywhere             anywhere             tcp dpt:sip
to:172.18.0.2:5060
DNAT       udp  --  anywhere             anywhere             udp dpt:sip
to:172.18.0.2:5060
DNAT       tcp  --  anywhere             anywhere             tcp dpt:3478
to:172.18.0.2:3478
DNAT       udp  --  anywhere             anywhere             udp dpt:3478
to:172.18.0.2:3478
DNAT       tcp  --  anywhere             anywhere             tcp dpt:32786
to:172.18.0.2:22
DNAT       tcp  --  anywhere             anywhere             tcp dpt:32787
to:172.18.0.3:22
DNAT       tcp  --  anywhere             anywhere             tcp dpt:32788
to:172.18.0.4:22
root at swarm-master:~#


The bono container is opening 5060 port only on 10.0.1.2 IP. So, even if
the host machine is exposing the port, container is unable to get the
packets.
Do you agree? Is there a way I can configure the bono container to run on
all interfaces?


-Sarbajit

P.S. I deployed Clearwater on a single node docker and that works fine.



On Fri, Sep 23, 2016 at 4:22 PM, Graeme Robertson (projectclearwater.org) <
g at projectclearwater.org> wrote:

> Hi Sarbajit,
>
>
>
> That is very strange ? netstat thinks there is a process listening on port
> 5060 on the host, but netcat doesn?t. I?m not really sure what further to
> suggest. I?ve got a couple of ideas for things you could do to try and
> narrow down the problem, but I?m afraid there?s nothing particularly
> concrete.
>
>
>
> You could try stopping the Bono container and running netcat as both a
> server and a client on the host to check that the host hasn?t got some
> weird iptables rule blocking the connection. It might also be worth taking
> packet captures to try and work out exactly what?s going on.
>
>
>
> Also, have you tried just deploying on a single host using Docker (i.e.
> not using Docker Swarm)? It would be good to verify that that works.
>
>
>
> I hope you manage to get to the bottom of the problem!
>
>
>
> Thanks,
> Graeme
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sarbajit Chatterjee
> *Sent:* 22 September 2016 12:14
>
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Hi Graeme,
>
>
>
> I can connect to 5060 port in 10.0.1.2 from sprout container.
>
>
>
> root at bb818f0a5535:/# nc -v -z 10.0.1.2 5060
>
> Connection to 10.0.1.2 5060 port [tcp/sip] succeeded!
>
> root at bb818f0a5535:/#
>
>
>
>
>
> Docker PS on the cluster shows bono container exposing port 5060
>
>
>
>
>
> CONTAINER ID        IMAGE                                        COMMAND
>                CREATED             STATUS              PORTS
>
>
>             NAMES
>
> bb818f0a5535        swarm-node:5000/clearwaterdocker_sprout
>  "/usr/bin/supervisord"   47 hours ago        Up 47 hours         5052/tcp,
> 5054/tcp, 10.109.190.10:32822->22/tcp
>
>                         swarm-node/clearwaterdocker_sprout_1
>
> 9ff210721451        swarm-node:5000/clearwaterdocker_homer
> "/usr/bin/supervisord"   47 hours ago        Up 47 hours         7888/tcp,
> 10.109.190.9:32788->22/tcp
>
>                          swarm-master/clearwaterdocker_homer_1
>
> 51653420c979        swarm-node:5000/clearwaterdocker_homestead
> "/usr/bin/supervisord"   47 hours ago        Up 47 hours
> 8888-8889/tcp, 10.109.190.9:32787->22/tcp
>
>                                   swarm-master/clearwaterdocker_
> homestead_1
>
> e994b17b4563        swarm-node:5000/clearwaterdocker_ellis
> "/usr/bin/supervisord"   47 hours ago        Up 47 hours
> 10.109.190.10:80->80/tcp, 10.109.190.10:32821->22/tcp
>
>                                   swarm-node/clearwaterdocker_ellis_1
>
> 9837c4dab241        swarm-node:5000/clearwaterdocker_bono
>  "/usr/bin/supervisord"   47 hours ago        Up 47 hours
> 10.109.190.9:3478->3478/tcp, 10.109.190.9:3478->3478/udp,
> 10.109.190.9:5060->5060/tcp, 10.109.190.9:5062->5062/tcp,
> 10.109.190.9:5060->5060/udp, 5058/tcp, 10.109.190.9:32786->22/tcp
> swarm-master/clearwaterdocker_bono_1
>
> 3db967c58754        swarm-node:5000/clearwaterdocker_ralf
>  "/usr/bin/supervisord"   47 hours ago        Up 47 hours
> 10888/tcp, 10.109.190.10:32820->22/tcp
>
>                                    swarm-node/clearwaterdocker_ralf_1
>
> c499d05af8e7        quay.io/coreos/etcd:v2.2.5                   "/etcd
> -name etcd0 -a"   47 hours ago        Up 47 hours         2379-2380/tcp,
> 4001/tcp, 7001/tcp
>
>                   swarm-node/clearwaterdocker_etcd_1
>
> 5fe0c51979e7        ubuntu:14.04.5
> "/bin/bash"              8 days ago          Up 8 days
>
>
>
>
>
>
> But I can't reach the 5060 port from the host machine where it is
> launched. Though the port seems to be open.
>
>
>
>
>
> root at swarm-master:~# nc -v -z 10.109.190.9 5060
>
> nc: connect to 10.109.190.9 port 5060 (tcp) failed: Connection refused
>
> root at swarm-master:~#
>
> root at swarm-master:~# netstat -anp | grep 5060
>
> tcp6       0      0 :::5060                 :::*                    LISTEN
>      21818/docker-proxy
>
> udp6       0      0 :::5060                 :::*
>      21829/docker-proxy
>
> root at swarm-master:~#
>
>
>
>
>
> Any suggestion?
>
>
>
> -Sarbajit
>
>
>
>
>
> On Thu, Sep 22, 2016 at 4:08 PM, Graeme Robertson (projectclearwater.org)
> <g at projectclearwater.org> wrote:
>
> Hi Sarbajit,
>
>
>
> That output all looks fine, but it sounds as though the port mapping has
> failed ? i.e. port 5060 on the Bono container hasn?t been exposed as port
> 5060 on the host. I?m not sure why that would have failed. Can you run nc
> -z -v 10.0.1.2 5060 inside the Bono container (this should work, but it?s
> worth doing as a sanity check!). Then can you run docker ps in your
> clearwater-docker checkout? The output should include a line that looks
> something like 0b4058027844        clearwaterdocker_bono
> "/usr/bin/supervisord"   About a minute ago   Up About a minute
> 0.0.0.0:3478->3478/tcp, 0.0.0.0:3478->3478/udp, *0.0.0.0:5060->5060/tcp*,
> 0.0.0.0:5062->5062/tcp, * 0.0.0.0:5060->5060/udp*, 5058/tcp, 0.0.0.0:42513->22/tcp
> clearwaterdocker_bono_1, which should indicate that the port mapping is
> active. If this all looks fine it might be worth also running nc -v -z
> 10.109.190.9 5060 on the host that?s running the Bono container.
>
>
>
> Thanks,
>
> Graeme
>
>
> ------------------------------
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sarbajit Chatterjee
> *Sent:* 21 September 2016 19:39
>
>
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Hi Graeme,
>
>
>
> I'm using following command to run the livetest -
>
>
>
> rake test[example.com] TESTS="Basic*" SIGNUP_CODE=secret
> PROXY=10.109.190.9 ELLIS=10.109.190.10
>
>
>
> here PROXY ip is where the bono container is launched and ELLIS ip is
> where the ellis container is launched.
>
>
>
> The bono service seems to be running in the container -
>
>
>
> root at 9837c4dab241:/# ps -eaf | grep bono
>
> root       122     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/
> clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager
> --mgmt-local-ip=10.0.1.2 --sig-local-ip=10.0.1.2 --local-site=site1
> --remote-site= --remote-cassandra-seeds= --signaling-namespace=
> --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater
> --etcd-cluster-key=bono --log-level=3 --log-directory=/var/log/clearwater-cluster-manager
> --pidfile=/var/run/clearwater-cluster-manager.pid
>
> root       124     1  0 Sep19 ?        00:00:00 /bin/bash /etc/init.d/bono
> run
>
> root       139   124  0 Sep19 ?        00:00:00 /bin/bash
> /usr/share/clearwater/bin/run-in-signaling-namespace start-stop-daemon
> --start --quiet --exec /usr/share/clearwater/bin/bono --chuid bono --chdir
> /etc/clearwater -- --domain=example.com --localhost=10.0.1.2,10.0.1.2
> --alias=10.0.1.2 --pcscf=5060,5058 --webrtc-port=5062
> --routing-proxy=scscf.sprout,5052,50,600 --ralf=ralf:10888 --sas=0.0.0.0,
> bono at 10.0.1.2 --dns-server=127.0.0.11 --worker-threads=4
> --analytics=/var/log/bono --log-file=/var/log/bono --log-level=2
>
> bono       140   139  0 Sep19 ?        00:11:15
> /usr/share/clearwater/bin/bono --domain=example.com
> --localhost=10.0.1.2,10.0.1.2 --alias=10.0.1.2 --pcscf=5060,5058
> --webrtc-port=5062 --routing-proxy=scscf.sprout,5052,50,600
> --ralf=ralf:10888 --sas=0.0.0.0,bono at 10.0.1.2 --dns-server=127.0.0.11
> --worker-threads=4 --analytics=/var/log/bono --log-file=/var/log/bono
> --log-level=2
>
> root       322   293  0 17:48 ?        00:00:00 grep --color=auto bono
>
> root at 9837c4dab241:/#
>
> root at 9837c4dab241:/# netstat -planut | grep 5060
>
> tcp        0      0 10.0.1.2:5060           0.0.0.0:*
> LISTEN      -
>
> udp        0      0 10.0.1.2:5060           0.0.0.0:*
>       -
>
> root at 9837c4dab241:/#
>
>
>
>
>
> But the connection to bono is failing from livetest container as you had
> predicted.
>
>
>
> root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.9 5060
>
> nc: connect to 10.109.190.9 port 5060 (tcp) failed: Connection refused
>
> root at 40efba73deb5:~/clearwater-live-test#
>
>
>
>
>
> On checking the bono log, I see a series of errors like below
> in beginning of the log -
>
>
>
> 19-09-2016 15:41:44.612 UTC Status utils.cpp:591: Log level set to 2
>
> 19-09-2016 15:41:44.612 UTC Status main.cpp:1388: Access logging enabled
> to /var/log/bono
>
> 19-09-2016 15:41:44.613 UTC Warning main.cpp:1435: SAS server option was
> invalid or not configured - SAS is disabled
>
> 19-09-2016 15:41:44.613 UTC Warning main.cpp:1511: A registration expiry
> period should not be specified for P-CSCF
>
> 19-09-2016 15:41:44.613 UTC Status snmp_agent.cpp:117: AgentX agent
> initialised
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:105: Constructing
> LoadMonitor
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:106:    Target latency
> (usecs)   : 100000
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:107:    Max bucket
> size          : 1000
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:108:    Initial token
> fill rate/s: 100.000000
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:109:    Min token fill
> rate/s    : 10.000000
>
> 19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:144: Creating
> Cached Resolver using servers:
>
> 19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:154:
> 127.0.0.11
>
> 19-09-2016 15:41:44.613 UTC Status sipresolver.cpp:60: Created SIP resolver
>
> 19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5058
>
> 19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5060
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:855: Local host aliases:
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  172.18.0.2
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:
>
> 19-09-2016 15:41:44.639 UTC Status httpresolver.cpp:52: Created HTTP
> resolver
>
> 19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:114: Configuring
> HTTP Connection
>
> 19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:115:   Connection
> created for server ralf:10888
>
> 19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:116:   Connection
> will use a response timeout of 500ms
>
> 19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:72: Creating
> connection pool to scscf.sprout:5052
>
> 19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:73:   connections =
> 50, recycle time = 600 +/- 120 seconds
>
> 19-09-2016 15:41:44.649 UTC Status bono.cpp:3314: Create list of PBXes
>
> 19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:63: Loading plug-ins
> from /usr/share/clearwater/sprout/plugins
>
> 19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:158: Finished loading
> plug-ins
>
> 19-09-2016 15:41:44.652 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
> 19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f3df8 TCP connect()
> error: Connection refused [code=120111]
>
> 19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f5c38 TCP connect()
> error: Connection refused [code=120111]
>
>
>
>
>
> I have observed that in the bono container 5060 port is not listening in
> all interfaces while 5062 port is listening in all interfaces.
>
>
>
> root at 9837c4dab241:/var/log/bono# netstat -planut | grep LISTEN
>
> tcp        0      0 10.0.1.2:4000           0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 10.0.1.2:5058           0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 127.0.0.11:43395        0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 10.0.1.2:5060           0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 0.0.0.0:5062            0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 127.0.0.1:8080          0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 10.0.1.2:3478           0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 0.0.0.0:22              0.0.0.0:*
> LISTEN      9/sshd
>
> tcp6       0      0 :::22                   :::*                    LISTEN
>      9/sshd
>
> root at 9837c4dab241:/var/log/bono#
>
>
>
>
>
> Is it right to have 5060 port listen on only local port? Please help me to
> debug the issue.
>
>
>
> Thanks,
>
> Sarbajit
>
>
>
>
>
> On Wed, Sep 21, 2016 at 10:01 PM, Graeme Robertson (projectclearwater.org)
> <graeme at projectclearwater.org> wrote:
>
> Hi Sarbajit,
>
>
>
> I?ve had another look at this, and actually I think clearwater-live-test
> checks it can connect to Bono before it tries to provision numbers from
> Ellis, and it?s actually that connection that?s failing ? apologies!
>
>
>
> Can you do similar checks for the Bono container, i.e. connect to the Bono
> container and run ps -eaf | grep bono and run nc -z -v <ip> 5060 from
> your live test container (where <ip> is the IP of your Bono)?
>
>
>
> One other thought ? what command are you using to run the tests? You?ll
> need to set the PROXY option to your Bono IP and the ELLIS option to your
> Ellis IP.
>
>
>
> Thanks,
>
> Graeme
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sarbajit Chatterjee
> *Sent:* 21 September 2016 16:41
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Thanks Graeme for your reply. Here are the command outputs that you had
> asked -
>
>
>
> root at e994b17b4563:/# ps -eaf | grep ellis
>
> root       177     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/
> clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager
> --mgmt-local-ip=10.0.1.7 --sig-local-ip=10.0.1.7 --local-site=site1
> --remote-site= --remote-cassandra-seeds= --signaling-namespace=
> --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater
> --etcd-cluster-key=ellis --log-level=3 --log-directory=/var/log/clearwater-cluster-manager
> --pidfile=/var/run/clearwater-cluster-manager.pid
>
> root       180     1  0 Sep19 ?        00:00:00 /bin/sh /etc/init.d/ellis
> run
>
> ellis      185   180  0 Sep19 ?        00:00:05
> /usr/share/clearwater/ellis/env/bin/python -m metaswitch.ellis.main
>
> root       287   253  0 15:18 ?        00:00:00 grep --color=auto ellis
>
> root at e994b17b4563:/#
>
> root at e994b17b4563:/# ps -eaf | grep nginx
>
> root       179     1  0 Sep19 ?        00:00:00 nginx: master process
> /usr/sbin/nginx -g daemon off;
>
> www-data   186   179  0 Sep19 ?        00:00:16 nginx: worker process
>
> www-data   187   179  0 Sep19 ?        00:00:00 nginx: worker process
>
> www-data   188   179  0 Sep19 ?        00:00:16 nginx: worker process
>
> www-data   189   179  0 Sep19 ?        00:00:16 nginx: worker process
>
> root       289   253  0 15:19 ?        00:00:00 grep --color=auto nginx
>
> root at e994b17b4563:/#
>
> root at e994b17b4563:/# netstat -planut | grep nginx
>
> tcp6       0      0 :::80                   :::*                    LISTEN
>      179/nginx -g daemon
>
> root at e994b17b4563:/#
>
>
>
>
>
> I think both ellis and nginx are running fine inside the container. I can
> also open the ellis login page from a web browser. I also checked the MySQL
> DB in ellis container. I can see livetest user entry in users table and
> 1000 rows in numbers table.
>
>
>
> I can also connect to ellis (host IP 10.109.190.10) from my livetest
> container -
>
>
>
> root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.10 80
>
> Connection to 10.109.190.10 80 port [tcp/http] succeeded!
>
> root at 40efba73deb5:~/clearwater-live-test#
>
>
>
>
>
> Is this happening because Clearwater containers are spread across multiple
> hosts? What other areas I should check?
>
>
>
>
>
> Thanks,
>
> Sarbajit
>
>
>
>
>
> On Wed, Sep 21, 2016 at 6:09 PM, Graeme Robertson (projectclearwater.org)
> <graeme at projectclearwater.org> wrote:
>
> Hi Sarbajit,
>
>
>
> I don?t think we?ve never tried deploying Project Clearwater in a Docker
> Swarm cluster, but I don?t see any reason why it couldn?t work. The tests
> are failing very early ? they?re not able to connect to Ellis on port 80. I
> can think of a couple of reasons for this ? either Ellis isn?t running or
> the Ellis port mapping hasn?t worked for some reason.
>
>
>
> Can you connect to the Ellis container and run ps ?eaf | grep ellis and ps
> ?eaf | grep nginx to confirm that NGINX and Ellis are running? Can you
> also run sudo netstat -planut | grep nginx or something equivalent to
> check that NGINX is listening on port 80? If there?s a problem with either
> NGINX or Ellis we probably need to look in the logs at /var/log/nginx/ or
> /var/log/ellis/ on the Ellis container.
>
>
>
> If however that all looks fine, then it sounds like the port mapping has
> failed for some reason. Can you run nc -z <ip> 80 from the box you?re
> running the live tests on? This will scan for anything listening at
> <ip>:80 and will return successfully if it finds anything.
>
>
>
> Thanks,
>
> Graeme
>
>
> ------------------------------
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org
> <clearwater-bounces at lists.projectclearwater.org>] *On Behalf Of *Sarbajit
> Chatterjee
> *Sent:* 20 September 2016 15:05
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Hello,
>
>
>
> I am following the instructions from https://github.com/
> Metaswitch/clearwater-docker. I can successfully deploy it on a single
> Docker node but, the compose file does not work with Swarm cluster.
>
>
>
> I did try to modify the compose file like this -
>
>
>
>
>
> version: '2'
>
> services:
>
>   etcd:
>
>     image: quay.io/coreos/etcd:v2.2.5
>
>     command: >
>
>       -name etcd0
>
>       -advertise-client-urls http://etcd:2379,http://etcd:4001
>
>       -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001
>
>       -initial-advertise-peer-urls http://etcd:2380
>
>       -listen-peer-urls http://0.0.0.0:2380
>
>       -initial-cluster etcd0=http://etcd:2380
>
>       -initial-cluster-state new
>
>   bono:
>
>     image: swarm-node:5000/clearwaterdocker_bono
>
>     ports:
>
>       - 22
>
>       - "3478:3478"
>
>       - "3478:3478/udp"
>
>       - "5060:5060"
>
>       - "5060:5060/udp"
>
>       - "5062:5062"
>
>   sprout:
>
>     image: swarm-node:5000/clearwaterdocker_sprout
>
>     networks:
>
>       default:
>
>         aliases:
>
>           - scscf.sprout
>
>           - icscf.sprout
>
>     ports:
>
>       - 22
>
>   homestead:
>
>     image: swarm-node:5000/clearwaterdocker_homestead
>
>     ports:
>
>       - 22
>
>   homer:
>
>     image: swarm-node:5000/clearwaterdocker_homer
>
>     ports:
>
>       - 22
>
>   ralf:
>
>     image: swarm-node:5000/clearwaterdocker_ralf
>
>     ports:
>
>       - 22
>
>   ellis:
>
>     image: swarm-node:5000/clearwaterdocker_ellis
>
>     ports:
>
>       - 22
>
>       - "80:80"
>
>
>
>
>
> where swarm-node:5000 is the local docker registry and it hosts the
> pre-built images of Clearwater containers. Even though the deployment
> succeeded, clearwater-livetests are failing with following error -
>
>
>
>
>
> Basic Registration (TCP) - Failed
>
>   Errno::ECONNREFUSED thrown:
>
>    - Connection refused - connect(2)
>
>      - /usr/local/rvm/gems/ruby-1.9.3-p551/gems/quaff-0.7.3/lib/sources.rb:41:in
> `initialize'
>
>
>
>
>
> Any suggestions on how I can deploy Clearwater on a Swarm cluster?
>
>
>
> Thanks,
>
> Sarbajit
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160923/147df706/attachment.html>

From g at projectclearwater.org  Fri Sep 23 12:05:25 2016
From: g at projectclearwater.org (Graeme Robertson (projectclearwater.org))
Date: Fri, 23 Sep 2016 16:05:25 +0000
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster
	using	docker-compose
In-Reply-To: <CAN20VniUfLiESAxYFka=Pe=+T+MhzBP+HbxF5L+rCCSFtCHmoA@mail.gmail.com>
References: <CAN20VnjYp3aGAnrkMUn-WYEck6+uTi+HwB6PbbZUC9wuPB1W1w@mail.gmail.com>
	<CY4PR02MB261618E6A2A54183C25FAC46E3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VnhU6-_S71FK_F0QateTrNxG2j_WN2BPEyx3PdhxOhGhmQ@mail.gmail.com>
	<CY4PR02MB2616E42F5584556E6172483FE3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VngUB9yN6Td29LcV0p0Ths2eFON_fx+Go94uaevvykbRDA@mail.gmail.com>
	<CY4PR02MB2616F4AE6B78CC82364A1B10E3C90@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20Vni=Do=b6vEy6d3KMbb2HVvhp-jA7Uv07PKx8O0z=g3Kcg@mail.gmail.com>
	<CY4PR02MB2616B8AEAC1514546FD6BD82E3C80@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VniUfLiESAxYFka=Pe=+T+MhzBP+HbxF5L+rCCSFtCHmoA@mail.gmail.com>
Message-ID: <CY4PR02MB2616AD64C1A83AF2536678D5E3C80@CY4PR02MB2616.namprd02.prod.outlook.com>

Hi Sarbajit,

Ah, ok ? that makes sense! I didn?t realise your Bono container had multiple interfaces. Unfortunately Bono doesn?t support listening on multiple interfaces. It is listening on 10.0.1.2 because that is what local_ip is set to in /etc/clearwater/local_config, and that was determined by this script<https://github.com/Metaswitch/clearwater-infrastructure/blob/566ccbb9913ccb9dfbb452a9185b7bf93c32bc1d/debian/clearwater-auto-config-docker.init.d#L61>, which uses the first IP address it finds from the output of running hostname. You could potentially edit that script to find the ?right? IP address, but it would be much easier if your Bono container just had one interface.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sarbajit Chatterjee
Sent: 23 September 2016 12:07
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Hi Graeme,

I see the iptables nat rules that docker has created on the host machine, contains 172.18.0.2 IP of bono, not the other interface 10.0.1.2 IP.


root at swarm-master:~# iptables -t nat -L
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
DOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
DOCKER     all  --  anywhere            !loopback/8           ADDRTYPE match dst-type LOCAL

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  172.18.0.0/16<http://172.18.0.0/16>        anywhere
MASQUERADE  all  --  172.17.0.0/16<http://172.17.0.0/16>        anywhere
MASQUERADE  tcp  --  172.17.0.2           172.17.0.2           tcp dpt:3376
MASQUERADE  tcp  --  172.17.0.5           172.17.0.5           tcp dpt:http-alt
MASQUERADE  tcp  --  172.18.0.2           172.18.0.2           tcp dpt:5062
MASQUERADE  tcp  --  172.18.0.2           172.18.0.2           tcp dpt:sip
MASQUERADE  udp  --  172.18.0.2           172.18.0.2           udp dpt:sip
MASQUERADE  tcp  --  172.18.0.2           172.18.0.2           tcp dpt:3478
MASQUERADE  udp  --  172.18.0.2           172.18.0.2           udp dpt:3478
MASQUERADE  tcp  --  172.18.0.2           172.18.0.2           tcp dpt:ssh
MASQUERADE  tcp  --  172.18.0.3           172.18.0.3           tcp dpt:ssh
MASQUERADE  tcp  --  172.18.0.4           172.18.0.4           tcp dpt:ssh

Chain DOCKER (2 references)
target     prot opt source               destination
RETURN     all  --  anywhere             anywhere
RETURN     all  --  anywhere             anywhere
DNAT       tcp  --  anywhere             anywhere             tcp dpt:3376 to:172.17.0.2:3376<http://172.17.0.2:3376>
DNAT       tcp  --  anywhere             anywhere             tcp dpt:32774 to:172.17.0.5:8080<http://172.17.0.5:8080>
DNAT       tcp  --  anywhere             anywhere             tcp dpt:5062 to:172.18.0.2:5062<http://172.18.0.2:5062>
DNAT       tcp  --  anywhere             anywhere             tcp dpt:sip to:172.18.0.2:5060<http://172.18.0.2:5060>
DNAT       udp  --  anywhere             anywhere             udp dpt:sip to:172.18.0.2:5060<http://172.18.0.2:5060>
DNAT       tcp  --  anywhere             anywhere             tcp dpt:3478 to:172.18.0.2:3478<http://172.18.0.2:3478>
DNAT       udp  --  anywhere             anywhere             udp dpt:3478 to:172.18.0.2:3478<http://172.18.0.2:3478>
DNAT       tcp  --  anywhere             anywhere             tcp dpt:32786 to:172.18.0.2:22<http://172.18.0.2:22>
DNAT       tcp  --  anywhere             anywhere             tcp dpt:32787 to:172.18.0.3:22<http://172.18.0.3:22>
DNAT       tcp  --  anywhere             anywhere             tcp dpt:32788 to:172.18.0.4:22<http://172.18.0.4:22>
root at swarm-master:~#


The bono container is opening 5060 port only on 10.0.1.2 IP. So, even if the host machine is exposing the port, container is unable to get the packets.
Do you agree? Is there a way I can configure the bono container to run on all interfaces?


-Sarbajit

P.S. I deployed Clearwater on a single node docker and that works fine.



On Fri, Sep 23, 2016 at 4:22 PM, Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <g at projectclearwater.org<mailto:g at projectclearwater.org>> wrote:
Hi Sarbajit,

That is very strange ? netstat thinks there is a process listening on port 5060 on the host, but netcat doesn?t. I?m not really sure what further to suggest. I?ve got a couple of ideas for things you could do to try and narrow down the problem, but I?m afraid there?s nothing particularly concrete.

You could try stopping the Bono container and running netcat as both a server and a client on the host to check that the host hasn?t got some weird iptables rule blocking the connection. It might also be worth taking packet captures to try and work out exactly what?s going on.

Also, have you tried just deploying on a single host using Docker (i.e. not using Docker Swarm)? It would be good to verify that that works.

I hope you manage to get to the bottom of the problem!

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Sarbajit Chatterjee
Sent: 22 September 2016 12:14

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Hi Graeme,

I can connect to 5060 port in 10.0.1.2 from sprout container.

root at bb818f0a5535:/# nc -v -z 10.0.1.2 5060
Connection to 10.0.1.2 5060 port [tcp/sip] succeeded!
root at bb818f0a5535:/#


Docker PS on the cluster shows bono container exposing port 5060


CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS              PORTS                                                                                                                                                                                   NAMES
bb818f0a5535        swarm-node:5000/clearwaterdocker_sprout      "/usr/bin/supervisord"   47 hours ago        Up 47 hours         5052/tcp, 5054/tcp, 10.109.190.10:32822->22/tcp                                                                                                                                         swarm-node/clearwaterdocker_sprout_1
9ff210721451        swarm-node:5000/clearwaterdocker_homer       "/usr/bin/supervisord"   47 hours ago        Up 47 hours         7888/tcp, 10.109.190.9:32788->22/tcp                                                                                                                                                    swarm-master/clearwaterdocker_homer_1
51653420c979        swarm-node:5000/clearwaterdocker_homestead   "/usr/bin/supervisord"   47 hours ago        Up 47 hours         8888-8889/tcp, 10.109.190.9:32787->22/tcp                                                                                                                                               swarm-master/clearwaterdocker_homestead_1
e994b17b4563        swarm-node:5000/clearwaterdocker_ellis       "/usr/bin/supervisord"   47 hours ago        Up 47 hours         10.109.190.10:80->80/tcp, 10.109.190.10:32821->22/tcp                                                                                                                                   swarm-node/clearwaterdocker_ellis_1
9837c4dab241        swarm-node:5000/clearwaterdocker_bono        "/usr/bin/supervisord"   47 hours ago        Up 47 hours         10.109.190.9:3478->3478/tcp, 10.109.190.9:3478->3478/udp, 10.109.190.9:5060->5060/tcp, 10.109.190.9:5062->5062/tcp, 10.109.190.9:5060->5060/udp, 5058/tcp, 10.109.190.9:32786->22/tcp   swarm-master/clearwaterdocker_bono_1
3db967c58754        swarm-node:5000/clearwaterdocker_ralf        "/usr/bin/supervisord"   47 hours ago        Up 47 hours         10888/tcp, 10.109.190.10:32820->22/tcp                                                                                                                                                  swarm-node/clearwaterdocker_ralf_1
c499d05af8e7        quay.io/coreos/etcd:v2.2.5<http://quay.io/coreos/etcd:v2.2.5>                   "/etcd -name etcd0 -a"   47 hours ago        Up 47 hours         2379-2380/tcp, 4001/tcp, 7001/tcp                                                                                                                                                       swarm-node/clearwaterdocker_etcd_1
5fe0c51979e7        ubuntu:14.04.5                               "/bin/bash"              8 days ago          Up 8 days


But I can't reach the 5060 port from the host machine where it is launched. Though the port seems to be open.


root at swarm-master:~# nc -v -z 10.109.190.9 5060
nc: connect to 10.109.190.9 port 5060 (tcp) failed: Connection refused
root at swarm-master:~#
root at swarm-master:~# netstat -anp | grep 5060
tcp6       0      0 :::5060                 :::*                    LISTEN      21818/docker-proxy
udp6       0      0 :::5060                 :::*                                21829/docker-proxy
root at swarm-master:~#


Any suggestion?

-Sarbajit


On Thu, Sep 22, 2016 at 4:08 PM, Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <g at projectclearwater.org<mailto:g at projectclearwater.org>> wrote:
Hi Sarbajit,

That output all looks fine, but it sounds as though the port mapping has failed ? i.e. port 5060 on the Bono container hasn?t been exposed as port 5060 on the host. I?m not sure why that would have failed. Can you run nc -z -v 10.0.1.2 5060 inside the Bono container (this should work, but it?s worth doing as a sanity check!). Then can you run docker ps in your clearwater-docker checkout? The output should include a line that looks something like 0b4058027844        clearwaterdocker_bono        "/usr/bin/supervisord"   About a minute ago   Up About a minute   0.0.0.0:3478->3478/tcp, 0.0.0.0:3478->3478/udp, 0.0.0.0:5060->5060/tcp, 0.0.0.0:5062->5062/tcp, 0.0.0.0:5060->5060/udp, 5058/tcp, 0.0.0.0:42513->22/tcp   clearwaterdocker_bono_1, which should indicate that the port mapping is active. If this all looks fine it might be worth also running nc -v -z 10.109.190.9 5060 on the host that?s running the Bono container.

Thanks,
Graeme

________________________________
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Sarbajit Chatterjee
Sent: 21 September 2016 19:39

To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Hi Graeme,

I'm using following command to run the livetest -

rake test[example.com<http://example.com>] TESTS="Basic*" SIGNUP_CODE=secret PROXY=10.109.190.9 ELLIS=10.109.190.10

here PROXY ip is where the bono container is launched and ELLIS ip is where the ellis container is launched.

The bono service seems to be running in the container -

root at 9837c4dab241:/# ps -eaf | grep bono
root       122     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager --mgmt-local-ip=10.0.1.2 --sig-local-ip=10.0.1.2 --local-site=site1 --remote-site= --remote-cassandra-seeds= --signaling-namespace= --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater --etcd-cluster-key=bono --log-level=3 --log-directory=/var/log/clearwater-cluster-manager --pidfile=/var/run/clearwater-cluster-manager.pid
root       124     1  0 Sep19 ?        00:00:00 /bin/bash /etc/init.d/bono run
root       139   124  0 Sep19 ?        00:00:00 /bin/bash /usr/share/clearwater/bin/run-in-signaling-namespace start-stop-daemon --start --quiet --exec /usr/share/clearwater/bin/bono --chuid bono --chdir /etc/clearwater -- --domain=example.com<http://example.com> --localhost=10.0.1.2,10.0.1.2 --alias=10.0.1.2 --pcscf=5060,5058 --webrtc-port=5062 --routing-proxy=scscf.sprout,5052,50,600 --ralf=ralf:10888 --sas=0.0.0.0,bono at 10.0.1.2<mailto:bono at 10.0.1.2> --dns-server=127.0.0.11 --worker-threads=4 --analytics=/var/log/bono --log-file=/var/log/bono --log-level=2
bono       140   139  0 Sep19 ?        00:11:15 /usr/share/clearwater/bin/bono --domain=example.com<http://example.com> --localhost=10.0.1.2,10.0.1.2 --alias=10.0.1.2 --pcscf=5060,5058 --webrtc-port=5062 --routing-proxy=scscf.sprout,5052,50,600 --ralf=ralf:10888 --sas=0.0.0.0,bono at 10.0.1.2<mailto:bono at 10.0.1.2> --dns-server=127.0.0.11 --worker-threads=4 --analytics=/var/log/bono --log-file=/var/log/bono --log-level=2
root       322   293  0 17:48 ?        00:00:00 grep --color=auto bono
root at 9837c4dab241:/#
root at 9837c4dab241:/# netstat -planut | grep 5060
tcp        0      0 10.0.1.2:5060<http://10.0.1.2:5060>           0.0.0.0:*               LISTEN      -
udp        0      0 10.0.1.2:5060<http://10.0.1.2:5060>           0.0.0.0:*                           -
root at 9837c4dab241:/#


But the connection to bono is failing from livetest container as you had predicted.

root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.9 5060
nc: connect to 10.109.190.9 port 5060 (tcp) failed: Connection refused
root at 40efba73deb5:~/clearwater-live-test#


On checking the bono log, I see a series of errors like below in beginning of the log -

19-09-2016 15:41:44.612 UTC Status utils.cpp:591: Log level set to 2
19-09-2016 15:41:44.612 UTC Status main.cpp:1388: Access logging enabled to /var/log/bono
19-09-2016 15:41:44.613 UTC Warning main.cpp:1435: SAS server option was invalid or not configured - SAS is disabled
19-09-2016 15:41:44.613 UTC Warning main.cpp:1511: A registration expiry period should not be specified for P-CSCF
19-09-2016 15:41:44.613 UTC Status snmp_agent.cpp:117: AgentX agent initialised
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:105: Constructing LoadMonitor
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:106:    Target latency (usecs)   : 100000
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:107:    Max bucket size          : 1000
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:108:    Initial token fill rate/s: 100.000000
19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:109:    Min token fill rate/s    : 10.000000
19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:144: Creating Cached Resolver using servers:
19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:154:     127.0.0.11
19-09-2016 15:41:44.613 UTC Status sipresolver.cpp:60: Created SIP resolver
19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5058
19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5060
19-09-2016 15:41:44.638 UTC Status stack.cpp:855: Local host aliases:
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  172.18.0.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
19-09-2016 15:41:44.638 UTC Status stack.cpp:862:
19-09-2016 15:41:44.639 UTC Status httpresolver.cpp:52: Created HTTP resolver
19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:114: Configuring HTTP Connection
19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:115:   Connection created for server ralf:10888
19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:116:   Connection will use a response timeout of 500ms
19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:72: Creating connection pool to scscf.sprout:5052
19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:73:   connections = 50, recycle time = 600 +/- 120 seconds
19-09-2016 15:41:44.649 UTC Status bono.cpp:3314: Create list of PBXes
19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:63: Loading plug-ins from /usr/share/clearwater/sprout/plugins
19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:158: Finished loading plug-ins
19-09-2016 15:41:44.652 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):
19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f3df8 TCP connect() error: Connection refused [code=120111]
19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f5c38 TCP connect() error: Connection refused [code=120111]


I have observed that in the bono container 5060 port is not listening in all interfaces while 5062 port is listening in all interfaces.

root at 9837c4dab241:/var/log/bono# netstat -planut | grep LISTEN
tcp        0      0 10.0.1.2:4000<http://10.0.1.2:4000>           0.0.0.0:*               LISTEN      -
tcp        0      0 10.0.1.2:5058<http://10.0.1.2:5058>           0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.11:43395<http://127.0.0.11:43395>        0.0.0.0:*               LISTEN      -
tcp        0      0 10.0.1.2:5060<http://10.0.1.2:5060>           0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:5062<http://0.0.0.0:5062>            0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.1:8080<http://127.0.0.1:8080>          0.0.0.0:*               LISTEN      -
tcp        0      0 10.0.1.2:3478<http://10.0.1.2:3478>           0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:22<http://0.0.0.0:22>              0.0.0.0:*               LISTEN      9/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      9/sshd
root at 9837c4dab241:/var/log/bono#


Is it right to have 5060 port listen on only local port? Please help me to debug the issue.

Thanks,
Sarbajit


On Wed, Sep 21, 2016 at 10:01 PM, Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>> wrote:
Hi Sarbajit,

I?ve had another look at this, and actually I think clearwater-live-test checks it can connect to Bono before it tries to provision numbers from Ellis, and it?s actually that connection that?s failing ? apologies!

Can you do similar checks for the Bono container, i.e. connect to the Bono container and run ps -eaf | grep bono and run nc -z -v <ip> 5060 from your live test container (where <ip> is the IP of your Bono)?

One other thought ? what command are you using to run the tests? You?ll need to set the PROXY option to your Bono IP and the ELLIS option to your Ellis IP.

Thanks,
Graeme

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org<mailto:clearwater-bounces at lists.projectclearwater.org>] On Behalf Of Sarbajit Chatterjee
Sent: 21 September 2016 16:41
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Thanks Graeme for your reply. Here are the command outputs that you had asked -

root at e994b17b4563:/# ps -eaf | grep ellis
root       177     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager --mgmt-local-ip=10.0.1.7 --sig-local-ip=10.0.1.7 --local-site=site1 --remote-site= --remote-cassandra-seeds= --signaling-namespace= --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater --etcd-cluster-key=ellis --log-level=3 --log-directory=/var/log/clearwater-cluster-manager --pidfile=/var/run/clearwater-cluster-manager.pid
root       180     1  0 Sep19 ?        00:00:00 /bin/sh /etc/init.d/ellis run
ellis      185   180  0 Sep19 ?        00:00:05 /usr/share/clearwater/ellis/env/bin/python -m metaswitch.ellis.main
root       287   253  0 15:18 ?        00:00:00 grep --color=auto ellis
root at e994b17b4563:/#
root at e994b17b4563:/# ps -eaf | grep nginx
root       179     1  0 Sep19 ?        00:00:00 nginx: master process /usr/sbin/nginx -g daemon off;
www-data   186   179  0 Sep19 ?        00:00:16 nginx: worker process
www-data   187   179  0 Sep19 ?        00:00:00 nginx: worker process
www-data   188   179  0 Sep19 ?        00:00:16 nginx: worker process
www-data   189   179  0 Sep19 ?        00:00:16 nginx: worker process
root       289   253  0 15:19 ?        00:00:00 grep --color=auto nginx
root at e994b17b4563:/#
root at e994b17b4563:/# netstat -planut | grep nginx
tcp6       0      0 :::80                   :::*                    LISTEN      179/nginx -g daemon
root at e994b17b4563:/#


I think both ellis and nginx are running fine inside the container. I can also open the ellis login page from a web browser. I also checked the MySQL DB in ellis container. I can see livetest user entry in users table and 1000 rows in numbers table.

I can also connect to ellis (host IP 10.109.190.10) from my livetest container -

root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.10 80
Connection to 10.109.190.10 80 port [tcp/http] succeeded!
root at 40efba73deb5:~/clearwater-live-test#


Is this happening because Clearwater containers are spread across multiple hosts? What other areas I should check?


Thanks,
Sarbajit


On Wed, Sep 21, 2016 at 6:09 PM, Graeme Robertson (projectclearwater.org<http://projectclearwater.org>) <graeme at projectclearwater.org<mailto:graeme at projectclearwater.org>> wrote:
Hi Sarbajit,

I don?t think we?ve never tried deploying Project Clearwater in a Docker Swarm cluster, but I don?t see any reason why it couldn?t work. The tests are failing very early ? they?re not able to connect to Ellis on port 80. I can think of a couple of reasons for this ? either Ellis isn?t running or the Ellis port mapping hasn?t worked for some reason.

Can you connect to the Ellis container and run ps ?eaf | grep ellis and ps ?eaf | grep nginx to confirm that NGINX and Ellis are running? Can you also run sudo netstat -planut | grep nginx or something equivalent to check that NGINX is listening on port 80? If there?s a problem with either NGINX or Ellis we probably need to look in the logs at /var/log/nginx/ or /var/log/ellis/ on the Ellis container.

If however that all looks fine, then it sounds like the port mapping has failed for some reason. Can you run nc -z <ip> 80 from the box you?re running the live tests on? This will scan for anything listening at <ip>:80 and will return successfully if it finds anything.

Thanks,
Graeme

________________________________
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Sarbajit Chatterjee
Sent: 20 September 2016 15:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster using docker-compose

Hello,

I am following the instructions from https://github.com/Metaswitch/clearwater-docker. I can successfully deploy it on a single Docker node but, the compose file does not work with Swarm cluster.

I did try to modify the compose file like this -


version: '2'
services:
  etcd:
    image: quay.io/coreos/etcd:v2.2.5<http://quay.io/coreos/etcd:v2.2.5>
    command: >
      -name etcd0
      -advertise-client-urls http://etcd:2379,http://etcd:4001
      -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001
      -initial-advertise-peer-urls http://etcd:2380
      -listen-peer-urls http://0.0.0.0:2380
      -initial-cluster etcd0=http://etcd:2380
      -initial-cluster-state new
  bono:
    image: swarm-node:5000/clearwaterdocker_bono
    ports:
      - 22
      - "3478:3478"
      - "3478:3478/udp"
      - "5060:5060"
      - "5060:5060/udp"
      - "5062:5062"
  sprout:
    image: swarm-node:5000/clearwaterdocker_sprout
    networks:
      default:
        aliases:
          - scscf.sprout
          - icscf.sprout
    ports:
      - 22
  homestead:
    image: swarm-node:5000/clearwaterdocker_homestead
    ports:
      - 22
  homer:
    image: swarm-node:5000/clearwaterdocker_homer
    ports:
      - 22
  ralf:
    image: swarm-node:5000/clearwaterdocker_ralf
    ports:
      - 22
  ellis:
    image: swarm-node:5000/clearwaterdocker_ellis
    ports:
      - 22
      - "80:80"


where swarm-node:5000 is the local docker registry and it hosts the pre-built images of Clearwater containers. Even though the deployment succeeded, clearwater-livetests are failing with following error -


Basic Registration (TCP) - Failed
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2)
     - /usr/local/rvm/gems/ruby-1.9.3-p551/gems/quaff-0.7.3/lib/sources.rb:41:in `initialize'


Any suggestions on how I can deploy Clearwater on a Swarm cluster?

Thanks,
Sarbajit


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160923/45dfffc4/attachment.html>

From morrisyan123 at gmail.com  Tue Sep 27 10:21:38 2016
From: morrisyan123 at gmail.com (yan morris)
Date: Tue, 27 Sep 2016 22:21:38 +0800
Subject: [Project Clearwater] Using Ralf
Message-ID: <CAM_v+eRYJqSnBf1Ryk0YDfn3Fo=CDGU2D9qrbrPPFuge5tb6GQ@mail.gmail.com>

Hi ,

As I know , Ralf is used to provide offline billing.

However , I don't know how to use it , for example : setting billing rules

I want to know how to use it , could you tell me how to use it or are there

documents I can refer ?


Thanks a lot .


Morris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160927/414337c4/attachment.html>

From surender.s at hcl.com  Wed Sep 28 02:05:31 2016
From: surender.s at hcl.com (Surender Singh)
Date: Wed, 28 Sep 2016 06:05:31 +0000
Subject: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID: <16AE9ED83DBA5B4D85B058EAF39C918107E1C674@NDA-HCLT-MBS05.hclt.corp.hcl.in>

Hi Graeme,

Thanks for support.

my problem got resolved ..issue was at pbx end..

Regards
Surender Singh



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 22 September 2016 16:11
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 41, Issue 46

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. Re: SIP Trunking With PBX_aio
      (Graeme Robertson (projectclearwater.org))


----------------------------------------------------------------------

Message: 1
Date: Thu, 22 Sep 2016 10:38:37 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<g at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID:
	<CY4PR02MB2616419418B169728B755550E3C90 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="us-ascii"

Hi Surender,

I'm not sure Schahzad ever said he was routing to a PBX - it looked like he was just doing SIP trunking - but if he was routing to a PBX, presumably he was using a different PBX to you.

You have a BGCF rule which says that requests to 10.112.87.177 should be routed via sip:cw-aio:5058. It looks like cw-aio resolves to the IP address of your AIO node, and Bono listens on port 5058 for traffic from the core, so the message gets routed to Bono which acts as an IBCF and presumably routes the message out to your PBX. Does that answer your question?

Thanks,
Graeme

-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 22 September 2016 06:49
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP Trunking With PBX_aio

Hi Graeme,

Thanks for support..

But have seen one old  thread subject 'SIP Trunking  ' by Schahzad .His solution working fine without any changes at PBX end.

Please give comments on call scenarios for out calls to PBX

SIP Client ----> Sprout ---->ENUM Query(Find new request URI based on B number prefix) ------>BGCF(match the domain name with new request URI which is re-write by ENUM and find out the route value )

In my case route value is    ["<sip:cw-aio:5058;transport=UDP;lr;orig>"] and Domain is 10.112.87.177 .

But here I have confusion regarding route value '' sip:cw-aio:5058;transport=UDP;lr;orig '  what is mean of this.

Please give clarity  on route part ..


Regards
Surender Singh
8826292018

	

-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of clearwater-request at lists.projectclearwater.org
Sent: 21 September 2016 16:55
To: clearwater at lists.projectclearwater.org
Subject: Clearwater Digest, Vol 41, Issue 40

Send Clearwater mailing list submissions to
	clearwater at lists.projectclearwater.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org

or, via email, send a message with subject or body 'help' to
	clearwater-request at lists.projectclearwater.org

You can reach the person managing the list at
	clearwater-owner at lists.projectclearwater.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. Re: SIP Trunking With PBX_aio
      (Graeme Robertson (projectclearwater.org))


----------------------------------------------------------------------

Message: 1
Date: Wed, 21 Sep 2016 11:24:02 +0000
From: "Graeme Robertson (projectclearwater.org)"
	<graeme at projectclearwater.org>
To: "clearwater at lists.projectclearwater.org"
	<clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] SIP Trunking With PBX_aio
Message-ID:
	<CY4PR02MB2616A2D97C1BC38B1E842CC5E3F60 at CY4PR02MB2616.namprd02.prod.outlook.com>
	
Content-Type: text/plain; charset="us-ascii"

Hi Surender,



Project Clearwater is IMS compliant, and so I'm afraid it doesn't alter either the To header or the From header, and there is no trivial way to configure Project Clearwater to do this manipulation, or any plans to add this ability to Project Clearwater. It sounds as though your PBX is non-IMS compliant. As I mentioned before, if you really need to re-write the To header on your request, and you are not averse to writing a bit of code, Sprout provides a really easy-to-use API for developing your own app server. https://github.com/Metaswitch/greeter is a sample application server that uses this API and simply adds the header "Subject: Hello world!" to a SIP message - you could write something similar that changes to the To header on your SIP messages. Alternatively, Metaswitch has a commercial product called Perimeta<http://www.metaswitch.com/perimeta-session-border-controller-sbc> which can be deployed as an IBCF, and which has a SIP Message Manipulation Framework which you would be able to use to alter your To header.



>From the logs below, it looks as though the BGCF is now applying your route and the INVITE will presumably be going out through Bono, but unfortunately I don't think that will solve your problem.



Thanks,

Graeme



-----Original Message-----
From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Surender Singh
Sent: 20 September 2016 13:45
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] SIP Trunking With PBX_aio



Hi Greame,



Thanks for support .



But my issue is ,when I calling from A to B (Out call) its showing example.com in To_header and From_header . and same value going at my PBX ,due this might be  pbx unable to identify the value like 1234 at example.com<mailto:1234 at example.com>  instead of 1234 at 10.112.87.177<mailto:1234 at 10.112.87.177> . Can have any possibility to change it at IMS end.





I also change the configuration in Bgcf.json but I unable to check whether calls are forwarding through BGCF route or not .I found configuration like below... in one thread.





{

    "number_blocks" : [

        {

           "name" : "Internal numbers",

           "prefix" : "650555",

           "regex" : "!(^.*$)!sip:\\1 at example.com!"

        },

        {

            "name" : "External numbers",

            "prefix" : "",

            "regex" : "!(^.*$)!sip:\\1 at 10.112.87.177!"

        }





    ]

}







{

    "routes" : [

        {   "name" : "TO_PBX",

          "domain" : "10.112.87.177",

          "route" : ["<sip:cw-aio:5058;transport=UDP;lr;orig>"]

        }



    ]



}









20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:691: Cloned tdta0x7f744c14aea0 to tdta0x7f744c14e530

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:odi_fWh8yrqIzM at 10.112.87.250:5054;lr;orig;service=scscf>

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c14eb40 => txdata 0x7f744c14e5d8 mapping

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1587: scscf-0x7f744c14df40 pass initial request Request msg INVITE/cseq=1 (tdta0x7f744c14e530) to Sproutlet

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:431: S-CSCF received initial request

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 3

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:773: Route header references this system

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:786: Found ODI token fWh8yrqIzM

20-09-2016 16:37:12.508 UTC Debug aschain.h:131: AsChain inc ref 0x7f744c1364b0 -> 2

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:793: Original dialog for odi_fWh8yrqIzM found: AsChain-orig[0x7f744c1364b0]:2/1

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:832: Got our Route header, session case orig, OD=AsChain-orig[0x7f744c1364b0]:2/1

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:291: Served user from P-Served-User header

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 4

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:502: Found served user, so apply services

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:1153: Performing originating initiating request processing

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1178: Completed applying originating services

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: true, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 2

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2218: Translating URI

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2189: Performing ENUM translation for user 1234

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:240: Translating URI via JSON ENUM lookup

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:305: Comparing first 4 numbers of 1234 against prefix 650555

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:305: Comparing first 0 numbers of 1234 against prefix

20-09-2016 16:37:12.508 UTC Debug enumservice.cpp:312: Match found

20-09-2016 16:37:12.508 UTC Info enumservice.cpp:280: Number 1234 found, translated URI = sip:1234 at 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: false, treat_number_as_phone: true

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2249: Translated URI sip:1234 at 10.112.87.177 is a real SIP URI - replacing Request-URI

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1194: New URI string is sip:1234 at 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug scscfsproutlet.cpp:1210: Routing to BGCF

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:1446: Routing to BGCF sip:bgcf.cw-aio:5053;transport=TCP

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f744c14eb40

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1386: scscf-0x7f744c14df40 sending Request msg INVITE/cseq=1 (tdta0x7f744c14e530) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f744c14e5d8, fork = 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1914: scscf-0x7f744c14df40 transmitting request on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1928: scscf-0x7f744c14df40 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f744c14e530) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c14eb40 => txdata 0x7f744c14e5d8 mapping

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:bgcf.cw-aio:5053;transport=TCP;lr

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:329: Possible service name - bgcf

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:335: Hostname - cw-aio

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1154: Created Sproutlet bgcf-0x7f744c05f0b0 for Request msg INVITE/cseq=1 (tdta0x7f744c14e530)

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:2062: Routing Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0) (609 bytes) to upstream sproutlet scscf:

--start msg--



SIP/2.0 100 Trying

Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh

Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be

Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>

Call-ID: OV0U_26XyoJOUXCQ0B_6FA..

From: <sip:6505550962 at example.com>;tag=9b0c5d3e

To: <sip:1234 at example.com>

CSeq: 1 INVITE

Content-Length:  0





--end msg--

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1623: scscf-0x7f744c14df40 received provisional response Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0, state = Proceeding

20-09-2016 16:37:12.508 UTC Info scscfsproutlet.cpp:561: S-CSCF received response

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1413: scscf-0x7f744c14df40 sending Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 1 responses, 0 requests, 0 timers

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1836: Aggregating response with status code 100

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1853: Discard 100/INVITE response (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Debug pjsip: tdta0x7f744c15 Destroying txdata Response msg 100/INVITE/cseq=1 (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:2062: Routing Request msg INVITE/cseq=1 (tdta0x7f744c14e530) (1411 bytes) to downstream sproutlet bgcf:

--start msg--



INVITE sip:1234 at 10.112.87.177 SIP/2.0

Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh

Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>

Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be

Max-Forwards: 67

Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>

To: <sip:1234 at example.com>

From: <sip:6505550962 at example.com>;tag=9b0c5d3e

Call-ID: OV0U_26XyoJOUXCQ0B_6FA..

CSeq: 1 INVITE

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE

Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri

User-Agent: Z 3.9.32144 r32121

Allow-Events: presence, kpml

P-Asserted-Identity: <sip:6505550962 at example.com>

Session-Expires: 600

P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg

Route: <sip:bgcf.cw-aio:5053;transport=TCP;lr>

Content-Type: application/sdp

Content-Length:   241



v=0

o=Z 0 0 IN IP4 10.112.123.57

s=Z

c=IN IP4 10.112.123.57

t=0 0

m=audio 8000 RTP/AVP 3 110 8 0 97 101

a=rtpmap:110 speex/8000

a=rtpmap:97 iLBC/8000

a=fmtp:97 mode=30

a=rtpmap:101 telephone-event/8000

a=fmtp:101 0-16

a=sendrecv



--end msg--

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:691: Cloned tdta0x7f744c14e530 to tdta0x7f744c1515d0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1215: Remove top Route header Route: <sip:bgcf.cw-aio:5053;transport=TCP;lr>

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1735: Adding message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1587: bgcf-0x7f744c05f0b0 pass initial request Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) to Sproutlet

20-09-2016 16:37:12.508 UTC Debug acr.cpp:49: Created ACR (0x7f744c05f030)

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:2328: Not translating URI

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:169: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false

20-09-2016 16:37:12.508 UTC Debug uri_classifier.cpp:199: Classified URI as 5

20-09-2016 16:37:12.508 UTC Debug bgcfservice.cpp:188: Getting route for URI domain 10.112.87.177 via BGCF lookup

20-09-2016 16:37:12.508 UTC Info bgcfservice.cpp:198: Found route to domain 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1350: Sproutlet send_request 0x7f744c151be0

20-09-2016 16:37:12.508 UTC Verbose sproutletproxy.cpp:1386: bgcf-0x7f744c05f0b0 sending Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1750: Processing actions from sproutlet - 0 responses, 1 requests, 0 timers

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1790: Processing request 0x7f744c151678, fork = 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1914: bgcf-0x7f744c05f0b0 transmitting request on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1928: bgcf-0x7f744c05f0b0 store reference to non-ACK request Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) on fork 0

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:1742: Removing message 0x7f744c151be0 => txdata 0x7f744c151678 mapping

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:119: Find target Sproutlet for request

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:158: Found next routable URI: sip:cw-aio:5058;transport=UDP;lr;orig

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:190: No Sproutlet found using service name or host

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:196: Find default service for port 5058

20-09-2016 16:37:12.508 UTC Debug sproutletproxy.cpp:874: No local sproutlet matches request

20-09-2016 16:37:12.508 UTC Debug pjsip: tsx0x7f744c154 Transaction created for Request msg INVITE/cseq=1 (tdta0x7f744c1515d0)

20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1618: Added trail identifier 10262 to UAC transaction

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:490: Next hop node is encoded in top route header

20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:86: SIPResolver::resolve for name cw-aio, port 5058, transport 17, family 2

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:523: Attempt to parse cw-aio as IP address

20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:128: Port is specified

20-09-2016 16:37:12.508 UTC Debug sipresolver.cpp:296: Perform A/AAAA record lookup only, name = cw-aio

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:724: Removing record for scscf.cw-aio (type 1, expiry time 1474389247) from the expiry list

20-09-2016 16:37:12.508 UTC Verbose dnscachedresolver.cpp:245: Check cache for cw-aio type 1

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:251: No entry found in cache

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:254: Create cache entry pending query

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:302: Create and execute DNS query transaction

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:315: Wait for query responses

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:465: Received DNS response for cw-aio type A

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:90: Parsing DNS message

000000: 19378580 00010001 00000000 0663772d 61696f00 00010001 c00c0001 00010000    .7.. .... .... .cw- aio. .... .... ....

000020: 00000004 0a7057fa                                                          .... .pW.



20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:95: Parsing header at offset 0x0

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:98: 1 questions, 1 answers, 0 authorities, 0 additional records

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:103: Parsing question 1 at offset 0xc

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:229: Parsed domain name = cw-aio, encoded length = 8

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:112: Parsing answer 1 at offset 0x18

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:229: Parsed domain name = cw-aio, encoded length = 2

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:282: Resource Record NAME=cw-aio TYPE=A CLASS=IN TTL=0 RDLENGTH=4

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:287: Parse A record RDATA

20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:142: Answer records

cw-aio                  0       IN      A       10.112.87.250



20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:143: Authority records



20-09-2016 16:37:12.508 UTC Debug dnsparser.cpp:144: Additional records



20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:761: Adding record to cache entry, TTL=0, expiry=1474389432

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:765: Update cache entry expiry to 1474389432

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:707: Adding cw-aio to cache expiry list with deletion time of 1474389732

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:319: Received all query responses

20-09-2016 16:37:12.508 UTC Debug dnscachedresolver.cpp:347: Pulling 1 records from cache for cw-aio A

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:362: Found 1 A/AAAA records, randomizing

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:993: 10.112.87.250:5058 transport 17 has state: WHITE

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:993: 10.112.87.250:5058 transport 17 has state: WHITE

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:406: Added a server, now have 1 of 5

20-09-2016 16:37:12.508 UTC Debug baseresolver.cpp:447: Adding 0 servers from blacklist

20-09-2016 16:37:12.508 UTC Info pjutils.cpp:938: Resolved destination URI sip:cw-aio:5058;transport=UDP;lr;orig to 1 servers

20-09-2016 16:37:12.508 UTC Debug pjutils.cpp:490: Next hop node is encoded in top route header

20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1641: Next hop cw-aio is not a stateless proxy

20-09-2016 16:37:12.508 UTC Debug basicproxy.cpp:1655: Sending request for sip:1234 at 10.112.87.177

20-09-2016 16:37:12.508 UTC Debug pjsip: tsx0x7f744c154 Sending Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) in state Null

20-09-2016 16:37:12.508 UTC Debug pjsip:       endpoint Request msg INVITE/cseq=1 (tdta0x7f744c1515d0): skipping target resolution because address is already set

20-09-2016 16:37:12.508 UTC Debug pjsip:       endpoint Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) exceeds UDP size threshold (1300), sending with TCP

20-09-2016 16:37:12.508 UTC Verbose pjsip: tcpc0x7f744c15 TCP client transport created

20-09-2016 16:37:12.508 UTC Verbose pjsip: tcpc0x7f744c15 TCP transport 10.112.87.250:57743 is connecting to 10.112.87.250:5058...

20-09-2016 16:37:12.509 UTC Verbose common_sip_processing.cpp:136: TX 1504 bytes Request msg INVITE/cseq=1 (tdta0x7f744c1515d0) to TCP 10.112.87.250:5058:

--start msg--



INVITE sip:1234 at 10.112.87.177 SIP/2.0

Via: SIP/2.0/TCP 10.112.87.250:57743;rport;branch=z9hG4bKPjeUUjYygIls2DRu2LAAHqVGU27AHZ.-S3

Record-Route: <sip:scscf.cw-aio:5054;transport=TCP;lr;service=scscf;billing-role=charge-orig>

Via: SIP/2.0/TCP 10.112.87.250:45312;rport=45312;received=10.112.87.250;branch=z9hG4bKPjMCwNeVkjc1uawalD3By9r.lqGGy5PwHh

Record-Route: <sip:10.112.87.250:5058;transport=TCP;lr>

Record-Route: <sip:paRGfRUGv7 at cw-aio:5060;transport=TCP;lr>

Via: SIP/2.0/TCP 10.112.123.57:43259;received=10.112.123.57;branch=z9hG4bK-524287-1---fa5982366f8904be

Max-Forwards: 66

Contact: <sip:6505550962 at 10.112.123.57:43259;transport=tcp>

To: <sip:1234 at example.com>

From: <sip:6505550962 at example.com>;tag=9b0c5d3e

Call-ID: OV0U_26XyoJOUXCQ0B_6FA..

CSeq: 1 INVITE

Allow: INVITE, ACK, CANCEL, BYE, NOTIFY, REFER, MESSAGE, OPTIONS, INFO, SUBSCRIBE

Supported: replaces, norefersub, extended-refer, timer, outbound, path, X-cisco-serviceuri

User-Agent: Z 3.9.32144 r32121

Allow-Events: presence, kpml

P-Asserted-Identity: <sip:6505550962 at example.com>

Session-Expires: 600

P-Served-User: <sip:6505550962 at example.com>;sescase=orig;regstate=reg

Route: <sip:cw-aio:5058;transport=UDP;lr;orig>

Content-Type: application/sdp

Content-Length:   241



v=0

o=Z 0 0 IN IP4 10.112.123.57

s=Z

c=IN IP4 10.112.123.57

t=0 0

m=audio 8000 RTP/AVP 3 110 8 0 97 101

a=rtpmap:110 speex/8000

a=rtpmap:97 iLBC/8000

a=fmtp:97 mode=30

a=rtpmap:101 telephone-event/8000

a=fmtp:101 0-16

a=sendrecv



--end msg--

20-09-2016 16:37:12.509 UTC Debug pjsip: tsx0x7f744c154 State changed from Null to Calling, event=TX_MSG

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:213: tsx0x7f744c1546a8 - tu_on_tsx_state UAC, TSX_STATE TX_MSG state=Calling

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:1813: tsx0x7f744c1546a8 - uac_tsx = 0x7f744c154540, uas_tsx = 0x7f744c001b20

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:1821: TX_MSG event on current UAC transaction

20-09-2016 16:37:12.509 UTC Debug basicproxy.cpp:2134: Starting timer C

20-09-2016 16:37:12.509 UTC Debug thread_dispatcher.cpp:193: Worker thread completed processing message 0x7f744409acd8

20-09-2016 16:37:12.509 UTC Debug thread_dispatcher.cpp:199: Request latency = 8327us

20-09-2016 16:37:12.519 UTC Verbose pjsip: tcpc0x7f744c15 TCP transport 10.112.87.250:57743 is connected to 10.112.87.250:5058

20-09-2016 16:37:12.524 UTC Debug pjsip: sip_endpoint.c Processing incoming message: Response msg 100/INVITE/cseq=1 (rdata0x7f744c157410)

20-09-2016 16:37:12.524 UTC Verbose common_sip_processing.cpp:120: RX 864 bytes Response msg 100/INVITE/cseq=1 (rdata0x7f744c157410) from TCP 10.112.87.250:5058:

--start msg--






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160921/b9498df0/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 41, Issue 40
******************************************


::DISCLAIMER::
----------------------------------------------------------------------------------------------------------------------------------------------------

The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only.
E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents (with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates.
Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification, distribution and / or publication of this message without the prior written consent of authorized representative of HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately.
Before opening any email and/or attachments, please check them for viruses and other defects.

----------------------------------------------------------------------------------------------------------------------------------------------------


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org



------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org


------------------------------

End of Clearwater Digest, Vol 41, Issue 46
******************************************



From manikantha.tadi at gmail.com  Wed Sep 28 04:11:55 2016
From: manikantha.tadi at gmail.com (Manikantha Srinivas Tadi)
Date: Wed, 28 Sep 2016 13:41:55 +0530
Subject: [Project Clearwater] DNS Problem while registration of sip client
Message-ID: <CAHfUHgGs2sA6P+CaGVgigpRvsBaMPubGuKbmbQ5vnhQNxQjbAA@mail.gmail.com>

Hi,

We are trying clearwater IMS to setup a call between sip client.

How ever, while registering the sip client using CSIPSimple utility , We
are getting the following error logs in /var/log/bono and /var/log/sprout

Bono Logs :

8-09-2016 08:04:45.765 UTC Error dnscachedresolver.cpp:607: Failed to
retrieve record for ralf.cw.pramati.com: Could not contact DNS servers
28-09-2016 08:04:45.765 UTC Error httpclient.cpp:623: cURL failure with
cURL error code 6 (see man 3 libcurl-errors) and HTTP error code 404
28-09-2016 08:04:50.652 UTC Status alarm.cpp:62: sprout issued 1005.4 alarm
28-09-2016 08:04:52.100 UTC Warning (Net-SNMP): Warning: Failed to connect
to the agentx master agent ([NIL]):

Sprout Logs :

28-09-2016 08:08:26.816 UTC Status alarm.cpp:62: sprout issued 1001.3 alarm
28-09-2016 08:08:37.986 UTC Error dnscachedresolver.cpp:607: Failed to
retrieve record for hs.cw.pramati.com: Could not contact DNS servers
28-09-2016 08:08:37.986 UTC Error httpclient.cpp:623: cURL failure with
cURL error code 6 (see man 3 libcurl-errors) and HTTP error code 404
28-09-2016 08:08:56.816 UTC Status alarm.cpp:62: sprout issued 1001.3 alarm
28-09-2016 08:09:05.374 UTC Status load_monitor.cpp:285: Maximum incoming
request rate/second unchanged - only handled 20 requests in last 191050ms,
minimum threshold for a change is 23881.250000


Where as i am able to manually resolve the DNS Records for
ralf.cw.pramati.com and hs.cw.pramati.com on the bono and sprout nodes
respectively.

Could someone please help if we are missing something in this regards ?

Thanks
Manikantha.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160928/d3d55ac5/attachment.html>

From sarbajitc at gmail.com  Wed Sep 28 05:00:13 2016
From: sarbajitc at gmail.com (Sarbajit Chatterjee)
Date: Wed, 28 Sep 2016 14:30:13 +0530
Subject: [Project Clearwater] Deploy Clearwater in a Swarm cluster using
	docker-compose
In-Reply-To: <CY4PR02MB2616AD64C1A83AF2536678D5E3C80@CY4PR02MB2616.namprd02.prod.outlook.com>
References: <CAN20VnjYp3aGAnrkMUn-WYEck6+uTi+HwB6PbbZUC9wuPB1W1w@mail.gmail.com>
	<CY4PR02MB261618E6A2A54183C25FAC46E3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VnhU6-_S71FK_F0QateTrNxG2j_WN2BPEyx3PdhxOhGhmQ@mail.gmail.com>
	<CY4PR02MB2616E42F5584556E6172483FE3F60@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VngUB9yN6Td29LcV0p0Ths2eFON_fx+Go94uaevvykbRDA@mail.gmail.com>
	<CY4PR02MB2616F4AE6B78CC82364A1B10E3C90@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20Vni=Do=b6vEy6d3KMbb2HVvhp-jA7Uv07PKx8O0z=g3Kcg@mail.gmail.com>
	<CY4PR02MB2616B8AEAC1514546FD6BD82E3C80@CY4PR02MB2616.namprd02.prod.outlook.com>
	<CAN20VniUfLiESAxYFka=Pe=+T+MhzBP+HbxF5L+rCCSFtCHmoA@mail.gmail.com>
	<CY4PR02MB2616AD64C1A83AF2536678D5E3C80@CY4PR02MB2616.namprd02.prod.outlook.com>
Message-ID: <CAN20VnimkE4CmSg1OrV2WwYdaVSG1JFZDuCM=rc3w+AxbVRzpw@mail.gmail.com>

I was able to fix the problem by adding a container which does port
forwarding to bono. I have posted the compose file in my forked repo
https://github.com/sarbajitc/clearwater-docker. Now livetest is running
successfully. I also verified the setup using Zoiper client.

-Sarbajit

On Fri, Sep 23, 2016 at 9:35 PM, Graeme Robertson (projectclearwater.org) <
g at projectclearwater.org> wrote:

> Hi Sarbajit,
>
>
>
> Ah, ok ? that makes sense! I didn?t realise your Bono container had
> multiple interfaces. Unfortunately Bono doesn?t support listening on
> multiple interfaces. It is listening on 10.0.1.2 because that is what
> local_ip is set to in /etc/clearwater/local_config, and that was
> determined by this script
> <https://github.com/Metaswitch/clearwater-infrastructure/blob/566ccbb9913ccb9dfbb452a9185b7bf93c32bc1d/debian/clearwater-auto-config-docker.init.d#L61>,
> which uses the first IP address it finds from the output of running
> hostname. You could potentially edit that script to find the ?right? IP
> address, but it would be much easier if your Bono container just had one
> interface.
>
>
>
> Thanks,
>
> Graeme
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sarbajit Chatterjee
> *Sent:* 23 September 2016 12:07
>
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Hi Graeme,
>
>
>
> I see the iptables nat rules that docker has created on the host machine,
> contains 172.18.0.2 IP of bono, not the other interface 10.0.1.2 IP.
>
>
>
>
>
> root at swarm-master:~# iptables -t nat -L
>
> Chain PREROUTING (policy ACCEPT)
>
> target     prot opt source               destination
>
> DOCKER     all  --  anywhere             anywhere             ADDRTYPE
> match dst-type LOCAL
>
>
>
> Chain INPUT (policy ACCEPT)
>
> target     prot opt source               destination
>
>
>
> Chain OUTPUT (policy ACCEPT)
>
> target     prot opt source               destination
>
> DOCKER     all  --  anywhere            !loopback/8           ADDRTYPE
> match dst-type LOCAL
>
>
>
> Chain POSTROUTING (policy ACCEPT)
>
> target     prot opt source               destination
>
> MASQUERADE  all  --  172.18.0.0/16        anywhere
>
> MASQUERADE  all  --  172.17.0.0/16        anywhere
>
> MASQUERADE  tcp  --  172.17.0.2           172.17.0.2           tcp dpt:3376
>
> MASQUERADE  tcp  --  172.17.0.5           172.17.0.5           tcp
> dpt:http-alt
>
> MASQUERADE  tcp  --  172.18.0.2           172.18.0.2           tcp dpt:5062
>
> MASQUERADE  tcp  --  172.18.0.2           172.18.0.2           tcp dpt:sip
>
> MASQUERADE  udp  --  172.18.0.2           172.18.0.2           udp dpt:sip
>
> MASQUERADE  tcp  --  172.18.0.2           172.18.0.2           tcp dpt:3478
>
> MASQUERADE  udp  --  172.18.0.2           172.18.0.2           udp dpt:3478
>
> MASQUERADE  tcp  --  172.18.0.2           172.18.0.2           tcp dpt:ssh
>
> MASQUERADE  tcp  --  172.18.0.3           172.18.0.3           tcp dpt:ssh
>
> MASQUERADE  tcp  --  172.18.0.4           172.18.0.4           tcp dpt:ssh
>
>
>
> Chain DOCKER (2 references)
>
> target     prot opt source               destination
>
> RETURN     all  --  anywhere             anywhere
>
> RETURN     all  --  anywhere             anywhere
>
> DNAT       tcp  --  anywhere             anywhere             tcp dpt:3376
> to:172.17.0.2:3376
>
> DNAT       tcp  --  anywhere             anywhere             tcp
> dpt:32774 to:172.17.0.5:8080
>
> DNAT       tcp  --  anywhere             anywhere             tcp dpt:5062
> to:172.18.0.2:5062
>
> DNAT       tcp  --  anywhere             anywhere             tcp dpt:sip
> to:172.18.0.2:5060
>
> DNAT       udp  --  anywhere             anywhere             udp dpt:sip
> to:172.18.0.2:5060
>
> DNAT       tcp  --  anywhere             anywhere             tcp dpt:3478
> to:172.18.0.2:3478
>
> DNAT       udp  --  anywhere             anywhere             udp dpt:3478
> to:172.18.0.2:3478
>
> DNAT       tcp  --  anywhere             anywhere             tcp
> dpt:32786 to:172.18.0.2:22
>
> DNAT       tcp  --  anywhere             anywhere             tcp
> dpt:32787 to:172.18.0.3:22
>
> DNAT       tcp  --  anywhere             anywhere             tcp
> dpt:32788 to:172.18.0.4:22
>
> root at swarm-master:~#
>
>
>
>
>
> The bono container is opening 5060 port only on 10.0.1.2 IP. So, even if
> the host machine is exposing the port, container is unable to get the
> packets.
>
> Do you agree? Is there a way I can configure the bono container to run on
> all interfaces?
>
>
>
>
>
> -Sarbajit
>
>
>
> P.S. I deployed Clearwater on a single node docker and that works fine.
>
>
>
>
>
>
>
> On Fri, Sep 23, 2016 at 4:22 PM, Graeme Robertson (projectclearwater.org)
> <g at projectclearwater.org> wrote:
>
> Hi Sarbajit,
>
>
>
> That is very strange ? netstat thinks there is a process listening on port
> 5060 on the host, but netcat doesn?t. I?m not really sure what further to
> suggest. I?ve got a couple of ideas for things you could do to try and
> narrow down the problem, but I?m afraid there?s nothing particularly
> concrete.
>
>
>
> You could try stopping the Bono container and running netcat as both a
> server and a client on the host to check that the host hasn?t got some
> weird iptables rule blocking the connection. It might also be worth taking
> packet captures to try and work out exactly what?s going on.
>
>
>
> Also, have you tried just deploying on a single host using Docker (i.e.
> not using Docker Swarm)? It would be good to verify that that works.
>
>
>
> I hope you manage to get to the bottom of the problem!
>
>
>
> Thanks,
> Graeme
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sarbajit Chatterjee
> *Sent:* 22 September 2016 12:14
>
>
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Hi Graeme,
>
>
>
> I can connect to 5060 port in 10.0.1.2 from sprout container.
>
>
>
> root at bb818f0a5535:/# nc -v -z 10.0.1.2 5060
>
> Connection to 10.0.1.2 5060 port [tcp/sip] succeeded!
>
> root at bb818f0a5535:/#
>
>
>
>
>
> Docker PS on the cluster shows bono container exposing port 5060
>
>
>
>
>
> CONTAINER ID        IMAGE                                        COMMAND
>                CREATED             STATUS              PORTS
>
>
>             NAMES
>
> bb818f0a5535        swarm-node:5000/clearwaterdocker_sprout
>  "/usr/bin/supervisord"   47 hours ago        Up 47 hours         5052/tcp,
> 5054/tcp, 10.109.190.10:32822->22/tcp
>
>                         swarm-node/clearwaterdocker_sprout_1
>
> 9ff210721451        swarm-node:5000/clearwaterdocker_homer
> "/usr/bin/supervisord"   47 hours ago        Up 47 hours         7888/tcp,
> 10.109.190.9:32788->22/tcp
>
>                          swarm-master/clearwaterdocker_homer_1
>
> 51653420c979        swarm-node:5000/clearwaterdocker_homestead
> "/usr/bin/supervisord"   47 hours ago        Up 47 hours
> 8888-8889/tcp, 10.109.190.9:32787->22/tcp
>
>                                   swarm-master/clearwaterdocker_
> homestead_1
>
> e994b17b4563        swarm-node:5000/clearwaterdocker_ellis
> "/usr/bin/supervisord"   47 hours ago        Up 47 hours
> 10.109.190.10:80->80/tcp, 10.109.190.10:32821->22/tcp
>
>                                   swarm-node/clearwaterdocker_ellis_1
>
> 9837c4dab241        swarm-node:5000/clearwaterdocker_bono
>  "/usr/bin/supervisord"   47 hours ago        Up 47 hours
> 10.109.190.9:3478->3478/tcp, 10.109.190.9:3478->3478/udp,
> 10.109.190.9:5060->5060/tcp, 10.109.190.9:5062->5062/tcp,
> 10.109.190.9:5060->5060/udp, 5058/tcp, 10.109.190.9:32786->22/tcp
> swarm-master/clearwaterdocker_bono_1
>
> 3db967c58754        swarm-node:5000/clearwaterdocker_ralf
>  "/usr/bin/supervisord"   47 hours ago        Up 47 hours
> 10888/tcp, 10.109.190.10:32820->22/tcp
>
>                                    swarm-node/clearwaterdocker_ralf_1
>
> c499d05af8e7        quay.io/coreos/etcd:v2.2.5                   "/etcd
> -name etcd0 -a"   47 hours ago        Up 47 hours         2379-2380/tcp,
> 4001/tcp, 7001/tcp
>
>                   swarm-node/clearwaterdocker_etcd_1
>
> 5fe0c51979e7        ubuntu:14.04.5
> "/bin/bash"              8 days ago          Up 8 days
>
>
>
>
>
>
> But I can't reach the 5060 port from the host machine where it is
> launched. Though the port seems to be open.
>
>
>
>
>
> root at swarm-master:~# nc -v -z 10.109.190.9 5060
>
> nc: connect to 10.109.190.9 port 5060 (tcp) failed: Connection refused
>
> root at swarm-master:~#
>
> root at swarm-master:~# netstat -anp | grep 5060
>
> tcp6       0      0 :::5060                 :::*                    LISTEN
>      21818/docker-proxy
>
> udp6       0      0 :::5060                 :::*
>      21829/docker-proxy
>
> root at swarm-master:~#
>
>
>
>
>
> Any suggestion?
>
>
>
> -Sarbajit
>
>
>
>
>
> On Thu, Sep 22, 2016 at 4:08 PM, Graeme Robertson (projectclearwater.org)
> <g at projectclearwater.org> wrote:
>
> Hi Sarbajit,
>
>
>
> That output all looks fine, but it sounds as though the port mapping has
> failed ? i.e. port 5060 on the Bono container hasn?t been exposed as port
> 5060 on the host. I?m not sure why that would have failed. Can you run nc
> -z -v 10.0.1.2 5060 inside the Bono container (this should work, but it?s
> worth doing as a sanity check!). Then can you run docker ps in your
> clearwater-docker checkout? The output should include a line that looks
> something like 0b4058027844        clearwaterdocker_bono
> "/usr/bin/supervisord"   About a minute ago   Up About a minute
> 0.0.0.0:3478->3478/tcp, 0.0.0.0:3478->3478/udp, *0.0.0.0:5060->5060/tcp*,
> 0.0.0.0:5062->5062/tcp, * 0.0.0.0:5060->5060/udp*, 5058/tcp, 0.0.0.0:42513->22/tcp
> clearwaterdocker_bono_1, which should indicate that the port mapping is
> active. If this all looks fine it might be worth also running nc -v -z
> 10.109.190.9 5060 on the host that?s running the Bono container.
>
>
>
> Thanks,
>
> Graeme
>
>
> ------------------------------
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sarbajit Chatterjee
> *Sent:* 21 September 2016 19:39
>
>
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Hi Graeme,
>
>
>
> I'm using following command to run the livetest -
>
>
>
> rake test[example.com] TESTS="Basic*" SIGNUP_CODE=secret
> PROXY=10.109.190.9 ELLIS=10.109.190.10
>
>
>
> here PROXY ip is where the bono container is launched and ELLIS ip is
> where the ellis container is launched.
>
>
>
> The bono service seems to be running in the container -
>
>
>
> root at 9837c4dab241:/# ps -eaf | grep bono
>
> root       122     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/
> clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager
> --mgmt-local-ip=10.0.1.2 --sig-local-ip=10.0.1.2 --local-site=site1
> --remote-site= --remote-cassandra-seeds= --signaling-namespace=
> --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater
> --etcd-cluster-key=bono --log-level=3 --log-directory=/var/log/clearwater-cluster-manager
> --pidfile=/var/run/clearwater-cluster-manager.pid
>
> root       124     1  0 Sep19 ?        00:00:00 /bin/bash /etc/init.d/bono
> run
>
> root       139   124  0 Sep19 ?        00:00:00 /bin/bash
> /usr/share/clearwater/bin/run-in-signaling-namespace start-stop-daemon
> --start --quiet --exec /usr/share/clearwater/bin/bono --chuid bono --chdir
> /etc/clearwater -- --domain=example.com --localhost=10.0.1.2,10.0.1.2
> --alias=10.0.1.2 --pcscf=5060,5058 --webrtc-port=5062
> --routing-proxy=scscf.sprout,5052,50,600 --ralf=ralf:10888 --sas=0.0.0.0,
> bono at 10.0.1.2 --dns-server=127.0.0.11 --worker-threads=4
> --analytics=/var/log/bono --log-file=/var/log/bono --log-level=2
>
> bono       140   139  0 Sep19 ?        00:11:15
> /usr/share/clearwater/bin/bono --domain=example.com
> --localhost=10.0.1.2,10.0.1.2 --alias=10.0.1.2 --pcscf=5060,5058
> --webrtc-port=5062 --routing-proxy=scscf.sprout,5052,50,600
> --ralf=ralf:10888 --sas=0.0.0.0,bono at 10.0.1.2 --dns-server=127.0.0.11
> --worker-threads=4 --analytics=/var/log/bono --log-file=/var/log/bono
> --log-level=2
>
> root       322   293  0 17:48 ?        00:00:00 grep --color=auto bono
>
> root at 9837c4dab241:/#
>
> root at 9837c4dab241:/# netstat -planut | grep 5060
>
> tcp        0      0 10.0.1.2:5060           0.0.0.0:*
> LISTEN      -
>
> udp        0      0 10.0.1.2:5060           0.0.0.0:*
>       -
>
> root at 9837c4dab241:/#
>
>
>
>
>
> But the connection to bono is failing from livetest container as you had
> predicted.
>
>
>
> root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.9 5060
>
> nc: connect to 10.109.190.9 port 5060 (tcp) failed: Connection refused
>
> root at 40efba73deb5:~/clearwater-live-test#
>
>
>
>
>
> On checking the bono log, I see a series of errors like below
> in beginning of the log -
>
>
>
> 19-09-2016 15:41:44.612 UTC Status utils.cpp:591: Log level set to 2
>
> 19-09-2016 15:41:44.612 UTC Status main.cpp:1388: Access logging enabled
> to /var/log/bono
>
> 19-09-2016 15:41:44.613 UTC Warning main.cpp:1435: SAS server option was
> invalid or not configured - SAS is disabled
>
> 19-09-2016 15:41:44.613 UTC Warning main.cpp:1511: A registration expiry
> period should not be specified for P-CSCF
>
> 19-09-2016 15:41:44.613 UTC Status snmp_agent.cpp:117: AgentX agent
> initialised
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:105: Constructing
> LoadMonitor
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:106:    Target latency
> (usecs)   : 100000
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:107:    Max bucket
> size          : 1000
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:108:    Initial token
> fill rate/s: 100.000000
>
> 19-09-2016 15:41:44.613 UTC Status load_monitor.cpp:109:    Min token fill
> rate/s    : 10.000000
>
> 19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:144: Creating
> Cached Resolver using servers:
>
> 19-09-2016 15:41:44.613 UTC Status dnscachedresolver.cpp:154:
> 127.0.0.11
>
> 19-09-2016 15:41:44.613 UTC Status sipresolver.cpp:60: Created SIP resolver
>
> 19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5058
>
> 19-09-2016 15:41:44.637 UTC Status stack.cpp:419: Listening on port 5060
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:855: Local host aliases:
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  172.18.0.2
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:  10.0.1.2
>
> 19-09-2016 15:41:44.638 UTC Status stack.cpp:862:
>
> 19-09-2016 15:41:44.639 UTC Status httpresolver.cpp:52: Created HTTP
> resolver
>
> 19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:114: Configuring
> HTTP Connection
>
> 19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:115:   Connection
> created for server ralf:10888
>
> 19-09-2016 15:41:44.641 UTC Status httpconnection.cpp:116:   Connection
> will use a response timeout of 500ms
>
> 19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:72: Creating
> connection pool to scscf.sprout:5052
>
> 19-09-2016 15:41:44.642 UTC Status connection_pool.cpp:73:   connections =
> 50, recycle time = 600 +/- 120 seconds
>
> 19-09-2016 15:41:44.649 UTC Status bono.cpp:3314: Create list of PBXes
>
> 19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:63: Loading plug-ins
> from /usr/share/clearwater/sprout/plugins
>
> 19-09-2016 15:41:44.649 UTC Status pluginloader.cpp:158: Finished loading
> plug-ins
>
> 19-09-2016 15:41:44.652 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
> 19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f3df8 TCP connect()
> error: Connection refused [code=120111]
>
> 19-09-2016 15:41:44.653 UTC Error pjsip:  tcpc0x14f5c38 TCP connect()
> error: Connection refused [code=120111]
>
>
>
>
>
> I have observed that in the bono container 5060 port is not listening in
> all interfaces while 5062 port is listening in all interfaces.
>
>
>
> root at 9837c4dab241:/var/log/bono# netstat -planut | grep LISTEN
>
> tcp        0      0 10.0.1.2:4000           0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 10.0.1.2:5058           0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 127.0.0.11:43395        0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 10.0.1.2:5060           0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 0.0.0.0:5062            0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 127.0.0.1:8080          0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 10.0.1.2:3478           0.0.0.0:*
> LISTEN      -
>
> tcp        0      0 0.0.0.0:22              0.0.0.0:*
> LISTEN      9/sshd
>
> tcp6       0      0 :::22                   :::*                    LISTEN
>      9/sshd
>
> root at 9837c4dab241:/var/log/bono#
>
>
>
>
>
> Is it right to have 5060 port listen on only local port? Please help me to
> debug the issue.
>
>
>
> Thanks,
>
> Sarbajit
>
>
>
>
>
> On Wed, Sep 21, 2016 at 10:01 PM, Graeme Robertson (projectclearwater.org)
> <graeme at projectclearwater.org> wrote:
>
> Hi Sarbajit,
>
>
>
> I?ve had another look at this, and actually I think clearwater-live-test
> checks it can connect to Bono before it tries to provision numbers from
> Ellis, and it?s actually that connection that?s failing ? apologies!
>
>
>
> Can you do similar checks for the Bono container, i.e. connect to the Bono
> container and run ps -eaf | grep bono and run nc -z -v <ip> 5060 from
> your live test container (where <ip> is the IP of your Bono)?
>
>
>
> One other thought ? what command are you using to run the tests? You?ll
> need to set the PROXY option to your Bono IP and the ELLIS option to your
> Ellis IP.
>
>
>
> Thanks,
>
> Graeme
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Sarbajit Chatterjee
> *Sent:* 21 September 2016 16:41
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* Re: [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Thanks Graeme for your reply. Here are the command outputs that you had
> asked -
>
>
>
> root at e994b17b4563:/# ps -eaf | grep ellis
>
> root       177     1  0 Sep19 ?        00:00:10 /usr/share/clearwater/
> clearwater-cluster-manager/env/bin/python /usr/share/clearwater/bin/clearwater-cluster-manager
> --mgmt-local-ip=10.0.1.7 --sig-local-ip=10.0.1.7 --local-site=site1
> --remote-site= --remote-cassandra-seeds= --signaling-namespace=
> --uuid=18c7daf3-a098-47ae-962f-a3d57c0cff6f --etcd-key=clearwater
> --etcd-cluster-key=ellis --log-level=3 --log-directory=/var/log/clearwater-cluster-manager
> --pidfile=/var/run/clearwater-cluster-manager.pid
>
> root       180     1  0 Sep19 ?        00:00:00 /bin/sh /etc/init.d/ellis
> run
>
> ellis      185   180  0 Sep19 ?        00:00:05
> /usr/share/clearwater/ellis/env/bin/python -m metaswitch.ellis.main
>
> root       287   253  0 15:18 ?        00:00:00 grep --color=auto ellis
>
> root at e994b17b4563:/#
>
> root at e994b17b4563:/# ps -eaf | grep nginx
>
> root       179     1  0 Sep19 ?        00:00:00 nginx: master process
> /usr/sbin/nginx -g daemon off;
>
> www-data   186   179  0 Sep19 ?        00:00:16 nginx: worker process
>
> www-data   187   179  0 Sep19 ?        00:00:00 nginx: worker process
>
> www-data   188   179  0 Sep19 ?        00:00:16 nginx: worker process
>
> www-data   189   179  0 Sep19 ?        00:00:16 nginx: worker process
>
> root       289   253  0 15:19 ?        00:00:00 grep --color=auto nginx
>
> root at e994b17b4563:/#
>
> root at e994b17b4563:/# netstat -planut | grep nginx
>
> tcp6       0      0 :::80                   :::*                    LISTEN
>      179/nginx -g daemon
>
> root at e994b17b4563:/#
>
>
>
>
>
> I think both ellis and nginx are running fine inside the container. I can
> also open the ellis login page from a web browser. I also checked the MySQL
> DB in ellis container. I can see livetest user entry in users table and
> 1000 rows in numbers table.
>
>
>
> I can also connect to ellis (host IP 10.109.190.10) from my livetest
> container -
>
>
>
> root at 40efba73deb5:~/clearwater-live-test# nc -v -z 10.109.190.10 80
>
> Connection to 10.109.190.10 80 port [tcp/http] succeeded!
>
> root at 40efba73deb5:~/clearwater-live-test#
>
>
>
>
>
> Is this happening because Clearwater containers are spread across multiple
> hosts? What other areas I should check?
>
>
>
>
>
> Thanks,
>
> Sarbajit
>
>
>
>
>
> On Wed, Sep 21, 2016 at 6:09 PM, Graeme Robertson (projectclearwater.org)
> <graeme at projectclearwater.org> wrote:
>
> Hi Sarbajit,
>
>
>
> I don?t think we?ve never tried deploying Project Clearwater in a Docker
> Swarm cluster, but I don?t see any reason why it couldn?t work. The tests
> are failing very early ? they?re not able to connect to Ellis on port 80. I
> can think of a couple of reasons for this ? either Ellis isn?t running or
> the Ellis port mapping hasn?t worked for some reason.
>
>
>
> Can you connect to the Ellis container and run ps ?eaf | grep ellis and ps
> ?eaf | grep nginx to confirm that NGINX and Ellis are running? Can you
> also run sudo netstat -planut | grep nginx or something equivalent to
> check that NGINX is listening on port 80? If there?s a problem with either
> NGINX or Ellis we probably need to look in the logs at /var/log/nginx/ or
> /var/log/ellis/ on the Ellis container.
>
>
>
> If however that all looks fine, then it sounds like the port mapping has
> failed for some reason. Can you run nc -z <ip> 80 from the box you?re
> running the live tests on? This will scan for anything listening at
> <ip>:80 and will return successfully if it finds anything.
>
>
>
> Thanks,
>
> Graeme
>
>
> ------------------------------
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org
> <clearwater-bounces at lists.projectclearwater.org>] *On Behalf Of *Sarbajit
> Chatterjee
> *Sent:* 20 September 2016 15:05
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] Deploy Clearwater in a Swarm cluster
> using docker-compose
>
>
>
> Hello,
>
>
>
> I am following the instructions from https://github.com/
> Metaswitch/clearwater-docker. I can successfully deploy it on a single
> Docker node but, the compose file does not work with Swarm cluster.
>
>
>
> I did try to modify the compose file like this -
>
>
>
>
>
> version: '2'
>
> services:
>
>   etcd:
>
>     image: quay.io/coreos/etcd:v2.2.5
>
>     command: >
>
>       -name etcd0
>
>       -advertise-client-urls http://etcd:2379,http://etcd:4001
>
>       -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001
>
>       -initial-advertise-peer-urls http://etcd:2380
>
>       -listen-peer-urls http://0.0.0.0:2380
>
>       -initial-cluster etcd0=http://etcd:2380
>
>       -initial-cluster-state new
>
>   bono:
>
>     image: swarm-node:5000/clearwaterdocker_bono
>
>     ports:
>
>       - 22
>
>       - "3478:3478"
>
>       - "3478:3478/udp"
>
>       - "5060:5060"
>
>       - "5060:5060/udp"
>
>       - "5062:5062"
>
>   sprout:
>
>     image: swarm-node:5000/clearwaterdocker_sprout
>
>     networks:
>
>       default:
>
>         aliases:
>
>           - scscf.sprout
>
>           - icscf.sprout
>
>     ports:
>
>       - 22
>
>   homestead:
>
>     image: swarm-node:5000/clearwaterdocker_homestead
>
>     ports:
>
>       - 22
>
>   homer:
>
>     image: swarm-node:5000/clearwaterdocker_homer
>
>     ports:
>
>       - 22
>
>   ralf:
>
>     image: swarm-node:5000/clearwaterdocker_ralf
>
>     ports:
>
>       - 22
>
>   ellis:
>
>     image: swarm-node:5000/clearwaterdocker_ellis
>
>     ports:
>
>       - 22
>
>       - "80:80"
>
>
>
>
>
> where swarm-node:5000 is the local docker registry and it hosts the
> pre-built images of Clearwater containers. Even though the deployment
> succeeded, clearwater-livetests are failing with following error -
>
>
>
>
>
> Basic Registration (TCP) - Failed
>
>   Errno::ECONNREFUSED thrown:
>
>    - Connection refused - connect(2)
>
>      - /usr/local/rvm/gems/ruby-1.9.3-p551/gems/quaff-0.7.3/lib/sources.rb:41:in
> `initialize'
>
>
>
>
>
> Any suggestions on how I can deploy Clearwater on a Swarm cluster?
>
>
>
> Thanks,
>
> Sarbajit
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160928/dfb1aebe/attachment.html>

From rob at projectclearwater.org  Wed Sep 28 12:41:35 2016
From: rob at projectclearwater.org (Robert Day (projectclearwater.org))
Date: Wed, 28 Sep 2016 16:41:35 +0000
Subject: [Project Clearwater] Nidoking release note
Message-ID: <BY2PR02MB2149983FE0E2CCF986431B67F4CF0@BY2PR02MB2149.namprd02.prod.outlook.com>

The release for Project Clearwater sprint "Nidoking" has been cut. The code for this release is tagged as release-107 in github.

In this release, we've been focusing on our database infrastructure. We've added some retry logic<https://github.com/Metaswitch/cpp-common/pull/561> to our Cassandra connection code allowing us to try other Cassandra nodes when our first one fails, and we've kicked off investigations into upgrading our supported Cassandra version from 2.0.14 to the latest stable release.

This release includes the following bug fixes:


*         Bono doesn't SAS log rejected requests (https://github.com/Metaswitch/sprout/issues/1008)

*         Exception hit while processing ACK causes crash (https://github.com/Metaswitch/sprout/issues/1504)

*         Filtering - RFC3841 example (TCP) live test fails intermittently (https://github.com/Metaswitch/clearwater-live-test/issues/128)

*         Monit doesn't monitor etcd plugins (https://github.com/Metaswitch/clearwater-etcd/issues/317)

*         'error' log for dull signaling flow (https://github.com/Metaswitch/sprout/issues/1531)

*         Intermittent SNMP statistic failures (https://github.com/Metaswitch/clearwater-fv-test/issues/41)

To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html. If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova) has been updated for this release.

Rob

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160928/f9f742d6/attachment.html>

From eleanor at projectclearwater.org  Wed Sep 28 14:27:58 2016
From: eleanor at projectclearwater.org (Eleanor Merry (projectclearwater.org))
Date: Wed, 28 Sep 2016 18:27:58 +0000
Subject: [Project Clearwater] Using Ralf
In-Reply-To: <CAM_v+eRYJqSnBf1Ryk0YDfn3Fo=CDGU2D9qrbrPPFuge5tb6GQ@mail.gmail.com>
References: <CAM_v+eRYJqSnBf1Ryk0YDfn3Fo=CDGU2D9qrbrPPFuge5tb6GQ@mail.gmail.com>
Message-ID: <BL2PR02MB20842D9A5D41D0790853B0F09BCF0@BL2PR02MB2084.namprd02.prod.outlook.com>

Hi Morris,

There aren?t any rules that you need to set up for Ralf. To enable Ralf on your deployment, set ralf_hostname in /etc/clearwater/shared_config to a hostname that resolves to all of your Ralf nodes. With this set, Bono and Sprout will report P-CSCF and I-CSCF/S-CSCF chargeable events to Ralf.

Ralf then will send these events over the Rf interface to an external Charging Data Function (CDF) (and we don?t provide a CDF in Project Clearwater). The CDF then generates CDRs, which are billing records that you can then process (see https://en.wikipedia.org/wiki/Charging_data_record for more details of the information stored in billing records). How you can pull the CDRs from your CDF, or what processing the CDF does to the CDRs really depends on what CDF you have.

To integrate Ralf with a CDF, please follow the document at http://clearwater.readthedocs.io/en/latest/CDF_Integration.html. We?ve also blogged about Ralf and Rf billing at http://www.projectclearwater.org/rf-billing/, which you may find interesting.

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of yan morris
Sent: 27 September 2016 15:22
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] Using Ralf

Hi ,

As I know , Ralf is used to provide offline billing.

However , I don't know how to use it , for example : setting billing rules

I want to know how to use it , could you tell me how to use it or are there

documents I can refer ?


Thanks a lot .


Morris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160928/9d4fae6b/attachment.html>

From eleanor at projectclearwater.org  Wed Sep 28 15:55:23 2016
From: eleanor at projectclearwater.org (Eleanor Merry (projectclearwater.org))
Date: Wed, 28 Sep 2016 19:55:23 +0000
Subject: [Project Clearwater] DNS Problem while registration of sip
	client
In-Reply-To: <CAHfUHgGs2sA6P+CaGVgigpRvsBaMPubGuKbmbQ5vnhQNxQjbAA@mail.gmail.com>
References: <CAHfUHgGs2sA6P+CaGVgigpRvsBaMPubGuKbmbQ5vnhQNxQjbAA@mail.gmail.com>
Message-ID: <BL2PR02MB20845315176685200E81B5329BCF0@BL2PR02MB2084.namprd02.prod.outlook.com>

Hi Manikantha,

Can you run ?nslookup ralf.cw.pramati.com<http://ralf.cw.pramati.com>? on your nodes, and see what the ?Server? value is?

The clearwater processes look in /etc/resolv.conf for DNS configuration. If your nodes don?t use localhost (which you can tell as the Server value above won?t be 127.0.0.1) you?ll need to update the dnsmasq configuration to point to this server rather than localhost. You can find details of how to do this at: https://github.com/Metaswitch/clearwater-docs/wiki/Clearwater-DNS-Usage#client-configuration.

Ellie

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Manikantha Srinivas Tadi
Sent: 28 September 2016 09:12
To: clearwater at lists.projectclearwater.org
Subject: [Project Clearwater] DNS Problem while registration of sip client

Hi,

We are trying clearwater IMS to setup a call between sip client.

How ever, while registering the sip client using CSIPSimple utility , We are getting the following error logs in /var/log/bono and /var/log/sprout

Bono Logs :

8-09-2016 08:04:45.765 UTC Error dnscachedresolver.cpp:607: Failed to retrieve record for ralf.cw.pramati.com<http://ralf.cw.pramati.com>: Could not contact DNS servers
28-09-2016 08:04:45.765 UTC Error httpclient.cpp:623: cURL failure with cURL error code 6 (see man 3 libcurl-errors) and HTTP error code 404
28-09-2016 08:04:50.652 UTC Status alarm.cpp:62: sprout issued 1005.4 alarm
28-09-2016 08:04:52.100 UTC Warning (Net-SNMP): Warning: Failed to connect to the agentx master agent ([NIL]):

Sprout Logs :

28-09-2016 08:08:26.816 UTC Status alarm.cpp:62: sprout issued 1001.3 alarm
28-09-2016 08:08:37.986 UTC Error dnscachedresolver.cpp:607: Failed to retrieve record for hs.cw.pramati.com<http://hs.cw.pramati.com>: Could not contact DNS servers
28-09-2016 08:08:37.986 UTC Error httpclient.cpp:623: cURL failure with cURL error code 6 (see man 3 libcurl-errors) and HTTP error code 404
28-09-2016 08:08:56.816 UTC Status alarm.cpp:62: sprout issued 1001.3 alarm
28-09-2016 08:09:05.374 UTC Status load_monitor.cpp:285: Maximum incoming request rate/second unchanged - only handled 20 requests in last 191050ms, minimum threshold for a change is 23881.250000


Where as i am able to manually resolve the DNS Records for ralf.cw.pramati.com<http://ralf.cw.pramati.com> and hs.cw.pramati.com<http://hs.cw.pramati.com> on the bono and sprout nodes respectively.

Could someone please help if we are missing something in this regards ?

Thanks
Manikantha.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160928/4c0a9fe3/attachment.html>

From manikantha.tadi at gmail.com  Thu Sep 29 07:27:36 2016
From: manikantha.tadi at gmail.com (Manikantha Srinivas Tadi)
Date: Thu, 29 Sep 2016 16:57:36 +0530
Subject: [Project Clearwater] DNS Problem while registration of sip
	client
In-Reply-To: <BL2PR02MB20845315176685200E81B5329BCF0@BL2PR02MB2084.namprd02.prod.outlook.com>
References: <CAHfUHgGs2sA6P+CaGVgigpRvsBaMPubGuKbmbQ5vnhQNxQjbAA@mail.gmail.com>
	<BL2PR02MB20845315176685200E81B5329BCF0@BL2PR02MB2084.namprd02.prod.outlook.com>
Message-ID: <CAHfUHgF6GySop5GUX+cUyaNSTgkZP2tx38K6GQfU5V=-ev5haQ@mail.gmail.com>

Hi Ellie,

Thanks for the response, Registration and call between sip clients
succeeded.

Regards,
Manikantha.

On Thu, Sep 29, 2016 at 1:25 AM, Eleanor Merry (projectclearwater.org) <
eleanor at projectclearwater.org> wrote:

> Hi Manikantha,
>
>
>
> Can you run ?nslookup ralf.cw.pramati.com? on your nodes, and see what
> the ?Server? value is?
>
>
>
> The clearwater processes look in /etc/resolv.conf for DNS configuration.
> If your nodes don?t use localhost (which you can tell as the Server value
> above won?t be 127.0.0.1) you?ll need to update the dnsmasq configuration
> to point to this server rather than localhost. You can find details of how
> to do this at: https://github.com/Metaswitch/clearwater-docs/wiki/
> Clearwater-DNS-Usage#client-configuration.
>
>
>
> Ellie
>
>
>
> *From:* Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org]
> *On Behalf Of *Manikantha Srinivas Tadi
> *Sent:* 28 September 2016 09:12
> *To:* clearwater at lists.projectclearwater.org
> *Subject:* [Project Clearwater] DNS Problem while registration of sip
> client
>
>
>
> Hi,
>
>
>
> We are trying clearwater IMS to setup a call between sip client.
>
>
>
> How ever, while registering the sip client using CSIPSimple utility , We
> are getting the following error logs in /var/log/bono and /var/log/sprout
>
>
>
> Bono Logs :
>
>
>
> 8-09-2016 08:04:45.765 UTC Error dnscachedresolver.cpp:607: Failed to
> retrieve record for ralf.cw.pramati.com: Could not contact DNS servers
>
> 28-09-2016 08:04:45.765 UTC Error httpclient.cpp:623: cURL failure with
> cURL error code 6 (see man 3 libcurl-errors) and HTTP error code 404
>
> 28-09-2016 08:04:50.652 UTC Status alarm.cpp:62: sprout issued 1005.4 alarm
>
> 28-09-2016 08:04:52.100 UTC Warning (Net-SNMP): Warning: Failed to connect
> to the agentx master agent ([NIL]):
>
>
>
> Sprout Logs :
>
>
>
> 28-09-2016 08:08:26.816 UTC Status alarm.cpp:62: sprout issued 1001.3 alarm
>
> 28-09-2016 08:08:37.986 UTC Error dnscachedresolver.cpp:607: Failed to
> retrieve record for hs.cw.pramati.com: Could not contact DNS servers
>
> 28-09-2016 08:08:37.986 UTC Error httpclient.cpp:623: cURL failure with
> cURL error code 6 (see man 3 libcurl-errors) and HTTP error code 404
>
> 28-09-2016 08:08:56.816 UTC Status alarm.cpp:62: sprout issued 1001.3 alarm
>
> 28-09-2016 08:09:05.374 UTC Status load_monitor.cpp:285: Maximum incoming
> request rate/second unchanged - only handled 20 requests in last 191050ms,
> minimum threshold for a change is 23881.250000
>
>
>
>
>
> Where as i am able to manually resolve the DNS Records for
> ralf.cw.pramati.com and hs.cw.pramati.com on the bono and sprout nodes
> respectively.
>
>
>
> Could someone please help if we are missing something in this regards ?
>
>
>
> Thanks
>
> Manikantha.
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160929/19c31e63/attachment.html>

From liujuan.zhu at hpe.com  Fri Sep 30 01:21:33 2016
From: liujuan.zhu at hpe.com (Zhu, Liu-Juan (Linda, CMS CoE China))
Date: Fri, 30 Sep 2016 05:21:33 +0000
Subject: [Project Clearwater] Clearwater IMS in docker
Message-ID: <AT5PR84MB00194F77295D81642B3002D6E1C10@AT5PR84MB0019.NAMPRD84.PROD.OUTLOOK.COM>

Dear all,

I would like to consult 2 questions:


1)      Is it possible for us to get the docker image (all-in-one or distributed) officially ?

2)      Does the current source code provided from https://github.com/Metaswitch/clearwater-docker is good enough for installing the Clearwater IMS in docker successfully?

Thank you very much for your support and look forward our response at your earliest.


Best Regards,
Linda

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20160930/3fa9a85a/attachment.html>

