From tcchindeka at gmail.com  Wed May  2 09:29:46 2018
From: tcchindeka at gmail.com (Tapiwa Chindeka)
Date: Wed, 2 May 2018 15:29:46 +0200
Subject: [Project Clearwater] Ellis failed to update server error
Message-ID: <CAFFdzJQdsVGkbjaee_oeqK=Tk4R3JSokWKrFFuNB3i9nJJvyWw@mail.gmail.com>

Hi

I installed Clearwater in OpenStack using the heat templates. Now when I
try to create a number through Ellis, I get an error saying Failed to
update server. The logs from my web console show the following:


{"username": "tc....com", "full_name": "Tapiwa"} common.js:30:7
Updating URLs common.js:30:7
Old url
http://ellis.openstack.rhodes.com/index.html?data=%7B%22username%22%3A%20%22tc....com%22%2C%20%22full_name%22%3A%20%22Tapiwa%22%7D&message=Created&status=201&success=true#
common.js:30:7
Old url http://ellis.openstack.rhodes.com/ common.js:30:7
Old url http://ellis.openstack.rhodes.com/addressbook.html common.js:30:7
Old url common.js:30:7
Old url javascript:void(0)
common.js:30:7
Old url common.js:30:7
Old url
http://ellis.openstack.rhodes.com/index.html?data=%7B%22username%22%3A%20%22tc....com%22%2C%20%22full_name%22%3A%20%22Tapiwa%22%7D&message=Created&status=201&success=true#privacy-pane
common.js:30:7
Old url
http://ellis.openstack.rhodes.com/index.html?data=%7B%22username%22%3A%20%22tc....com%22%2C%20%22full_name%22%3A%20%22Tapiwa%22%7D&message=Created&status=201&success=true#redirect-pane
common.js:30:7
Old url
http://ellis.openstack.rhodes.com/index.html?data=%7B%22username%22%3A%20%22tc....com%22%2C%20%22full_name%22%3A%20%22Tapiwa%22%7D&message=Created&status=201&success=true#barring-pane
common.js:30:7
Old url
http://ellis.openstack.rhodes.com/index.html?data=%7B%22username%22%3A%20%22tc....com%22%2C%20%22full_name%22%3A%20%22Tapiwa%22%7D&message=Created&status=201&success=true#as-pane
common.js:30:7
Old url javascript:void(0)
common.js:30:7
Old url common.js:30:7
Old url
http://ellis.openstack.rhodes.com/index.html?data=%7B%22username%22%3A%20%22tc....com%22%2C%20%22full_name%22%3A%20%22Tapiwa%22%7D&message=Created&status=201&success=true#rule
common.js:30:7
Old url javascript:void(0)
common.js:30:7
Hash changed to '' common.js:30:7
Changing page from pleasewait to dashboard common.js:30:7
Fade out complete, fading in... common.js:30:7
Getting /accounts/tc....com/numbers/ common.js:30:7
Creating a number common.js:30:7
Posting to /accounts/tc....com/numbers/ common.js:30:7
{"status": 502, "message": "Bad Gateway", "reason": "Upstream request
failed", "detail": {"Upstream error": "502", "Upstream URL": "
http://hs-prov.openstack.rhodes.com:8889/private/6505550412%40openstack.rhodes.com"},
"error": true}



I also used  clearwater-diags-monitor to get diagnostics information
specifically connectivity_to_hs.openstack.rhodes.com.txt which shows:

; <<>> DiG 9.9.5-3ubuntu0.16-Ubuntu <<>> hs.openstack.rhodes.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 40176
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 1, ADDITIONAL: 2

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;hs.openstack.rhodes.com.       IN      A

;; ANSWER SECTION:
hs.openstack.rhodes.com. 30     IN      A       172.16.10.6

;; AUTHORITY SECTION:
openstack.rhodes.com.   3600    IN      NS      ns.openstack.rhodes.com.

;; ADDITIONAL SECTION:
ns.openstack.rhodes.com. 3600   IN      A       203.0.113.105

;; Query time: 20 msec
;; SERVER: 127.0.0.1#53(127.0.0.1)
;; WHEN: Wed May 02 13:09:11 UTC 2018
;; MSG SIZE  rcvd: 101

Check connectivity:
172.16.10.6:8888 FAILED


Any idea what the problem might be?

Kind regards
Tapiwa
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180502/510d15d8/attachment.html>

From arjunsinghbhai7 at gmail.com  Thu May  3 08:01:10 2018
From: arjunsinghbhai7 at gmail.com (Arjun Singh)
Date: Thu, 3 May 2018 17:31:10 +0530
Subject: [Project Clearwater] Failing in registration
Message-ID: <CA+PF-omYrXN=Vj34e1LWMTZCOvkMoCqj-fHaiQOdwhkWZY1tmw@mail.gmail.com>

Hi,
I was using manual installation of clearwater. I have installed six node
successfully and everthing is fine.

I am using virtualbox for installing the VMs and there I am using *host
only network* so that I got private IP for every VM and I make that IP as
local_ip as 192.168.56.xxx and public_ip is random or same as local_ip.

I can ping every node from every other node using their domain name and
also using their IPs.

I am facing problem while opening ellis.arjun.org in browser.
its giving like:



*Network Error (dns_unresolved_hostname) *
*Your requested host "ellis.arjun.org <http://ellis.arjun.org>" could not
be resolved by DNS.*

*ellis log is also not showing anything even I have set the log_level=5 in
user_settings*


[*ellis]ubuntu at elli:/var/log/ellis$ cat ellis_20180503T190000Z.txt*
03-05-2018 19:00:04.484 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.27ms
03-05-2018 19:00:14.532 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.29ms
03-05-2018 19:00:24.597 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.26ms
03-05-2018 19:00:34.645 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.24ms
03-05-2018 19:00:44.701 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.27ms
03-05-2018 19:00:54.764 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.28ms
03-05-2018 19:01:04.817 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.26ms
03-05-2018 19:01:14.873 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.31ms
03-05-2018 19:01:24.924 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.25ms
03-05-2018 19:01:34.961 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.29ms
03-05-2018 19:01:45.014 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.29ms
03-05-2018 19:01:55.082 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.20ms
03-05-2018 19:02:05.122 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0) 0.20ms


*[ellis]ubuntu at elli:~$ ping sprout.arjun.org <http://sprout.arjun.org>*
PING sprout.imsamruta.org (192.168.56.112) 56(84) bytes of data.
64 bytes from 192.168.56.112: icmp_seq=1 ttl=64 time=0.430 ms
64 bytes from 192.168.56.112: icmp_seq=2 ttl=64 time=0.432 ms
64 bytes from 192.168.56.112: icmp_seq=3 ttl=64 time=0.540 ms



*[ellis]ubuntu at elli:~$ netstat -tupln*
(No info could be read for "-p": geteuid()=1000 but you should be root.)
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
     PID/Program name
tcp        0      0 127.0.0.1:53            0.0.0.0:*               LISTEN
    -
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
    -
tcp        0      0 127.0.0.1:2812          0.0.0.0:*               LISTEN
    -
tcp        0      0 127.0.0.1:8000          0.0.0.0:*               LISTEN
    -
tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN
    -
tcp        0      0 192.168.56.114:2380     0.0.0.0:*               LISTEN
    -
tcp6       0      0 ::1:53                  :::*                    LISTEN
    -
tcp6       0      0 :::22                   :::*                    LISTEN
    -
tcp6       0      0 :::4000                 :::*                    LISTEN
    -
tcp6       0      0 :::80                   :::*                    LISTEN
    -
udp        0      0 0.0.0.0:18146           0.0.0.0:*
     -
udp        0      0 127.0.0.1:53            0.0.0.0:*
     -
udp        0      0 0.0.0.0:68              0.0.0.0:*
     -
udp        0      0 192.168.56.114:123      0.0.0.0:*
     -
udp        0      0 10.0.2.15:123           0.0.0.0:*
     -
udp        0      0 127.0.0.1:123           0.0.0.0:*
     -
udp        0      0 0.0.0.0:123             0.0.0.0:*
     -
udp6       0      0 ::1:53                  :::*
    -
udp6       0      0 fe80::a00:27ff:fe93:123 :::*
    -
udp6       0      0 fe80::a00:27ff:fe14:123 :::*
    -
udp6       0      0 ::1:123                 :::*
    -
udp6       0      0 :::123                  :::*
    -
udp6       0      0 :::12576                :::*
    -




*[sprout]ubuntu at sprou:~$ netstat -tupln*
(No info could be read for "-p": geteuid()=1000 but you should be root.)
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
     PID/Program name
tcp        0      0 127.0.0.1:53            0.0.0.0:*               LISTEN
    -
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
    -
tcp        0      0 127.0.0.1:2812          0.0.0.0:*               LISTEN
    -
tcp        0      0 192.168.56.112:5052     0.0.0.0:*               LISTEN
    -
tcp        0      0 192.168.56.112:5053     0.0.0.0:*               LISTEN
    -
tcp        0      0 192.168.56.112:5054     0.0.0.0:*               LISTEN
    -
tcp        0      0 192.168.56.112:5055     0.0.0.0:*               LISTEN
    -
tcp        0      0 192.168.56.112:9888     0.0.0.0:*               LISTEN
    -
tcp        0      0 127.0.0.1:8000          0.0.0.0:*               LISTEN
    -
tcp        0      0 192.168.56.112:2380     0.0.0.0:*               LISTEN
    -
tcp6       0      0 ::1:53                  :::*                    LISTEN
    -
tcp6       0      0 :::22                   :::*                    LISTEN
    -
tcp6       0      0 :::4000                 :::*                    LISTEN
    -
udp        0      0 192.168.56.112:5052     0.0.0.0:*
     -
udp        0      0 192.168.56.112:5053     0.0.0.0:*
     -
udp        0      0 192.168.56.112:5054     0.0.0.0:*
     -
udp        0      0 192.168.56.112:5055     0.0.0.0:*
     -
udp        0      0 0.0.0.0:30370           0.0.0.0:*
     -
udp        0      0 127.0.0.1:53            0.0.0.0:*
     -
udp        0      0 0.0.0.0:68              0.0.0.0:*
     -
udp        0      0 192.168.56.112:123      0.0.0.0:*
     -
udp        0      0 10.0.2.15:123           0.0.0.0:*
     -
udp        0      0 127.0.0.1:123           0.0.0.0:*
     -
udp        0      0 0.0.0.0:123             0.0.0.0:*
     -
udp6       0      0 :::15002                :::*
    -
udp6       0      0 ::1:53                  :::*
    -
udp6       0      0 fe80::a00:27ff:fe69:123 :::*
    -
udp6       0      0 fe80::a00:27ff:fe2b:123 :::*
    -
udp6       0      0 ::1:123                 :::*
    -
udp6       0      0 :::123                  :::*
    -




Even I tried to make a call using stress testing, I have created the
subscribers on vellum node, that was successful. I am able to ping sprout
or anyother node from stress node but
initial registration itself is failed:

*[]ubuntu@(none):~$ /usr/share/clearwater/bin/run_stress arjun.org
<http://arjun.org> 20 1*
Starting initial registration, will take 0 seconds
Initial registration failed - see
/var/log/clearwater-sip-stress/5398_initial_reg_errors.log for details of
the errors

*cat  /var/log/clearwater-sip-stress/5398_initial_reg_errors.log*
2018-05-03      22:03:48.050591 1525365228.050591: Aborting call on
unexpected message for Call-Id '20-5405 at 192.168
Via: SIP/2.0/TCP 192.168.56.116:32926
;received=192.168.56.116;branch=z9hG4bK-5405-20-0
Call-ID: 20-5405 at 192.168.56.116
From: <sip:2010000004 at arjun.org>;tag=5405SIPpTag0020
To: <sip:2010000004 at arjun.org>;tag=z9hG4bKPjXJj-5EN6o8CATIEn9fdaPbynYS8Iuf0F
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f20"
Content-Length:  0


*[]ubuntu@(none):~$ netstat -tupln*
(No info could be read for "-p": geteuid()=1000 but you should be root.)
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
     PID/Program name
tcp        0      0 127.0.0.1:53            0.0.0.0:*               LISTEN
    -
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
    -
tcp        0      0 127.0.0.1:2812          0.0.0.0:*               LISTEN
    -
tcp6       0      0 ::1:53                  :::*                    LISTEN
    -
tcp6       0      0 :::22                   :::*                    LISTEN
    -
udp        0      0 0.0.0.0:42737           0.0.0.0:*
     -
udp        0      0 127.0.0.1:53            0.0.0.0:*
     -
udp        0      0 0.0.0.0:68              0.0.0.0:*
     -
udp        0      0 192.168.56.116:123      0.0.0.0:*
     -
udp        0      0 10.0.2.15:123           0.0.0.0:*
     -
udp        0      0 127.0.0.1:123           0.0.0.0:*
     -
udp        0      0 0.0.0.0:123             0.0.0.0:*
     -
udp6       0      0 ::1:53                  :::*
    -
udp6       0      0 fe80::a00:27ff:fe79:123 :::*
    -
udp6       0      0 fe80::a00:27ff:fedf:123 :::*
    -
udp6       0      0 ::1:123                 :::*
    -
udp6       0      0 :::123                  :::*
    -
udp6       0      0 :::47700                :::*
    -



Please guide me what I am doing wrong and how do I fix it, I have just
followed the document given.

Regards,
Arjun
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180503/f164a1fe/attachment.html>

From arjunsinghbhai7 at gmail.com  Thu May  3 08:03:51 2018
From: arjunsinghbhai7 at gmail.com (Arjun Singh)
Date: Thu, 3 May 2018 17:33:51 +0530
Subject: [Project Clearwater] Failing in registration
In-Reply-To: <CA+PF-omYrXN=Vj34e1LWMTZCOvkMoCqj-fHaiQOdwhkWZY1tmw@mail.gmail.com>
References: <CA+PF-omYrXN=Vj34e1LWMTZCOvkMoCqj-fHaiQOdwhkWZY1tmw@mail.gmail.com>
Message-ID: <CA+PF-o=LD1StPwMEQMe5kbFMe2OdTtQ1qf8SSdFta7i-3n1qdQ@mail.gmail.com>

missed the complete the log of stress node :

*2018-05-03      22:03:47.910668 1525365227.910668: Aborting call on
unexpected message for Call-Id '8-5405 at 192.168.56.116
<8-5405 at 192.168.56.116>': while expecting '401' (index 1), received
'SIP/2.0 403 Forbidden*
Via: SIP/2.0/TCP 192.168.56.116:46460
;received=192.168.56.116;branch=z9hG4bK-5405-8-0
Call-ID: 8-5405 at 192.168.56.116
From: <sip:2010000014 at arjun.org>;tag=5405SIPpTag008
To: <sip:2010000014 at arjun.org>;tag=z9hG4bKPjB7yZvH.mfJ8DEJzQyiVg31OynkyLghPU
CSeq: 1 REGISTER
P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f8"
Content-Length:  0

thanks


On Thu, May 3, 2018 at 5:31 PM, Arjun Singh <arjunsinghbhai7 at gmail.com>
wrote:

> Hi,
> I was using manual installation of clearwater. I have installed six node
> successfully and everthing is fine.
>
> I am using virtualbox for installing the VMs and there I am using *host
> only network* so that I got private IP for every VM and I make that IP as
> local_ip as 192.168.56.xxx and public_ip is random or same as local_ip.
>
> I can ping every node from every other node using their domain name and
> also using their IPs.
>
> I am facing problem while opening ellis.arjun.org in browser.
> its giving like:
>
>
>
> *Network Error (dns_unresolved_hostname) *
> *Your requested host "ellis.arjun.org <http://ellis.arjun.org>" could not
> be resolved by DNS.*
>
> *ellis log is also not showing anything even I have set the log_level=5 in
> user_settings*
>
>
> [*ellis]ubuntu at elli:/var/log/ellis$ cat ellis_20180503T190000Z.txt*
> 03-05-2018 19:00:04.484 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.27ms
> 03-05-2018 19:00:14.532 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.29ms
> 03-05-2018 19:00:24.597 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.26ms
> 03-05-2018 19:00:34.645 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.24ms
> 03-05-2018 19:00:44.701 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.27ms
> 03-05-2018 19:00:54.764 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.28ms
> 03-05-2018 19:01:04.817 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.26ms
> 03-05-2018 19:01:14.873 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.31ms
> 03-05-2018 19:01:24.924 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.25ms
> 03-05-2018 19:01:34.961 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.29ms
> 03-05-2018 19:01:45.014 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.29ms
> 03-05-2018 19:01:55.082 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.20ms
> 03-05-2018 19:02:05.122 UTC INFO web.py:1447: 200 GET /ping (0.0.0.0)
> 0.20ms
>
>
> *[ellis]ubuntu at elli:~$ ping sprout.arjun.org <http://sprout.arjun.org>*
> PING sprout.imsamruta.org (192.168.56.112) 56(84) bytes of data.
> 64 bytes from 192.168.56.112: icmp_seq=1 ttl=64 time=0.430 ms
> 64 bytes from 192.168.56.112: icmp_seq=2 ttl=64 time=0.432 ms
> 64 bytes from 192.168.56.112: icmp_seq=3 ttl=64 time=0.540 ms
>
>
>
> *[ellis]ubuntu at elli:~$ netstat -tupln*
> (No info could be read for "-p": geteuid()=1000 but you should be root.)
> Active Internet connections (only servers)
> Proto Recv-Q Send-Q Local Address           Foreign Address         State
>      PID/Program name
> tcp        0      0 127.0.0.1:53            0.0.0.0:*
>  LISTEN      -
> tcp        0      0 0.0.0.0:22              0.0.0.0:*
>  LISTEN      -
> tcp        0      0 127.0.0.1:2812          0.0.0.0:*
>  LISTEN      -
> tcp        0      0 127.0.0.1:8000          0.0.0.0:*
>  LISTEN      -
> tcp        0      0 127.0.0.1:3306          0.0.0.0:*
>  LISTEN      -
> tcp        0      0 192.168.56.114:2380     0.0.0.0:*
>  LISTEN      -
> tcp6       0      0 ::1:53                  :::*
> LISTEN      -
> tcp6       0      0 :::22                   :::*
> LISTEN      -
> tcp6       0      0 :::4000                 :::*
> LISTEN      -
> tcp6       0      0 :::80                   :::*
> LISTEN      -
> udp        0      0 0.0.0.0:18146           0.0.0.0:*
>        -
> udp        0      0 127.0.0.1:53            0.0.0.0:*
>        -
> udp        0      0 0.0.0.0:68              0.0.0.0:*
>        -
> udp        0      0 192.168.56.114:123      0.0.0.0:*
>        -
> udp        0      0 10.0.2.15:123           0.0.0.0:*
>        -
> udp        0      0 127.0.0.1:123           0.0.0.0:*
>        -
> udp        0      0 0.0.0.0:123             0.0.0.0:*
>        -
> udp6       0      0 ::1:53                  :::*
>       -
> udp6       0      0 fe80::a00:27ff:fe93:123 :::*
>       -
> udp6       0      0 fe80::a00:27ff:fe14:123 :::*
>       -
> udp6       0      0 ::1:123                 :::*
>       -
> udp6       0      0 :::123                  :::*
>       -
> udp6       0      0 :::12576                :::*
>       -
>
>
>
>
> *[sprout]ubuntu at sprou:~$ netstat -tupln*
> (No info could be read for "-p": geteuid()=1000 but you should be root.)
> Active Internet connections (only servers)
> Proto Recv-Q Send-Q Local Address           Foreign Address         State
>      PID/Program name
> tcp        0      0 127.0.0.1:53            0.0.0.0:*
>  LISTEN      -
> tcp        0      0 0.0.0.0:22              0.0.0.0:*
>  LISTEN      -
> tcp        0      0 127.0.0.1:2812          0.0.0.0:*
>  LISTEN      -
> tcp        0      0 192.168.56.112:5052     0.0.0.0:*
>  LISTEN      -
> tcp        0      0 192.168.56.112:5053     0.0.0.0:*
>  LISTEN      -
> tcp        0      0 192.168.56.112:5054     0.0.0.0:*
>  LISTEN      -
> tcp        0      0 192.168.56.112:5055     0.0.0.0:*
>  LISTEN      -
> tcp        0      0 192.168.56.112:9888     0.0.0.0:*
>  LISTEN      -
> tcp        0      0 127.0.0.1:8000          0.0.0.0:*
>  LISTEN      -
> tcp        0      0 192.168.56.112:2380     0.0.0.0:*
>  LISTEN      -
> tcp6       0      0 ::1:53                  :::*
> LISTEN      -
> tcp6       0      0 :::22                   :::*
> LISTEN      -
> tcp6       0      0 :::4000                 :::*
> LISTEN      -
> udp        0      0 192.168.56.112:5052     0.0.0.0:*
>        -
> udp        0      0 192.168.56.112:5053     0.0.0.0:*
>        -
> udp        0      0 192.168.56.112:5054     0.0.0.0:*
>        -
> udp        0      0 192.168.56.112:5055     0.0.0.0:*
>        -
> udp        0      0 0.0.0.0:30370           0.0.0.0:*
>        -
> udp        0      0 127.0.0.1:53            0.0.0.0:*
>        -
> udp        0      0 0.0.0.0:68              0.0.0.0:*
>        -
> udp        0      0 192.168.56.112:123      0.0.0.0:*
>        -
> udp        0      0 10.0.2.15:123           0.0.0.0:*
>        -
> udp        0      0 127.0.0.1:123           0.0.0.0:*
>        -
> udp        0      0 0.0.0.0:123             0.0.0.0:*
>        -
> udp6       0      0 :::15002                :::*
>       -
> udp6       0      0 ::1:53                  :::*
>       -
> udp6       0      0 fe80::a00:27ff:fe69:123 :::*
>       -
> udp6       0      0 fe80::a00:27ff:fe2b:123 :::*
>       -
> udp6       0      0 ::1:123                 :::*
>       -
> udp6       0      0 :::123                  :::*
>       -
>
>
>
>
> Even I tried to make a call using stress testing, I have created the
> subscribers on vellum node, that was successful. I am able to ping sprout
> or anyother node from stress node but
> initial registration itself is failed:
>
> *[]ubuntu@(none):~$ /usr/share/clearwater/bin/run_stress arjun.org
> <http://arjun.org> 20 1*
> Starting initial registration, will take 0 seconds
> Initial registration failed - see /var/log/clearwater-sip-
> stress/5398_initial_reg_errors.log for details of the errors
>
> *cat  /var/log/clearwater-sip-stress/5398_initial_reg_errors.log*
> 2018-05-03      22:03:48.050591 1525365228.050591: Aborting call on
> unexpected message for Call-Id '20-5405 at 192.168
> Via: SIP/2.0/TCP 192.168.56.116:32926;received=
> 192.168.56.116;branch=z9hG4bK-5405-20-0
> Call-ID: 20-5405 at 192.168.56.116
> From: <sip:2010000004 at arjun.org>;tag=5405SIPpTag0020
> To: <sip:2010000004 at arjun.org>;tag=z9hG4bKPjXJj-
> 5EN6o8CATIEn9fdaPbynYS8Iuf0F
> CSeq: 1 REGISTER
> P-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f20"
> Content-Length:  0
>
>
> *[]ubuntu@(none):~$ netstat -tupln*
> (No info could be read for "-p": geteuid()=1000 but you should be root.)
> Active Internet connections (only servers)
> Proto Recv-Q Send-Q Local Address           Foreign Address         State
>      PID/Program name
> tcp        0      0 127.0.0.1:53            0.0.0.0:*
>  LISTEN      -
> tcp        0      0 0.0.0.0:22              0.0.0.0:*
>  LISTEN      -
> tcp        0      0 127.0.0.1:2812          0.0.0.0:*
>  LISTEN      -
> tcp6       0      0 ::1:53                  :::*
> LISTEN      -
> tcp6       0      0 :::22                   :::*
> LISTEN      -
> udp        0      0 0.0.0.0:42737           0.0.0.0:*
>        -
> udp        0      0 127.0.0.1:53            0.0.0.0:*
>        -
> udp        0      0 0.0.0.0:68              0.0.0.0:*
>        -
> udp        0      0 192.168.56.116:123      0.0.0.0:*
>        -
> udp        0      0 10.0.2.15:123           0.0.0.0:*
>        -
> udp        0      0 127.0.0.1:123           0.0.0.0:*
>        -
> udp        0      0 0.0.0.0:123             0.0.0.0:*
>        -
> udp6       0      0 ::1:53                  :::*
>       -
> udp6       0      0 fe80::a00:27ff:fe79:123 :::*
>       -
> udp6       0      0 fe80::a00:27ff:fedf:123 :::*
>       -
> udp6       0      0 ::1:123                 :::*
>       -
> udp6       0      0 :::123                  :::*
>       -
> udp6       0      0 :::47700                :::*
>       -
>
>
>
> Please guide me what I am doing wrong and how do I fix it, I have just
> followed the document given.
>
> Regards,
> Arjun
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180503/2217d26a/attachment.html>

From Matthew.Davis.2 at team.telstra.com  Thu May  3 20:46:17 2018
From: Matthew.Davis.2 at team.telstra.com (Davis, Matthew)
Date: Fri, 4 May 2018 00:46:17 +0000
Subject: [Project Clearwater] Deploying Clearwater using Heat Template
In-Reply-To: <CADabj4YOaJYfU782rVFhU3h6=k_9iq=DLMt37CfNnBHK=S7d2Q@mail.gmail.com>
References: <CAKHfQt7+g8thaq7+7os6jZWfD+kZX7o0VE3K-ctF-6VnBQfhig@mail.gmail.com>
	<CADabj4YOaJYfU782rVFhU3h6=k_9iq=DLMt37CfNnBHK=S7d2Q@mail.gmail.com>
Message-ID: <ME2PR01MB288429523B58AA17DD0F1A76C2860@ME2PR01MB2884.ausprd01.prod.outlook.com>

Hi,

I?ve tried that, and now I get the error:

ERROR: Failed to validate: : resources.mgmt_network: : Property network not assigned

Or

ERROR: Failed to validate: : resources.sig_network: : Property network not assigned

(It alternates between the two each time I try)

I?ve tried running `heat stack-create` (as per the README) but that gives a warning about deprecated commands. So I?ve also tried

openstack stack create --parameter $PARAMS -t clearwater.yaml clearwater_heat

But the result is the same.

I?ve also tried validation (both with the heat and openstack commands), but validation of templates doesn?t work recursively. The validator only validated clearwater.yaml, not any others.

I can?t see any property in any file named just ?network?, except for constraints like ?neutron.network?.


Thanks,
Matt

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of tahir Masood
Sent: Tuesday, 17 April 2018 1:53 AM
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Deploying Clearwater using Heat Template

Download all the yaml files to your openstack controller
Create the key using the command provided in the read me of git hub repo and deploy the stack.
Make sure sure you are in the folder where all other yaml files are present
Clearwater.yaml is the main file used to trigger the stack creation all other will be called by this file automatically

Regards
Tahir

On Mon, 16 Apr 2018, 8:47 PM joehary ar, <joehary at gmail.com<mailto:joehary at gmail.com>> wrote:
Hi,

I would like to seek support and guidance on how I can deploy clearwater using heat template based on the yaml file https://github.com/Metaswitch/clearwater-heat

May I know anybody successful try this template?

1. Do i need to download all the file in git onto my openstack folder?
2. Just create heat stack with only one yaml file --> clearwater.yaml ?

Please assist me further.

Thank you.

Regards,
Joe
_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180504/b8e65a34/attachment.html>

From amirhatami1000 at gmail.com  Tue May  8 07:08:25 2018
From: amirhatami1000 at gmail.com (Amir Moahammad Hatami)
Date: Tue, 8 May 2018 15:38:25 +0430
Subject: [Project Clearwater] simple questions about clearwater
Message-ID: <CAEtN4MAAhHphC_KT0uyQWfBEbvgwR01NvTDmbUA4gZQwfxgj6Q@mail.gmail.com>

Hi,

I am a beginner working with Clearwater. I want to set it up and do some
stress tests. I have a few questions.

1- I am using OpenStack Ansible, and QEMU is the hype visor. Is there any
problems using them or they are fine?

2- In complete installation I can see somewhere that writes that at least 6
VMs are needed and somewhere else it is written that at least 6 Machines
are needed. I am confused if i should use 6 different machines or I can
just set up 6 VMs in one machine and set it up. (Actually I don't have six
different machines in my setup)

3- In case if installing all in one package, is it necessary to openstack?

Thanks very much
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180508/46f7f169/attachment.html>

From Matthew.Davis.2 at team.telstra.com  Tue May  8 20:22:35 2018
From: Matthew.Davis.2 at team.telstra.com (Davis, Matthew)
Date: Wed, 9 May 2018 00:22:35 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
Message-ID: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>

I'm having issues with Homestead on Kubernetes as well.

What was the bug in the script you used? I just followed the instructions in the README.

The stable branch is the same as the master branch at the moment, so switching to the stable branch won't help.

If I set up the firewalls or DNS wrong, will that cause homestead to not come up?

Here are my Kubernetes logs:

$ kubectl logs homestead-prov-564fb46b67-c9csz
2018-05-09 00:19:43,630 CRIT Supervisor running as root (no user in config file)
2018-05-09 00:19:43,630 WARN Included extra file "/etc/supervisor/conf.d/socket-factory.supervisord.con
f" during parsing
2018-05-09 00:19:43,630 WARN Included extra file "/etc/supervisor/conf.d/clearwater-group.conf" during parsing
2018-05-09 00:19:43,631 WARN Included extra file "/etc/supervisor/conf.d/nginx.conf" during parsing
2018-05-09 00:19:43,631 WARN Included extra file "/etc/supervisor/conf.d/homestead-prov.conf" during parsing
2018-05-09 00:19:43,631 WARN Included extra file "/etc/supervisor/conf.d/snmpd.conf" during parsing
2018-05-09 00:19:43,631 WARN Included extra file "/etc/supervisor/conf.d/clearwater-infrastructure.conf" during parsing
2018-05-09 00:19:43,631 WARN Included extra file "/etc/supervisor/conf.d/supervisord.conf" during parsing
2018-05-09 00:19:43,651 INFO RPC interface 'supervisor' initialized
2018-05-09 00:19:43,652 CRIT Server 'unix_http_server' running without any HTTP authentication checking
2018-05-09 00:19:43,652 INFO supervisord started with pid 1
2018-05-09 00:19:44,655 INFO spawned: 'snmpd' with pid 7
2018-05-09 00:19:44,656 INFO spawnerr: can't find command '/usr/share/clearwater/bin/clearwater-socket-factory-sig-wrapper'
2018-05-09 00:19:44,658 INFO spawned: 'clearwater-infrastructure' with pid 8
2018-05-09 00:19:44,667 INFO spawnerr: can't find command '/usr/share/clearwater/bin/clearwater-socket-factory-mgmt-wrapper'
2018-05-09 00:19:44,676 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-09 00:19:44,676 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-09 00:19:45,635 INFO exited: snmpd (exit status 0; expected)
2018-05-09 00:19:45,636 CRIT reaped unknown pid 48)
2018-05-09 00:19:46,638 INFO spawnerr: can't find command '/usr/share/clearwater/bin/clearwater-socket-factory-sig-wrapper'
2018-05-09 00:19:46,639 INFO spawnerr: can't find command '/usr/share/clearwater/bin/clearwater-socket-factory-mgmt-wrapper'
2018-05-09 00:19:47,626 CRIT reaped unknown pid 52)
2018-05-09 00:19:47,627 CRIT reaped unknown pid 53)
2018-05-09 00:19:48,717 INFO spawnerr: can't find command '/usr/share/clearwater/bin/clearwater-socket-factory-sig-wrapper'
2018-05-09 00:19:48,717 INFO spawnerr: can't find command '/usr/share/clearwater/bin/clearwater-socket-factory-mgmt-wrapper'
2018-05-09 00:19:48,818 CRIT reaped unknown pid 183)
2018-05-09 00:19:48,818 CRIT reaped unknown pid 184)
2018-05-09 00:19:48,836 CRIT reaped unknown pid 206)

$ kubectl logs homestead-84bf4458d9-5rq4z homestead
2018-05-09 00:15:25,992 INFO exited: homestead (exit status 0; not expected)
2018-05-09 00:15:36,005 INFO spawned: 'homestead' with pid 2014
2018-05-09 00:15:36,187 INFO exited: homestead (exit status 0; not expected)
2018-05-09 00:15:47,200 INFO spawned: 'homestead' with pid 2205
2018-05-09 00:15:47,373 INFO exited: homestead (exit status 0; not expected)
2018-05-09 00:15:48,198 WARN received SIGTERM indicating exit request
2018-05-09 00:15:48,199 INFO waiting for nginx, homestead, socket-factory-sig, socket-factory-mgmt to die
2018-05-09 00:15:48,199 INFO stopped: socket-factory-mgmt (terminated by SIGTERM)
2018-05-09 00:15:50,202 INFO stopped: socket-factory-sig (terminated by SIGTERM)
2018-05-09 00:15:50,207 INFO stopped: nginx (exit status 0)

ubuntu at aks-build:~$ kubectl get pods
NAME                              READY     STATUS             RESTARTS   AGE
astaire-9bb9d79f6-8zfw7           2/2       Running            0          16h
bono-86f7c46886-k6khg             2/2       Running            0          16h
cassandra-7777c47568-56kqr        1/1       Running            0          16h
cassandra-7777c47568-k7psl        1/1       Running            0          16h
cassandra-7777c47568-tr4cs        1/1       Running            0          16h
chronos-5f56966bcd-fhqgh          2/2       Running            0          16h
ellis-54dcc4476c-k6dfz            1/1       Running            0          16h
etcd-69c59b7d87-8pc5g             1/1       Running            0          16h
homer-7d9fb5755d-nbqd9            1/1       Running            0          16h
homestead-84bf4458d9-5rq4z        1/2       CrashLoopBackOff   253        16h
homestead-prov-564fb46b67-c9csz   0/1       CrashLoopBackOff   252        16h
ralf-b8fdcc64f-77dg6              2/2       Running            0          16h
sprout-67656f5f9d-vtvtb           2/2       Running            0          16h


Regards,


Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180509/f0f2de6a/attachment.html>

From tcchindeka at gmail.com  Wed May  9 01:30:40 2018
From: tcchindeka at gmail.com (Tapiwa Chindeka)
Date: Wed, 09 May 2018 05:30:40 +0000
Subject: [Project Clearwater] simple questions about clearwater
In-Reply-To: <CAEtN4MAAhHphC_KT0uyQWfBEbvgwR01NvTDmbUA4gZQwfxgj6Q@mail.gmail.com>
References: <CAEtN4MAAhHphC_KT0uyQWfBEbvgwR01NvTDmbUA4gZQwfxgj6Q@mail.gmail.com>
Message-ID: <CAFFdzJQtaybmWP=pCRDE5YW7dqs6uPgF2FX+WnkHJtTAQNzv0w@mail.gmail.com>

Hi Moahammad

I'm not sure about the answer to your first question. The 6 machines they
mention are your 6 VMs. You don't need 6 physical machines. Lastly, you
don't need OpenStack for the all in one deployment.

Regards
Tapiwa

On Tue, 8 May 2018, 13:09 Amir Moahammad Hatami, <amirhatami1000 at gmail.com>
wrote:

> Hi,
>
> I am a beginner working with Clearwater. I want to set it up and do some
> stress tests. I have a few questions.
>
> 1- I am using OpenStack Ansible, and QEMU is the hype visor. Is there any
> problems using them or they are fine?
>
> 2- In complete installation I can see somewhere that writes that at least
> 6 VMs are needed and somewhere else it is written that at least 6 Machines
> are needed. I am confused if i should use 6 different machines or I can
> just set up 6 VMs in one machine and set it up. (Actually I don't have six
> different machines in my setup)
>
> 3- In case if installing all in one package, is it necessary to openstack?
>
> Thanks very much
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
>
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180509/54fc99d1/attachment.html>

From ameliagdhn at gmail.com  Fri May 11 12:02:33 2018
From: ameliagdhn at gmail.com (amelia putri Anggadhini)
Date: Fri, 11 May 2018 23:02:33 +0700
Subject: [Project Clearwater] Query regarding Ralf CTF AVPs
Message-ID: <CABqEO-pQ9Vjbgd48229ZDgOZwvRecWNAdN9B-zxigPfYEzX1gw@mail.gmail.com>

Hi,

Currently i'm trying to develop a volume based offline charging system by
integrating Clearwater with RestComm jDiameter and i want to use the
Accounting Information AVPs, such as Accounting Input Octets etc. But the
AVPs contained in the ACR sent by Ralf doesn't seem to include those AVPs.
Is it an expected behaviour from Ralf or did i miss something?

Thanks in advance,

Amelia
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180511/dc85cba1/attachment.html>

From Matthew.Davis.2 at team.telstra.com  Mon May 14 02:04:56 2018
From: Matthew.Davis.2 at team.telstra.com (Davis, Matthew)
Date: Mon, 14 May 2018 06:04:56 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
Message-ID: <MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>

Hi everyone,


I've fixed the issue I had with homestead-prov and submitted the patch as this pull request: https://github.com/Metaswitch/clearwater-docker/pull/89

However now I just run into another issue, so homestead-prov is giving me the same error as homestead. The error message is not verbose. Does anyone know how to make it more verbose? I have no idea what's happening.


```
2018-05-14 05:47:41,369 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:41,370 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-sig entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-mgmt entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,477 INFO exited: snmpd (exit status 0; expected)
2018-05-14 05:47:42,478 CRIT reaped unknown pid 61)
2018-05-14 05:47:44,572 CRIT reaped unknown pid 65)
2018-05-14 05:47:44,576 CRIT reaped unknown pid 66)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 235)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 236)
2018-05-14 05:47:45,902 CRIT reaped unknown pid 257)
2018-05-14 05:47:48,153 CRIT reaped unknown pid 294)
2018-05-14 05:47:48,226 INFO spawned: 'homestead-prov' with pid 297
2018-05-14 05:47:48,237 INFO spawned: 'nginx' with pid 298
2018-05-14 05:47:48,747 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:49,750 INFO spawned: 'homestead-prov' with pid 324
2018-05-14 05:47:49,750 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:50,089 INFO exited: clearwater-infrastructure (exit status 0; expected)
2018-05-14 05:47:50,335 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:52,339 INFO spawned: 'homestead-prov' with pid 335
2018-05-14 05:47:53,324 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:56,330 INFO spawned: 'homestead-prov' with pid 346
2018-05-14 05:47:56,902 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:57,904 INFO gave up: homestead-prov entered FATAL state, too many start retries too quickly
```

Regards,
Matt Davis
Telstra | Graduate Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180514/980b2855/attachment.html>

From amirhatami1000 at gmail.com  Mon May 14 06:32:47 2018
From: amirhatami1000 at gmail.com (Amir Moahammad Hatami)
Date: Mon, 14 May 2018 15:02:47 +0430
Subject: [Project Clearwater] Clearwater Digest, Vol 61, Issue 5
In-Reply-To: <mailman.1.1525795201.62035.clearwater_lists.projectclearwater.org@lists.projectclearwater.org>
References: <mailman.1.1525795201.62035.clearwater_lists.projectclearwater.org@lists.projectclearwater.org>
Message-ID: <CAEtN4MB9Tx5rAvPMTh-duR-3TA33zSAVtDvHjemHrVr_raa9fQ@mail.gmail.com>

Thanks, If I install all the 6 VMs in a single machine would that be
necessary to use OpenStack?



?

On Tue, May 8, 2018 at 8:30 PM, <
clearwater-request at lists.projectclearwater.org> wrote:

> Send Clearwater mailing list submissions to
>         clearwater at lists.projectclearwater.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.projectclearwater.org/mailman/
> listinfo/clearwater_lists.projectclearwater.org
>
> or, via email, send a message with subject or body 'help' to
>         clearwater-request at lists.projectclearwater.org
>
> You can reach the person managing the list at
>         clearwater-owner at lists.projectclearwater.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Clearwater digest..."
>
>
> Today's Topics:
>
>    1. simple questions about clearwater (Amir Moahammad Hatami)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 8 May 2018 15:38:25 +0430
> From: Amir Moahammad Hatami <amirhatami1000 at gmail.com>
> To: clearwater at lists.projectclearwater.org
> Subject: [Project Clearwater] simple questions about clearwater
> Message-ID:
>         <CAEtN4MAAhHphC_KT0uyQWfBEbvgwR01NvTDmbUA4gZQw
> fxgj6Q at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Hi,
>
> I am a beginner working with Clearwater. I want to set it up and do some
> stress tests. I have a few questions.
>
> 1- I am using OpenStack Ansible, and QEMU is the hype visor. Is there any
> problems using them or they are fine?
>
> 2- In complete installation I can see somewhere that writes that at least 6
> VMs are needed and somewhere else it is written that at least 6 Machines
> are needed. I am confused if i should use 6 different machines or I can
> just set up 6 VMs in one machine and set it up. (Actually I don't have six
> different machines in my setup)
>
> 3- In case if installing all in one package, is it necessary to openstack?
>
> Thanks very much
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.
> projectclearwater.org/attachments/20180508/46f7f169/attachment-0001.html>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
> ------------------------------
>
> End of Clearwater Digest, Vol 61, Issue 5
> *****************************************
>



-- 
Amir Mohammad Hatami
B.Sc in Electrical Engineering
Sharif University of technology
http://ee.sharif.ir/~hatami
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180514/f39802fb/attachment.html>

From fescarosbuechs at vmware.com  Mon May 14 06:41:55 2018
From: fescarosbuechs at vmware.com (Frank Escaros-Buechsel)
Date: Mon, 14 May 2018 10:41:55 +0000
Subject: [Project Clearwater] Clearwater Digest, Vol 61, Issue 5
In-Reply-To: <CAEtN4MB9Tx5rAvPMTh-duR-3TA33zSAVtDvHjemHrVr_raa9fQ@mail.gmail.com>
References: <mailman.1.1525795201.62035.clearwater_lists.projectclearwater.org@lists.projectclearwater.org>,
	<CAEtN4MB9Tx5rAvPMTh-duR-3TA33zSAVtDvHjemHrVr_raa9fQ@mail.gmail.com>
Message-ID: <0EB64FE9-0153-4737-94CB-AD3D7A3123AD@vmware.com>

Amir,
Openstack is an IaaS cloud platform allowing you to consume certain things via api and have a more or less automated experience (last time I tried to onboard on openstack was less than automated though for me, I haven?t tried in a while).

You don?t technically need openstack for the all in one image or the fully distributed deployment at all, you can deploy the single vm or all 6 vms on the same virtualisation host or several virtualisation hosts however you choose as long as the networking between the vms is in place (or for the single vm if you opt for an all in one deployment).

So in short, while openstack can make your life easier for certain management operations when using Clearwater you don?t have to use it. A direct hypervisor deploy will work just fine as well.

Regards,
Frank


Sent from VMware Boxer

On 14 May 2018 at 11:34:24 IST, Amir Moahammad Hatami <amirhatami1000 at gmail.com> wrote:
Thanks, If I install all the 6 VMs in a single machine would that be necessary to use OpenStack?



?

On Tue, May 8, 2018 at 8:30 PM, <clearwater-request at lists.projectclearwater.org<mailto:clearwater-request at lists.projectclearwater.org>> wrote:
Send Clearwater mailing list submissions to
        clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>

To subscribe or unsubscribe via the World Wide Web, visit
        http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.projectclearwater.org_mailman_listinfo_clearwater-5Flists.projectclearwater.org&d=DwMFaQ&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=qsX7A0xN3K1_SEQ5pQAIto7a09W7fC3w6QvZoOhDBNo&s=iikjc9zb2GaP7iwrN97eeLdwlkWn1E82OEtRjsmzRxQ&e=>

or, via email, send a message with subject or body 'help' to
        clearwater-request at lists.projectclearwater.org<mailto:clearwater-request at lists.projectclearwater.org>

You can reach the person managing the list at
        clearwater-owner at lists.projectclearwater.org<mailto:clearwater-owner at lists.projectclearwater.org>

When replying, please edit your Subject line so it is more specific
than "Re: Contents of Clearwater digest..."


Today's Topics:

   1. simple questions about clearwater (Amir Moahammad Hatami)


----------------------------------------------------------------------

Message: 1
Date: Tue, 8 May 2018 15:38:25 +0430
From: Amir Moahammad Hatami <amirhatami1000 at gmail.com<mailto:amirhatami1000 at gmail.com>>
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] simple questions about clearwater
Message-ID:
        <CAEtN4MAAhHphC_KT0uyQWfBEbvgwR01NvTDmbUA4gZQwfxgj6Q at mail.gmail.com<mailto:CAEtN4MAAhHphC_KT0uyQWfBEbvgwR01NvTDmbUA4gZQwfxgj6Q at mail.gmail.com>>
Content-Type: text/plain; charset="utf-8"

Hi,

I am a beginner working with Clearwater. I want to set it up and do some
stress tests. I have a few questions.

1- I am using OpenStack Ansible, and QEMU is the hype visor. Is there any
problems using them or they are fine?

2- In complete installation I can see somewhere that writes that at least 6
VMs are needed and somewhere else it is written that at least 6 Machines
are needed. I am confused if i should use 6 different machines or I can
just set up 6 VMs in one machine and set it up. (Actually I don't have six
different machines in my setup)

3- In case if installing all in one package, is it necessary to openstack?

Thanks very much
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180508/46f7f169/attachment-0001.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.projectclearwater.org_pipermail_clearwater-5Flists.projectclearwater.org_attachments_20180508_46f7f169_attachment-2D0001.html&d=DwMFaQ&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=qsX7A0xN3K1_SEQ5pQAIto7a09W7fC3w6QvZoOhDBNo&s=McwyTXTxId32DXnovpf_9szXzTZ7eqrSRo5pS05j3Qs&e=>>

------------------------------

Subject: Digest Footer

_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.projectclearwater.org_mailman_listinfo_clearwater-5Flists.projectclearwater.org&d=DwMFaQ&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=qsX7A0xN3K1_SEQ5pQAIto7a09W7fC3w6QvZoOhDBNo&s=iikjc9zb2GaP7iwrN97eeLdwlkWn1E82OEtRjsmzRxQ&e=>


------------------------------

End of Clearwater Digest, Vol 61, Issue 5
*****************************************



--
Amir Mohammad Hatami
B.Sc in Electrical Engineering
Sharif University of technology
http://ee.sharif.ir/~hatami<https://urldefense.proofpoint.com/v2/url?u=http-3A__ee.sharif.ir_-7Ehatami&d=DwMFaQ&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=qsX7A0xN3K1_SEQ5pQAIto7a09W7fC3w6QvZoOhDBNo&s=jlWInbU95Gfs-cLJPtslVufcd1d1i1_zxV6aB0v_Mro&e=>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180514/61063a0c/attachment.html>

From richard.whitehouse at projectclearwater.org  Tue May 15 13:23:52 2018
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Tue, 15 May 2018 17:23:52 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
Message-ID: <BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>

Matthew,

Sorry to hear you are having problems deploying Clearwater on Docker.

I think that the socket factory error is actually fairly benign.

Does this happen every time you create your deployment?

Can you provide the config map that you are providing when deploying on Kubernetes?

Can you connect into one of the pods that is failing to run homestead-prov and grab the logs. The relevant ones will be under /var/log/homestead-prov inside the container.

We cannot reproduce your issue on our kubernetes setup - can you provide details of your kubernetes setup - what version of k8s and what networking configuration you are using?

>From the diagnostics we have at the moment, the most likely scenario is that the Homestead and Homestead Prov containers are unable to contact Cassandra - if that's the case they will continually restart to attempt to resolve the problem.



Richard


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 14 May 2018 07:05
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi everyone,

I've fixed the issue I had with homestead-prov and submitted the patch as this pull request: https://github.com/Metaswitch/clearwater-docker/pull/89

However now I just run into another issue, so homestead-prov is giving me the same error as homestead. The error message is not verbose. Does anyone know how to make it more verbose? I have no idea what's happening.


```
2018-05-14 05:47:41,369 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:41,370 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-sig entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-mgmt entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,477 INFO exited: snmpd (exit status 0; expected)
2018-05-14 05:47:42,478 CRIT reaped unknown pid 61)
2018-05-14 05:47:44,572 CRIT reaped unknown pid 65)
2018-05-14 05:47:44,576 CRIT reaped unknown pid 66)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 235)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 236)
2018-05-14 05:47:45,902 CRIT reaped unknown pid 257)
2018-05-14 05:47:48,153 CRIT reaped unknown pid 294)
2018-05-14 05:47:48,226 INFO spawned: 'homestead-prov' with pid 297
2018-05-14 05:47:48,237 INFO spawned: 'nginx' with pid 298
2018-05-14 05:47:48,747 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:49,750 INFO spawned: 'homestead-prov' with pid 324
2018-05-14 05:47:49,750 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:50,089 INFO exited: clearwater-infrastructure (exit status 0; expected)
2018-05-14 05:47:50,335 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:52,339 INFO spawned: 'homestead-prov' with pid 335
2018-05-14 05:47:53,324 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:56,330 INFO spawned: 'homestead-prov' with pid 346
2018-05-14 05:47:56,902 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:57,904 INFO gave up: homestead-prov entered FATAL state, too many start retries too quickly
```

Regards,
Matt Davis
Telstra | Graduate Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180515/73ad8e47/attachment.html>

From richard.whitehouse at projectclearwater.org  Tue May 15 13:27:53 2018
From: richard.whitehouse at projectclearwater.org (Richard Whitehouse (projectclearwater.org))
Date: Tue, 15 May 2018 17:27:53 +0000
Subject: [Project Clearwater] Deploying Clearwater using Heat Template
In-Reply-To: <ME2PR01MB288429523B58AA17DD0F1A76C2860@ME2PR01MB2884.ausprd01.prod.outlook.com>
References: <CAKHfQt7+g8thaq7+7os6jZWfD+kZX7o0VE3K-ctF-6VnBQfhig@mail.gmail.com>
	<CADabj4YOaJYfU782rVFhU3h6=k_9iq=DLMt37CfNnBHK=S7d2Q@mail.gmail.com>
	<ME2PR01MB288429523B58AA17DD0F1A76C2860@ME2PR01MB2884.ausprd01.prod.outlook.com>
Message-ID: <BN6PR02MB3362CE9FFF12A5C76B813B95F0930@BN6PR02MB3362.namprd02.prod.outlook.com>

Matt,

What version of OpenStack are you running against? ? it sounds like there may be compatibility issues with heat templates.

Can you provide the full command ? or at least the set of parameters that you are provide to stack create?


Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 04 May 2018 01:46
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Deploying Clearwater using Heat Template

Hi,

I?ve tried that, and now I get the error:

ERROR: Failed to validate: : resources.mgmt_network: : Property network not assigned

Or

ERROR: Failed to validate: : resources.sig_network: : Property network not assigned

(It alternates between the two each time I try)

I?ve tried running `heat stack-create` (as per the README) but that gives a warning about deprecated commands. So I?ve also tried

openstack stack create --parameter $PARAMS -t clearwater.yaml clearwater_heat

But the result is the same.

I?ve also tried validation (both with the heat and openstack commands), but validation of templates doesn?t work recursively. The validator only validated clearwater.yaml, not any others.

I can?t see any property in any file named just ?network?, except for constraints like ?neutron.network?.


Thanks,
Matt

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of tahir Masood
Sent: Tuesday, 17 April 2018 1:53 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Deploying Clearwater using Heat Template

Download all the yaml files to your openstack controller
Create the key using the command provided in the read me of git hub repo and deploy the stack.
Make sure sure you are in the folder where all other yaml files are present
Clearwater.yaml is the main file used to trigger the stack creation all other will be called by this file automatically

Regards
Tahir

On Mon, 16 Apr 2018, 8:47 PM joehary ar, <joehary at gmail.com<mailto:joehary at gmail.com>> wrote:
Hi,

I would like to seek support and guidance on how I can deploy clearwater using heat template based on the yaml file https://github.com/Metaswitch/clearwater-heat

May I know anybody successful try this template?

1. Do i need to download all the file in git onto my openstack folder?
2. Just create heat stack with only one yaml file --> clearwater.yaml ?

Please assist me further.

Thank you.

Regards,
Joe
_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180515/59689439/attachment.html>

From Matthew.Davis.2 at team.telstra.com  Tue May 15 20:49:44 2018
From: Matthew.Davis.2 at team.telstra.com (Davis, Matthew)
Date: Wed, 16 May 2018 00:49:44 +0000
Subject: [Project Clearwater] Deploying Clearwater using Heat Template
In-Reply-To: <BN6PR02MB3362CE9FFF12A5C76B813B95F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
References: <CAKHfQt7+g8thaq7+7os6jZWfD+kZX7o0VE3K-ctF-6VnBQfhig@mail.gmail.com>
	<CADabj4YOaJYfU782rVFhU3h6=k_9iq=DLMt37CfNnBHK=S7d2Q@mail.gmail.com>
	<ME2PR01MB288429523B58AA17DD0F1A76C2860@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<BN6PR02MB3362CE9FFF12A5C76B813B95F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
Message-ID: <ME2PR01MB2884A52D9CF23A8D11667175C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>

Hi Richard,

It was either Newton or Mitaka. I?m not sure. For unrelated reasons that deployment has been destroyed and my colleague is currently installing Ocata. Once that?s up I?ll give it another try.

As for the exact command, unfortunately I can?t remember the contents of $PARAMS. I lost the script which had $PARAMS, because that openstack deployment was destroyed.

So unfortunately it looks like we can?t solve this issue at the moment. I?ll update this thread once I have a chance to try on Ocata.


Thanks,
Matt



From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Wednesday, 16 May 2018 3:28 AM
To: clearwater at lists.projectclearwater.org; Davis, Matthew <Matthew.Davis.2 at team.telstra.com>
Subject: RE: [Project Clearwater] Deploying Clearwater using Heat Template

Matt,

What version of OpenStack are you running against? ? it sounds like there may be compatibility issues with heat templates.

Can you provide the full command ? or at least the set of parameters that you are provide to stack create?


Richard

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 04 May 2018 01:46
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Deploying Clearwater using Heat Template

Hi,

I?ve tried that, and now I get the error:

ERROR: Failed to validate: : resources.mgmt_network: : Property network not assigned

Or

ERROR: Failed to validate: : resources.sig_network: : Property network not assigned

(It alternates between the two each time I try)

I?ve tried running `heat stack-create` (as per the README) but that gives a warning about deprecated commands. So I?ve also tried

openstack stack create --parameter $PARAMS -t clearwater.yaml clearwater_heat

But the result is the same.

I?ve also tried validation (both with the heat and openstack commands), but validation of templates doesn?t work recursively. The validator only validated clearwater.yaml, not any others.

I can?t see any property in any file named just ?network?, except for constraints like ?neutron.network?.


Thanks,
Matt

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of tahir Masood
Sent: Tuesday, 17 April 2018 1:53 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Deploying Clearwater using Heat Template

Download all the yaml files to your openstack controller
Create the key using the command provided in the read me of git hub repo and deploy the stack.
Make sure sure you are in the folder where all other yaml files are present
Clearwater.yaml is the main file used to trigger the stack creation all other will be called by this file automatically

Regards
Tahir

On Mon, 16 Apr 2018, 8:47 PM joehary ar, <joehary at gmail.com<mailto:joehary at gmail.com>> wrote:
Hi,

I would like to seek support and guidance on how I can deploy clearwater using heat template based on the yaml file https://github.com/Metaswitch/clearwater-heat

May I know anybody successful try this template?

1. Do i need to download all the file in git onto my openstack folder?
2. Just create heat stack with only one yaml file --> clearwater.yaml ?

Please assist me further.

Thank you.

Regards,
Joe
_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180516/e855cade/attachment.html>

From Matthew.Davis.2 at team.telstra.com  Tue May 15 23:35:28 2018
From: Matthew.Davis.2 at team.telstra.com (Davis, Matthew)
Date: Wed, 16 May 2018 03:35:28 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
Message-ID: <ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>

Hi Richard,

Tldr: I've made some progress since I sent that email. I've found and fixed bugs to do with dnsmasq, but now I'm stuck.
The systemd logs for homestead still say: "Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."
I'm not even sure whether the dnsmasq bugs were related.

First, to answer your questions:

Yes this happens every time I deploy.

My configmap:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
Events:  <none>
```

I'm deploying on Azure's kubernetes service (AKS). I've enabled HTTP Application Routing<https://docs.microsoft.com/en-us/azure/aks/http-application-routing>, which means that Azure manages DNS for me. The only unusual step is that Azure requires that I add ingresses to do so. The DNS records point to each service (bono, ellis, sprout etc). I can ping "cassandra" from the homestead pod.

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:22:21Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.6", GitCommit:"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1", GitTreeState:"clean", BuildDate:"2018-03-21T15:13:31Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}

Getting the logs is a race against time, since kubernetes keeps terminating the pod while I'm in there.
After several attempts, I managed to get the homestead logs (same as homestead-prov logs)

```
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:123: Creating Cached Resolver using servers:
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:133:     10.0.0.10
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Error static_dns_cache.cpp:89: DNS config file /etc/clearwater/dns.json mis
sing
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status main.cpp:653: Using local impu store: astaire.2991678f-f9dd-4098-8fe
8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.129 UTC [7f6379c8e7c0] Status http_connection_pool.cpp:37: Connection pool will use calculated res
ponse timeout of 550ms
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:35: Configuring HTTP Connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:36:   Connection created for server sprout.2991678f
-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io:9888
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status main.cpp:1013: No HSS configured - using Homestead-prov
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:266: Configuring store connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:267:   Hostname:  cassandra.2991678f-f9dd-4098-8
fe8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:268:   Port:      9160
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:296: Configuring store worker pool
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:297:   Threads:   10
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:298:   Max Queue: 0
15-05-2018 07:05:40.131 UTC [7f6366ffd700] Status alarm.cpp:244: Reraising all alarms with a known state
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Error main.cpp:1056: Failed to initialize the Cassandra store with error co
de 3.
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Status main.cpp:1057: Homestead is shutting down
```

The full logs are just this chunk repeating over and over until everything shuts down.

I think the crucial error message is DNS config file /etc/clearwater/dns.json missing

In the all-in-one installation, that file is just:

{
  "hostnames": [
  ]
}

Is it ok to manually insert that using the Dockerfile? Or should there be other entries in there for a non-AIO installation?

dns.json is created by the dnsmasq service.

During the container build, there is a red error message saying "dnsmasq: setting capabilities failed: Operation not permitted"
(There are a lot of red error messages during build. Is that an issue?)

I noticed that dnsmasq was not running in homestead docker, yet it is running in my working all-in-one installation. (There are also a bunch of other services which are halted. Is that an issue?)

I tried starting dnsmasq manually, it wouldn't start. I looked online and the solution is to add user=root to the file /etc/dnsmasq.conf.

I tried modifying the Dockerfile to manually append that string to the file.  (I'm assuming that the order of lines in /etc/dnsmasq.conf does not matter)

Then dnsmasq still doesn't come up. I get an error about how the port it wants is already taken.

So then I tried modifying the Dockerfile to paste in dns.conf from the AIO installation. (The one with an empty list of hosts)

Now the missing file error is gone, but I still get the other error:

"Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."

How can I debug this?

All this time I thought that error was caused by dns issues. Is it because dnsmasq still won't start, so dns.json is missing hosts? Or is that unrelated?
"ping cassandra" works from the homestead pod.

Other Remaining questions:

*        Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?

o   {  "hostnames": [ ]}

o   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?

o   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?

*        There are many other red error messages during build. Does that matter?

*        There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?

o   Services running in AIO:

?  [ ? ]  clearwater-auto-config-generic

?   [ + ]  clearwater-cluster-manager

?   [ + ]  clearwater-config-manager

?   [ + ]  clearwater-diags-monitor

?   [ - ]  clearwater-etcd

?   [ + ]  clearwater-infrastructure

?   [ ? ]  clearwater-memcached

?   [ + ]  clearwater-queue-manager

o   Services running (or not) in docker homestead:

?  [ ? ]  clearwater-auto-config-docker

?   [ + ]  clearwater-cluster-manager

?   [ - ]  clearwater-config-manager

?   [ - ]  clearwater-diags-monitor

?   [ + ]  clearwater-etcd

?   [ + ]  clearwater-infrastructure

?   [ - ]  clearwater-queue-manager


Thanks,

Matthew Davis
Telstra | CTO | Cloud SDN NFV

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Wednesday, 16 May 2018 3:24 AM
To: clearwater at lists.projectclearwater.org; Davis, Matthew <Matthew.Davis.2 at team.telstra.com>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Matthew,

Sorry to hear you are having problems deploying Clearwater on Docker.

I think that the socket factory error is actually fairly benign.

Does this happen every time you create your deployment?

Can you provide the config map that you are providing when deploying on Kubernetes?

Can you connect into one of the pods that is failing to run homestead-prov and grab the logs. The relevant ones will be under /var/log/homestead-prov inside the container.

We cannot reproduce your issue on our kubernetes setup - can you provide details of your kubernetes setup - what version of k8s and what networking configuration you are using?

>From the diagnostics we have at the moment, the most likely scenario is that the Homestead and Homestead Prov containers are unable to contact Cassandra - if that's the case they will continually restart to attempt to resolve the problem.



Richard


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 14 May 2018 07:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi everyone,
I've fixed the issue I had with homestead-prov and submitted the patch as this pull request: https://github.com/Metaswitch/clearwater-docker/pull/89

However now I just run into another issue, so homestead-prov is giving me the same error as homestead. The error message is not verbose. Does anyone know how to make it more verbose? I have no idea what's happening.


```
2018-05-14 05:47:41,369 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:41,370 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-sig entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-mgmt entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,477 INFO exited: snmpd (exit status 0; expected)
2018-05-14 05:47:42,478 CRIT reaped unknown pid 61)
2018-05-14 05:47:44,572 CRIT reaped unknown pid 65)
2018-05-14 05:47:44,576 CRIT reaped unknown pid 66)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 235)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 236)
2018-05-14 05:47:45,902 CRIT reaped unknown pid 257)
2018-05-14 05:47:48,153 CRIT reaped unknown pid 294)
2018-05-14 05:47:48,226 INFO spawned: 'homestead-prov' with pid 297
2018-05-14 05:47:48,237 INFO spawned: 'nginx' with pid 298
2018-05-14 05:47:48,747 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:49,750 INFO spawned: 'homestead-prov' with pid 324
2018-05-14 05:47:49,750 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:50,089 INFO exited: clearwater-infrastructure (exit status 0; expected)
2018-05-14 05:47:50,335 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:52,339 INFO spawned: 'homestead-prov' with pid 335
2018-05-14 05:47:53,324 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:56,330 INFO spawned: 'homestead-prov' with pid 346
2018-05-14 05:47:56,902 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:57,904 INFO gave up: homestead-prov entered FATAL state, too many start retries too quickly
```

Regards,
Matt Davis
Telstra | Graduate Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180516/c1de075d/attachment.html>

From Mark.Perryman at metaswitch.com  Wed May 16 06:55:28 2018
From: Mark.Perryman at metaswitch.com (Mark Perryman)
Date: Wed, 16 May 2018 10:55:28 +0000
Subject: [Project Clearwater] Query regarding Ralf CTF AVPs
Message-ID: <CY4PR02MB2518C2AC4606601D06031A1B8F920@CY4PR02MB2518.namprd02.prod.outlook.com>

Amelia

As far as I know, Ralf does not include the Diameter Accounting Extensions AVPs, so this is expected behaviour.

Hope that helps,

Mark.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180516/7995075d/attachment.html>

From Mark.Perryman at metaswitch.com  Wed May 16 07:12:35 2018
From: Mark.Perryman at metaswitch.com (Mark Perryman)
Date: Wed, 16 May 2018 11:12:35 +0000
Subject: [Project Clearwater] simple questions about clearwater
Message-ID: <CY4PR02MB2518A0BA475E697B24DA64868F920@CY4PR02MB2518.namprd02.prod.outlook.com>

Amir,

I am not aware of any issues with your proposed setup (OpenStack Ansible/QEMU), though I don't think it is one we have used, so I would be interested in whether you encounter any issues.

Please let me know if you have any further questions.

Mark.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180516/fccc8440/attachment.html>

From Benjamin.Laing at metaswitch.com  Wed May 16 10:14:58 2018
From: Benjamin.Laing at metaswitch.com (Benjamin Laing)
Date: Wed, 16 May 2018 14:14:58 +0000
Subject: [Project Clearwater] Failing in registration
Message-ID: <CY4PR02MB2517858315BB403193F8A43EF1920@CY4PR02MB2517.namprd02.prod.outlook.com>

Hi Arjun,

Thanks for your question and apologies in the delay getting back to you.

It looks like your host machine is having trouble resolving the DNS entry. As per http://clearwater.readthedocs.io/en/stable/Installation_Instructions.html#installation-methods option 3,  a manual install "requires manually configuring every machine, firewalls and DNS entries". A few things to try:

  *   Make sure you DNS is set up as per the docs http://clearwater.readthedocs.io/en/stable/Clearwater_DNS_Usage.html
  *   Are you trying to access Ellis from your local machine? Does http://192.168.56.116/ work? If not, can you ping that address from your local machine?
  *   Have you considered using one of the automated installation methods -  e.g. the All-in-one image or using a Chef server - as these will configure the DNS for you? See choices 1 and 2 in http://clearwater.readthedocs.io/en/stable/Installation_Instructions.html#installation-methods ?

Best,

Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180516/1af16225/attachment.html>

From Adam.Lindley at metaswitch.com  Wed May 16 13:04:54 2018
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Wed, 16 May 2018 17:04:54 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
Message-ID: <CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>

Hi Matthew,

I've had a look into your issues deploying under kubernetes, and think I've narrowed down the search a good amount. Details below.
First though I'll take a run through the smaller questions you've raised below, as I think we can knock some of them off fairly quickly.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
[AJL] You shouldn't need it in the containers. In our VMs we found that some operations were fairly heavy on DNS queries, and so provided dnsmasq as a means of providing caching. The need for this has decreased quite a bit now, and as we move forward in the microservice space it makes less sense to have it around.

     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
This will likely be down to how the container networking works, but it won't cause any problems in running under kubernetes.

     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
[AJL] This is a benign error in this case, so we haven't dug down into it. It shouldn't impact your deployment at all.

  *   There are many other red error messages during build. Does that matter?
[AJL] No, these don't matter. I see a number of error messages during the build. A lot of these are dependencies of some of our packages raising error messages when being installed in this environment

  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
[AJL] The AIO has a number of different requirements that we don't have in the container environment, and vice versa, so the fact that there is a different set doesn't mean you have issues there
As a couple of examples for the differences you pulled out below:

  *   The auto config packages are used to enable specific setup in different environments. E.g. setting up required config files that other deployments do not rely on. In the docker case, we need to read environment variables and pass them through to the system configuration files
  *   Clearwater-etcd is our interface into an etcd cluster, and is used for sharing common configuration around a cluster. On the AIO this simply increases complexity, as we have no need for it, but in docker/kubernetes, we want the services to share configuration through an underlying etcd cluster.
I don't believe any of the differences you pulled out below would be leading to the issues you're seeing, but it's a nice analysis :)
The missing dns.json config file is also currently just a benign error in the containers, raised by some expectations we have when running in VMs.


So, on to the main issue:
In getting your deployment up and running, the key issue is definitely going to be the 'Failed to initialize the Cassandra store' one. This indicates the Homestead and Homestead-prov services have been unable to reach the cassandra cluster, and without that they won't be able to get up and running.
As you've already found, getting logs out and doing more detailed debugging in containers presents its own set of problems. To temporarily prevent kubernetes from automatically restarting pods, you can (as you may have already) remove the liveness checking sections from the generated deployment files. This isn't ideal, but it should allow you time to do some more detailed debugging. You may also want to reduce the number of replicas of the pods down to one for now, to make it easier to follow traffic. Should just be a case of changing the .depl files and redeploying.

The log indicates you're getting error code 3 back from attempting to setup the Cassandra store, which is a 'connection error', as opposed to what you would get if the dns was simply unresolvable (I have tested that on our internal setup, and the logs would be clearly stating it was unresolvable). This is definitely leading me to suspect network connectivity issues into the cassandra pods. The fact that you can ping between the two however suggests that it's more complex than simply being unable to send any traffic between them.

To test this, I tweaked the cassandra configuration (running just a single pod for simplicity) so that it was listening on a port that wasn't 9160, which is the port that both homestead and homestead prov attempt to connect to. With this setup in place, I saw both of these pods repeatedly failing with the same error code as yours.
Because of this I would suggest the following next steps in tracking down the problem:

  *   Double check that your networking configuration allows traffic through to the cassandra pods on port 9160. The homestead and homestead prov processes will require this.
  *   Connect to the Cassandra pods, and verify that they are indeed listening for traffic on port 9160
     *   `netstat -planut` is my default for this, but to each their own
     *   /var/log/cassandra/system.log may have more information if something strange is going on here
  *   Manually check the connectivity between the pods, where you could try
     *   Connect to e.g. a homestead pod, and try running `nc <cassandra hostname> 9160`. I'm seeing this instantly return if the port is not open/listened on.
     *   Install tcpdump on both sides, and see whether you see traffic arriving in the cassandra pods at all. This isn't a very container, but getting a dump from both cassandra and homestead sides, to see what point in the flow is failing would be a decent way to work out where the issue lies.

Sadly, I don't have access to an AKS setup right now, so can't investigate the ingress rules component at all. I suspect that there's something slightly unusual happening there that is leading to traffic being unable to reach port 9160.
Hopefully this should give you a better idea of what the source of the issue is here. If this doesn't show anything up, and you're seeing traffic from homestead arrive in the cassandra pods and going back out we may have to run some more testing

Cheers,
Adam


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 16 May 2018 04:35
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>; clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Richard,

Tldr: I've made some progress since I sent that email. I've found and fixed bugs to do with dnsmasq, but now I'm stuck.
The systemd logs for homestead still say: "Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."
I'm not even sure whether the dnsmasq bugs were related.

First, to answer your questions:

Yes this happens every time I deploy.

My configmap:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
Events:  <none>
```

I'm deploying on Azure's kubernetes service (AKS). I've enabled HTTP Application Routing<https://docs.microsoft.com/en-us/azure/aks/http-application-routing>, which means that Azure manages DNS for me. The only unusual step is that Azure requires that I add ingresses to do so. The DNS records point to each service (bono, ellis, sprout etc). I can ping "cassandra" from the homestead pod.

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:22:21Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.6", GitCommit:"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1", GitTreeState:"clean", BuildDate:"2018-03-21T15:13:31Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}

Getting the logs is a race against time, since kubernetes keeps terminating the pod while I'm in there.
After several attempts, I managed to get the homestead logs (same as homestead-prov logs)

```
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:123: Creating Cached Resolver using servers:
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:133:     10.0.0.10
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Error static_dns_cache.cpp:89: DNS config file /etc/clearwater/dns.json mis
sing
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status main.cpp:653: Using local impu store: astaire.2991678f-f9dd-4098-8fe
8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.129 UTC [7f6379c8e7c0] Status http_connection_pool.cpp:37: Connection pool will use calculated res
ponse timeout of 550ms
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:35: Configuring HTTP Connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:36:   Connection created for server sprout.2991678f
-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io:9888
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status main.cpp:1013: No HSS configured - using Homestead-prov
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:266: Configuring store connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:267:   Hostname:  cassandra.2991678f-f9dd-4098-8
fe8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:268:   Port:      9160
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:296: Configuring store worker pool
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:297:   Threads:   10
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:298:   Max Queue: 0
15-05-2018 07:05:40.131 UTC [7f6366ffd700] Status alarm.cpp:244: Reraising all alarms with a known state
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Error main.cpp:1056: Failed to initialize the Cassandra store with error co
de 3.
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Status main.cpp:1057: Homestead is shutting down
```

The full logs are just this chunk repeating over and over until everything shuts down.

I think the crucial error message is DNS config file /etc/clearwater/dns.json missing

In the all-in-one installation, that file is just:

{
  "hostnames": [
  ]
}

Is it ok to manually insert that using the Dockerfile? Or should there be other entries in there for a non-AIO installation?

dns.json is created by the dnsmasq service.

During the container build, there is a red error message saying "dnsmasq: setting capabilities failed: Operation not permitted"
(There are a lot of red error messages during build. Is that an issue?)

I noticed that dnsmasq was not running in homestead docker, yet it is running in my working all-in-one installation. (There are also a bunch of other services which are halted. Is that an issue?)

I tried starting dnsmasq manually, it wouldn't start. I looked online and the solution is to add user=root to the file /etc/dnsmasq.conf.

I tried modifying the Dockerfile to manually append that string to the file.  (I'm assuming that the order of lines in /etc/dnsmasq.conf does not matter)

Then dnsmasq still doesn't come up. I get an error about how the port it wants is already taken.

So then I tried modifying the Dockerfile to paste in dns.conf from the AIO installation. (The one with an empty list of hosts)

Now the missing file error is gone, but I still get the other error:

"Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."

How can I debug this?

All this time I thought that error was caused by dns issues. Is it because dnsmasq still won't start, so dns.json is missing hosts? Or is that unrelated?
"ping cassandra" works from the homestead pod.
Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
  *   There are many other red error messages during build. Does that matter?
  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
     *   Services running in AIO:
        *   [ ? ]  clearwater-auto-config-generic
        *    [ + ]  clearwater-cluster-manager
        *    [ + ]  clearwater-config-manager
        *    [ + ]  clearwater-diags-monitor
        *    [ - ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ ? ]  clearwater-memcached
        *    [ + ]  clearwater-queue-manager
     *   Services running (or not) in docker homestead:
        *   [ ? ]  clearwater-auto-config-docker
        *    [ + ]  clearwater-cluster-manager
        *    [ - ]  clearwater-config-manager
        *    [ - ]  clearwater-diags-monitor
        *    [ + ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ - ]  clearwater-queue-manager


Thanks,

Matthew Davis
Telstra | CTO | Cloud SDN NFV

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Wednesday, 16 May 2018 3:24 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Matthew,

Sorry to hear you are having problems deploying Clearwater on Docker.

I think that the socket factory error is actually fairly benign.

Does this happen every time you create your deployment?

Can you provide the config map that you are providing when deploying on Kubernetes?

Can you connect into one of the pods that is failing to run homestead-prov and grab the logs. The relevant ones will be under /var/log/homestead-prov inside the container.

We cannot reproduce your issue on our kubernetes setup - can you provide details of your kubernetes setup - what version of k8s and what networking configuration you are using?

>From the diagnostics we have at the moment, the most likely scenario is that the Homestead and Homestead Prov containers are unable to contact Cassandra - if that's the case they will continually restart to attempt to resolve the problem.



Richard


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 14 May 2018 07:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi everyone,
I've fixed the issue I had with homestead-prov and submitted the patch as this pull request: https://github.com/Metaswitch/clearwater-docker/pull/89

However now I just run into another issue, so homestead-prov is giving me the same error as homestead. The error message is not verbose. Does anyone know how to make it more verbose? I have no idea what's happening.


```
2018-05-14 05:47:41,369 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:41,370 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-sig entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-mgmt entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,477 INFO exited: snmpd (exit status 0; expected)
2018-05-14 05:47:42,478 CRIT reaped unknown pid 61)
2018-05-14 05:47:44,572 CRIT reaped unknown pid 65)
2018-05-14 05:47:44,576 CRIT reaped unknown pid 66)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 235)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 236)
2018-05-14 05:47:45,902 CRIT reaped unknown pid 257)
2018-05-14 05:47:48,153 CRIT reaped unknown pid 294)
2018-05-14 05:47:48,226 INFO spawned: 'homestead-prov' with pid 297
2018-05-14 05:47:48,237 INFO spawned: 'nginx' with pid 298
2018-05-14 05:47:48,747 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:49,750 INFO spawned: 'homestead-prov' with pid 324
2018-05-14 05:47:49,750 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:50,089 INFO exited: clearwater-infrastructure (exit status 0; expected)
2018-05-14 05:47:50,335 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:52,339 INFO spawned: 'homestead-prov' with pid 335
2018-05-14 05:47:53,324 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:56,330 INFO spawned: 'homestead-prov' with pid 346
2018-05-14 05:47:56,902 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:57,904 INFO gave up: homestead-prov entered FATAL state, too many start retries too quickly
```

Regards,
Matt Davis
Telstra | Graduate Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180516/dcd815e9/attachment.html>

From Matthew.Davis.2 at team.telstra.com  Thu May 17 04:31:35 2018
From: Matthew.Davis.2 at team.telstra.com (Davis, Matthew)
Date: Thu, 17 May 2018 08:31:35 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
Message-ID: <ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>

Hi Adam,


*        `netstat -planut` shows that Cassandra is listening on 9160 from all ports

*        `nc cassandra 9160` did not exit immediately. It just hangs without any output

*        `ping cassandra` works, but `dig cassandra` shows no IP addresses

I tried changing my env-vars configmap from 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io to default.svc.cluster.local. This means that the pods do spin up successfully, and that error message goes away. What is env-vars supposed to be set to?
The docs say: "a ZONE key set to the domain of your Kubernetes cluster". So I set it to the domain of my cluster, and that seems to be the cause of all my problems. If I set it to something which isn't the domain of the cluster, the pods come up. So now I'm confused, is it supposed to be set to the domain of my cluster or not?
Is there some extra step I need to do to configure kubernetes to know that it's domain is 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io? I'm pretty sure it already knows, but how can I check?

Now all the pods are up. I can't make a call or run tests successfully. I wonder whether this is because my deployment has no idea what that it's domain is actually 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io.

In a browser, ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io takes me to the sign up page. I successfully created an account. I think this is good proof that the dns, ingresses and firewalls are set up properly. (albeit only for port 80, but I've checked the other ports. And by 'properly' I mean 'probably too loosely')

I've noticed that all my dns records (set up by Azure) point to the same ip address. Is this right, or not? (I still get confused by how kubernetes shares ip addresses around).
Here's my ellis ingress in kubernetes (required for Azure to set up dns for me)

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: ellis
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: ellis
            servicePort: 80
```

And for bono

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: bono
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: bono
            servicePort: 3478
        - backend:
            serviceName: bono
            servicePort: 5060
        - backend:
            serviceName: bono
            servicePort: 5062
```

I don't understand ingresses well. I'm not certain that this is right. But records appear in the dns server with names 'bono' and 'ellis' pointing to the ip address of something.
The example in the Azure docs<https://docs.microsoft.com/en-us/azure/aks/http-application-routing> has 'path: /`. I'm not sure whether I need that or not in my ingresses. With or without that line, the result is the same.

I've tried with ingresses for just bono and ellis, and also ingresses for every module (except etcd). The result is the same. (If I don't have ingresses for a module, it has no DNS record)



I have no idea what parameters to use when running the tests. The documentation doesn't say. Similarly I have no idea what to use when configuring my SIP client. What's the "domain"? What's the "proxy"?
Here's a bunch of permutations of test arguments which I ran, and the associated error messages:


*        rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret

o   Basic Call - Mainline (TCP) - Failed

o     SocketError thrown:

o      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)

o        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'

o        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'

o        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'

o        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'

*        rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret

o   Basic Call - Mainline (TCP) - Failed

o     SocketError thrown:

o      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)

o        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'

o        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'

o        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'

o        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'

*        rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret

o   Basic Call - Mainline (TCP) - Failed

o     SocketError thrown:

o      - getaddrinfo: Name or service not known

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'

*        rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret

o   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m

o     SocketError thrown:

o      - getaddrinfo: Name or service not known

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'

*        rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"

o   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m

o     URI::InvalidURIError thrown:

o      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts"

o        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'

o        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'

o        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'

o        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'

o        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'

o        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'

o        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'

*        rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"

o   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m

o     URI::InvalidURIError thrown:

o      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts"

o        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'

o        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'

o        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'

o        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'

o        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'

o        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'

o        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'

*        rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"

o   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m

o     URI::InvalidURIError thrown:

o      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts"

o        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'

o        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'

o        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'

o        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'

*        rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"

o   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m

o     URI::InvalidURIError thrown:

o      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts"

o        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'

o        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'

o        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'

o        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'

o        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'

*        rake test[default.svc.cluster.local] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

o   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m

o     SocketError thrown:

o      - getaddrinfo: Name or service not known

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'

*        rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

o   This one took a really long time before failing. So maybe it's the closest?

o   Basic Call - Mainline (TCP) - Failed

o     Errno::ETIMEDOUT thrown:

o      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'

*        rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

o   Basic Call - Mainline (TCP) - Failed

o     SocketError thrown:

o      - getaddrinfo: Name or service not known

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'

*        rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

o   Basic Call - Mainline (TCP) - Failed

o     Errno::ETIMEDOUT thrown:

o      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'

o        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'


When I try to register with a SIP client I get a bunch of 404 and 408 (timeout) error codes. Here's what I'm trying to use. I'm basing this off what worked for the AIO installation with this SIP client (Twinkle, one of the ones on Ubuntu)

*        Your Name: Matthew

*        User Name: 6505550375

o   Not sip:6505550375

o   Not 6505550375 at ...

*        Domain: default.svc.cluster.local, or is it example.com? The Clearwater kubernetes readme says you should make a call like an AIO installation, which is example.com.

*        Authentication name: 6505550375 at default.svc.cluster.local<mailto:6505550375 at default.svc.cluster.local>

*        Password: the one generated by the web page. Not the one you use to sign into the web page.

*        Proxy server: not sure. What should I use?

o   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

o   bono-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

o   ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

o   ellis-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

o   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

o   example.com

o   default.svc.cluster.local

After running a bunch of failed tests and trying manually to register on a SIP client, here are the logs I see:


*        /var/log/bono/bono_current.txt on the bono pod:

o   17-05-2018 08:00:18.268 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 25

o   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state

o   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm

o   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm

o   17-05-2018 08:00:54.278 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 32

o   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state

o   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm

o   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm

o   17-05-2018 08:01:17.285 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 20

*         Ellis: /var/log/ellis/ellis_20180517T070000Z.txt

o

o   17-05-2018 07:44:01.833 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local

o   17-05-2018 07:44:01.848 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/

o   17-05-2018 07:44:01.857 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets/e74929ec-21ea-48ac-95b6-68115894313d

o   17-05-2018 07:44:01.876 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets

o   17-05-2018 07:44:01.883 UTC INFO homestead.py:144: {"associated_implicit_registration_sets": ["e74929ec-21ea-48ac-95b6-68115894313d"]}

o   17-05-2018 07:44:01.883 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles

o   17-05-2018 07:44:01.913 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/public_ids/sip%3A6505550400%40default.svc.cluster.local

o   17-05-2018 07:44:01.966 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/public/sip%3A6505550400%40default.svc.cluster.local/service_profile

o   17-05-2018 07:44:01.976 UTC INFO homestead.py:255: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/filter_criteria

o   17-05-2018 07:44:01.980 UTC INFO xdm.py:29: Sending HTTP PUT request to http://homer.default.svc.cluster.local:7888/org.etsi.ngn.simservs/users/sip%3A6505550400%40default.svc.cluster.local/simservs.xml

o   17-05-2018 07:44:02.028 UTC INFO web.py:1447: 200 POST /accounts/live.tests at example.com/numbers/ (0.0.0.0) 209.96ms

Here are my firewall rules. My understanding is that these are the rules for a firewall between pods and the outside world. I think that communications between pods don't get filtered by these rules, but I added all the inter-pod ports just in case.

Inbound and Outbound TCP: 22,2380,4000,80,443,5060,5062,3478,5058,5054,5052,9888,8888,8889,10888,7888,11211,7000,7253,11311,9160
Inbound and Outbound UDP: 161,162,5060,32768-65535,3478

Any ideas where to go from here? My main question is what command should I run for rake tests?
Should I try to run the rake tests from a test pod within the cluster?

Thanks,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV
M  0415 762 868  | E  Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Thursday, 17 May 2018 3:05 AM
To: clearwater at lists.projectclearwater.org; Davis, Matthew <Matthew.Davis.2 at team.telstra.com>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

I've had a look into your issues deploying under kubernetes, and think I've narrowed down the search a good amount. Details below.
First though I'll take a run through the smaller questions you've raised below, as I think we can knock some of them off fairly quickly.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
[AJL] You shouldn't need it in the containers. In our VMs we found that some operations were fairly heavy on DNS queries, and so provided dnsmasq as a means of providing caching. The need for this has decreased quite a bit now, and as we move forward in the microservice space it makes less sense to have it around.

     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
This will likely be down to how the container networking works, but it won't cause any problems in running under kubernetes.

     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
[AJL] This is a benign error in this case, so we haven't dug down into it. It shouldn't impact your deployment at all.

  *   There are many other red error messages during build. Does that matter?
[AJL] No, these don't matter. I see a number of error messages during the build. A lot of these are dependencies of some of our packages raising error messages when being installed in this environment

  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
[AJL] The AIO has a number of different requirements that we don't have in the container environment, and vice versa, so the fact that there is a different set doesn't mean you have issues there
As a couple of examples for the differences you pulled out below:

  *   The auto config packages are used to enable specific setup in different environments. E.g. setting up required config files that other deployments do not rely on. In the docker case, we need to read environment variables and pass them through to the system configuration files
  *   Clearwater-etcd is our interface into an etcd cluster, and is used for sharing common configuration around a cluster. On the AIO this simply increases complexity, as we have no need for it, but in docker/kubernetes, we want the services to share configuration through an underlying etcd cluster.
I don't believe any of the differences you pulled out below would be leading to the issues you're seeing, but it's a nice analysis :)
The missing dns.json config file is also currently just a benign error in the containers, raised by some expectations we have when running in VMs.


So, on to the main issue:
In getting your deployment up and running, the key issue is definitely going to be the 'Failed to initialize the Cassandra store' one. This indicates the Homestead and Homestead-prov services have been unable to reach the cassandra cluster, and without that they won't be able to get up and running.
As you've already found, getting logs out and doing more detailed debugging in containers presents its own set of problems. To temporarily prevent kubernetes from automatically restarting pods, you can (as you may have already) remove the liveness checking sections from the generated deployment files. This isn't ideal, but it should allow you time to do some more detailed debugging. You may also want to reduce the number of replicas of the pods down to one for now, to make it easier to follow traffic. Should just be a case of changing the .depl files and redeploying.

The log indicates you're getting error code 3 back from attempting to setup the Cassandra store, which is a 'connection error', as opposed to what you would get if the dns was simply unresolvable (I have tested that on our internal setup, and the logs would be clearly stating it was unresolvable). This is definitely leading me to suspect network connectivity issues into the cassandra pods. The fact that you can ping between the two however suggests that it's more complex than simply being unable to send any traffic between them.

To test this, I tweaked the cassandra configuration (running just a single pod for simplicity) so that it was listening on a port that wasn't 9160, which is the port that both homestead and homestead prov attempt to connect to. With this setup in place, I saw both of these pods repeatedly failing with the same error code as yours.
Because of this I would suggest the following next steps in tracking down the problem:

  *   Double check that your networking configuration allows traffic through to the cassandra pods on port 9160. The homestead and homestead prov processes will require this.
  *   Connect to the Cassandra pods, and verify that they are indeed listening for traffic on port 9160
     *   `netstat -planut` is my default for this, but to each their own
     *   /var/log/cassandra/system.log may have more information if something strange is going on here
  *   Manually check the connectivity between the pods, where you could try
     *   Connect to e.g. a homestead pod, and try running `nc <cassandra hostname> 9160`. I'm seeing this instantly return if the port is not open/listened on.
     *   Install tcpdump on both sides, and see whether you see traffic arriving in the cassandra pods at all. This isn't a very container, but getting a dump from both cassandra and homestead sides, to see what point in the flow is failing would be a decent way to work out where the issue lies.

Sadly, I don't have access to an AKS setup right now, so can't investigate the ingress rules component at all. I suspect that there's something slightly unusual happening there that is leading to traffic being unable to reach port 9160.
Hopefully this should give you a better idea of what the source of the issue is here. If this doesn't show anything up, and you're seeing traffic from homestead arrive in the cassandra pods and going back out we may have to run some more testing

Cheers,
Adam


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 16 May 2018 04:35
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Richard,

Tldr: I've made some progress since I sent that email. I've found and fixed bugs to do with dnsmasq, but now I'm stuck.
The systemd logs for homestead still say: "Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."
I'm not even sure whether the dnsmasq bugs were related.

First, to answer your questions:

Yes this happens every time I deploy.

My configmap:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
Events:  <none>
```

I'm deploying on Azure's kubernetes service (AKS). I've enabled HTTP Application Routing<https://docs.microsoft.com/en-us/azure/aks/http-application-routing>, which means that Azure manages DNS for me. The only unusual step is that Azure requires that I add ingresses to do so. The DNS records point to each service (bono, ellis, sprout etc). I can ping "cassandra" from the homestead pod.

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:22:21Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.6", GitCommit:"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1", GitTreeState:"clean", BuildDate:"2018-03-21T15:13:31Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}

Getting the logs is a race against time, since kubernetes keeps terminating the pod while I'm in there.
After several attempts, I managed to get the homestead logs (same as homestead-prov logs)

```
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:123: Creating Cached Resolver using servers:
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:133:     10.0.0.10
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Error static_dns_cache.cpp:89: DNS config file /etc/clearwater/dns.json mis
sing
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status main.cpp:653: Using local impu store: astaire.2991678f-f9dd-4098-8fe
8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.129 UTC [7f6379c8e7c0] Status http_connection_pool.cpp:37: Connection pool will use calculated res
ponse timeout of 550ms
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:35: Configuring HTTP Connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:36:   Connection created for server sprout.2991678f
-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io:9888
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status main.cpp:1013: No HSS configured - using Homestead-prov
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:266: Configuring store connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:267:   Hostname:  cassandra.2991678f-f9dd-4098-8
fe8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:268:   Port:      9160
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:296: Configuring store worker pool
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:297:   Threads:   10
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:298:   Max Queue: 0
15-05-2018 07:05:40.131 UTC [7f6366ffd700] Status alarm.cpp:244: Reraising all alarms with a known state
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Error main.cpp:1056: Failed to initialize the Cassandra store with error co
de 3.
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Status main.cpp:1057: Homestead is shutting down
```

The full logs are just this chunk repeating over and over until everything shuts down.

I think the crucial error message is DNS config file /etc/clearwater/dns.json missing

In the all-in-one installation, that file is just:

{
  "hostnames": [
  ]
}

Is it ok to manually insert that using the Dockerfile? Or should there be other entries in there for a non-AIO installation?

dns.json is created by the dnsmasq service.

During the container build, there is a red error message saying "dnsmasq: setting capabilities failed: Operation not permitted"
(There are a lot of red error messages during build. Is that an issue?)

I noticed that dnsmasq was not running in homestead docker, yet it is running in my working all-in-one installation. (There are also a bunch of other services which are halted. Is that an issue?)

I tried starting dnsmasq manually, it wouldn't start. I looked online and the solution is to add user=root to the file /etc/dnsmasq.conf.

I tried modifying the Dockerfile to manually append that string to the file.  (I'm assuming that the order of lines in /etc/dnsmasq.conf does not matter)

Then dnsmasq still doesn't come up. I get an error about how the port it wants is already taken.

So then I tried modifying the Dockerfile to paste in dns.conf from the AIO installation. (The one with an empty list of hosts)

Now the missing file error is gone, but I still get the other error:

"Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."

How can I debug this?

All this time I thought that error was caused by dns issues. Is it because dnsmasq still won't start, so dns.json is missing hosts? Or is that unrelated?
"ping cassandra" works from the homestead pod.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
  *   There are many other red error messages during build. Does that matter?
  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
     *   Services running in AIO:
        *   [ ? ]  clearwater-auto-config-generic
        *    [ + ]  clearwater-cluster-manager
        *    [ + ]  clearwater-config-manager
        *    [ + ]  clearwater-diags-monitor
        *    [ - ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ ? ]  clearwater-memcached
        *    [ + ]  clearwater-queue-manager
     *   Services running (or not) in docker homestead:
        *   [ ? ]  clearwater-auto-config-docker
        *    [ + ]  clearwater-cluster-manager
        *    [ - ]  clearwater-config-manager
        *    [ - ]  clearwater-diags-monitor
        *    [ + ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ - ]  clearwater-queue-manager


Thanks,

Matthew Davis
Telstra | CTO | Cloud SDN NFV

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Wednesday, 16 May 2018 3:24 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Matthew,

Sorry to hear you are having problems deploying Clearwater on Docker.

I think that the socket factory error is actually fairly benign.

Does this happen every time you create your deployment?

Can you provide the config map that you are providing when deploying on Kubernetes?

Can you connect into one of the pods that is failing to run homestead-prov and grab the logs. The relevant ones will be under /var/log/homestead-prov inside the container.

We cannot reproduce your issue on our kubernetes setup - can you provide details of your kubernetes setup - what version of k8s and what networking configuration you are using?

>From the diagnostics we have at the moment, the most likely scenario is that the Homestead and Homestead Prov containers are unable to contact Cassandra - if that's the case they will continually restart to attempt to resolve the problem.



Richard


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 14 May 2018 07:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi everyone,
I've fixed the issue I had with homestead-prov and submitted the patch as this pull request: https://github.com/Metaswitch/clearwater-docker/pull/89

However now I just run into another issue, so homestead-prov is giving me the same error as homestead. The error message is not verbose. Does anyone know how to make it more verbose? I have no idea what's happening.


```
2018-05-14 05:47:41,369 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:41,370 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-sig entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-mgmt entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,477 INFO exited: snmpd (exit status 0; expected)
2018-05-14 05:47:42,478 CRIT reaped unknown pid 61)
2018-05-14 05:47:44,572 CRIT reaped unknown pid 65)
2018-05-14 05:47:44,576 CRIT reaped unknown pid 66)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 235)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 236)
2018-05-14 05:47:45,902 CRIT reaped unknown pid 257)
2018-05-14 05:47:48,153 CRIT reaped unknown pid 294)
2018-05-14 05:47:48,226 INFO spawned: 'homestead-prov' with pid 297
2018-05-14 05:47:48,237 INFO spawned: 'nginx' with pid 298
2018-05-14 05:47:48,747 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:49,750 INFO spawned: 'homestead-prov' with pid 324
2018-05-14 05:47:49,750 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:50,089 INFO exited: clearwater-infrastructure (exit status 0; expected)
2018-05-14 05:47:50,335 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:52,339 INFO spawned: 'homestead-prov' with pid 335
2018-05-14 05:47:53,324 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:56,330 INFO spawned: 'homestead-prov' with pid 346
2018-05-14 05:47:56,902 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:57,904 INFO gave up: homestead-prov entered FATAL state, too many start retries too quickly
```

Regards,
Matt Davis
Telstra | Graduate Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180517/82713682/attachment.html>

From Benjamin.Laing at metaswitch.com  Thu May 17 09:35:21 2018
From: Benjamin.Laing at metaswitch.com (Benjamin Laing)
Date: Thu, 17 May 2018 13:35:21 +0000
Subject: [Project Clearwater] Ellis failed to update server error
Message-ID: <CY4PR02MB25177CCF517CEF300A793DC2F1910@CY4PR02MB2517.namprd02.prod.outlook.com>

Hi Tapiwa,

Apologies in the delay in getting back to you!

Potentially there is a compatibility issue between the heat templates and OpenStack (see http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/2018-May/003943.html )- which version of OpenStack are you running?

Thanks,

Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180517/1004ae3d/attachment.html>

From Adam.Lindley at metaswitch.com  Thu May 17 13:04:31 2018
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Thu, 17 May 2018 17:04:31 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
Message-ID: <CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>

Hey Matthew,

I think we're making some solid progress, and I have a decent idea what was going on here. I think I have a better idea of what the AKS platform is doing, and should hopefully be able to explain why you were seeing the issues you were. Next steps on resolving your current issues below :)

So, first thing is the confusion over domains. I think there are two separate 'domains' here, as well as two separate DNS solutions doing slightly different things, and so the terms are a bit overloaded. As I see it there you have:

  *   AKS
     *   AKS domain, which is a globally routable web domain, '2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io', specific to your AKS account
     *   AKS DNS, which automatically creates entries based on Kubernetes ingress rules to provide access to specific pods at this domain, as you have set up for Ellis and Bono.
     *   This domain is what you originally passed in to the Project Clearwater pods, through the config map, and so is what our processes were attempting to connect to


  *   Kubernetes
     *   Kubernetes cluster domain, default.svc.cluster.local, which is a domain internal to the kubernetes cluster (the default value)
     *   Kubernetes DNS (probably kube-dns), where Kubernetes will automatically create records for all deployed pods and services, based on the configuration files you used in deploying.
     *   This domain is what is set in e.g. resolv.conf in your containers, and so is what you connect to if you run e.g. `nc cassandra 9160`

So, I think the way the issue you had getting the pods running manifested in such a hard to diagnose way is:

  *   You are able to contact the cassandra pods internally, e.g. using `nc cassandra 9160`, as this is going via the Kubernetes domain based DNS. i.e. cassandra.default.svc.cluster.local resolves to the internal IP of your cassandra pods.
  *   The clearwater processes were trying to use the AKS domain provided in `ZONE`, i.e. "cassandra.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io". As you noted, all the AKS DNS addresses resolve to the same IP. I believe this is the IP of the Kubernetes host, where the ingress rules you set up direct traffic for specific services through via the host IP.
  *   Because this resolved to an IP, we didn't see a DNS lookup failure, but it wasn't an IP at which we could connect to cassandra, because there are no ingress rules set up.

I think there's a simple solution here, based on the assumption that you want to be able to use the AKS domain as your subscriber home domain. We should simply be able to set that up as an additional home domain in the system.
To do this, we simply need to set up your configmap with the ZONE key set as `default.svc.cluster.local`, and the additional argument `--from-literal=ADDITIONAL_SHARED_CONFIG=additional_home_domains=2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io`

However, as you are currently provisioning subscribers through the Ellis web UI, and not integrating with an external HSS, I don't think we need to do this at the moment.
We can add it in if we see it become an issue down the line.


Next steps - Live tests
Good to see you've got the live test suite set up and running, though not successfully at the moment. In terms of the right command, I think the attempt you highlighted as taking a long time is the correct one. That is:
rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

You don't need to be running within the cluster for it to work; as long as your Bono and Ellis services are accessible from your test box, that will be fine.
While we're hitting errors, you can add ` TESTS="Basic call - mainline"` onto the command to simply run the first test only. This will limit the amount of error output you see on each run, and make it a bit easier to go about debugging.

The fact that the test command returned the error `Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060` suggests to me that there is potentially an issue in the ingress rules here. It is not returning a `getaddrinfo` error, so I believe the DNS is resolving to an IP, but that Bono is not reachable via that IP on port 5060.
I think your best next step here is to run some networking checks on this connection:

  *   From the box you are running the live tests on, make sure the bono.<aks domain> address resolves, the IP is reachable, and then try using `nc` to connect to port 5060.
  *   Run some tcpdumps in the bono pod to see if you are seeing any traffic reaching it on port 5060 when running the live tests

Hopefully this should help you track down at what point the traffic is getting stuck.

>From the Bono logs you've included, it looks like there's just no traffic reaching the process at all, and so I would suspect there's something unusual in the ingress rules setup. I don't know enough about the Azure http controller behaviour to be able to spot anything that looks particularly out of place at the moment though.
Equally, I am slightly worried that given that it is called 'http-application-routing' there may be some logic in place in the controller that will be interfering with non-http traffic. If we don't see any SIP traffic reaching the bono pod, we could potentially test if this is the case by using something like `curl` to send an http message to `bono.<aks domain>:5060`, and see if we see it reach the bono pod. If HTTP passes through, but SIP is blocked, I think we would need to dig some more into AKS configuration to work out what is going on there.

If you want to do testing with a sip client, I think you have most of the configuration correct, but I haven't used the Twinkle softclient. We have some docs on using sofclients, at http://clearwater.readthedocs.io/en/stable/Making_your_first_call.html, that you've probably already seen. Hopefully should be a decent guide. For the proxy, you'll want to use bono.<aks domain>, as that is what will resolve to the right IP address. However, if the  live tests are failing to connect to Bono, I think you will see similar issues down this path. We should focus on working out what is causing that problem first.

Hopefully once the connectivity issue to Bono is sorted, we should be fully up and running. Let us know how it goes, or if you have any questions on the distinction between the AKS domain and the Kubernetes/clearwater deployment domain.

Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 17 May 2018 09:32
To: Adam Lindley <Adam.Lindley at metaswitch.com>; clearwater at lists.projectclearwater.org
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,


  *   `netstat -planut` shows that Cassandra is listening on 9160 from all ports
  *   `nc cassandra 9160` did not exit immediately. It just hangs without any output
  *   `ping cassandra` works, but `dig cassandra` shows no IP addresses

I tried changing my env-vars configmap from 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io to default.svc.cluster.local. This means that the pods do spin up successfully, and that error message goes away. What is env-vars supposed to be set to?
The docs say: "a ZONE key set to the domain of your Kubernetes cluster". So I set it to the domain of my cluster, and that seems to be the cause of all my problems. If I set it to something which isn't the domain of the cluster, the pods come up. So now I'm confused, is it supposed to be set to the domain of my cluster or not?
Is there some extra step I need to do to configure kubernetes to know that it's domain is 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io? I'm pretty sure it already knows, but how can I check?

Now all the pods are up. I can't make a call or run tests successfully. I wonder whether this is because my deployment has no idea what that it's domain is actually 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io.

In a browser, ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io takes me to the sign up page. I successfully created an account. I think this is good proof that the dns, ingresses and firewalls are set up properly. (albeit only for port 80, but I've checked the other ports. And by 'properly' I mean 'probably too loosely')

I've noticed that all my dns records (set up by Azure) point to the same ip address. Is this right, or not? (I still get confused by how kubernetes shares ip addresses around).
Here's my ellis ingress in kubernetes (required for Azure to set up dns for me)

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: ellis
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: ellis
            servicePort: 80
```

And for bono

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: bono
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: bono
            servicePort: 3478
        - backend:
            serviceName: bono
            servicePort: 5060
        - backend:
            serviceName: bono
            servicePort: 5062
```

I don't understand ingresses well. I'm not certain that this is right. But records appear in the dns server with names 'bono' and 'ellis' pointing to the ip address of something.
The example in the Azure docs<https://docs.microsoft.com/en-us/azure/aks/http-application-routing> has 'path: /`. I'm not sure whether I need that or not in my ingresses. With or without that line, the result is the same.

I've tried with ingresses for just bono and ellis, and also ingresses for every module (except etcd). The result is the same. (If I don't have ingresses for a module, it has no DNS record)



I have no idea what parameters to use when running the tests. The documentation doesn't say. Similarly I have no idea what to use when configuring my SIP client. What's the "domain"? What's the "proxy"?
Here's a bunch of permutations of test arguments which I ran, and the associated error messages:


  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
  *   rake test[default.svc.cluster.local] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   This one took a really long time before failing. So maybe it's the closest?
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'


When I try to register with a SIP client I get a bunch of 404 and 408 (timeout) error codes. Here's what I'm trying to use. I'm basing this off what worked for the AIO installation with this SIP client (Twinkle, one of the ones on Ubuntu)

  *   Your Name: Matthew
  *   User Name: 6505550375
     *   Not sip:6505550375
     *   Not 6505550375 at ...
  *   Domain: default.svc.cluster.local, or is it example.com? The Clearwater kubernetes readme says you should make a call like an AIO installation, which is example.com.
  *   Authentication name: 6505550375 at default.svc.cluster.local<mailto:6505550375 at default.svc.cluster.local>
  *   Password: the one generated by the web page. Not the one you use to sign into the web page.
  *   Proxy server: not sure. What should I use?
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   example.com
     *   default.svc.cluster.local

After running a bunch of failed tests and trying manually to register on a SIP client, here are the logs I see:


  *   /var/log/bono/bono_current.txt on the bono pod:
     *   17-05-2018 08:00:18.268 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 25
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:00:54.278 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 32
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:01:17.285 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 20
  *    Ellis: /var/log/ellis/ellis_20180517T070000Z.txt
     *
     *   17-05-2018 07:44:01.833 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.848 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/
     *   17-05-2018 07:44:01.857 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets/e74929ec-21ea-48ac-95b6-68115894313d
     *   17-05-2018 07:44:01.876 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:144: {"associated_implicit_registration_sets": ["e74929ec-21ea-48ac-95b6-68115894313d"]}
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles
     *   17-05-2018 07:44:01.913 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/public_ids/sip%3A6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.966 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/public/sip%3A6505550400%40default.svc.cluster.local/service_profile
     *   17-05-2018 07:44:01.976 UTC INFO homestead.py:255: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/filter_criteria
     *   17-05-2018 07:44:01.980 UTC INFO xdm.py:29: Sending HTTP PUT request to http://homer.default.svc.cluster.local:7888/org.etsi.ngn.simservs/users/sip%3A6505550400%40default.svc.cluster.local/simservs.xml
     *   17-05-2018 07:44:02.028 UTC INFO web.py:1447: 200 POST /accounts/live.tests at example.com/numbers/<mailto:/accounts/live.tests at example.com/numbers/> (0.0.0.0) 209.96ms

Here are my firewall rules. My understanding is that these are the rules for a firewall between pods and the outside world. I think that communications between pods don't get filtered by these rules, but I added all the inter-pod ports just in case.

Inbound and Outbound TCP: 22,2380,4000,80,443,5060,5062,3478,5058,5054,5052,9888,8888,8889,10888,7888,11211,7000,7253,11311,9160
Inbound and Outbound UDP: 161,162,5060,32768-65535,3478

Any ideas where to go from here? My main question is what command should I run for rake tests?
Should I try to run the rake tests from a test pod within the cluster?

Thanks,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV
M  0415 762 868  | E  Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Thursday, 17 May 2018 3:05 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

I've had a look into your issues deploying under kubernetes, and think I've narrowed down the search a good amount. Details below.
First though I'll take a run through the smaller questions you've raised below, as I think we can knock some of them off fairly quickly.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
[AJL] You shouldn't need it in the containers. In our VMs we found that some operations were fairly heavy on DNS queries, and so provided dnsmasq as a means of providing caching. The need for this has decreased quite a bit now, and as we move forward in the microservice space it makes less sense to have it around.

     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
This will likely be down to how the container networking works, but it won't cause any problems in running under kubernetes.

     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
[AJL] This is a benign error in this case, so we haven't dug down into it. It shouldn't impact your deployment at all.

  *   There are many other red error messages during build. Does that matter?
[AJL] No, these don't matter. I see a number of error messages during the build. A lot of these are dependencies of some of our packages raising error messages when being installed in this environment

  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
[AJL] The AIO has a number of different requirements that we don't have in the container environment, and vice versa, so the fact that there is a different set doesn't mean you have issues there
As a couple of examples for the differences you pulled out below:

  *   The auto config packages are used to enable specific setup in different environments. E.g. setting up required config files that other deployments do not rely on. In the docker case, we need to read environment variables and pass them through to the system configuration files
  *   Clearwater-etcd is our interface into an etcd cluster, and is used for sharing common configuration around a cluster. On the AIO this simply increases complexity, as we have no need for it, but in docker/kubernetes, we want the services to share configuration through an underlying etcd cluster.
I don't believe any of the differences you pulled out below would be leading to the issues you're seeing, but it's a nice analysis :)
The missing dns.json config file is also currently just a benign error in the containers, raised by some expectations we have when running in VMs.


So, on to the main issue:
In getting your deployment up and running, the key issue is definitely going to be the 'Failed to initialize the Cassandra store' one. This indicates the Homestead and Homestead-prov services have been unable to reach the cassandra cluster, and without that they won't be able to get up and running.
As you've already found, getting logs out and doing more detailed debugging in containers presents its own set of problems. To temporarily prevent kubernetes from automatically restarting pods, you can (as you may have already) remove the liveness checking sections from the generated deployment files. This isn't ideal, but it should allow you time to do some more detailed debugging. You may also want to reduce the number of replicas of the pods down to one for now, to make it easier to follow traffic. Should just be a case of changing the .depl files and redeploying.

The log indicates you're getting error code 3 back from attempting to setup the Cassandra store, which is a 'connection error', as opposed to what you would get if the dns was simply unresolvable (I have tested that on our internal setup, and the logs would be clearly stating it was unresolvable). This is definitely leading me to suspect network connectivity issues into the cassandra pods. The fact that you can ping between the two however suggests that it's more complex than simply being unable to send any traffic between them.

To test this, I tweaked the cassandra configuration (running just a single pod for simplicity) so that it was listening on a port that wasn't 9160, which is the port that both homestead and homestead prov attempt to connect to. With this setup in place, I saw both of these pods repeatedly failing with the same error code as yours.
Because of this I would suggest the following next steps in tracking down the problem:

  *   Double check that your networking configuration allows traffic through to the cassandra pods on port 9160. The homestead and homestead prov processes will require this.
  *   Connect to the Cassandra pods, and verify that they are indeed listening for traffic on port 9160
     *   `netstat -planut` is my default for this, but to each their own
     *   /var/log/cassandra/system.log may have more information if something strange is going on here
  *   Manually check the connectivity between the pods, where you could try
     *   Connect to e.g. a homestead pod, and try running `nc <cassandra hostname> 9160`. I'm seeing this instantly return if the port is not open/listened on.
     *   Install tcpdump on both sides, and see whether you see traffic arriving in the cassandra pods at all. This isn't a very container, but getting a dump from both cassandra and homestead sides, to see what point in the flow is failing would be a decent way to work out where the issue lies.

Sadly, I don't have access to an AKS setup right now, so can't investigate the ingress rules component at all. I suspect that there's something slightly unusual happening there that is leading to traffic being unable to reach port 9160.
Hopefully this should give you a better idea of what the source of the issue is here. If this doesn't show anything up, and you're seeing traffic from homestead arrive in the cassandra pods and going back out we may have to run some more testing

Cheers,
Adam


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 16 May 2018 04:35
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Richard,

Tldr: I've made some progress since I sent that email. I've found and fixed bugs to do with dnsmasq, but now I'm stuck.
The systemd logs for homestead still say: "Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."
I'm not even sure whether the dnsmasq bugs were related.

First, to answer your questions:

Yes this happens every time I deploy.

My configmap:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
Events:  <none>
```

I'm deploying on Azure's kubernetes service (AKS). I've enabled HTTP Application Routing<https://docs.microsoft.com/en-us/azure/aks/http-application-routing>, which means that Azure manages DNS for me. The only unusual step is that Azure requires that I add ingresses to do so. The DNS records point to each service (bono, ellis, sprout etc). I can ping "cassandra" from the homestead pod.

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:22:21Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.6", GitCommit:"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1", GitTreeState:"clean", BuildDate:"2018-03-21T15:13:31Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}

Getting the logs is a race against time, since kubernetes keeps terminating the pod while I'm in there.
After several attempts, I managed to get the homestead logs (same as homestead-prov logs)

```
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:123: Creating Cached Resolver using servers:
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:133:     10.0.0.10
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Error static_dns_cache.cpp:89: DNS config file /etc/clearwater/dns.json mis
sing
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status main.cpp:653: Using local impu store: astaire.2991678f-f9dd-4098-8fe
8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.129 UTC [7f6379c8e7c0] Status http_connection_pool.cpp:37: Connection pool will use calculated res
ponse timeout of 550ms
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:35: Configuring HTTP Connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:36:   Connection created for server sprout.2991678f
-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io:9888
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status main.cpp:1013: No HSS configured - using Homestead-prov
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:266: Configuring store connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:267:   Hostname:  cassandra.2991678f-f9dd-4098-8
fe8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:268:   Port:      9160
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:296: Configuring store worker pool
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:297:   Threads:   10
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:298:   Max Queue: 0
15-05-2018 07:05:40.131 UTC [7f6366ffd700] Status alarm.cpp:244: Reraising all alarms with a known state
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Error main.cpp:1056: Failed to initialize the Cassandra store with error co
de 3.
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Status main.cpp:1057: Homestead is shutting down
```

The full logs are just this chunk repeating over and over until everything shuts down.

I think the crucial error message is DNS config file /etc/clearwater/dns.json missing

In the all-in-one installation, that file is just:

{
  "hostnames": [
  ]
}

Is it ok to manually insert that using the Dockerfile? Or should there be other entries in there for a non-AIO installation?

dns.json is created by the dnsmasq service.

During the container build, there is a red error message saying "dnsmasq: setting capabilities failed: Operation not permitted"
(There are a lot of red error messages during build. Is that an issue?)

I noticed that dnsmasq was not running in homestead docker, yet it is running in my working all-in-one installation. (There are also a bunch of other services which are halted. Is that an issue?)

I tried starting dnsmasq manually, it wouldn't start. I looked online and the solution is to add user=root to the file /etc/dnsmasq.conf.

I tried modifying the Dockerfile to manually append that string to the file.  (I'm assuming that the order of lines in /etc/dnsmasq.conf does not matter)

Then dnsmasq still doesn't come up. I get an error about how the port it wants is already taken.

So then I tried modifying the Dockerfile to paste in dns.conf from the AIO installation. (The one with an empty list of hosts)

Now the missing file error is gone, but I still get the other error:

"Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."

How can I debug this?

All this time I thought that error was caused by dns issues. Is it because dnsmasq still won't start, so dns.json is missing hosts? Or is that unrelated?
"ping cassandra" works from the homestead pod.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
  *   There are many other red error messages during build. Does that matter?
  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
     *   Services running in AIO:
        *   [ ? ]  clearwater-auto-config-generic
        *    [ + ]  clearwater-cluster-manager
        *    [ + ]  clearwater-config-manager
        *    [ + ]  clearwater-diags-monitor
        *    [ - ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ ? ]  clearwater-memcached
        *    [ + ]  clearwater-queue-manager
     *   Services running (or not) in docker homestead:
        *   [ ? ]  clearwater-auto-config-docker
        *    [ + ]  clearwater-cluster-manager
        *    [ - ]  clearwater-config-manager
        *    [ - ]  clearwater-diags-monitor
        *    [ + ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ - ]  clearwater-queue-manager


Thanks,

Matthew Davis
Telstra | CTO | Cloud SDN NFV

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Wednesday, 16 May 2018 3:24 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Matthew,

Sorry to hear you are having problems deploying Clearwater on Docker.

I think that the socket factory error is actually fairly benign.

Does this happen every time you create your deployment?

Can you provide the config map that you are providing when deploying on Kubernetes?

Can you connect into one of the pods that is failing to run homestead-prov and grab the logs. The relevant ones will be under /var/log/homestead-prov inside the container.

We cannot reproduce your issue on our kubernetes setup - can you provide details of your kubernetes setup - what version of k8s and what networking configuration you are using?

>From the diagnostics we have at the moment, the most likely scenario is that the Homestead and Homestead Prov containers are unable to contact Cassandra - if that's the case they will continually restart to attempt to resolve the problem.



Richard


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 14 May 2018 07:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi everyone,
I've fixed the issue I had with homestead-prov and submitted the patch as this pull request: https://github.com/Metaswitch/clearwater-docker/pull/89

However now I just run into another issue, so homestead-prov is giving me the same error as homestead. The error message is not verbose. Does anyone know how to make it more verbose? I have no idea what's happening.


```
2018-05-14 05:47:41,369 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:41,370 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-sig entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-mgmt entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,477 INFO exited: snmpd (exit status 0; expected)
2018-05-14 05:47:42,478 CRIT reaped unknown pid 61)
2018-05-14 05:47:44,572 CRIT reaped unknown pid 65)
2018-05-14 05:47:44,576 CRIT reaped unknown pid 66)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 235)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 236)
2018-05-14 05:47:45,902 CRIT reaped unknown pid 257)
2018-05-14 05:47:48,153 CRIT reaped unknown pid 294)
2018-05-14 05:47:48,226 INFO spawned: 'homestead-prov' with pid 297
2018-05-14 05:47:48,237 INFO spawned: 'nginx' with pid 298
2018-05-14 05:47:48,747 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:49,750 INFO spawned: 'homestead-prov' with pid 324
2018-05-14 05:47:49,750 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:50,089 INFO exited: clearwater-infrastructure (exit status 0; expected)
2018-05-14 05:47:50,335 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:52,339 INFO spawned: 'homestead-prov' with pid 335
2018-05-14 05:47:53,324 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:56,330 INFO spawned: 'homestead-prov' with pid 346
2018-05-14 05:47:56,902 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:57,904 INFO gave up: homestead-prov entered FATAL state, too many start retries too quickly
```

Regards,
Matt Davis
Telstra | Graduate Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180517/cadd786e/attachment.html>

From Matthew.Davis.2 at team.telstra.com  Fri May 18 04:13:52 2018
From: Matthew.Davis.2 at team.telstra.com (Davis, Matthew)
Date: Fri, 18 May 2018 08:13:52 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
Message-ID: <ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>

Hi Adam,

Thanks for that.


*        From the machine I'm running tests on, I can't ping bono.<ask-domain> or ellis.<aks-domain>. But I can open ellis.<aks-domain> in a browser (or with wget). I suspect this is because ping has no port number, so kubernetes doesn't know which ingress it should hand the packet to. `dig ellis.<aks-domain>` gives the ip address listed with `kubectl get ingress`

*        Curl:

o   `curl ellis.<ask-domain>:80` it returns a blank string immediately.

o   `curl ellis.<ask-domain>:81` - times out

o   `curl bono.<ask-domain>:5060` - times out

?  Nothing in tcpdump

*        When I run `kubectl get ingress` I only see port 80, even though I configured them to look on other ports. I suspect that ingresses only accept connections from port 80, and then on the internal side of the ingress they can work on any port. I don't know how to verify this.

Edit: It seems like this setup is temperamental. Yesterday and this morning I could open ellis in my browser and create an account. After tinkering around I've gone back to that setup, but it doesn't work. (timeout). Hmm.

My next step will be to install on a vanilla kubernetes environment. But I expect that will come with it's own challenges and even more limitations. (I won't have something exposing Kubernetes' DNS entries externally, and I won't be able to run tests on any real devices because this cluster will be in our corporate sandpit which is fenced off from everything).

I've tried to follow the alternative for DNS: https://github.com/Metaswitch/clearwater-docker#alternative-configuration-for-exposing-bono-and-ellis-services
I don't understand that part of the instructions.


*        Where do I add the 'nodePort' line.

*        Where did the number 30080 come from?

Regards,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 18 May 2018 3:05 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com>; clearwater at lists.projectclearwater.org
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

I think we're making some solid progress, and I have a decent idea what was going on here. I think I have a better idea of what the AKS platform is doing, and should hopefully be able to explain why you were seeing the issues you were. Next steps on resolving your current issues below :)

So, first thing is the confusion over domains. I think there are two separate 'domains' here, as well as two separate DNS solutions doing slightly different things, and so the terms are a bit overloaded. As I see it there you have:

  *   AKS
     *   AKS domain, which is a globally routable web domain, '2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io', specific to your AKS account
     *   AKS DNS, which automatically creates entries based on Kubernetes ingress rules to provide access to specific pods at this domain, as you have set up for Ellis and Bono.
     *   This domain is what you originally passed in to the Project Clearwater pods, through the config map, and so is what our processes were attempting to connect to


  *   Kubernetes
     *   Kubernetes cluster domain, default.svc.cluster.local, which is a domain internal to the kubernetes cluster (the default value)
     *   Kubernetes DNS (probably kube-dns), where Kubernetes will automatically create records for all deployed pods and services, based on the configuration files you used in deploying.
     *   This domain is what is set in e.g. resolv.conf in your containers, and so is what you connect to if you run e.g. `nc cassandra 9160`

So, I think the way the issue you had getting the pods running manifested in such a hard to diagnose way is:

  *   You are able to contact the cassandra pods internally, e.g. using `nc cassandra 9160`, as this is going via the Kubernetes domain based DNS. i.e. cassandra.default.svc.cluster.local resolves to the internal IP of your cassandra pods.
  *   The clearwater processes were trying to use the AKS domain provided in `ZONE`, i.e. "cassandra.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io". As you noted, all the AKS DNS addresses resolve to the same IP. I believe this is the IP of the Kubernetes host, where the ingress rules you set up direct traffic for specific services through via the host IP.
  *   Because this resolved to an IP, we didn't see a DNS lookup failure, but it wasn't an IP at which we could connect to cassandra, because there are no ingress rules set up.

I think there's a simple solution here, based on the assumption that you want to be able to use the AKS domain as your subscriber home domain. We should simply be able to set that up as an additional home domain in the system.
To do this, we simply need to set up your configmap with the ZONE key set as `default.svc.cluster.local`, and the additional argument `--from-literal=ADDITIONAL_SHARED_CONFIG=additional_home_domains=2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io`

However, as you are currently provisioning subscribers through the Ellis web UI, and not integrating with an external HSS, I don't think we need to do this at the moment.
We can add it in if we see it become an issue down the line.


Next steps - Live tests
Good to see you've got the live test suite set up and running, though not successfully at the moment. In terms of the right command, I think the attempt you highlighted as taking a long time is the correct one. That is:
rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

You don't need to be running within the cluster for it to work; as long as your Bono and Ellis services are accessible from your test box, that will be fine.
While we're hitting errors, you can add ` TESTS="Basic call - mainline"` onto the command to simply run the first test only. This will limit the amount of error output you see on each run, and make it a bit easier to go about debugging.

The fact that the test command returned the error `Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060` suggests to me that there is potentially an issue in the ingress rules here. It is not returning a `getaddrinfo` error, so I believe the DNS is resolving to an IP, but that Bono is not reachable via that IP on port 5060.
I think your best next step here is to run some networking checks on this connection:

  *   From the box you are running the live tests on, make sure the bono.<aks domain> address resolves, the IP is reachable, and then try using `nc` to connect to port 5060.
  *   Run some tcpdumps in the bono pod to see if you are seeing any traffic reaching it on port 5060 when running the live tests

Hopefully this should help you track down at what point the traffic is getting stuck.

>From the Bono logs you've included, it looks like there's just no traffic reaching the process at all, and so I would suspect there's something unusual in the ingress rules setup. I don't know enough about the Azure http controller behaviour to be able to spot anything that looks particularly out of place at the moment though.
Equally, I am slightly worried that given that it is called 'http-application-routing' there may be some logic in place in the controller that will be interfering with non-http traffic. If we don't see any SIP traffic reaching the bono pod, we could potentially test if this is the case by using something like `curl` to send an http message to `bono.<aks domain>:5060`, and see if we see it reach the bono pod. If HTTP passes through, but SIP is blocked, I think we would need to dig some more into AKS configuration to work out what is going on there.

If you want to do testing with a sip client, I think you have most of the configuration correct, but I haven't used the Twinkle softclient. We have some docs on using sofclients, at http://clearwater.readthedocs.io/en/stable/Making_your_first_call.html, that you've probably already seen. Hopefully should be a decent guide. For the proxy, you'll want to use bono.<aks domain>, as that is what will resolve to the right IP address. However, if the  live tests are failing to connect to Bono, I think you will see similar issues down this path. We should focus on working out what is causing that problem first.

Hopefully once the connectivity issue to Bono is sorted, we should be fully up and running. Let us know how it goes, or if you have any questions on the distinction between the AKS domain and the Kubernetes/clearwater deployment domain.

Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 17 May 2018 09:32
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,


  *   `netstat -planut` shows that Cassandra is listening on 9160 from all ports
  *   `nc cassandra 9160` did not exit immediately. It just hangs without any output
  *   `ping cassandra` works, but `dig cassandra` shows no IP addresses

I tried changing my env-vars configmap from 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io to default.svc.cluster.local. This means that the pods do spin up successfully, and that error message goes away. What is env-vars supposed to be set to?
The docs say: "a ZONE key set to the domain of your Kubernetes cluster". So I set it to the domain of my cluster, and that seems to be the cause of all my problems. If I set it to something which isn't the domain of the cluster, the pods come up. So now I'm confused, is it supposed to be set to the domain of my cluster or not?
Is there some extra step I need to do to configure kubernetes to know that it's domain is 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io? I'm pretty sure it already knows, but how can I check?

Now all the pods are up. I can't make a call or run tests successfully. I wonder whether this is because my deployment has no idea what that it's domain is actually 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io.

In a browser, ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io takes me to the sign up page. I successfully created an account. I think this is good proof that the dns, ingresses and firewalls are set up properly. (albeit only for port 80, but I've checked the other ports. And by 'properly' I mean 'probably too loosely')

I've noticed that all my dns records (set up by Azure) point to the same ip address. Is this right, or not? (I still get confused by how kubernetes shares ip addresses around).
Here's my ellis ingress in kubernetes (required for Azure to set up dns for me)

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: ellis
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: ellis
            servicePort: 80
```

And for bono

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: bono
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: bono
            servicePort: 3478
        - backend:
            serviceName: bono
            servicePort: 5060
        - backend:
            serviceName: bono
            servicePort: 5062
```

I don't understand ingresses well. I'm not certain that this is right. But records appear in the dns server with names 'bono' and 'ellis' pointing to the ip address of something.
The example in the Azure docs<https://docs.microsoft.com/en-us/azure/aks/http-application-routing> has 'path: /`. I'm not sure whether I need that or not in my ingresses. With or without that line, the result is the same.

I've tried with ingresses for just bono and ellis, and also ingresses for every module (except etcd). The result is the same. (If I don't have ingresses for a module, it has no DNS record)



I have no idea what parameters to use when running the tests. The documentation doesn't say. Similarly I have no idea what to use when configuring my SIP client. What's the "domain"? What's the "proxy"?
Here's a bunch of permutations of test arguments which I ran, and the associated error messages:


  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
  *   rake test[default.svc.cluster.local] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   This one took a really long time before failing. So maybe it's the closest?
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'


When I try to register with a SIP client I get a bunch of 404 and 408 (timeout) error codes. Here's what I'm trying to use. I'm basing this off what worked for the AIO installation with this SIP client (Twinkle, one of the ones on Ubuntu)

  *   Your Name: Matthew
  *   User Name: 6505550375
     *   Not sip:6505550375
     *   Not 6505550375 at ...
  *   Domain: default.svc.cluster.local, or is it example.com? The Clearwater kubernetes readme says you should make a call like an AIO installation, which is example.com.
  *   Authentication name: 6505550375 at default.svc.cluster.local<mailto:6505550375 at default.svc.cluster.local>
  *   Password: the one generated by the web page. Not the one you use to sign into the web page.
  *   Proxy server: not sure. What should I use?
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   example.com
     *   default.svc.cluster.local

After running a bunch of failed tests and trying manually to register on a SIP client, here are the logs I see:


  *   /var/log/bono/bono_current.txt on the bono pod:
     *   17-05-2018 08:00:18.268 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 25
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:00:54.278 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 32
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:01:17.285 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 20
  *    Ellis: /var/log/ellis/ellis_20180517T070000Z.txt
     *
     *   17-05-2018 07:44:01.833 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.848 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/
     *   17-05-2018 07:44:01.857 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets/e74929ec-21ea-48ac-95b6-68115894313d
     *   17-05-2018 07:44:01.876 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:144: {"associated_implicit_registration_sets": ["e74929ec-21ea-48ac-95b6-68115894313d"]}
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles
     *   17-05-2018 07:44:01.913 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/public_ids/sip%3A6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.966 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/public/sip%3A6505550400%40default.svc.cluster.local/service_profile
     *   17-05-2018 07:44:01.976 UTC INFO homestead.py:255: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/filter_criteria
     *   17-05-2018 07:44:01.980 UTC INFO xdm.py:29: Sending HTTP PUT request to http://homer.default.svc.cluster.local:7888/org.etsi.ngn.simservs/users/sip%3A6505550400%40default.svc.cluster.local/simservs.xml
     *   17-05-2018 07:44:02.028 UTC INFO web.py:1447: 200 POST /accounts/live.tests at example.com/numbers/<mailto:/accounts/live.tests at example.com/numbers/> (0.0.0.0) 209.96ms

Here are my firewall rules. My understanding is that these are the rules for a firewall between pods and the outside world. I think that communications between pods don't get filtered by these rules, but I added all the inter-pod ports just in case.

Inbound and Outbound TCP: 22,2380,4000,80,443,5060,5062,3478,5058,5054,5052,9888,8888,8889,10888,7888,11211,7000,7253,11311,9160
Inbound and Outbound UDP: 161,162,5060,32768-65535,3478

Any ideas where to go from here? My main question is what command should I run for rake tests?
Should I try to run the rake tests from a test pod within the cluster?

Thanks,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV
M  0415 762 868  | E  Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Thursday, 17 May 2018 3:05 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

I've had a look into your issues deploying under kubernetes, and think I've narrowed down the search a good amount. Details below.
First though I'll take a run through the smaller questions you've raised below, as I think we can knock some of them off fairly quickly.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
[AJL] You shouldn't need it in the containers. In our VMs we found that some operations were fairly heavy on DNS queries, and so provided dnsmasq as a means of providing caching. The need for this has decreased quite a bit now, and as we move forward in the microservice space it makes less sense to have it around.

     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
This will likely be down to how the container networking works, but it won't cause any problems in running under kubernetes.

     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
[AJL] This is a benign error in this case, so we haven't dug down into it. It shouldn't impact your deployment at all.

  *   There are many other red error messages during build. Does that matter?
[AJL] No, these don't matter. I see a number of error messages during the build. A lot of these are dependencies of some of our packages raising error messages when being installed in this environment

  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
[AJL] The AIO has a number of different requirements that we don't have in the container environment, and vice versa, so the fact that there is a different set doesn't mean you have issues there
As a couple of examples for the differences you pulled out below:

  *   The auto config packages are used to enable specific setup in different environments. E.g. setting up required config files that other deployments do not rely on. In the docker case, we need to read environment variables and pass them through to the system configuration files
  *   Clearwater-etcd is our interface into an etcd cluster, and is used for sharing common configuration around a cluster. On the AIO this simply increases complexity, as we have no need for it, but in docker/kubernetes, we want the services to share configuration through an underlying etcd cluster.
I don't believe any of the differences you pulled out below would be leading to the issues you're seeing, but it's a nice analysis :)
The missing dns.json config file is also currently just a benign error in the containers, raised by some expectations we have when running in VMs.


So, on to the main issue:
In getting your deployment up and running, the key issue is definitely going to be the 'Failed to initialize the Cassandra store' one. This indicates the Homestead and Homestead-prov services have been unable to reach the cassandra cluster, and without that they won't be able to get up and running.
As you've already found, getting logs out and doing more detailed debugging in containers presents its own set of problems. To temporarily prevent kubernetes from automatically restarting pods, you can (as you may have already) remove the liveness checking sections from the generated deployment files. This isn't ideal, but it should allow you time to do some more detailed debugging. You may also want to reduce the number of replicas of the pods down to one for now, to make it easier to follow traffic. Should just be a case of changing the .depl files and redeploying.

The log indicates you're getting error code 3 back from attempting to setup the Cassandra store, which is a 'connection error', as opposed to what you would get if the dns was simply unresolvable (I have tested that on our internal setup, and the logs would be clearly stating it was unresolvable). This is definitely leading me to suspect network connectivity issues into the cassandra pods. The fact that you can ping between the two however suggests that it's more complex than simply being unable to send any traffic between them.

To test this, I tweaked the cassandra configuration (running just a single pod for simplicity) so that it was listening on a port that wasn't 9160, which is the port that both homestead and homestead prov attempt to connect to. With this setup in place, I saw both of these pods repeatedly failing with the same error code as yours.
Because of this I would suggest the following next steps in tracking down the problem:

  *   Double check that your networking configuration allows traffic through to the cassandra pods on port 9160. The homestead and homestead prov processes will require this.
  *   Connect to the Cassandra pods, and verify that they are indeed listening for traffic on port 9160
     *   `netstat -planut` is my default for this, but to each their own
     *   /var/log/cassandra/system.log may have more information if something strange is going on here
  *   Manually check the connectivity between the pods, where you could try
     *   Connect to e.g. a homestead pod, and try running `nc <cassandra hostname> 9160`. I'm seeing this instantly return if the port is not open/listened on.
     *   Install tcpdump on both sides, and see whether you see traffic arriving in the cassandra pods at all. This isn't a very container, but getting a dump from both cassandra and homestead sides, to see what point in the flow is failing would be a decent way to work out where the issue lies.

Sadly, I don't have access to an AKS setup right now, so can't investigate the ingress rules component at all. I suspect that there's something slightly unusual happening there that is leading to traffic being unable to reach port 9160.
Hopefully this should give you a better idea of what the source of the issue is here. If this doesn't show anything up, and you're seeing traffic from homestead arrive in the cassandra pods and going back out we may have to run some more testing

Cheers,
Adam


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 16 May 2018 04:35
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Richard,

Tldr: I've made some progress since I sent that email. I've found and fixed bugs to do with dnsmasq, but now I'm stuck.
The systemd logs for homestead still say: "Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."
I'm not even sure whether the dnsmasq bugs were related.

First, to answer your questions:

Yes this happens every time I deploy.

My configmap:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
Events:  <none>
```

I'm deploying on Azure's kubernetes service (AKS). I've enabled HTTP Application Routing<https://docs.microsoft.com/en-us/azure/aks/http-application-routing>, which means that Azure manages DNS for me. The only unusual step is that Azure requires that I add ingresses to do so. The DNS records point to each service (bono, ellis, sprout etc). I can ping "cassandra" from the homestead pod.

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:22:21Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.6", GitCommit:"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1", GitTreeState:"clean", BuildDate:"2018-03-21T15:13:31Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}

Getting the logs is a race against time, since kubernetes keeps terminating the pod while I'm in there.
After several attempts, I managed to get the homestead logs (same as homestead-prov logs)

```
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:123: Creating Cached Resolver using servers:
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:133:     10.0.0.10
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Error static_dns_cache.cpp:89: DNS config file /etc/clearwater/dns.json mis
sing
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status main.cpp:653: Using local impu store: astaire.2991678f-f9dd-4098-8fe
8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.129 UTC [7f6379c8e7c0] Status http_connection_pool.cpp:37: Connection pool will use calculated res
ponse timeout of 550ms
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:35: Configuring HTTP Connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:36:   Connection created for server sprout.2991678f
-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io:9888
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status main.cpp:1013: No HSS configured - using Homestead-prov
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:266: Configuring store connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:267:   Hostname:  cassandra.2991678f-f9dd-4098-8
fe8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:268:   Port:      9160
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:296: Configuring store worker pool
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:297:   Threads:   10
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:298:   Max Queue: 0
15-05-2018 07:05:40.131 UTC [7f6366ffd700] Status alarm.cpp:244: Reraising all alarms with a known state
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Error main.cpp:1056: Failed to initialize the Cassandra store with error co
de 3.
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Status main.cpp:1057: Homestead is shutting down
```

The full logs are just this chunk repeating over and over until everything shuts down.

I think the crucial error message is DNS config file /etc/clearwater/dns.json missing

In the all-in-one installation, that file is just:

{
  "hostnames": [
  ]
}

Is it ok to manually insert that using the Dockerfile? Or should there be other entries in there for a non-AIO installation?

dns.json is created by the dnsmasq service.

During the container build, there is a red error message saying "dnsmasq: setting capabilities failed: Operation not permitted"
(There are a lot of red error messages during build. Is that an issue?)

I noticed that dnsmasq was not running in homestead docker, yet it is running in my working all-in-one installation. (There are also a bunch of other services which are halted. Is that an issue?)

I tried starting dnsmasq manually, it wouldn't start. I looked online and the solution is to add user=root to the file /etc/dnsmasq.conf.

I tried modifying the Dockerfile to manually append that string to the file.  (I'm assuming that the order of lines in /etc/dnsmasq.conf does not matter)

Then dnsmasq still doesn't come up. I get an error about how the port it wants is already taken.

So then I tried modifying the Dockerfile to paste in dns.conf from the AIO installation. (The one with an empty list of hosts)

Now the missing file error is gone, but I still get the other error:

"Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."

How can I debug this?

All this time I thought that error was caused by dns issues. Is it because dnsmasq still won't start, so dns.json is missing hosts? Or is that unrelated?
"ping cassandra" works from the homestead pod.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
  *   There are many other red error messages during build. Does that matter?
  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
     *   Services running in AIO:
        *   [ ? ]  clearwater-auto-config-generic
        *    [ + ]  clearwater-cluster-manager
        *    [ + ]  clearwater-config-manager
        *    [ + ]  clearwater-diags-monitor
        *    [ - ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ ? ]  clearwater-memcached
        *    [ + ]  clearwater-queue-manager
     *   Services running (or not) in docker homestead:
        *   [ ? ]  clearwater-auto-config-docker
        *    [ + ]  clearwater-cluster-manager
        *    [ - ]  clearwater-config-manager
        *    [ - ]  clearwater-diags-monitor
        *    [ + ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ - ]  clearwater-queue-manager


Thanks,

Matthew Davis
Telstra | CTO | Cloud SDN NFV

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Wednesday, 16 May 2018 3:24 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Matthew,

Sorry to hear you are having problems deploying Clearwater on Docker.

I think that the socket factory error is actually fairly benign.

Does this happen every time you create your deployment?

Can you provide the config map that you are providing when deploying on Kubernetes?

Can you connect into one of the pods that is failing to run homestead-prov and grab the logs. The relevant ones will be under /var/log/homestead-prov inside the container.

We cannot reproduce your issue on our kubernetes setup - can you provide details of your kubernetes setup - what version of k8s and what networking configuration you are using?

>From the diagnostics we have at the moment, the most likely scenario is that the Homestead and Homestead Prov containers are unable to contact Cassandra - if that's the case they will continually restart to attempt to resolve the problem.



Richard


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 14 May 2018 07:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi everyone,
I've fixed the issue I had with homestead-prov and submitted the patch as this pull request: https://github.com/Metaswitch/clearwater-docker/pull/89

However now I just run into another issue, so homestead-prov is giving me the same error as homestead. The error message is not verbose. Does anyone know how to make it more verbose? I have no idea what's happening.


```
2018-05-14 05:47:41,369 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:41,370 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-sig entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-mgmt entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,477 INFO exited: snmpd (exit status 0; expected)
2018-05-14 05:47:42,478 CRIT reaped unknown pid 61)
2018-05-14 05:47:44,572 CRIT reaped unknown pid 65)
2018-05-14 05:47:44,576 CRIT reaped unknown pid 66)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 235)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 236)
2018-05-14 05:47:45,902 CRIT reaped unknown pid 257)
2018-05-14 05:47:48,153 CRIT reaped unknown pid 294)
2018-05-14 05:47:48,226 INFO spawned: 'homestead-prov' with pid 297
2018-05-14 05:47:48,237 INFO spawned: 'nginx' with pid 298
2018-05-14 05:47:48,747 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:49,750 INFO spawned: 'homestead-prov' with pid 324
2018-05-14 05:47:49,750 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:50,089 INFO exited: clearwater-infrastructure (exit status 0; expected)
2018-05-14 05:47:50,335 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:52,339 INFO spawned: 'homestead-prov' with pid 335
2018-05-14 05:47:53,324 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:56,330 INFO spawned: 'homestead-prov' with pid 346
2018-05-14 05:47:56,902 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:57,904 INFO gave up: homestead-prov entered FATAL state, too many start retries too quickly
```

Regards,
Matt Davis
Telstra | Graduate Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180518/18ddef8f/attachment.html>

From Adam.Lindley at metaswitch.com  Fri May 18 13:45:42 2018
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Fri, 18 May 2018 17:45:42 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>
Message-ID: <CY1PR0201MB0971545314ED91FB7F233AC1E2900@CY1PR0201MB0971.namprd02.prod.outlook.com>

Hey Matthew,

No worries. Glad we're making headway on this. It will be useful to work out any of the kinks in using platforms like AKS so that we can continue to deploy easily in as many situations as possible.

>From the info you've sent below, it looks like there's an issue in the AKS configuration/ingress rules meaning there isn't anything in place for Bono. Without a rule to pass traffic through port 5060 to Bono, we're going to have a tough time getting calls running.
>From my experience with them, I think ingress rules should work on any port combinations, not just port 80. I would however suggest, if possible, clearing out the existing rules and then re-configuring the Ellis and Bono ingress rules from scratch, so that we can eliminate anything unusual that might have got in place while running through the other issues.
If you do this, and things still seem out of place, send over some more details on what the setup you're following is, and some diagnostics/info on what it all looks like.

To answer your direct questions:

  *   The port number 30080 came from the fact that some platforms have a restricted set of external ports available to expose. I believe we first saw this using GKE, but essentially, some platforms reserve low values in the port ranges for platform system processes, or exposing their own web UIs, and so we can't always guarantee that port 80 will be usable as the external facing port. Mapping port 30080 on the host through to 80 on the Ellis service however has worked well on other platforms, and just means you need to tweak the web address you access it over.
However, this may clash with some of the AKS behaviour, and if you are able to map directly through on port 80, that will work just fine. I suggest you lean more on AKS documentation on this side of things, as the Azure provided DNS etc may have special requirements or behaviours.
  *   The nodeport yaml configuration should look something like the below, taking Ellis as an example, with the 'nodePort' entry simply in addition to the existing 'name' and 'port' entries. i.e.:
apiVersion: v1
kind: Service
metadata:
  name: ellis
spec:
  type: NodePort
  ports:
  - name: "http"
    port: 80
    NodePort: 30080
  selector:
    service: ellis


  *   Bono may prove more challenging in the AKS environment, as it currently requires having a single static IP address. As the docs have it, we found the solution in GKE to be using a LoadBalancer type exposure, with static IP address. You will hopefully be able to do something very similar in AKS, where the IP of the Kubernetes host, already exposed by the AKS DNS records, is probably the right choice to start working with. I would suggest searching up a bit in the AKS documentation to see if there's anything in there about exposing services with a static IP, or potentially there are other support channels available for the Azure side of things.

I don't think we've quite reached an impasse on getting running with AKS here, but if you're keen to set up your own Kubernetes deployment that is definitely a possibility. As you've noted, there will be some limitations, particularly centred around networking. However, this is something we have definitely done before, so once those limitations are dealt with it should work as expected.
A few potentially helpful guiding points:

  *   If you are able to set up your Kubernetes cluster using a service network that is routable on the wider system network, potentially using simpler CNI network plugins, you can build a solution where your pod IPs are directly accessible, and so the need for any special NodePort or LoadBalancer options can be removed. This is very much a tech interest set up though, and not necessarily how we see things happening in the wider context of cloud native containerisation.
  *   You would potentially have to manage your own external DNS service if you want external components to be able to resolve your Kubernetes services. However, this should be as simple as running a simple DNS server on a box somewhere, and delegating your Kubernetes domain down to the Kubernetes DNS service. Again, there are likely to be many solutions to this problem, with lots of different information and guidance available. This will probably make it harder to get started, but should mean you're able to set up the solution you want.

Potentially, if you're able to let us know more about what sort of set up you're after, and what your requirements and available kit are, we may be able to help steer towards the right solution.

Hope this helps,
Adam


From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 18 May 2018 09:14
To: Adam Lindley <Adam.Lindley at metaswitch.com>; clearwater at lists.projectclearwater.org
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,

Thanks for that.


  *   From the machine I'm running tests on, I can't ping bono.<ask-domain> or ellis.<aks-domain>. But I can open ellis.<aks-domain> in a browser (or with wget). I suspect this is because ping has no port number, so kubernetes doesn't know which ingress it should hand the packet to. `dig ellis.<aks-domain>` gives the ip address listed with `kubectl get ingress`
  *   Curl:
     *   `curl ellis.<ask-domain>:80` it returns a blank string immediately.
     *   `curl ellis.<ask-domain>:81` - times out
     *   `curl bono.<ask-domain>:5060` - times out
        *   Nothing in tcpdump
  *   When I run `kubectl get ingress` I only see port 80, even though I configured them to look on other ports. I suspect that ingresses only accept connections from port 80, and then on the internal side of the ingress they can work on any port. I don't know how to verify this.

Edit: It seems like this setup is temperamental. Yesterday and this morning I could open ellis in my browser and create an account. After tinkering around I've gone back to that setup, but it doesn't work. (timeout). Hmm.

My next step will be to install on a vanilla kubernetes environment. But I expect that will come with it's own challenges and even more limitations. (I won't have something exposing Kubernetes' DNS entries externally, and I won't be able to run tests on any real devices because this cluster will be in our corporate sandpit which is fenced off from everything).

I've tried to follow the alternative for DNS: https://github.com/Metaswitch/clearwater-docker#alternative-configuration-for-exposing-bono-and-ellis-services
I don't understand that part of the instructions.

  *   Where do I add the 'nodePort' line.

  *   Where did the number 30080 come from?

Regards,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 18 May 2018 3:05 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

I think we're making some solid progress, and I have a decent idea what was going on here. I think I have a better idea of what the AKS platform is doing, and should hopefully be able to explain why you were seeing the issues you were. Next steps on resolving your current issues below :)

So, first thing is the confusion over domains. I think there are two separate 'domains' here, as well as two separate DNS solutions doing slightly different things, and so the terms are a bit overloaded. As I see it there you have:

  *   AKS
     *   AKS domain, which is a globally routable web domain, '2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io', specific to your AKS account
     *   AKS DNS, which automatically creates entries based on Kubernetes ingress rules to provide access to specific pods at this domain, as you have set up for Ellis and Bono.
     *   This domain is what you originally passed in to the Project Clearwater pods, through the config map, and so is what our processes were attempting to connect to


  *   Kubernetes
     *   Kubernetes cluster domain, default.svc.cluster.local, which is a domain internal to the kubernetes cluster (the default value)
     *   Kubernetes DNS (probably kube-dns), where Kubernetes will automatically create records for all deployed pods and services, based on the configuration files you used in deploying.
     *   This domain is what is set in e.g. resolv.conf in your containers, and so is what you connect to if you run e.g. `nc cassandra 9160`

So, I think the way the issue you had getting the pods running manifested in such a hard to diagnose way is:

  *   You are able to contact the cassandra pods internally, e.g. using `nc cassandra 9160`, as this is going via the Kubernetes domain based DNS. i.e. cassandra.default.svc.cluster.local resolves to the internal IP of your cassandra pods.
  *   The clearwater processes were trying to use the AKS domain provided in `ZONE`, i.e. "cassandra.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io". As you noted, all the AKS DNS addresses resolve to the same IP. I believe this is the IP of the Kubernetes host, where the ingress rules you set up direct traffic for specific services through via the host IP.
  *   Because this resolved to an IP, we didn't see a DNS lookup failure, but it wasn't an IP at which we could connect to cassandra, because there are no ingress rules set up.

I think there's a simple solution here, based on the assumption that you want to be able to use the AKS domain as your subscriber home domain. We should simply be able to set that up as an additional home domain in the system.
To do this, we simply need to set up your configmap with the ZONE key set as `default.svc.cluster.local`, and the additional argument `--from-literal=ADDITIONAL_SHARED_CONFIG=additional_home_domains=2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io`

However, as you are currently provisioning subscribers through the Ellis web UI, and not integrating with an external HSS, I don't think we need to do this at the moment.
We can add it in if we see it become an issue down the line.


Next steps - Live tests
Good to see you've got the live test suite set up and running, though not successfully at the moment. In terms of the right command, I think the attempt you highlighted as taking a long time is the correct one. That is:
rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

You don't need to be running within the cluster for it to work; as long as your Bono and Ellis services are accessible from your test box, that will be fine.
While we're hitting errors, you can add ` TESTS="Basic call - mainline"` onto the command to simply run the first test only. This will limit the amount of error output you see on each run, and make it a bit easier to go about debugging.

The fact that the test command returned the error `Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060` suggests to me that there is potentially an issue in the ingress rules here. It is not returning a `getaddrinfo` error, so I believe the DNS is resolving to an IP, but that Bono is not reachable via that IP on port 5060.
I think your best next step here is to run some networking checks on this connection:

  *   From the box you are running the live tests on, make sure the bono.<aks domain> address resolves, the IP is reachable, and then try using `nc` to connect to port 5060.
  *   Run some tcpdumps in the bono pod to see if you are seeing any traffic reaching it on port 5060 when running the live tests

Hopefully this should help you track down at what point the traffic is getting stuck.

>From the Bono logs you've included, it looks like there's just no traffic reaching the process at all, and so I would suspect there's something unusual in the ingress rules setup. I don't know enough about the Azure http controller behaviour to be able to spot anything that looks particularly out of place at the moment though.
Equally, I am slightly worried that given that it is called 'http-application-routing' there may be some logic in place in the controller that will be interfering with non-http traffic. If we don't see any SIP traffic reaching the bono pod, we could potentially test if this is the case by using something like `curl` to send an http message to `bono.<aks domain>:5060`, and see if we see it reach the bono pod. If HTTP passes through, but SIP is blocked, I think we would need to dig some more into AKS configuration to work out what is going on there.

If you want to do testing with a sip client, I think you have most of the configuration correct, but I haven't used the Twinkle softclient. We have some docs on using sofclients, at http://clearwater.readthedocs.io/en/stable/Making_your_first_call.html, that you've probably already seen. Hopefully should be a decent guide. For the proxy, you'll want to use bono.<aks domain>, as that is what will resolve to the right IP address. However, if the  live tests are failing to connect to Bono, I think you will see similar issues down this path. We should focus on working out what is causing that problem first.

Hopefully once the connectivity issue to Bono is sorted, we should be fully up and running. Let us know how it goes, or if you have any questions on the distinction between the AKS domain and the Kubernetes/clearwater deployment domain.

Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 17 May 2018 09:32
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,


  *   `netstat -planut` shows that Cassandra is listening on 9160 from all ports
  *   `nc cassandra 9160` did not exit immediately. It just hangs without any output
  *   `ping cassandra` works, but `dig cassandra` shows no IP addresses

I tried changing my env-vars configmap from 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io to default.svc.cluster.local. This means that the pods do spin up successfully, and that error message goes away. What is env-vars supposed to be set to?
The docs say: "a ZONE key set to the domain of your Kubernetes cluster". So I set it to the domain of my cluster, and that seems to be the cause of all my problems. If I set it to something which isn't the domain of the cluster, the pods come up. So now I'm confused, is it supposed to be set to the domain of my cluster or not?
Is there some extra step I need to do to configure kubernetes to know that it's domain is 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io? I'm pretty sure it already knows, but how can I check?

Now all the pods are up. I can't make a call or run tests successfully. I wonder whether this is because my deployment has no idea what that it's domain is actually 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io.

In a browser, ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io takes me to the sign up page. I successfully created an account. I think this is good proof that the dns, ingresses and firewalls are set up properly. (albeit only for port 80, but I've checked the other ports. And by 'properly' I mean 'probably too loosely')

I've noticed that all my dns records (set up by Azure) point to the same ip address. Is this right, or not? (I still get confused by how kubernetes shares ip addresses around).
Here's my ellis ingress in kubernetes (required for Azure to set up dns for me)

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: ellis
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: ellis
            servicePort: 80
```

And for bono

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: bono
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: bono
            servicePort: 3478
        - backend:
            serviceName: bono
            servicePort: 5060
        - backend:
            serviceName: bono
            servicePort: 5062
```

I don't understand ingresses well. I'm not certain that this is right. But records appear in the dns server with names 'bono' and 'ellis' pointing to the ip address of something.
The example in the Azure docs<https://docs.microsoft.com/en-us/azure/aks/http-application-routing> has 'path: /`. I'm not sure whether I need that or not in my ingresses. With or without that line, the result is the same.

I've tried with ingresses for just bono and ellis, and also ingresses for every module (except etcd). The result is the same. (If I don't have ingresses for a module, it has no DNS record)



I have no idea what parameters to use when running the tests. The documentation doesn't say. Similarly I have no idea what to use when configuring my SIP client. What's the "domain"? What's the "proxy"?
Here's a bunch of permutations of test arguments which I ran, and the associated error messages:


  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
  *   rake test[default.svc.cluster.local] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   This one took a really long time before failing. So maybe it's the closest?
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'


When I try to register with a SIP client I get a bunch of 404 and 408 (timeout) error codes. Here's what I'm trying to use. I'm basing this off what worked for the AIO installation with this SIP client (Twinkle, one of the ones on Ubuntu)

  *   Your Name: Matthew
  *   User Name: 6505550375
     *   Not sip:6505550375
     *   Not 6505550375 at ...
  *   Domain: default.svc.cluster.local, or is it example.com? The Clearwater kubernetes readme says you should make a call like an AIO installation, which is example.com.
  *   Authentication name: 6505550375 at default.svc.cluster.local<mailto:6505550375 at default.svc.cluster.local>
  *   Password: the one generated by the web page. Not the one you use to sign into the web page.
  *   Proxy server: not sure. What should I use?
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   example.com
     *   default.svc.cluster.local

After running a bunch of failed tests and trying manually to register on a SIP client, here are the logs I see:


  *   /var/log/bono/bono_current.txt on the bono pod:
     *   17-05-2018 08:00:18.268 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 25
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:00:54.278 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 32
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:01:17.285 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 20
  *    Ellis: /var/log/ellis/ellis_20180517T070000Z.txt
     *
     *   17-05-2018 07:44:01.833 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.848 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/
     *   17-05-2018 07:44:01.857 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets/e74929ec-21ea-48ac-95b6-68115894313d
     *   17-05-2018 07:44:01.876 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:144: {"associated_implicit_registration_sets": ["e74929ec-21ea-48ac-95b6-68115894313d"]}
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles
     *   17-05-2018 07:44:01.913 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/public_ids/sip%3A6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.966 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/public/sip%3A6505550400%40default.svc.cluster.local/service_profile
     *   17-05-2018 07:44:01.976 UTC INFO homestead.py:255: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/filter_criteria
     *   17-05-2018 07:44:01.980 UTC INFO xdm.py:29: Sending HTTP PUT request to http://homer.default.svc.cluster.local:7888/org.etsi.ngn.simservs/users/sip%3A6505550400%40default.svc.cluster.local/simservs.xml
     *   17-05-2018 07:44:02.028 UTC INFO web.py:1447: 200 POST /accounts/live.tests at example.com/numbers/<mailto:/accounts/live.tests at example.com/numbers/> (0.0.0.0) 209.96ms

Here are my firewall rules. My understanding is that these are the rules for a firewall between pods and the outside world. I think that communications between pods don't get filtered by these rules, but I added all the inter-pod ports just in case.

Inbound and Outbound TCP: 22,2380,4000,80,443,5060,5062,3478,5058,5054,5052,9888,8888,8889,10888,7888,11211,7000,7253,11311,9160
Inbound and Outbound UDP: 161,162,5060,32768-65535,3478

Any ideas where to go from here? My main question is what command should I run for rake tests?
Should I try to run the rake tests from a test pod within the cluster?

Thanks,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV
M  0415 762 868  | E  Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Thursday, 17 May 2018 3:05 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

I've had a look into your issues deploying under kubernetes, and think I've narrowed down the search a good amount. Details below.
First though I'll take a run through the smaller questions you've raised below, as I think we can knock some of them off fairly quickly.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
[AJL] You shouldn't need it in the containers. In our VMs we found that some operations were fairly heavy on DNS queries, and so provided dnsmasq as a means of providing caching. The need for this has decreased quite a bit now, and as we move forward in the microservice space it makes less sense to have it around.

     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
This will likely be down to how the container networking works, but it won't cause any problems in running under kubernetes.

     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
[AJL] This is a benign error in this case, so we haven't dug down into it. It shouldn't impact your deployment at all.

  *   There are many other red error messages during build. Does that matter?
[AJL] No, these don't matter. I see a number of error messages during the build. A lot of these are dependencies of some of our packages raising error messages when being installed in this environment

  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
[AJL] The AIO has a number of different requirements that we don't have in the container environment, and vice versa, so the fact that there is a different set doesn't mean you have issues there
As a couple of examples for the differences you pulled out below:

  *   The auto config packages are used to enable specific setup in different environments. E.g. setting up required config files that other deployments do not rely on. In the docker case, we need to read environment variables and pass them through to the system configuration files
  *   Clearwater-etcd is our interface into an etcd cluster, and is used for sharing common configuration around a cluster. On the AIO this simply increases complexity, as we have no need for it, but in docker/kubernetes, we want the services to share configuration through an underlying etcd cluster.
I don't believe any of the differences you pulled out below would be leading to the issues you're seeing, but it's a nice analysis :)
The missing dns.json config file is also currently just a benign error in the containers, raised by some expectations we have when running in VMs.


So, on to the main issue:
In getting your deployment up and running, the key issue is definitely going to be the 'Failed to initialize the Cassandra store' one. This indicates the Homestead and Homestead-prov services have been unable to reach the cassandra cluster, and without that they won't be able to get up and running.
As you've already found, getting logs out and doing more detailed debugging in containers presents its own set of problems. To temporarily prevent kubernetes from automatically restarting pods, you can (as you may have already) remove the liveness checking sections from the generated deployment files. This isn't ideal, but it should allow you time to do some more detailed debugging. You may also want to reduce the number of replicas of the pods down to one for now, to make it easier to follow traffic. Should just be a case of changing the .depl files and redeploying.

The log indicates you're getting error code 3 back from attempting to setup the Cassandra store, which is a 'connection error', as opposed to what you would get if the dns was simply unresolvable (I have tested that on our internal setup, and the logs would be clearly stating it was unresolvable). This is definitely leading me to suspect network connectivity issues into the cassandra pods. The fact that you can ping between the two however suggests that it's more complex than simply being unable to send any traffic between them.

To test this, I tweaked the cassandra configuration (running just a single pod for simplicity) so that it was listening on a port that wasn't 9160, which is the port that both homestead and homestead prov attempt to connect to. With this setup in place, I saw both of these pods repeatedly failing with the same error code as yours.
Because of this I would suggest the following next steps in tracking down the problem:

  *   Double check that your networking configuration allows traffic through to the cassandra pods on port 9160. The homestead and homestead prov processes will require this.
  *   Connect to the Cassandra pods, and verify that they are indeed listening for traffic on port 9160
     *   `netstat -planut` is my default for this, but to each their own
     *   /var/log/cassandra/system.log may have more information if something strange is going on here
  *   Manually check the connectivity between the pods, where you could try
     *   Connect to e.g. a homestead pod, and try running `nc <cassandra hostname> 9160`. I'm seeing this instantly return if the port is not open/listened on.
     *   Install tcpdump on both sides, and see whether you see traffic arriving in the cassandra pods at all. This isn't a very container, but getting a dump from both cassandra and homestead sides, to see what point in the flow is failing would be a decent way to work out where the issue lies.

Sadly, I don't have access to an AKS setup right now, so can't investigate the ingress rules component at all. I suspect that there's something slightly unusual happening there that is leading to traffic being unable to reach port 9160.
Hopefully this should give you a better idea of what the source of the issue is here. If this doesn't show anything up, and you're seeing traffic from homestead arrive in the cassandra pods and going back out we may have to run some more testing

Cheers,
Adam


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 16 May 2018 04:35
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Richard,

Tldr: I've made some progress since I sent that email. I've found and fixed bugs to do with dnsmasq, but now I'm stuck.
The systemd logs for homestead still say: "Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."
I'm not even sure whether the dnsmasq bugs were related.

First, to answer your questions:

Yes this happens every time I deploy.

My configmap:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
Events:  <none>
```

I'm deploying on Azure's kubernetes service (AKS). I've enabled HTTP Application Routing<https://docs.microsoft.com/en-us/azure/aks/http-application-routing>, which means that Azure manages DNS for me. The only unusual step is that Azure requires that I add ingresses to do so. The DNS records point to each service (bono, ellis, sprout etc). I can ping "cassandra" from the homestead pod.

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:22:21Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.6", GitCommit:"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1", GitTreeState:"clean", BuildDate:"2018-03-21T15:13:31Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}

Getting the logs is a race against time, since kubernetes keeps terminating the pod while I'm in there.
After several attempts, I managed to get the homestead logs (same as homestead-prov logs)

```
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:123: Creating Cached Resolver using servers:
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:133:     10.0.0.10
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Error static_dns_cache.cpp:89: DNS config file /etc/clearwater/dns.json mis
sing
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status main.cpp:653: Using local impu store: astaire.2991678f-f9dd-4098-8fe
8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.129 UTC [7f6379c8e7c0] Status http_connection_pool.cpp:37: Connection pool will use calculated res
ponse timeout of 550ms
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:35: Configuring HTTP Connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:36:   Connection created for server sprout.2991678f
-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io:9888
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status main.cpp:1013: No HSS configured - using Homestead-prov
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:266: Configuring store connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:267:   Hostname:  cassandra.2991678f-f9dd-4098-8
fe8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:268:   Port:      9160
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:296: Configuring store worker pool
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:297:   Threads:   10
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:298:   Max Queue: 0
15-05-2018 07:05:40.131 UTC [7f6366ffd700] Status alarm.cpp:244: Reraising all alarms with a known state
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Error main.cpp:1056: Failed to initialize the Cassandra store with error co
de 3.
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Status main.cpp:1057: Homestead is shutting down
```

The full logs are just this chunk repeating over and over until everything shuts down.

I think the crucial error message is DNS config file /etc/clearwater/dns.json missing

In the all-in-one installation, that file is just:

{
  "hostnames": [
  ]
}

Is it ok to manually insert that using the Dockerfile? Or should there be other entries in there for a non-AIO installation?

dns.json is created by the dnsmasq service.

During the container build, there is a red error message saying "dnsmasq: setting capabilities failed: Operation not permitted"
(There are a lot of red error messages during build. Is that an issue?)

I noticed that dnsmasq was not running in homestead docker, yet it is running in my working all-in-one installation. (There are also a bunch of other services which are halted. Is that an issue?)

I tried starting dnsmasq manually, it wouldn't start. I looked online and the solution is to add user=root to the file /etc/dnsmasq.conf.

I tried modifying the Dockerfile to manually append that string to the file.  (I'm assuming that the order of lines in /etc/dnsmasq.conf does not matter)

Then dnsmasq still doesn't come up. I get an error about how the port it wants is already taken.

So then I tried modifying the Dockerfile to paste in dns.conf from the AIO installation. (The one with an empty list of hosts)

Now the missing file error is gone, but I still get the other error:

"Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."

How can I debug this?

All this time I thought that error was caused by dns issues. Is it because dnsmasq still won't start, so dns.json is missing hosts? Or is that unrelated?
"ping cassandra" works from the homestead pod.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
  *   There are many other red error messages during build. Does that matter?
  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
     *   Services running in AIO:
        *   [ ? ]  clearwater-auto-config-generic
        *    [ + ]  clearwater-cluster-manager
        *    [ + ]  clearwater-config-manager
        *    [ + ]  clearwater-diags-monitor
        *    [ - ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ ? ]  clearwater-memcached
        *    [ + ]  clearwater-queue-manager
     *   Services running (or not) in docker homestead:
        *   [ ? ]  clearwater-auto-config-docker
        *    [ + ]  clearwater-cluster-manager
        *    [ - ]  clearwater-config-manager
        *    [ - ]  clearwater-diags-monitor
        *    [ + ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ - ]  clearwater-queue-manager


Thanks,

Matthew Davis
Telstra | CTO | Cloud SDN NFV

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Wednesday, 16 May 2018 3:24 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Matthew,

Sorry to hear you are having problems deploying Clearwater on Docker.

I think that the socket factory error is actually fairly benign.

Does this happen every time you create your deployment?

Can you provide the config map that you are providing when deploying on Kubernetes?

Can you connect into one of the pods that is failing to run homestead-prov and grab the logs. The relevant ones will be under /var/log/homestead-prov inside the container.

We cannot reproduce your issue on our kubernetes setup - can you provide details of your kubernetes setup - what version of k8s and what networking configuration you are using?

>From the diagnostics we have at the moment, the most likely scenario is that the Homestead and Homestead Prov containers are unable to contact Cassandra - if that's the case they will continually restart to attempt to resolve the problem.



Richard


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 14 May 2018 07:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi everyone,
I've fixed the issue I had with homestead-prov and submitted the patch as this pull request: https://github.com/Metaswitch/clearwater-docker/pull/89

However now I just run into another issue, so homestead-prov is giving me the same error as homestead. The error message is not verbose. Does anyone know how to make it more verbose? I have no idea what's happening.


```
2018-05-14 05:47:41,369 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:41,370 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-sig entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-mgmt entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,477 INFO exited: snmpd (exit status 0; expected)
2018-05-14 05:47:42,478 CRIT reaped unknown pid 61)
2018-05-14 05:47:44,572 CRIT reaped unknown pid 65)
2018-05-14 05:47:44,576 CRIT reaped unknown pid 66)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 235)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 236)
2018-05-14 05:47:45,902 CRIT reaped unknown pid 257)
2018-05-14 05:47:48,153 CRIT reaped unknown pid 294)
2018-05-14 05:47:48,226 INFO spawned: 'homestead-prov' with pid 297
2018-05-14 05:47:48,237 INFO spawned: 'nginx' with pid 298
2018-05-14 05:47:48,747 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:49,750 INFO spawned: 'homestead-prov' with pid 324
2018-05-14 05:47:49,750 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:50,089 INFO exited: clearwater-infrastructure (exit status 0; expected)
2018-05-14 05:47:50,335 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:52,339 INFO spawned: 'homestead-prov' with pid 335
2018-05-14 05:47:53,324 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:56,330 INFO spawned: 'homestead-prov' with pid 346
2018-05-14 05:47:56,902 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:57,904 INFO gave up: homestead-prov entered FATAL state, too many start retries too quickly
```

Regards,
Matt Davis
Telstra | Graduate Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180518/e43ed992/attachment.html>

From amirhatami1000 at gmail.com  Sat May 19 10:24:52 2018
From: amirhatami1000 at gmail.com (Amir Moahammad Hatami)
Date: Sat, 19 May 2018 18:54:52 +0430
Subject: [Project Clearwater] Chef server and clients are down at the end of
 installation.
Message-ID: <CAEtN4MAS+cbsBiTV0bqarGvKoaZK9Ua9obBKujaviRcbnXLHHA@mail.gmail.com>

Hi,

I am using the Amazon AWS to install Clearwater deployment. During the
execution of the last command  *<knife deployment resize -E clearwater -V>
*The two machines for chef server and chef client are down. Any ideas on
why ? I have attached a snapshot of the terminal.

Thanks



?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180519/40ca7a5a/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image.png
Type: image/png
Size: 93213 bytes
Desc: not available
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180519/40ca7a5a/attachment.png>

From amirhatami1000 at gmail.com  Mon May 21 08:46:53 2018
From: amirhatami1000 at gmail.com (Amir Moahammad Hatami)
Date: Mon, 21 May 2018 17:16:53 +0430
Subject: [Project Clearwater] Chef server and clients are down at the end of
 installation
Message-ID: <CAEtN4MC8DWr5etE1n755YvE39aqb-M4jhx6H5Cb7SX17nPgGZQ@mail.gmail.com>

Hi,

I am using the Amazon AWS to install Clearwater deployment. During the
execution of the last command  *<knife deployment resize -E clearwater -V>
*The two machines for chef server and chef client are down. Any ideas on
why ? Since the picture is not attached I copied terminal contents here.



?JSON Attributes: {:clearwater=>{:index=>1, :site=>1, :seagull=>nil,
:ralf=>nil}}
INFO: Successfully created clearwater-vellum-1
INFO: Preparing to quiesce extra nodes
INFO: Quiescing extra nodes
INFO: Cleaning config
INFO: Cleaning deployment...
INFO: Will delete following broken servers: ["UNNAMED", "UNNAMED"]
INFO: Found broken box
Instance ID: i-034c67037e5dac3f9
Flavor: t2.medium
Image: ami-009f5845b05f2cc55
Region: us-east-1
Availability Zone: us-east-1a
Security Groups: launch-wizard-2
SSH Key: Keypair1
Root Device Type: ebs
Public DNS Name: ec2-34-226-202-201.compute-1.amazonaws.com
Public IP Address: 34.226.202.201
Private DNS Name: ip-172-31-28-205.ec2.internal
Private IP Address: 172.31.28.205

WARNING: Deleted server i-034c67037e5dac3f9
WARNING: Corresponding node and client for the i-034c67037e5dac3f9 server
were not deleted and remain registered with the Chef Server
INFO: Found broken box
Instance ID: i-068272c582d1dc7e4
Flavor: t2.small
Image: ami-0666102b02895e832
Region: us-east-1
Availability Zone: us-east-1d
Security Groups: launch-wizard-2
SSH Key: Keypair1
Root Device Type: ebs
Public DNS Name: ec2-18-206-231-145.compute-1.amazonaws.com
Public IP Address: 18.206.231.145
Private DNS Name: ip-172-31-85-97.ec2.internal
Private IP Address: 172.31.85.97

WARNING: Deleted server i-068272c582d1dc7e4
WARNING: Corresponding node and client for the i-068272c582d1dc7e4 server
were not deleted and remain registered with the Chef Server

Broadcast message from root at ip-172-31-85-97
        (unknown) at 12:41 ...

The system is going down for power off NOW!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180521/7dc3be54/attachment.html>

From tcchindeka at gmail.com  Tue May 22 03:21:30 2018
From: tcchindeka at gmail.com (Tapiwa Chindeka)
Date: Tue, 22 May 2018 09:21:30 +0200
Subject: [Project Clearwater] Ellis failed to update server error
In-Reply-To: <CY4PR02MB25177CCF517CEF300A793DC2F1910@CY4PR02MB2517.namprd02.prod.outlook.com>
References: <CY4PR02MB25177CCF517CEF300A793DC2F1910@CY4PR02MB2517.namprd02.prod.outlook.com>
Message-ID: <CAFFdzJToj3r==hTKV8jSEL_KmH7wC=6+Jw+uak5ND1uL6w0CKQ@mail.gmail.com>

Hi Ben

I am using OpenStack Ocata.

My parameters file has the following:

parameters:
  public_mgmt_net_id: 1f6cabf2-e6a4-4baf-8cc4-7af1b7d06067
  private_mgmt_net_cidr: 172.16.11.0/24
  private_mgmt_net_gateway: 172.16.11.1
  private_mgmt_net_pool_start: 172.16.11.2
  private_mgmt_net_pool_end: 172.16.11.254
  external_mgmt_dns_ip: 203.0.113.1
  public_sig_net_id: 1f6cabf2-e6a4-4baf-8cc4-7af1b7d06067
  private_sig_net_ip_version: "4"
  private_sig_net_cidr: 172.16.10.0/24
  private_sig_net_gateway: 172.16.10.1
  private_sig_net_pool_start: 172.16.10.2
  private_sig_net_pool_end: 172.16.10.254
  external_sig_dns_ip: 203.0.113.1
  flavor: m4.small
  key_name: cw-key
  repo_url: http://repo.cw-ngv.com/stable
  zone: openstack.rhodes.com
  image: ubuntu 14.04 LTS
  dn_range_start: "6505550000"
  dn_range_length: "1000"
  dnssec_key: *generated DNS private key*
  bono_cluster_size: 1
  sprout_cluster_size: 1
  homer_cluster_size: 1
  dime_cluster_size: 1
  vellum_cluster_size: 1

Kind regards
Tapiwa



On Thu, May 17, 2018 at 3:35 PM, Benjamin Laing <
Benjamin.Laing at metaswitch.com> wrote:

> Hi Tapiwa,
>
>
>
> Apologies in the delay in getting back to you!
>
>
>
> Potentially there is a compatibility issue between the heat templates and
> OpenStack (see http://lists.projectclearwater.org/
> pipermail/clearwater_lists.projectclearwater.org/2018-May/003943.html )?
> which version of OpenStack are you running?
>
>
>
> Thanks,
>
>
>
> Ben
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180522/0463137e/attachment.html>

From Matthew.Davis.2 at team.telstra.com  Tue May 22 04:15:00 2018
From: Matthew.Davis.2 at team.telstra.com (Davis, Matthew)
Date: Tue, 22 May 2018 08:15:00 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <CY1PR0201MB0971545314ED91FB7F233AC1E2900@CY1PR0201MB0971.namprd02.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971545314ED91FB7F233AC1E2900@CY1PR0201MB0971.namprd02.prod.outlook.com>
Message-ID: <ME2PR01MB288450EF460A39FEA9E1D724C2940@ME2PR01MB2884.ausprd01.prod.outlook.com>

Hi Adam,

I heard back from a contact we have in Microsoft. They seemed to suggest that selecting 'advanced networking' instead of 'basic' (which is what I was using) might make each pod directly addressable, but from within an azure vnet (so I'd have to find a way to expose that to the public internet for my SIP clients). I'm halfway through trying that at the moment. I'll update that thread once I run some rake tests.

As far as a vanilla kubernetes installation goes, I tried that today and couldn't get it working. Can you help me with that?
I'm trying to just get the simplest setup possible. For now my goal is just to get something working somewhere. (Incidentally this is why I initially tried Azure Kubernetes Service with Application Routing)

So in my vanilla installation (i.e. kubernetes on openstack) I just used 1 bono pod. (Once I get this simple install working, then I'll try more complicated setups)

How am I supposed to run the rake tests?

My cluster is at 10.3.1.76. I changed ellis-svc to a nodePort type.

I'm trying:

rake test[default.svc.cluster.local] PROXY="10.3.1.76" SIGNUP_CODE=secret ELLIS=10.3.1.76:30080 TESTS="Basic call - mainline"

The error message is:

Basic Call - Mainline (TCP) - Failed
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2) for "10.3.1.76" port 5060
     - /home/ubuntu/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     - /home/ubuntu/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     - /home/ubuntu/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     - /home/ubuntu/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
     - /home/ubuntu/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new_source'

The logs in bono in /var/log/bono/bono_current.txt did not show any additional lines during the test. The logs are currently:

22-05-2018 08:11:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1012.3 alarm
22-05-2018 08:11:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1013.3 alarm
22-05-2018 08:12:02.410 UTC [7f59f0ff1700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 20
22-05-2018 08:12:04.667 UTC [7f5a15023700] Status alarm.cpp:244: Reraising all alarms with a known state
22-05-2018 08:12:04.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1012.3 alarm
22-05-2018 08:12:04.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1013.3 alarm
22-05-2018 08:12:26.414 UTC [7f59f0ff1700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 5
22-05-2018 08:12:34.667 UTC [7f5a15023700] Status alarm.cpp:244: Reraising all alarms with a known state
22-05-2018 08:12:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1012.3 alarm
22-05-2018 08:12:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1013.3 alarm

When I try manually in a GUI SIP client I get a 503 error.

I haven't done any DNS stuff for this deployment. As I said, I'm trying to keep it simple, just so that I have something working. My understanding is that I only need the DNS server if I want SIP clients to access the server via a domain name instead of an IP address. Is that correct?

When I run `nc 10.3.1.76 30080 -v` it connects successfully.
When I run `nc 10.3.1.76 5060 -v`  fails (connection refused), both from my laptop and from one of the pods which isn't bono. Since I get 'connection refused' when running that from within the cluster, that means it's not a firewall issue. (The firewall surrounds the whole cluster, and doesn't get between pods).

Thanks,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Saturday, 19 May 2018 3:46 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com>; clearwater at lists.projectclearwater.org
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

No worries. Glad we're making headway on this. It will be useful to work out any of the kinks in using platforms like AKS so that we can continue to deploy easily in as many situations as possible.

>From the info you've sent below, it looks like there's an issue in the AKS configuration/ingress rules meaning there isn't anything in place for Bono. Without a rule to pass traffic through port 5060 to Bono, we're going to have a tough time getting calls running.
>From my experience with them, I think ingress rules should work on any port combinations, not just port 80. I would however suggest, if possible, clearing out the existing rules and then re-configuring the Ellis and Bono ingress rules from scratch, so that we can eliminate anything unusual that might have got in place while running through the other issues.
If you do this, and things still seem out of place, send over some more details on what the setup you're following is, and some diagnostics/info on what it all looks like.

To answer your direct questions:

  *   The port number 30080 came from the fact that some platforms have a restricted set of external ports available to expose. I believe we first saw this using GKE, but essentially, some platforms reserve low values in the port ranges for platform system processes, or exposing their own web UIs, and so we can't always guarantee that port 80 will be usable as the external facing port. Mapping port 30080 on the host through to 80 on the Ellis service however has worked well on other platforms, and just means you need to tweak the web address you access it over.
However, this may clash with some of the AKS behaviour, and if you are able to map directly through on port 80, that will work just fine. I suggest you lean more on AKS documentation on this side of things, as the Azure provided DNS etc may have special requirements or behaviours.
  *   The nodeport yaml configuration should look something like the below, taking Ellis as an example, with the 'nodePort' entry simply in addition to the existing 'name' and 'port' entries. i.e.:
apiVersion: v1
kind: Service
metadata:
  name: ellis
spec:
  type: NodePort
  ports:
  - name: "http"
    port: 80
    NodePort: 30080
  selector:
    service: ellis


  *   Bono may prove more challenging in the AKS environment, as it currently requires having a single static IP address. As the docs have it, we found the solution in GKE to be using a LoadBalancer type exposure, with static IP address. You will hopefully be able to do something very similar in AKS, where the IP of the Kubernetes host, already exposed by the AKS DNS records, is probably the right choice to start working with. I would suggest searching up a bit in the AKS documentation to see if there's anything in there about exposing services with a static IP, or potentially there are other support channels available for the Azure side of things.

I don't think we've quite reached an impasse on getting running with AKS here, but if you're keen to set up your own Kubernetes deployment that is definitely a possibility. As you've noted, there will be some limitations, particularly centred around networking. However, this is something we have definitely done before, so once those limitations are dealt with it should work as expected.
A few potentially helpful guiding points:

  *   If you are able to set up your Kubernetes cluster using a service network that is routable on the wider system network, potentially using simpler CNI network plugins, you can build a solution where your pod IPs are directly accessible, and so the need for any special NodePort or LoadBalancer options can be removed. This is very much a tech interest set up though, and not necessarily how we see things happening in the wider context of cloud native containerisation.
  *   You would potentially have to manage your own external DNS service if you want external components to be able to resolve your Kubernetes services. However, this should be as simple as running a simple DNS server on a box somewhere, and delegating your Kubernetes domain down to the Kubernetes DNS service. Again, there are likely to be many solutions to this problem, with lots of different information and guidance available. This will probably make it harder to get started, but should mean you're able to set up the solution you want.

Potentially, if you're able to let us know more about what sort of set up you're after, and what your requirements and available kit are, we may be able to help steer towards the right solution.

Hope this helps,
Adam


From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 18 May 2018 09:14
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,

Thanks for that.


  *   From the machine I'm running tests on, I can't ping bono.<ask-domain> or ellis.<aks-domain>. But I can open ellis.<aks-domain> in a browser (or with wget). I suspect this is because ping has no port number, so kubernetes doesn't know which ingress it should hand the packet to. `dig ellis.<aks-domain>` gives the ip address listed with `kubectl get ingress`
  *   Curl:
     *   `curl ellis.<ask-domain>:80` it returns a blank string immediately.
     *   `curl ellis.<ask-domain>:81` - times out
     *   `curl bono.<ask-domain>:5060` - times out
        *   Nothing in tcpdump
  *   When I run `kubectl get ingress` I only see port 80, even though I configured them to look on other ports. I suspect that ingresses only accept connections from port 80, and then on the internal side of the ingress they can work on any port. I don't know how to verify this.

Edit: It seems like this setup is temperamental. Yesterday and this morning I could open ellis in my browser and create an account. After tinkering around I've gone back to that setup, but it doesn't work. (timeout). Hmm.

My next step will be to install on a vanilla kubernetes environment. But I expect that will come with it's own challenges and even more limitations. (I won't have something exposing Kubernetes' DNS entries externally, and I won't be able to run tests on any real devices because this cluster will be in our corporate sandpit which is fenced off from everything).

I've tried to follow the alternative for DNS: https://github.com/Metaswitch/clearwater-docker#alternative-configuration-for-exposing-bono-and-ellis-services
I don't understand that part of the instructions.

  *   Where do I add the 'nodePort' line.

  *   Where did the number 30080 come from?

Regards,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 18 May 2018 3:05 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

I think we're making some solid progress, and I have a decent idea what was going on here. I think I have a better idea of what the AKS platform is doing, and should hopefully be able to explain why you were seeing the issues you were. Next steps on resolving your current issues below :)

So, first thing is the confusion over domains. I think there are two separate 'domains' here, as well as two separate DNS solutions doing slightly different things, and so the terms are a bit overloaded. As I see it there you have:

  *   AKS
     *   AKS domain, which is a globally routable web domain, '2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io', specific to your AKS account
     *   AKS DNS, which automatically creates entries based on Kubernetes ingress rules to provide access to specific pods at this domain, as you have set up for Ellis and Bono.
     *   This domain is what you originally passed in to the Project Clearwater pods, through the config map, and so is what our processes were attempting to connect to


  *   Kubernetes
     *   Kubernetes cluster domain, default.svc.cluster.local, which is a domain internal to the kubernetes cluster (the default value)
     *   Kubernetes DNS (probably kube-dns), where Kubernetes will automatically create records for all deployed pods and services, based on the configuration files you used in deploying.
     *   This domain is what is set in e.g. resolv.conf in your containers, and so is what you connect to if you run e.g. `nc cassandra 9160`

So, I think the way the issue you had getting the pods running manifested in such a hard to diagnose way is:

  *   You are able to contact the cassandra pods internally, e.g. using `nc cassandra 9160`, as this is going via the Kubernetes domain based DNS. i.e. cassandra.default.svc.cluster.local resolves to the internal IP of your cassandra pods.
  *   The clearwater processes were trying to use the AKS domain provided in `ZONE`, i.e. "cassandra.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io". As you noted, all the AKS DNS addresses resolve to the same IP. I believe this is the IP of the Kubernetes host, where the ingress rules you set up direct traffic for specific services through via the host IP.
  *   Because this resolved to an IP, we didn't see a DNS lookup failure, but it wasn't an IP at which we could connect to cassandra, because there are no ingress rules set up.

I think there's a simple solution here, based on the assumption that you want to be able to use the AKS domain as your subscriber home domain. We should simply be able to set that up as an additional home domain in the system.
To do this, we simply need to set up your configmap with the ZONE key set as `default.svc.cluster.local`, and the additional argument `--from-literal=ADDITIONAL_SHARED_CONFIG=additional_home_domains=2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io`

However, as you are currently provisioning subscribers through the Ellis web UI, and not integrating with an external HSS, I don't think we need to do this at the moment.
We can add it in if we see it become an issue down the line.


Next steps - Live tests
Good to see you've got the live test suite set up and running, though not successfully at the moment. In terms of the right command, I think the attempt you highlighted as taking a long time is the correct one. That is:
rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

You don't need to be running within the cluster for it to work; as long as your Bono and Ellis services are accessible from your test box, that will be fine.
While we're hitting errors, you can add ` TESTS="Basic call - mainline"` onto the command to simply run the first test only. This will limit the amount of error output you see on each run, and make it a bit easier to go about debugging.

The fact that the test command returned the error `Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060` suggests to me that there is potentially an issue in the ingress rules here. It is not returning a `getaddrinfo` error, so I believe the DNS is resolving to an IP, but that Bono is not reachable via that IP on port 5060.
I think your best next step here is to run some networking checks on this connection:

  *   From the box you are running the live tests on, make sure the bono.<aks domain> address resolves, the IP is reachable, and then try using `nc` to connect to port 5060.
  *   Run some tcpdumps in the bono pod to see if you are seeing any traffic reaching it on port 5060 when running the live tests

Hopefully this should help you track down at what point the traffic is getting stuck.

>From the Bono logs you've included, it looks like there's just no traffic reaching the process at all, and so I would suspect there's something unusual in the ingress rules setup. I don't know enough about the Azure http controller behaviour to be able to spot anything that looks particularly out of place at the moment though.
Equally, I am slightly worried that given that it is called 'http-application-routing' there may be some logic in place in the controller that will be interfering with non-http traffic. If we don't see any SIP traffic reaching the bono pod, we could potentially test if this is the case by using something like `curl` to send an http message to `bono.<aks domain>:5060`, and see if we see it reach the bono pod. If HTTP passes through, but SIP is blocked, I think we would need to dig some more into AKS configuration to work out what is going on there.

If you want to do testing with a sip client, I think you have most of the configuration correct, but I haven't used the Twinkle softclient. We have some docs on using sofclients, at http://clearwater.readthedocs.io/en/stable/Making_your_first_call.html, that you've probably already seen. Hopefully should be a decent guide. For the proxy, you'll want to use bono.<aks domain>, as that is what will resolve to the right IP address. However, if the  live tests are failing to connect to Bono, I think you will see similar issues down this path. We should focus on working out what is causing that problem first.

Hopefully once the connectivity issue to Bono is sorted, we should be fully up and running. Let us know how it goes, or if you have any questions on the distinction between the AKS domain and the Kubernetes/clearwater deployment domain.

Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 17 May 2018 09:32
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,


  *   `netstat -planut` shows that Cassandra is listening on 9160 from all ports
  *   `nc cassandra 9160` did not exit immediately. It just hangs without any output
  *   `ping cassandra` works, but `dig cassandra` shows no IP addresses

I tried changing my env-vars configmap from 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io to default.svc.cluster.local. This means that the pods do spin up successfully, and that error message goes away. What is env-vars supposed to be set to?
The docs say: "a ZONE key set to the domain of your Kubernetes cluster". So I set it to the domain of my cluster, and that seems to be the cause of all my problems. If I set it to something which isn't the domain of the cluster, the pods come up. So now I'm confused, is it supposed to be set to the domain of my cluster or not?
Is there some extra step I need to do to configure kubernetes to know that it's domain is 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io? I'm pretty sure it already knows, but how can I check?

Now all the pods are up. I can't make a call or run tests successfully. I wonder whether this is because my deployment has no idea what that it's domain is actually 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io.

In a browser, ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io takes me to the sign up page. I successfully created an account. I think this is good proof that the dns, ingresses and firewalls are set up properly. (albeit only for port 80, but I've checked the other ports. And by 'properly' I mean 'probably too loosely')

I've noticed that all my dns records (set up by Azure) point to the same ip address. Is this right, or not? (I still get confused by how kubernetes shares ip addresses around).
Here's my ellis ingress in kubernetes (required for Azure to set up dns for me)

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: ellis
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: ellis
            servicePort: 80
```

And for bono

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: bono
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: bono
            servicePort: 3478
        - backend:
            serviceName: bono
            servicePort: 5060
        - backend:
            serviceName: bono
            servicePort: 5062
```

I don't understand ingresses well. I'm not certain that this is right. But records appear in the dns server with names 'bono' and 'ellis' pointing to the ip address of something.
The example in the Azure docs<https://docs.microsoft.com/en-us/azure/aks/http-application-routing> has 'path: /`. I'm not sure whether I need that or not in my ingresses. With or without that line, the result is the same.

I've tried with ingresses for just bono and ellis, and also ingresses for every module (except etcd). The result is the same. (If I don't have ingresses for a module, it has no DNS record)



I have no idea what parameters to use when running the tests. The documentation doesn't say. Similarly I have no idea what to use when configuring my SIP client. What's the "domain"? What's the "proxy"?
Here's a bunch of permutations of test arguments which I ran, and the associated error messages:


  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
  *   rake test[default.svc.cluster.local] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   This one took a really long time before failing. So maybe it's the closest?
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'


When I try to register with a SIP client I get a bunch of 404 and 408 (timeout) error codes. Here's what I'm trying to use. I'm basing this off what worked for the AIO installation with this SIP client (Twinkle, one of the ones on Ubuntu)

  *   Your Name: Matthew
  *   User Name: 6505550375
     *   Not sip:6505550375
     *   Not 6505550375 at ...
  *   Domain: default.svc.cluster.local, or is it example.com? The Clearwater kubernetes readme says you should make a call like an AIO installation, which is example.com.
  *   Authentication name: 6505550375 at default.svc.cluster.local<mailto:6505550375 at default.svc.cluster.local>
  *   Password: the one generated by the web page. Not the one you use to sign into the web page.
  *   Proxy server: not sure. What should I use?
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   example.com
     *   default.svc.cluster.local

After running a bunch of failed tests and trying manually to register on a SIP client, here are the logs I see:


  *   /var/log/bono/bono_current.txt on the bono pod:
     *   17-05-2018 08:00:18.268 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 25
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:00:54.278 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 32
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:01:17.285 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 20
  *    Ellis: /var/log/ellis/ellis_20180517T070000Z.txt
     *
     *   17-05-2018 07:44:01.833 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.848 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/
     *   17-05-2018 07:44:01.857 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets/e74929ec-21ea-48ac-95b6-68115894313d
     *   17-05-2018 07:44:01.876 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:144: {"associated_implicit_registration_sets": ["e74929ec-21ea-48ac-95b6-68115894313d"]}
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles
     *   17-05-2018 07:44:01.913 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/public_ids/sip%3A6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.966 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/public/sip%3A6505550400%40default.svc.cluster.local/service_profile
     *   17-05-2018 07:44:01.976 UTC INFO homestead.py:255: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/filter_criteria
     *   17-05-2018 07:44:01.980 UTC INFO xdm.py:29: Sending HTTP PUT request to http://homer.default.svc.cluster.local:7888/org.etsi.ngn.simservs/users/sip%3A6505550400%40default.svc.cluster.local/simservs.xml
     *   17-05-2018 07:44:02.028 UTC INFO web.py:1447: 200 POST /accounts/live.tests at example.com/numbers/<mailto:/accounts/live.tests at example.com/numbers/> (0.0.0.0) 209.96ms

Here are my firewall rules. My understanding is that these are the rules for a firewall between pods and the outside world. I think that communications between pods don't get filtered by these rules, but I added all the inter-pod ports just in case.

Inbound and Outbound TCP: 22,2380,4000,80,443,5060,5062,3478,5058,5054,5052,9888,8888,8889,10888,7888,11211,7000,7253,11311,9160
Inbound and Outbound UDP: 161,162,5060,32768-65535,3478

Any ideas where to go from here? My main question is what command should I run for rake tests?
Should I try to run the rake tests from a test pod within the cluster?

Thanks,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV
M  0415 762 868  | E  Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Thursday, 17 May 2018 3:05 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

I've had a look into your issues deploying under kubernetes, and think I've narrowed down the search a good amount. Details below.
First though I'll take a run through the smaller questions you've raised below, as I think we can knock some of them off fairly quickly.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
[AJL] You shouldn't need it in the containers. In our VMs we found that some operations were fairly heavy on DNS queries, and so provided dnsmasq as a means of providing caching. The need for this has decreased quite a bit now, and as we move forward in the microservice space it makes less sense to have it around.

     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
This will likely be down to how the container networking works, but it won't cause any problems in running under kubernetes.

     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
[AJL] This is a benign error in this case, so we haven't dug down into it. It shouldn't impact your deployment at all.

  *   There are many other red error messages during build. Does that matter?
[AJL] No, these don't matter. I see a number of error messages during the build. A lot of these are dependencies of some of our packages raising error messages when being installed in this environment

  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
[AJL] The AIO has a number of different requirements that we don't have in the container environment, and vice versa, so the fact that there is a different set doesn't mean you have issues there
As a couple of examples for the differences you pulled out below:

  *   The auto config packages are used to enable specific setup in different environments. E.g. setting up required config files that other deployments do not rely on. In the docker case, we need to read environment variables and pass them through to the system configuration files
  *   Clearwater-etcd is our interface into an etcd cluster, and is used for sharing common configuration around a cluster. On the AIO this simply increases complexity, as we have no need for it, but in docker/kubernetes, we want the services to share configuration through an underlying etcd cluster.
I don't believe any of the differences you pulled out below would be leading to the issues you're seeing, but it's a nice analysis :)
The missing dns.json config file is also currently just a benign error in the containers, raised by some expectations we have when running in VMs.


So, on to the main issue:
In getting your deployment up and running, the key issue is definitely going to be the 'Failed to initialize the Cassandra store' one. This indicates the Homestead and Homestead-prov services have been unable to reach the cassandra cluster, and without that they won't be able to get up and running.
As you've already found, getting logs out and doing more detailed debugging in containers presents its own set of problems. To temporarily prevent kubernetes from automatically restarting pods, you can (as you may have already) remove the liveness checking sections from the generated deployment files. This isn't ideal, but it should allow you time to do some more detailed debugging. You may also want to reduce the number of replicas of the pods down to one for now, to make it easier to follow traffic. Should just be a case of changing the .depl files and redeploying.

The log indicates you're getting error code 3 back from attempting to setup the Cassandra store, which is a 'connection error', as opposed to what you would get if the dns was simply unresolvable (I have tested that on our internal setup, and the logs would be clearly stating it was unresolvable). This is definitely leading me to suspect network connectivity issues into the cassandra pods. The fact that you can ping between the two however suggests that it's more complex than simply being unable to send any traffic between them.

To test this, I tweaked the cassandra configuration (running just a single pod for simplicity) so that it was listening on a port that wasn't 9160, which is the port that both homestead and homestead prov attempt to connect to. With this setup in place, I saw both of these pods repeatedly failing with the same error code as yours.
Because of this I would suggest the following next steps in tracking down the problem:

  *   Double check that your networking configuration allows traffic through to the cassandra pods on port 9160. The homestead and homestead prov processes will require this.
  *   Connect to the Cassandra pods, and verify that they are indeed listening for traffic on port 9160
     *   `netstat -planut` is my default for this, but to each their own
     *   /var/log/cassandra/system.log may have more information if something strange is going on here
  *   Manually check the connectivity between the pods, where you could try
     *   Connect to e.g. a homestead pod, and try running `nc <cassandra hostname> 9160`. I'm seeing this instantly return if the port is not open/listened on.
     *   Install tcpdump on both sides, and see whether you see traffic arriving in the cassandra pods at all. This isn't a very container, but getting a dump from both cassandra and homestead sides, to see what point in the flow is failing would be a decent way to work out where the issue lies.

Sadly, I don't have access to an AKS setup right now, so can't investigate the ingress rules component at all. I suspect that there's something slightly unusual happening there that is leading to traffic being unable to reach port 9160.
Hopefully this should give you a better idea of what the source of the issue is here. If this doesn't show anything up, and you're seeing traffic from homestead arrive in the cassandra pods and going back out we may have to run some more testing

Cheers,
Adam


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 16 May 2018 04:35
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Richard,

Tldr: I've made some progress since I sent that email. I've found and fixed bugs to do with dnsmasq, but now I'm stuck.
The systemd logs for homestead still say: "Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."
I'm not even sure whether the dnsmasq bugs were related.

First, to answer your questions:

Yes this happens every time I deploy.

My configmap:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
Events:  <none>
```

I'm deploying on Azure's kubernetes service (AKS). I've enabled HTTP Application Routing<https://docs.microsoft.com/en-us/azure/aks/http-application-routing>, which means that Azure manages DNS for me. The only unusual step is that Azure requires that I add ingresses to do so. The DNS records point to each service (bono, ellis, sprout etc). I can ping "cassandra" from the homestead pod.

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:22:21Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.6", GitCommit:"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1", GitTreeState:"clean", BuildDate:"2018-03-21T15:13:31Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}

Getting the logs is a race against time, since kubernetes keeps terminating the pod while I'm in there.
After several attempts, I managed to get the homestead logs (same as homestead-prov logs)

```
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:123: Creating Cached Resolver using servers:
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:133:     10.0.0.10
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Error static_dns_cache.cpp:89: DNS config file /etc/clearwater/dns.json mis
sing
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status main.cpp:653: Using local impu store: astaire.2991678f-f9dd-4098-8fe
8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.129 UTC [7f6379c8e7c0] Status http_connection_pool.cpp:37: Connection pool will use calculated res
ponse timeout of 550ms
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:35: Configuring HTTP Connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:36:   Connection created for server sprout.2991678f
-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io:9888
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status main.cpp:1013: No HSS configured - using Homestead-prov
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:266: Configuring store connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:267:   Hostname:  cassandra.2991678f-f9dd-4098-8
fe8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:268:   Port:      9160
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:296: Configuring store worker pool
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:297:   Threads:   10
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:298:   Max Queue: 0
15-05-2018 07:05:40.131 UTC [7f6366ffd700] Status alarm.cpp:244: Reraising all alarms with a known state
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Error main.cpp:1056: Failed to initialize the Cassandra store with error co
de 3.
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Status main.cpp:1057: Homestead is shutting down
```

The full logs are just this chunk repeating over and over until everything shuts down.

I think the crucial error message is DNS config file /etc/clearwater/dns.json missing

In the all-in-one installation, that file is just:

{
  "hostnames": [
  ]
}

Is it ok to manually insert that using the Dockerfile? Or should there be other entries in there for a non-AIO installation?

dns.json is created by the dnsmasq service.

During the container build, there is a red error message saying "dnsmasq: setting capabilities failed: Operation not permitted"
(There are a lot of red error messages during build. Is that an issue?)

I noticed that dnsmasq was not running in homestead docker, yet it is running in my working all-in-one installation. (There are also a bunch of other services which are halted. Is that an issue?)

I tried starting dnsmasq manually, it wouldn't start. I looked online and the solution is to add user=root to the file /etc/dnsmasq.conf.

I tried modifying the Dockerfile to manually append that string to the file.  (I'm assuming that the order of lines in /etc/dnsmasq.conf does not matter)

Then dnsmasq still doesn't come up. I get an error about how the port it wants is already taken.

So then I tried modifying the Dockerfile to paste in dns.conf from the AIO installation. (The one with an empty list of hosts)

Now the missing file error is gone, but I still get the other error:

"Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."

How can I debug this?

All this time I thought that error was caused by dns issues. Is it because dnsmasq still won't start, so dns.json is missing hosts? Or is that unrelated?
"ping cassandra" works from the homestead pod.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
  *   There are many other red error messages during build. Does that matter?
  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
     *   Services running in AIO:
        *   [ ? ]  clearwater-auto-config-generic
        *    [ + ]  clearwater-cluster-manager
        *    [ + ]  clearwater-config-manager
        *    [ + ]  clearwater-diags-monitor
        *    [ - ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ ? ]  clearwater-memcached
        *    [ + ]  clearwater-queue-manager
     *   Services running (or not) in docker homestead:
        *   [ ? ]  clearwater-auto-config-docker
        *    [ + ]  clearwater-cluster-manager
        *    [ - ]  clearwater-config-manager
        *    [ - ]  clearwater-diags-monitor
        *    [ + ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ - ]  clearwater-queue-manager


Thanks,

Matthew Davis
Telstra | CTO | Cloud SDN NFV

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Wednesday, 16 May 2018 3:24 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Matthew,

Sorry to hear you are having problems deploying Clearwater on Docker.

I think that the socket factory error is actually fairly benign.

Does this happen every time you create your deployment?

Can you provide the config map that you are providing when deploying on Kubernetes?

Can you connect into one of the pods that is failing to run homestead-prov and grab the logs. The relevant ones will be under /var/log/homestead-prov inside the container.

We cannot reproduce your issue on our kubernetes setup - can you provide details of your kubernetes setup - what version of k8s and what networking configuration you are using?

>From the diagnostics we have at the moment, the most likely scenario is that the Homestead and Homestead Prov containers are unable to contact Cassandra - if that's the case they will continually restart to attempt to resolve the problem.



Richard


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 14 May 2018 07:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi everyone,
I've fixed the issue I had with homestead-prov and submitted the patch as this pull request: https://github.com/Metaswitch/clearwater-docker/pull/89

However now I just run into another issue, so homestead-prov is giving me the same error as homestead. The error message is not verbose. Does anyone know how to make it more verbose? I have no idea what's happening.


```
2018-05-14 05:47:41,369 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:41,370 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-sig entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-mgmt entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,477 INFO exited: snmpd (exit status 0; expected)
2018-05-14 05:47:42,478 CRIT reaped unknown pid 61)
2018-05-14 05:47:44,572 CRIT reaped unknown pid 65)
2018-05-14 05:47:44,576 CRIT reaped unknown pid 66)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 235)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 236)
2018-05-14 05:47:45,902 CRIT reaped unknown pid 257)
2018-05-14 05:47:48,153 CRIT reaped unknown pid 294)
2018-05-14 05:47:48,226 INFO spawned: 'homestead-prov' with pid 297
2018-05-14 05:47:48,237 INFO spawned: 'nginx' with pid 298
2018-05-14 05:47:48,747 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:49,750 INFO spawned: 'homestead-prov' with pid 324
2018-05-14 05:47:49,750 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:50,089 INFO exited: clearwater-infrastructure (exit status 0; expected)
2018-05-14 05:47:50,335 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:52,339 INFO spawned: 'homestead-prov' with pid 335
2018-05-14 05:47:53,324 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:56,330 INFO spawned: 'homestead-prov' with pid 346
2018-05-14 05:47:56,902 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:57,904 INFO gave up: homestead-prov entered FATAL state, too many start retries too quickly
```

Regards,
Matt Davis
Telstra | Graduate Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180522/aaf9540c/attachment.html>

From William.Yates at metaswitch.com  Tue May 22 04:41:01 2018
From: William.Yates at metaswitch.com (William Yates)
Date: Tue, 22 May 2018 08:41:01 +0000
Subject: [Project Clearwater] 
 =?utf-8?q?Release_note_for_Sprint_Zam=C3=AE?= =?utf-8?q?n?=
In-Reply-To: <BLUPR0201MB1490EDCA3422D3B75EEE4D0E95880@BLUPR0201MB1490.namprd02.prod.outlook.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
	<BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D9674D4@ex-mbx-prd02.gci.com>,
	<BLUPR0201MB149036A21130C85A1A66829E95B40@BLUPR0201MB1490.namprd02.prod.outlook.com>,
	<9ED8EE39-46CC-4466-8A61-168201C9DDBC@gci.com>
	<1B813795-9351-41F9-9831-B07F54D3AC30@vmware.com>,
	<A7E1158244C52C428F0B61BDC9CF760C016D98E595@ex-mbx-prd02.gci.com>
	<54A349FD-766F-4DFE-B775-56C65C396E83@vmware.com>
	<BN6PR05MB3074301874BF4C5590B190A2B9890@BN6PR05MB3074.namprd05.prod.outlook.com>
	<BLUPR0201MB1490EDCA3422D3B75EEE4D0E95880@BLUPR0201MB1490.namprd02.prod.outlook.com>
Message-ID: <BY2PR0201MB1496BC904FBB20D650DD95EC95940@BY2PR0201MB1496.namprd02.prod.outlook.com>

Frank,

Apologies for the slow turnaround on this question.

> What is the actual intended disk configuration for the AIO image based on vSphere? Which virtual storage controller is the target and how many vdisks should the resulting VM have, as currently the OVA through vCenter as well as through ESXi deploys with a single disk.
It should be a single disk on the IDE 0 controller.
You can see how the device creation is done in virtualbox in here:  https://github.com/Metaswitch/clearwater-vm-images/blob/master/ubuntu-ovf/make_ovf.sh

Cheers,
Will

From: William Yates
Sent: 24 April 2018 10:40
To: clearwater at lists.projectclearwater.org
Subject: RE: [Project Clearwater] Release note for Sprint Zam?n

Thanks Frank, we'll get back on the SATA0 issue.

Roger, would you be happy to raise a github issue for this, and we'll consider fixing it in a future stable release ?

Also, the manual install offers another workaround, described here: https://clearwater.readthedocs.io/en/stable/All_in_one_Images.html#manual-build

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Frank Escaros-Buechsel
Sent: 24 April 2018 00:09
To: Roger Case <case at gci.com>
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Hi Roger,
After reading the documentation I do not think that a vCenter deploy is extensively tested as only ESXi deploy is mentioned in the clearwater wiki.

I get exactly the same error message as you do when I try to deploy through vCenter, yet I am able to successfully deploy the OVA directly to ESXi.

@Clearwater team:
I modified the OVF file after extraction and removed the offending fields from Roger?s screenshot which also allows me then to deploy successfully through vCenter.
I have copied the new contents of the OVF file to https://pastebin.com/raw/NStkhajn for your review.
When trying to boot the VM it is complaining about not being able to connect to SATA0 controller.
What is the actual intended disk configuration for the AIO image based on vSphere? Which virtual storage controller is the target and how many vdisks should the resulting VM have, as currently the OVA through vCenter as well as through ESXi deploys with a single disk.

Ellis and Bono do seem to come up successfully despite the error I am seeing, which makes me think it?s purely cosmetic but with knowing the correct disk controller type for vSphere which is intended by you I can make further modifications to the OVF to avoid the cosmetic error as well:

[cw-aio]ubuntu at cw-aio:~$ netstat -ano | grep 5060
tcp        0      0 10.27.32.111:5060       0.0.0.0:*               LISTEN      off (0.00/0/0)
udp        0      0 10.27.32.111:5060       0.0.0.0:*                           off (0.00/0/0)

Kind Regards,
Frank Escaros-Buechsel
Consulting Architect NFV
fescarosbuechs at vmware.com<mailto:fescarosbuechs at vmware.com>
Parnell  House, Barrack Square, Ballincollig, Co. Cork, P31 PF68, IRL
Twitter: @fbuechsel

Advanced notice of absence:


[VMwareLogo]

From: Frank Escaros-Buechsel
Sent: Friday 20 April 2018 22:03
To: Roger Case <case at gci.com>
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Hi Roger,
Looks like a malformed ofv file for the vsphere build you are using, I can try to reproduce in my lab over the weekend.
Frank Escaros-Buechsel
Consulting Architect NFV
Parnell House, Barrack Square, Ballincollig, Co. Cork
P31 PF68, IRL
Twitter: @fbuechsel

On 20 Apr 2018, at 22:58, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:
Hi Frank,


  1.  Where would I get the checksum to compare with the one I derived from the downloaded file?
  2.  Unzip OK.
  3.  Checksums of .ovf and .vmdk match those in the .mf file
  4.  Same error when selecting all three unzipped files to deploy ovf.
  5.  Removing the .mf file and attempting to deploy the template using just the .ovf and .vmdk results in different errors:


<image001.png>

Thanks,

Roger

From: Frank Escaros-Buechsel [mailto:fescarosbuechs at vmware.com]
Sent: Friday, April 20, 2018 9:12 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Roger Case
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
1)Verify the checksum to ensure you don?t have a corrupt download
2)unzip the ova into a folder, you will get an ovf, vmdk and mf file
3)run sha1 sum on all files separately, compare output against the values in mf file, if one deviates update the value in mf to correct one
4)do not zip again, deploy through web client and select ovf and all vmdk files

This should let you deploy.

Another way would be to follow step 1 and 2 and simply delete the mf file, as this will skip the checksum check (not recommended for environments which need to proof authenticity of uploaded files though)

Hope that helps.

Frank


Sent from VMware Boxer
On 20 April 2018 at 19:27:24 GMT+3, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:
Hi Will,

No, also 6.0.
Previous AIO, however.
Other Metaswitch .OVA still work....

Sent from my GCiPhone

On Apr 20, 2018, at 01:49, William Yates <William.Yates at metaswitch.com<mailto:William.Yates at metaswitch.com>> wrote:
[External Email]

Hi Roger,



When this worked before, was it on a previous version of vSphere?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 19 April 2018 20:15
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Hi Will,



vSphere 6.0, vCenter Server 6.5.



Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of William Yates
Sent: Thursday, April 19, 2018 2:58 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi Roger,



Please could you tell me what version of vSphere you are using?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 18 April 2018 21:59
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?



Am I missing something here?  This worked before.  Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=2qwlj9Q3_bQcSmxUEMuhYsYgR9cLAYq8Yow_4Awb6sg&e=>)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.



This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.



We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.





This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.



To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__docs.projectclearwater.org_en_stable_Upgrading-5Fa-5FClearwater-5Fdeployment.html&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=qcTPSTk5rbrrb9tKuc7vPepjpenEiPPEJ2lTtsmp9Es&e=>.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova<https://urldefense.proofpoint.com/v2/url?u=http-3A__vm-2Dimages.cw-2Dngv.com_cw-2Daio.ova&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=PUMku_aSKgNoiXkqZ8ant31BoZ-iIM3wJCW24TKelsU&e=>) has been updated for this release.





Cheers,



Richard


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.projectclearwater.org_mailman_listinfo_clearwater-5Flists.projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=Z5v_f1O7HlUlYiseOcH3vO5bpWsuXKfrYT-IaaL4saE&e=>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180522/de5b2879/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.gif
Type: image/gif
Size: 4572 bytes
Desc: image001.gif
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180522/de5b2879/attachment.gif>

From William.Yates at metaswitch.com  Tue May 22 05:51:37 2018
From: William.Yates at metaswitch.com (William Yates)
Date: Tue, 22 May 2018 09:51:37 +0000
Subject: [Project Clearwater] 
 =?utf-8?q?Release_note_for_Sprint_Zam=C3=AE?= =?utf-8?q?n?=
In-Reply-To: <BY2PR0201MB1496BC904FBB20D650DD95EC95940@BY2PR0201MB1496.namprd02.prod.outlook.com>
References: <BN6PR02MB3362E310B291CB05FA381AA7F0BE0@BN6PR02MB3362.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D966953@ex-mbx-prd02.gci.com>
	<BLUPR0201MB1490E818677AF0CA5DDC3C6995B50@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<A7E1158244C52C428F0B61BDC9CF760C016D9674D4@ex-mbx-prd02.gci.com>,
	<BLUPR0201MB149036A21130C85A1A66829E95B40@BLUPR0201MB1490.namprd02.prod.outlook.com>,
	<9ED8EE39-46CC-4466-8A61-168201C9DDBC@gci.com>
	<1B813795-9351-41F9-9831-B07F54D3AC30@vmware.com>,
	<A7E1158244C52C428F0B61BDC9CF760C016D98E595@ex-mbx-prd02.gci.com>
	<54A349FD-766F-4DFE-B775-56C65C396E83@vmware.com>
	<BN6PR05MB3074301874BF4C5590B190A2B9890@BN6PR05MB3074.namprd05.prod.outlook.com>
	<BLUPR0201MB1490EDCA3422D3B75EEE4D0E95880@BLUPR0201MB1490.namprd02.prod.outlook.com>
	<BY2PR0201MB1496BC904FBB20D650DD95EC95940@BY2PR0201MB1496.namprd02.prod.outlook.com>
Message-ID: <BY2PR0201MB1496B7A1DA8C660CA7F34E2895940@BY2PR0201MB1496.namprd02.prod.outlook.com>

I raised an issue on github to keep track of this: https://github.com/Metaswitch/project-clearwater-issues/issues/31

Cheers,
Will


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of William Yates
Sent: 22 May 2018 09:41
To: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Frank,

Apologies for the slow turnaround on this question.

> What is the actual intended disk configuration for the AIO image based on vSphere? Which virtual storage controller is the target and how many vdisks should the resulting VM have, as currently the OVA through vCenter as well as through ESXi deploys with a single disk.
It should be a single disk on the IDE 0 controller.
You can see how the device creation is done in virtualbox in here:  https://github.com/Metaswitch/clearwater-vm-images/blob/master/ubuntu-ovf/make_ovf.sh

Cheers,
Will

From: William Yates
Sent: 24 April 2018 10:40
To: clearwater at lists.projectclearwater.org
Subject: RE: [Project Clearwater] Release note for Sprint Zam?n

Thanks Frank, we'll get back on the SATA0 issue.

Roger, would you be happy to raise a github issue for this, and we'll consider fixing it in a future stable release ?

Also, the manual install offers another workaround, described here: https://clearwater.readthedocs.io/en/stable/All_in_one_Images.html#manual-build

Cheers,
Will

From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Frank Escaros-Buechsel
Sent: 24 April 2018 00:09
To: Roger Case <case at gci.com>
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Hi Roger,
After reading the documentation I do not think that a vCenter deploy is extensively tested as only ESXi deploy is mentioned in the clearwater wiki.

I get exactly the same error message as you do when I try to deploy through vCenter, yet I am able to successfully deploy the OVA directly to ESXi.

@Clearwater team:
I modified the OVF file after extraction and removed the offending fields from Roger?s screenshot which also allows me then to deploy successfully through vCenter.
I have copied the new contents of the OVF file to https://pastebin.com/raw/NStkhajn for your review.
When trying to boot the VM it is complaining about not being able to connect to SATA0 controller.
What is the actual intended disk configuration for the AIO image based on vSphere? Which virtual storage controller is the target and how many vdisks should the resulting VM have, as currently the OVA through vCenter as well as through ESXi deploys with a single disk.

Ellis and Bono do seem to come up successfully despite the error I am seeing, which makes me think it?s purely cosmetic but with knowing the correct disk controller type for vSphere which is intended by you I can make further modifications to the OVF to avoid the cosmetic error as well:

[cw-aio]ubuntu at cw-aio:~$ netstat -ano | grep 5060
tcp        0      0 10.27.32.111:5060       0.0.0.0:*               LISTEN      off (0.00/0/0)
udp        0      0 10.27.32.111:5060       0.0.0.0:*                           off (0.00/0/0)

Kind Regards,
Frank Escaros-Buechsel
Consulting Architect NFV
fescarosbuechs at vmware.com<mailto:fescarosbuechs at vmware.com>
Parnell  House, Barrack Square, Ballincollig, Co. Cork, P31 PF68, IRL
Twitter: @fbuechsel

Advanced notice of absence:


[VMwareLogo]

From: Frank Escaros-Buechsel
Sent: Friday 20 April 2018 22:03
To: Roger Case <case at gci.com>
Cc: clearwater at lists.projectclearwater.org
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

Hi Roger,
Looks like a malformed ofv file for the vsphere build you are using, I can try to reproduce in my lab over the weekend.
Frank Escaros-Buechsel
Consulting Architect NFV
Parnell House, Barrack Square, Ballincollig, Co. Cork
P31 PF68, IRL
Twitter: @fbuechsel

On 20 Apr 2018, at 22:58, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:
Hi Frank,


  1.  Where would I get the checksum to compare with the one I derived from the downloaded file?
  2.  Unzip OK.
  3.  Checksums of .ovf and .vmdk match those in the .mf file
  4.  Same error when selecting all three unzipped files to deploy ovf.
  5.  Removing the .mf file and attempting to deploy the template using just the .ovf and .vmdk results in different errors:


<image001.png>

Thanks,

Roger

From: Frank Escaros-Buechsel [mailto:fescarosbuechs at vmware.com]
Sent: Friday, April 20, 2018 9:12 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Roger Case
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n

[External Email]
1)Verify the checksum to ensure you don?t have a corrupt download
2)unzip the ova into a folder, you will get an ovf, vmdk and mf file
3)run sha1 sum on all files separately, compare output against the values in mf file, if one deviates update the value in mf to correct one
4)do not zip again, deploy through web client and select ovf and all vmdk files

This should let you deploy.

Another way would be to follow step 1 and 2 and simply delete the mf file, as this will skip the checksum check (not recommended for environments which need to proof authenticity of uploaded files though)

Hope that helps.

Frank


Sent from VMware Boxer
On 20 April 2018 at 19:27:24 GMT+3, Roger Case <case at gci.com<mailto:case at gci.com>> wrote:
Hi Will,

No, also 6.0.
Previous AIO, however.
Other Metaswitch .OVA still work....

Sent from my GCiPhone

On Apr 20, 2018, at 01:49, William Yates <William.Yates at metaswitch.com<mailto:William.Yates at metaswitch.com>> wrote:
[External Email]

Hi Roger,



When this worked before, was it on a previous version of vSphere?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 19 April 2018 20:15
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Hi Will,



vSphere 6.0, vCenter Server 6.5.



Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of William Yates
Sent: Thursday, April 19, 2018 2:58 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi Roger,



Please could you tell me what version of vSphere you are using?



Cheers,

Will



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Roger Case
Sent: 18 April 2018 21:59
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Release note for Sprint Zam?n



Attempt to deploy vSphere OVF Template using downloaded cw-aio.ova from link below results in ?The provided manifest file is invalid: Invalid OVF checksum algorithm: SHA1.?



Am I missing something here?  This worked before.  Thanks!



Roger



From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Richard Whitehouse (projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=2qwlj9Q3_bQcSmxUEMuhYsYgR9cLAYq8Yow_4Awb6sg&e=>)
Sent: Tuesday, April 10, 2018 7:32 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: [Project Clearwater] Release note for Sprint Zam?n



[External Email]

Hi all,

The release for Project Clearwater sprint ?Zam?n? has been cut. The code for this release is tagged as release-130 in GitHub.



This is a big release for us! Over the last year, we?ve put a huge amount of effort into increasing Project Clearwater?s quality, as we know that reliability is extremely important to most users of IMS. We?re proud of where we?ve got to, and we now recommend using this release whenever possible. To mark that, we are declaring Zam?n our first ?stable? release.



We think that Project Clearwater is now a well featured IMS core, built on a great architecture. It?s time to move our core development to a new phase, focussed more on stability, and less on new features. That?s going to mean that we won?t expect to tag new releases as frequently. It?s also going to free us up to spend more time blogging about Project Clearwater and IMS: watch this space.





This release includes a number of bug fixes which improves Clearwater?s reliability at performance and load.



To upgrade to this release, follow the instructions at http://docs.projectclearwater.org/en/stable/Upgrading_a_Clearwater_deployment.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__docs.projectclearwater.org_en_stable_Upgrading-5Fa-5FClearwater-5Fdeployment.html&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=qcTPSTk5rbrrb9tKuc7vPepjpenEiPPEJ2lTtsmp9Es&e=>.  If you are deploying an all-in-one node, the standard image (http://vm-images.cw-ngv.com/cw-aio.ova<https://urldefense.proofpoint.com/v2/url?u=http-3A__vm-2Dimages.cw-2Dngv.com_cw-2Daio.ova&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=PUMku_aSKgNoiXkqZ8ant31BoZ-iIM3wJCW24TKelsU&e=>) has been updated for this release.





Cheers,



Richard


_______________________________________________
Clearwater mailing list
Clearwater at lists.projectclearwater.org<mailto:Clearwater at lists.projectclearwater.org>
http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.projectclearwater.org<https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.projectclearwater.org_mailman_listinfo_clearwater-5Flists.projectclearwater.org&d=DwMF-g&c=uilaK90D4TOVoH58JNXRgQ&r=-TWQ2venLlorDb_G_vIJLhFd9harQeN_bur4qh28ZZY&m=nZ74kNJCYThrHTDSA3J_sWXkn3mizY8ISYbSJBKg24Y&s=Z5v_f1O7HlUlYiseOcH3vO5bpWsuXKfrYT-IaaL4saE&e=>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180522/fd634413/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.gif
Type: image/gif
Size: 4572 bytes
Desc: image001.gif
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180522/fd634413/attachment.gif>

From Adam.Lindley at metaswitch.com  Tue May 22 11:20:41 2018
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Tue, 22 May 2018 15:20:41 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <ME2PR01MB288450EF460A39FEA9E1D724C2940@ME2PR01MB2884.ausprd01.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971545314ED91FB7F233AC1E2900@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB288450EF460A39FEA9E1D724C2940@ME2PR01MB2884.ausprd01.prod.outlook.com>
Message-ID: <BY1PR0201MB09689BB9A1645396B16AE810E2940@BY1PR0201MB0968.namprd02.prod.outlook.com>

Hi Matthew,

Sounds like interesting progress on the Azure side. I'd be really interested in the details of how you get it up and running if this approach works, and we'd be really happy to be able to integrate some steps into our docs to expand the platforms we can easily get running on. Do update with any progress you make there :)

As for your vanilla Kubernetes setup, as far as I can see you're having issues connecting to Bono because the service/pod is not exposed; you're trying to connect to port 5060 on your Kubernetes host, without any service set up on the port. This is why your tests and sip client are failing, and nothing is hitting the Bono process.
Can you give some more information on the network setup of your Kubernetes rig? What network plugin/CNI have you configured it with? And are the pod networks purely internal to the cluster? Or have you set them up as externally routable?

As documented in the clearwater-docker README<https://github.com/Metaswitch/clearwater-docker/blob/master/README.md#alternative-configuration-for-exposing-bono-and-ellis-services>, exposing Bono presents a few challenges. In our internal setups, we have in the past either ended up using GRE tunnels to route traffic directly into pod networks (not simple, and a fair pain to set up), or have deployed Kubernetes using fairly basic network infrastructure, and making the pod networks directly routable using the kubenet plugin and some simple forwarding rules. This means that we are able to route directly into the bono pod.
In your case, you may well have deployed with an overlay network, and an internal only pod network, which makes things a bit harder.

The right solution here is to find a way to route traffic through on port 5060 direct into your bono pod. As in our README, GKE provides a load balancer that is able to do this, and internally we've used the above methods. I'm not sure what the best approach available on current Kubernetes is.

However, as a stop gap to get you a bit further forward for now, I have played around and have a suggestion for something that may work for now. With a few manual tweaks we should be able to set bono up to listen as the P-CSCF on a different port, expose that as a nodeport, and have bono view itself as recording the same IP as the Kubernetes host.
To do this, try the following steps:

  *   Set up the bono service with a NodePort (similar to ellis). This will be something like port 30060.
  *   Modify the bono-depl file:
     *   add `name: PUBLIC_IP` and `value: <k8s host IP>` to the `env:` section.
     *   Remove the liveness and readiness probes relating to port 5060. We will be changing the port bono listens on manually, and without this change the containers will be killed and restarted under our feet
  *   Deploy all the pods and services
  *   Connect to the bono pod, and modify `/etc/init.d/bono`
     *   Change the value of the line `--pcscf=5060,5058` to `--pcscf=<nodePort>,5058` , with the same nodeport you set in bono-svc
  *   Restart the bono service using `service bono restart` from within the container

This should leave your bono pod able to receive traffic through a nodePort connection on the Kubernetes host IP. You should be able to run the live tests with the command below, and should be able to get a GUI SIP client registering as well.

rake test[default.svc.cw-k8s.test] PROXY=10.3.1.76 PROXY_PORT=<bono nodeport> SIGNUP_CODE='secret' ELLIS=10.3.1.76:30080

If you get more connection errors, as in your below error output, try tcpdumping in different locations along the call path to find out where traffic is getting dropped. The test failure output should give a reasonable clue as to what is going wrong.

Keep us updated. Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 22 May 2018 09:15
To: Adam Lindley <Adam.Lindley at metaswitch.com>; clearwater at lists.projectclearwater.org
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,

I heard back from a contact we have in Microsoft. They seemed to suggest that selecting 'advanced networking' instead of 'basic' (which is what I was using) might make each pod directly addressable, but from within an azure vnet (so I'd have to find a way to expose that to the public internet for my SIP clients). I'm halfway through trying that at the moment. I'll update that thread once I run some rake tests.

As far as a vanilla kubernetes installation goes, I tried that today and couldn't get it working. Can you help me with that?
I'm trying to just get the simplest setup possible. For now my goal is just to get something working somewhere. (Incidentally this is why I initially tried Azure Kubernetes Service with Application Routing)

So in my vanilla installation (i.e. kubernetes on openstack) I just used 1 bono pod. (Once I get this simple install working, then I'll try more complicated setups)

How am I supposed to run the rake tests?

My cluster is at 10.3.1.76. I changed ellis-svc to a nodePort type.

I'm trying:

rake test[default.svc.cluster.local] PROXY="10.3.1.76" SIGNUP_CODE=secret ELLIS=10.3.1.76:30080 TESTS="Basic call - mainline"

The error message is:

Basic Call - Mainline (TCP) - Failed
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2) for "10.3.1.76" port 5060
     - /home/ubuntu/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     - /home/ubuntu/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     - /home/ubuntu/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     - /home/ubuntu/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
     - /home/ubuntu/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new_source'

The logs in bono in /var/log/bono/bono_current.txt did not show any additional lines during the test. The logs are currently:

22-05-2018 08:11:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1012.3 alarm
22-05-2018 08:11:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1013.3 alarm
22-05-2018 08:12:02.410 UTC [7f59f0ff1700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 20
22-05-2018 08:12:04.667 UTC [7f5a15023700] Status alarm.cpp:244: Reraising all alarms with a known state
22-05-2018 08:12:04.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1012.3 alarm
22-05-2018 08:12:04.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1013.3 alarm
22-05-2018 08:12:26.414 UTC [7f59f0ff1700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 5
22-05-2018 08:12:34.667 UTC [7f5a15023700] Status alarm.cpp:244: Reraising all alarms with a known state
22-05-2018 08:12:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1012.3 alarm
22-05-2018 08:12:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1013.3 alarm

When I try manually in a GUI SIP client I get a 503 error.

I haven't done any DNS stuff for this deployment. As I said, I'm trying to keep it simple, just so that I have something working. My understanding is that I only need the DNS server if I want SIP clients to access the server via a domain name instead of an IP address. Is that correct?

When I run `nc 10.3.1.76 30080 -v` it connects successfully.
When I run `nc 10.3.1.76 5060 -v`  fails (connection refused), both from my laptop and from one of the pods which isn't bono. Since I get 'connection refused' when running that from within the cluster, that means it's not a firewall issue. (The firewall surrounds the whole cluster, and doesn't get between pods).

Thanks,
Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Saturday, 19 May 2018 3:46 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

No worries. Glad we're making headway on this. It will be useful to work out any of the kinks in using platforms like AKS so that we can continue to deploy easily in as many situations as possible.

>From the info you've sent below, it looks like there's an issue in the AKS configuration/ingress rules meaning there isn't anything in place for Bono. Without a rule to pass traffic through port 5060 to Bono, we're going to have a tough time getting calls running.
>From my experience with them, I think ingress rules should work on any port combinations, not just port 80. I would however suggest, if possible, clearing out the existing rules and then re-configuring the Ellis and Bono ingress rules from scratch, so that we can eliminate anything unusual that might have got in place while running through the other issues.
If you do this, and things still seem out of place, send over some more details on what the setup you're following is, and some diagnostics/info on what it all looks like.

To answer your direct questions:

  *   The port number 30080 came from the fact that some platforms have a restricted set of external ports available to expose. I believe we first saw this using GKE, but essentially, some platforms reserve low values in the port ranges for platform system processes, or exposing their own web UIs, and so we can't always guarantee that port 80 will be usable as the external facing port. Mapping port 30080 on the host through to 80 on the Ellis service however has worked well on other platforms, and just means you need to tweak the web address you access it over.
However, this may clash with some of the AKS behaviour, and if you are able to map directly through on port 80, that will work just fine. I suggest you lean more on AKS documentation on this side of things, as the Azure provided DNS etc may have special requirements or behaviours.
  *   The nodeport yaml configuration should look something like the below, taking Ellis as an example, with the 'nodePort' entry simply in addition to the existing 'name' and 'port' entries. i.e.:
apiVersion: v1
kind: Service
metadata:
  name: ellis
spec:
  type: NodePort
  ports:
  - name: "http"
    port: 80
    NodePort: 30080
  selector:
    service: ellis


  *   Bono may prove more challenging in the AKS environment, as it currently requires having a single static IP address. As the docs have it, we found the solution in GKE to be using a LoadBalancer type exposure, with static IP address. You will hopefully be able to do something very similar in AKS, where the IP of the Kubernetes host, already exposed by the AKS DNS records, is probably the right choice to start working with. I would suggest searching up a bit in the AKS documentation to see if there's anything in there about exposing services with a static IP, or potentially there are other support channels available for the Azure side of things.

I don't think we've quite reached an impasse on getting running with AKS here, but if you're keen to set up your own Kubernetes deployment that is definitely a possibility. As you've noted, there will be some limitations, particularly centred around networking. However, this is something we have definitely done before, so once those limitations are dealt with it should work as expected.
A few potentially helpful guiding points:

  *   If you are able to set up your Kubernetes cluster using a service network that is routable on the wider system network, potentially using simpler CNI network plugins, you can build a solution where your pod IPs are directly accessible, and so the need for any special NodePort or LoadBalancer options can be removed. This is very much a tech interest set up though, and not necessarily how we see things happening in the wider context of cloud native containerisation.
  *   You would potentially have to manage your own external DNS service if you want external components to be able to resolve your Kubernetes services. However, this should be as simple as running a simple DNS server on a box somewhere, and delegating your Kubernetes domain down to the Kubernetes DNS service. Again, there are likely to be many solutions to this problem, with lots of different information and guidance available. This will probably make it harder to get started, but should mean you're able to set up the solution you want.

Potentially, if you're able to let us know more about what sort of set up you're after, and what your requirements and available kit are, we may be able to help steer towards the right solution.

Hope this helps,
Adam


From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 18 May 2018 09:14
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,

Thanks for that.


  *   From the machine I'm running tests on, I can't ping bono.<ask-domain> or ellis.<aks-domain>. But I can open ellis.<aks-domain> in a browser (or with wget). I suspect this is because ping has no port number, so kubernetes doesn't know which ingress it should hand the packet to. `dig ellis.<aks-domain>` gives the ip address listed with `kubectl get ingress`
  *   Curl:
     *   `curl ellis.<ask-domain>:80` it returns a blank string immediately.
     *   `curl ellis.<ask-domain>:81` - times out
     *   `curl bono.<ask-domain>:5060` - times out
        *   Nothing in tcpdump
  *   When I run `kubectl get ingress` I only see port 80, even though I configured them to look on other ports. I suspect that ingresses only accept connections from port 80, and then on the internal side of the ingress they can work on any port. I don't know how to verify this.

Edit: It seems like this setup is temperamental. Yesterday and this morning I could open ellis in my browser and create an account. After tinkering around I've gone back to that setup, but it doesn't work. (timeout). Hmm.

My next step will be to install on a vanilla kubernetes environment. But I expect that will come with it's own challenges and even more limitations. (I won't have something exposing Kubernetes' DNS entries externally, and I won't be able to run tests on any real devices because this cluster will be in our corporate sandpit which is fenced off from everything).

I've tried to follow the alternative for DNS: https://github.com/Metaswitch/clearwater-docker#alternative-configuration-for-exposing-bono-and-ellis-services
I don't understand that part of the instructions.

  *   Where do I add the 'nodePort' line.

  *   Where did the number 30080 come from?

Regards,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 18 May 2018 3:05 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

I think we're making some solid progress, and I have a decent idea what was going on here. I think I have a better idea of what the AKS platform is doing, and should hopefully be able to explain why you were seeing the issues you were. Next steps on resolving your current issues below :)

So, first thing is the confusion over domains. I think there are two separate 'domains' here, as well as two separate DNS solutions doing slightly different things, and so the terms are a bit overloaded. As I see it there you have:

  *   AKS
     *   AKS domain, which is a globally routable web domain, '2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io', specific to your AKS account
     *   AKS DNS, which automatically creates entries based on Kubernetes ingress rules to provide access to specific pods at this domain, as you have set up for Ellis and Bono.
     *   This domain is what you originally passed in to the Project Clearwater pods, through the config map, and so is what our processes were attempting to connect to


  *   Kubernetes
     *   Kubernetes cluster domain, default.svc.cluster.local, which is a domain internal to the kubernetes cluster (the default value)
     *   Kubernetes DNS (probably kube-dns), where Kubernetes will automatically create records for all deployed pods and services, based on the configuration files you used in deploying.
     *   This domain is what is set in e.g. resolv.conf in your containers, and so is what you connect to if you run e.g. `nc cassandra 9160`

So, I think the way the issue you had getting the pods running manifested in such a hard to diagnose way is:

  *   You are able to contact the cassandra pods internally, e.g. using `nc cassandra 9160`, as this is going via the Kubernetes domain based DNS. i.e. cassandra.default.svc.cluster.local resolves to the internal IP of your cassandra pods.
  *   The clearwater processes were trying to use the AKS domain provided in `ZONE`, i.e. "cassandra.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io". As you noted, all the AKS DNS addresses resolve to the same IP. I believe this is the IP of the Kubernetes host, where the ingress rules you set up direct traffic for specific services through via the host IP.
  *   Because this resolved to an IP, we didn't see a DNS lookup failure, but it wasn't an IP at which we could connect to cassandra, because there are no ingress rules set up.

I think there's a simple solution here, based on the assumption that you want to be able to use the AKS domain as your subscriber home domain. We should simply be able to set that up as an additional home domain in the system.
To do this, we simply need to set up your configmap with the ZONE key set as `default.svc.cluster.local`, and the additional argument `--from-literal=ADDITIONAL_SHARED_CONFIG=additional_home_domains=2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io`

However, as you are currently provisioning subscribers through the Ellis web UI, and not integrating with an external HSS, I don't think we need to do this at the moment.
We can add it in if we see it become an issue down the line.


Next steps - Live tests
Good to see you've got the live test suite set up and running, though not successfully at the moment. In terms of the right command, I think the attempt you highlighted as taking a long time is the correct one. That is:
rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

You don't need to be running within the cluster for it to work; as long as your Bono and Ellis services are accessible from your test box, that will be fine.
While we're hitting errors, you can add ` TESTS="Basic call - mainline"` onto the command to simply run the first test only. This will limit the amount of error output you see on each run, and make it a bit easier to go about debugging.

The fact that the test command returned the error `Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060` suggests to me that there is potentially an issue in the ingress rules here. It is not returning a `getaddrinfo` error, so I believe the DNS is resolving to an IP, but that Bono is not reachable via that IP on port 5060.
I think your best next step here is to run some networking checks on this connection:

  *   From the box you are running the live tests on, make sure the bono.<aks domain> address resolves, the IP is reachable, and then try using `nc` to connect to port 5060.
  *   Run some tcpdumps in the bono pod to see if you are seeing any traffic reaching it on port 5060 when running the live tests

Hopefully this should help you track down at what point the traffic is getting stuck.

>From the Bono logs you've included, it looks like there's just no traffic reaching the process at all, and so I would suspect there's something unusual in the ingress rules setup. I don't know enough about the Azure http controller behaviour to be able to spot anything that looks particularly out of place at the moment though.
Equally, I am slightly worried that given that it is called 'http-application-routing' there may be some logic in place in the controller that will be interfering with non-http traffic. If we don't see any SIP traffic reaching the bono pod, we could potentially test if this is the case by using something like `curl` to send an http message to `bono.<aks domain>:5060`, and see if we see it reach the bono pod. If HTTP passes through, but SIP is blocked, I think we would need to dig some more into AKS configuration to work out what is going on there.

If you want to do testing with a sip client, I think you have most of the configuration correct, but I haven't used the Twinkle softclient. We have some docs on using sofclients, at http://clearwater.readthedocs.io/en/stable/Making_your_first_call.html, that you've probably already seen. Hopefully should be a decent guide. For the proxy, you'll want to use bono.<aks domain>, as that is what will resolve to the right IP address. However, if the  live tests are failing to connect to Bono, I think you will see similar issues down this path. We should focus on working out what is causing that problem first.

Hopefully once the connectivity issue to Bono is sorted, we should be fully up and running. Let us know how it goes, or if you have any questions on the distinction between the AKS domain and the Kubernetes/clearwater deployment domain.

Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 17 May 2018 09:32
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,


  *   `netstat -planut` shows that Cassandra is listening on 9160 from all ports
  *   `nc cassandra 9160` did not exit immediately. It just hangs without any output
  *   `ping cassandra` works, but `dig cassandra` shows no IP addresses

I tried changing my env-vars configmap from 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io to default.svc.cluster.local. This means that the pods do spin up successfully, and that error message goes away. What is env-vars supposed to be set to?
The docs say: "a ZONE key set to the domain of your Kubernetes cluster". So I set it to the domain of my cluster, and that seems to be the cause of all my problems. If I set it to something which isn't the domain of the cluster, the pods come up. So now I'm confused, is it supposed to be set to the domain of my cluster or not?
Is there some extra step I need to do to configure kubernetes to know that it's domain is 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io? I'm pretty sure it already knows, but how can I check?

Now all the pods are up. I can't make a call or run tests successfully. I wonder whether this is because my deployment has no idea what that it's domain is actually 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io.

In a browser, ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io takes me to the sign up page. I successfully created an account. I think this is good proof that the dns, ingresses and firewalls are set up properly. (albeit only for port 80, but I've checked the other ports. And by 'properly' I mean 'probably too loosely')

I've noticed that all my dns records (set up by Azure) point to the same ip address. Is this right, or not? (I still get confused by how kubernetes shares ip addresses around).
Here's my ellis ingress in kubernetes (required for Azure to set up dns for me)

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: ellis
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: ellis
            servicePort: 80
```

And for bono

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: bono
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: bono
            servicePort: 3478
        - backend:
            serviceName: bono
            servicePort: 5060
        - backend:
            serviceName: bono
            servicePort: 5062
```

I don't understand ingresses well. I'm not certain that this is right. But records appear in the dns server with names 'bono' and 'ellis' pointing to the ip address of something.
The example in the Azure docs<https://docs.microsoft.com/en-us/azure/aks/http-application-routing> has 'path: /`. I'm not sure whether I need that or not in my ingresses. With or without that line, the result is the same.

I've tried with ingresses for just bono and ellis, and also ingresses for every module (except etcd). The result is the same. (If I don't have ingresses for a module, it has no DNS record)



I have no idea what parameters to use when running the tests. The documentation doesn't say. Similarly I have no idea what to use when configuring my SIP client. What's the "domain"? What's the "proxy"?
Here's a bunch of permutations of test arguments which I ran, and the associated error messages:


  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
  *   rake test[default.svc.cluster.local] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   This one took a really long time before failing. So maybe it's the closest?
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'


When I try to register with a SIP client I get a bunch of 404 and 408 (timeout) error codes. Here's what I'm trying to use. I'm basing this off what worked for the AIO installation with this SIP client (Twinkle, one of the ones on Ubuntu)

  *   Your Name: Matthew
  *   User Name: 6505550375
     *   Not sip:6505550375
     *   Not 6505550375 at ...
  *   Domain: default.svc.cluster.local, or is it example.com? The Clearwater kubernetes readme says you should make a call like an AIO installation, which is example.com.
  *   Authentication name: 6505550375 at default.svc.cluster.local<mailto:6505550375 at default.svc.cluster.local>
  *   Password: the one generated by the web page. Not the one you use to sign into the web page.
  *   Proxy server: not sure. What should I use?
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   example.com
     *   default.svc.cluster.local

After running a bunch of failed tests and trying manually to register on a SIP client, here are the logs I see:


  *   /var/log/bono/bono_current.txt on the bono pod:
     *   17-05-2018 08:00:18.268 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 25
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:00:54.278 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 32
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:01:17.285 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 20
  *    Ellis: /var/log/ellis/ellis_20180517T070000Z.txt
     *
     *   17-05-2018 07:44:01.833 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.848 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/
     *   17-05-2018 07:44:01.857 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets/e74929ec-21ea-48ac-95b6-68115894313d
     *   17-05-2018 07:44:01.876 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:144: {"associated_implicit_registration_sets": ["e74929ec-21ea-48ac-95b6-68115894313d"]}
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles
     *   17-05-2018 07:44:01.913 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/public_ids/sip%3A6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.966 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/public/sip%3A6505550400%40default.svc.cluster.local/service_profile
     *   17-05-2018 07:44:01.976 UTC INFO homestead.py:255: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/filter_criteria
     *   17-05-2018 07:44:01.980 UTC INFO xdm.py:29: Sending HTTP PUT request to http://homer.default.svc.cluster.local:7888/org.etsi.ngn.simservs/users/sip%3A6505550400%40default.svc.cluster.local/simservs.xml
     *   17-05-2018 07:44:02.028 UTC INFO web.py:1447: 200 POST /accounts/live.tests at example.com/numbers/<mailto:/accounts/live.tests at example.com/numbers/> (0.0.0.0) 209.96ms

Here are my firewall rules. My understanding is that these are the rules for a firewall between pods and the outside world. I think that communications between pods don't get filtered by these rules, but I added all the inter-pod ports just in case.

Inbound and Outbound TCP: 22,2380,4000,80,443,5060,5062,3478,5058,5054,5052,9888,8888,8889,10888,7888,11211,7000,7253,11311,9160
Inbound and Outbound UDP: 161,162,5060,32768-65535,3478

Any ideas where to go from here? My main question is what command should I run for rake tests?
Should I try to run the rake tests from a test pod within the cluster?

Thanks,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV
M  0415 762 868  | E  Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Thursday, 17 May 2018 3:05 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

I've had a look into your issues deploying under kubernetes, and think I've narrowed down the search a good amount. Details below.
First though I'll take a run through the smaller questions you've raised below, as I think we can knock some of them off fairly quickly.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
[AJL] You shouldn't need it in the containers. In our VMs we found that some operations were fairly heavy on DNS queries, and so provided dnsmasq as a means of providing caching. The need for this has decreased quite a bit now, and as we move forward in the microservice space it makes less sense to have it around.

     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
This will likely be down to how the container networking works, but it won't cause any problems in running under kubernetes.

     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
[AJL] This is a benign error in this case, so we haven't dug down into it. It shouldn't impact your deployment at all.

  *   There are many other red error messages during build. Does that matter?
[AJL] No, these don't matter. I see a number of error messages during the build. A lot of these are dependencies of some of our packages raising error messages when being installed in this environment

  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
[AJL] The AIO has a number of different requirements that we don't have in the container environment, and vice versa, so the fact that there is a different set doesn't mean you have issues there
As a couple of examples for the differences you pulled out below:

  *   The auto config packages are used to enable specific setup in different environments. E.g. setting up required config files that other deployments do not rely on. In the docker case, we need to read environment variables and pass them through to the system configuration files
  *   Clearwater-etcd is our interface into an etcd cluster, and is used for sharing common configuration around a cluster. On the AIO this simply increases complexity, as we have no need for it, but in docker/kubernetes, we want the services to share configuration through an underlying etcd cluster.
I don't believe any of the differences you pulled out below would be leading to the issues you're seeing, but it's a nice analysis :)
The missing dns.json config file is also currently just a benign error in the containers, raised by some expectations we have when running in VMs.


So, on to the main issue:
In getting your deployment up and running, the key issue is definitely going to be the 'Failed to initialize the Cassandra store' one. This indicates the Homestead and Homestead-prov services have been unable to reach the cassandra cluster, and without that they won't be able to get up and running.
As you've already found, getting logs out and doing more detailed debugging in containers presents its own set of problems. To temporarily prevent kubernetes from automatically restarting pods, you can (as you may have already) remove the liveness checking sections from the generated deployment files. This isn't ideal, but it should allow you time to do some more detailed debugging. You may also want to reduce the number of replicas of the pods down to one for now, to make it easier to follow traffic. Should just be a case of changing the .depl files and redeploying.

The log indicates you're getting error code 3 back from attempting to setup the Cassandra store, which is a 'connection error', as opposed to what you would get if the dns was simply unresolvable (I have tested that on our internal setup, and the logs would be clearly stating it was unresolvable). This is definitely leading me to suspect network connectivity issues into the cassandra pods. The fact that you can ping between the two however suggests that it's more complex than simply being unable to send any traffic between them.

To test this, I tweaked the cassandra configuration (running just a single pod for simplicity) so that it was listening on a port that wasn't 9160, which is the port that both homestead and homestead prov attempt to connect to. With this setup in place, I saw both of these pods repeatedly failing with the same error code as yours.
Because of this I would suggest the following next steps in tracking down the problem:

  *   Double check that your networking configuration allows traffic through to the cassandra pods on port 9160. The homestead and homestead prov processes will require this.
  *   Connect to the Cassandra pods, and verify that they are indeed listening for traffic on port 9160
     *   `netstat -planut` is my default for this, but to each their own
     *   /var/log/cassandra/system.log may have more information if something strange is going on here
  *   Manually check the connectivity between the pods, where you could try
     *   Connect to e.g. a homestead pod, and try running `nc <cassandra hostname> 9160`. I'm seeing this instantly return if the port is not open/listened on.
     *   Install tcpdump on both sides, and see whether you see traffic arriving in the cassandra pods at all. This isn't a very container, but getting a dump from both cassandra and homestead sides, to see what point in the flow is failing would be a decent way to work out where the issue lies.

Sadly, I don't have access to an AKS setup right now, so can't investigate the ingress rules component at all. I suspect that there's something slightly unusual happening there that is leading to traffic being unable to reach port 9160.
Hopefully this should give you a better idea of what the source of the issue is here. If this doesn't show anything up, and you're seeing traffic from homestead arrive in the cassandra pods and going back out we may have to run some more testing

Cheers,
Adam


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 16 May 2018 04:35
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Richard,

Tldr: I've made some progress since I sent that email. I've found and fixed bugs to do with dnsmasq, but now I'm stuck.
The systemd logs for homestead still say: "Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."
I'm not even sure whether the dnsmasq bugs were related.

First, to answer your questions:

Yes this happens every time I deploy.

My configmap:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
Events:  <none>
```

I'm deploying on Azure's kubernetes service (AKS). I've enabled HTTP Application Routing<https://docs.microsoft.com/en-us/azure/aks/http-application-routing>, which means that Azure manages DNS for me. The only unusual step is that Azure requires that I add ingresses to do so. The DNS records point to each service (bono, ellis, sprout etc). I can ping "cassandra" from the homestead pod.

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:22:21Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.6", GitCommit:"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1", GitTreeState:"clean", BuildDate:"2018-03-21T15:13:31Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}

Getting the logs is a race against time, since kubernetes keeps terminating the pod while I'm in there.
After several attempts, I managed to get the homestead logs (same as homestead-prov logs)

```
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:123: Creating Cached Resolver using servers:
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:133:     10.0.0.10
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Error static_dns_cache.cpp:89: DNS config file /etc/clearwater/dns.json mis
sing
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status main.cpp:653: Using local impu store: astaire.2991678f-f9dd-4098-8fe
8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.129 UTC [7f6379c8e7c0] Status http_connection_pool.cpp:37: Connection pool will use calculated res
ponse timeout of 550ms
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:35: Configuring HTTP Connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:36:   Connection created for server sprout.2991678f
-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io:9888
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status main.cpp:1013: No HSS configured - using Homestead-prov
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:266: Configuring store connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:267:   Hostname:  cassandra.2991678f-f9dd-4098-8
fe8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:268:   Port:      9160
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:296: Configuring store worker pool
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:297:   Threads:   10
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:298:   Max Queue: 0
15-05-2018 07:05:40.131 UTC [7f6366ffd700] Status alarm.cpp:244: Reraising all alarms with a known state
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Error main.cpp:1056: Failed to initialize the Cassandra store with error co
de 3.
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Status main.cpp:1057: Homestead is shutting down
```

The full logs are just this chunk repeating over and over until everything shuts down.

I think the crucial error message is DNS config file /etc/clearwater/dns.json missing

In the all-in-one installation, that file is just:

{
  "hostnames": [
  ]
}

Is it ok to manually insert that using the Dockerfile? Or should there be other entries in there for a non-AIO installation?

dns.json is created by the dnsmasq service.

During the container build, there is a red error message saying "dnsmasq: setting capabilities failed: Operation not permitted"
(There are a lot of red error messages during build. Is that an issue?)

I noticed that dnsmasq was not running in homestead docker, yet it is running in my working all-in-one installation. (There are also a bunch of other services which are halted. Is that an issue?)

I tried starting dnsmasq manually, it wouldn't start. I looked online and the solution is to add user=root to the file /etc/dnsmasq.conf.

I tried modifying the Dockerfile to manually append that string to the file.  (I'm assuming that the order of lines in /etc/dnsmasq.conf does not matter)

Then dnsmasq still doesn't come up. I get an error about how the port it wants is already taken.

So then I tried modifying the Dockerfile to paste in dns.conf from the AIO installation. (The one with an empty list of hosts)

Now the missing file error is gone, but I still get the other error:

"Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."

How can I debug this?

All this time I thought that error was caused by dns issues. Is it because dnsmasq still won't start, so dns.json is missing hosts? Or is that unrelated?
"ping cassandra" works from the homestead pod.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
  *   There are many other red error messages during build. Does that matter?
  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
     *   Services running in AIO:
        *   [ ? ]  clearwater-auto-config-generic
        *    [ + ]  clearwater-cluster-manager
        *    [ + ]  clearwater-config-manager
        *    [ + ]  clearwater-diags-monitor
        *    [ - ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ ? ]  clearwater-memcached
        *    [ + ]  clearwater-queue-manager
     *   Services running (or not) in docker homestead:
        *   [ ? ]  clearwater-auto-config-docker
        *    [ + ]  clearwater-cluster-manager
        *    [ - ]  clearwater-config-manager
        *    [ - ]  clearwater-diags-monitor
        *    [ + ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ - ]  clearwater-queue-manager


Thanks,

Matthew Davis
Telstra | CTO | Cloud SDN NFV

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Wednesday, 16 May 2018 3:24 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Matthew,

Sorry to hear you are having problems deploying Clearwater on Docker.

I think that the socket factory error is actually fairly benign.

Does this happen every time you create your deployment?

Can you provide the config map that you are providing when deploying on Kubernetes?

Can you connect into one of the pods that is failing to run homestead-prov and grab the logs. The relevant ones will be under /var/log/homestead-prov inside the container.

We cannot reproduce your issue on our kubernetes setup - can you provide details of your kubernetes setup - what version of k8s and what networking configuration you are using?

>From the diagnostics we have at the moment, the most likely scenario is that the Homestead and Homestead Prov containers are unable to contact Cassandra - if that's the case they will continually restart to attempt to resolve the problem.



Richard


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 14 May 2018 07:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi everyone,
I've fixed the issue I had with homestead-prov and submitted the patch as this pull request: https://github.com/Metaswitch/clearwater-docker/pull/89

However now I just run into another issue, so homestead-prov is giving me the same error as homestead. The error message is not verbose. Does anyone know how to make it more verbose? I have no idea what's happening.


```
2018-05-14 05:47:41,369 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:41,370 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-sig entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-mgmt entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,477 INFO exited: snmpd (exit status 0; expected)
2018-05-14 05:47:42,478 CRIT reaped unknown pid 61)
2018-05-14 05:47:44,572 CRIT reaped unknown pid 65)
2018-05-14 05:47:44,576 CRIT reaped unknown pid 66)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 235)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 236)
2018-05-14 05:47:45,902 CRIT reaped unknown pid 257)
2018-05-14 05:47:48,153 CRIT reaped unknown pid 294)
2018-05-14 05:47:48,226 INFO spawned: 'homestead-prov' with pid 297
2018-05-14 05:47:48,237 INFO spawned: 'nginx' with pid 298
2018-05-14 05:47:48,747 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:49,750 INFO spawned: 'homestead-prov' with pid 324
2018-05-14 05:47:49,750 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:50,089 INFO exited: clearwater-infrastructure (exit status 0; expected)
2018-05-14 05:47:50,335 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:52,339 INFO spawned: 'homestead-prov' with pid 335
2018-05-14 05:47:53,324 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:56,330 INFO spawned: 'homestead-prov' with pid 346
2018-05-14 05:47:56,902 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:57,904 INFO gave up: homestead-prov entered FATAL state, too many start retries too quickly
```

Regards,
Matt Davis
Telstra | Graduate Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180522/b2493c68/attachment.html>

From Benjamin.Laing at metaswitch.com  Wed May 23 03:00:53 2018
From: Benjamin.Laing at metaswitch.com (Benjamin Laing)
Date: Wed, 23 May 2018 07:00:53 +0000
Subject: [Project Clearwater] Ellis failed to update server error
Message-ID: <CY4PR02MB2517F1001D30AAF45E916933F16B0@CY4PR02MB2517.namprd02.prod.outlook.com>

Hi Tapiwa,

I believe we've seen this running on Ocata, so potentially that's a red herring. It's looking like Ellis can't connect to Dime via hs-prov.openstack.rhodes.com:8889<http://hs-prov.openstack.rhodes.com:8889/private/6505550412%40openstack.rhodes.com%22%7D>:

  *   Is Dime powered on?
  *   What's the output when you run `dig hs-prov.openstack.rhodes.com` from Ellis?
  *   Can you ping 172.16.10.6 from Ellis?
  *   What are the hs_hostname / hs_hostname_mgmt and hs_provisioning_hostname settings in shared_config?

All the best,

Ben

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180523/96669564/attachment.html>

From tcchindeka at gmail.com  Wed May 23 10:05:53 2018
From: tcchindeka at gmail.com (Tapiwa Chindeka)
Date: Wed, 23 May 2018 16:05:53 +0200
Subject: [Project Clearwater] Ellis failed to update server error
In-Reply-To: <CY4PR02MB2517F1001D30AAF45E916933F16B0@CY4PR02MB2517.namprd02.prod.outlook.com>
References: <CY4PR02MB2517F1001D30AAF45E916933F16B0@CY4PR02MB2517.namprd02.prod.outlook.com>
Message-ID: <CAFFdzJSGvNxfRGzz4_KKOzMXj3ONVOnLq2fRyq3D4-gszAYazQ@mail.gmail.com>

Hi Ben


   - Yes Dime is powered on
   -  dig hs-prov.openstack.rhodes.com gives me:

; <<>> DiG 9.9.5-3ubuntu0.16-Ubuntu <<>> hs-prov.openstack.rhodes.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 4626
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 1, ADDITIONAL: 2

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;hs-prov.openstack.rhodes.com.  IN      A

;; ANSWER SECTION:
hs-prov.openstack.rhodes.com. 30 IN     A       172.16.11.15

;; AUTHORITY SECTION:
openstack.rhodes.com.   3600    IN      NS      ns.openstack.rhodes.com.

;; ADDITIONAL SECTION:
ns.openstack.rhodes.com. 3600   IN      A       203.0.113.105

;; Query time: 36 msec
;; SERVER: 127.0.0.1#53(127.0.0.1)
;; WHEN: Wed May 23 13:36:20 UTC 2018
;; MSG SIZE  rcvd: 106



   - I can't ping 172.16.10.6 from ellis. Ellis is only part of the
   clearwater-private-management subnet, 172.16.11.0/24 and 172.16.10.6 is
   the private signaling IP of Dime. I can however ping Dime on 172.16.11.15

These are the configurations in shared_config:

home_domain=openstack.rhodes.com
sprout_hostname=sprout.openstack.rhodes.com
hs_hostname=hs.openstack.rhodes.com:8888
hs_provisioning_hostname=hs-prov.openstack.rhodes.com:8889
ralf_hostname=ralf.openstack.rhodes.com:10888
xdms_hostname=homer.openstack.rhodes.com:7888
sprout_impi_store=vellum.openstack.rhodes.com
sprout_registration_store=vellum.openstack.rhodes.com
homestead_impu_store=vellum.openstack.rhodes.com
cassandra_hostname=vellum.openstack.rhodes.com
chronos_hostname=vellum.openstack.rhodes.com
ralf_session_store=vellum.openstack.rhodes.com
upstream_port=0
# Email server configuration
smtp_smarthost=localhost
smtp_username=username
smtp_password=password
email_recovery_sender=clearwater at example.org
# Keys
signup_key=secret
turn_workaround=secret
ellis_api_key=secret
ellis_cookie_key=secret


Thank you,
Tapiwa

On Wed, May 23, 2018 at 9:00 AM, Benjamin Laing <
Benjamin.Laing at metaswitch.com> wrote:

> Hi Tapiwa,
>
>
>
> I believe we?ve seen this running on Ocata, so potentially that?s a red
> herring. It?s looking like Ellis can?t connect to Dime via
> hs-prov.openstack.rhodes.com:8889
> <http://hs-prov.openstack.rhodes.com:8889/private/6505550412%40openstack.rhodes.com%22%7D>
> :
>
>    - Is Dime powered on?
>    - What?s the output when you run `dig hs-prov.openstack.rhodes.com`
>    from Ellis?
>    - Can you ping 172.16.10.6 from Ellis?
>    - What are the hs_hostname / hs_hostname_mgmt and
>    hs_provisioning_hostname settings in shared_config?
>
>
>
> All the best,
>
>
>
> Ben
>
>
>
> _______________________________________________
> Clearwater mailing list
> Clearwater at lists.projectclearwater.org
> http://lists.projectclearwater.org/mailman/listinfo/clearwater_lists.
> projectclearwater.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180523/66e63b6c/attachment.html>

From Alan.Kwon at interoptechnologies.com  Wed May 23 16:21:43 2018
From: Alan.Kwon at interoptechnologies.com (Kwon, Alan)
Date: Wed, 23 May 2018 16:21:43 -0400
Subject: [Project Clearwater] PSI routing at terminating network
Message-ID: <D72B3787.53989%Alan.Kwon@interoptechnologies.com>

Hi,

I?m trying to understand the routing logic in Sprout. Basically, it?s a SUBSCRIBE coming from another network (NNI) with ReqURI set to a PSI:

SUBSCRIBE sip:ImService at sc01.ivc.iot1.com:5510;as.session=4 SIP/2.0
Via: SIP/2.0/UDP 172.27.0.140;branch=z9hG4bK08B0100666d326454c4
From: <tel:+18152579478>;tag=gK08e-gK00e-492495c9-c6df-4eaf-8016-c126fbac5136
To: <sip:ImService at sc01.ivc.iot1.com;as.session=4>
Call-ID: f6fa61ef-a3df-4c30-9069-1ee0a3957f58
CSeq: 2 SUBSCRIBE
Max-Forwards: 61
Expires: 3600
P-Preferred-Identity: <tel:+18152579478>
Contact: <sip:ImService at sc01.sales2.iot1.com:5510;as.session=2>;+sip.instance="<urn:gsma:imei:35197909-002681->";+g.oma.sip-im
Event: conference
Accept: application/conference-info+xml
Accept-Contact: *;+g.oma.sip-im
P-Asserted-Identity: <sip:+18152579478 at sales2.iot1.com>
P-Asserted-Identity: <tel:+18152579478>
Supported: eventlist
Record-Route: <sip:172.27.0.140:5060;lr;transport=udp>
P-Charging-Vector: icid-value=dab7cbc0-4038-1036-00-00-00-50-56-bb-09-00;icid-generated-at=172.27.0.140
Content-Length: 0

>From the Sprout log, I can see I-CSCF performing LIR and getting a successful result:

22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug icscfsproutlet.cpp:551: Terminating request
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer
_sip: true, treat_number_as_phone: false
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:172: Classified URI as 5
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug icscfrouter.cpp:431: Perform LIR - impu sip:ImService at sc01.ivc.iot1.com, originating false, auth_type None
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug a_record_resolver.cpp:57: ARecordResolver::resolve_iter for host dime.ivc.iot1.com, port 8888, family 2
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug utils.cpp:446: Attempt to parse dime.ivc.iot1.com as IP address
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Verbose dnscachedresolver.cpp:468: Check cache for dime.ivc.iot1.com type 1
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug dnscachedresolver.cpp:578: Pulling 2 records from cache for dime.ivc.iot1.com A
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug baseresolver.cpp:192: Found 2 A/AAAA records, creating iterator
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug utils.cpp:446: Attempt to parse dime.ivc.iot1.com as IP address
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug baseresolver.cpp:812: Attempting to get 1 targets for host:dime.ivc.iot1.com. allowed_host_state = 3
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug baseresolver.cpp:587: 172.27.0.51:8888;transport=TCP has state: WHITE
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug baseresolver.cpp:587: 172.27.0.52:8888;transport=TCP has state: WHITE
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug baseresolver.cpp:587: 172.27.0.51:8888;transport=TCP has state: WHITE
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug baseresolver.cpp:883: Added a whitelisted server to targets, now have 1 of 1
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug connection_pool.h:207: Request for connection to IP: 172.27.0.51, port: 8888
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug connection_pool.h:229: No existing connection in pool, create one
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug http_connection_pool.cpp:42: Allocated CURL handle 0x7faafc31d9a0
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug connection_pool.h:231: Created new connection 0x7faafc336bb0
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug httpclient.cpp:557: Set CURLOPT_RESOLVE: dime.ivc.iot1.com:8888:172.27.0.51
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug httpclient.cpp:588: Sending HTTP request : http://dime.ivc.iot1.com:8888/impu/sip%3AImService%40sc01.ivc.iot1.com/l
ocation (trying 172.27.0.51)
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug thread_dispatcher.cpp:117: Pausing stopwatch due to HTTP request to http://dime.ivc.iot1.com:8888/impu/sip%3AImServ
ice%40sc01.ivc.iot1.com/location
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:956: Received header http/1.1200ok with value
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:957: Header pointer: 0x7fa9fbf6a000
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:956: Received header content-length with value 75
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:957: Header pointer: 0x7fa9fbf6a000
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:956: Received header content-type with value text/plain
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:957: Header pointer: 0x7fa9fbf6a000
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:956: Received header  with value
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:957: Header pointer: 0x7fa9fbf6a000
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug thread_dispatcher.cpp:123: Resuming stopwatch after HTTP request to http://dime.ivc.iot1.com:8888/impu/sip%3AImService%40sc01.ivc.iot1.com/location
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:628: Received HTTP response: status=200, doc={"result-code":2001,"mandatory-capabilities":[],"optional-capabilities":[]}
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug baseresolver.cpp:672: Successful response from  172.27.0.51:8888;transport=TCP
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug connection_pool.h:244: Release connection to IP: 172.27.0.51, port: 8888 to pool
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug event_statistic_accumulator.cpp:32: Accumulate 6770 for 0x1f10670
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug event_statistic_accumulator.cpp:32: Accumulate 6770 for 0x1f10718
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug event_statistic_accumulator.cpp:32: Accumulate 6770 for 0x1f19fb0
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug event_statistic_accumulator.cpp:32: Accumulate 6770 for 0x1f19ff8
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug icscfrouter.cpp:246: HSS returned capabilities
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug acr.cpp:621: Storing Server-Capabilities
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug scscfselector.cpp:263: Selected S-CSCF is sip:scscf.sprout.ivc.iot1.com:5054
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug icscfrouter.cpp:118: SCSCF selected: sip:scscf.sprout.ivc.iot1.com:5054

When the request gets to S-CSCF, I was expecting it to evaluate iFC for the PSI (sip:ImService at sc01.ivc.iot1.com), however, it seems to skip the service evaluation:

22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('icscf'->'scscf'/'scscf-proxy')
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('icscf'->'scscf'/'scscf-proxy')
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug sproutletproxy.cpp:2517: Internal network function boundary: yes
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug pjutils.cpp:736: Cloned tdta0x7faafc1562c0 to tdta0x7faafc2dac10
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug sproutletproxy.cpp:2517: Internal network function boundary: yes
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug pjutils.cpp:736: Cloned tdta0x7faafc1562c0 to tdta0x7faafc2dac10
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug sproutletproxy.cpp:1450: Remove top Route header Route: <sip:sprout.ivc.iot1.com;lr;service=scscf-proxy>
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug sproutletproxy.cpp:2115: Adding message 0x7faafc2db220 => txdata 0x7faafc2dacb8 mapping
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Verbose sproutletproxy.cpp:1946: scscf-proxy-0x7faafc305ee0 pass initial request Request msg SUBSCRIBE/cseq=2 (tdta0x7faa
fc2dac10) to Sproutlet
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Info scscfsproutlet.cpp:471: S-CSCF received initial request
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_
sip: true, treat_number_as_phone: false
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:172: Classified URI as 3
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug scscfsproutlet.cpp:945: Route header references this system
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug scscfsproutlet.cpp:991: No ODI token, or invalid ODI token, on request - logging ICID marker dab7cbc0-4038-1036-00-
00-00-50-56-bb-09-00 for B2BUA AS correlation
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug scscfsproutlet.cpp:1004: Got our Route header, session case term, OD=None
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer
_sip: true, treat_number_as_phone: false
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:172: Classified URI as 5
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug scscfsproutlet.cpp:1348: URI is not locally hosted
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug acr.cpp:1797: Create RalfACR for node type S-CSCF with role Terminating
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug acr.cpp:24: Created ACR (0x7faafc168d50)
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug acr.cpp:170: Created S-CSCF Ralf ACR
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug acr.cpp:29: Destroyed ACR (0x7faafc168d50)
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:172: Classified URI as 5
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Info scscfsproutlet.cpp:656: Route request without applying services

Could you please explain why the iFC evaluation is skipped? 24-229, Section 5.4.3.3 seems to imply that it should build an ordered list of iFC if no original dialog identifier is present in the topmost Route header of the incoming request, which seems to be the case here.

Thank you,



[cid:7F5F1D25-E9CF-4FB6-957D-E4E41857C139]<http://www.interoptechnologies.com/>




ALAN KWON
Senior Software Engineer




T: +1 972-753-1865 (Texas)
F: +1 239-425-6845

Confidentiality Notice: The information in this e-mail and in any attachment may contain information which is legally privileged. It is intended only for the attention and use of the named recipient. If you are not the intended recipient, you are not authorized to retain, disclose, copy or distribute the message and/or any of its attachments. If you received this e-mail in error, please notify me and delete this message.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180523/8de949ea/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0E8CB2C0-CC34-4B10-814C-7B349D41FD79[27].png
Type: image/png
Size: 3922 bytes
Desc: 0E8CB2C0-CC34-4B10-814C-7B349D41FD79[27].png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180523/8de949ea/attachment.png>

From Matthew.Davis.2 at team.telstra.com  Thu May 24 01:19:46 2018
From: Matthew.Davis.2 at team.telstra.com (Davis, Matthew)
Date: Thu, 24 May 2018 05:19:46 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <BY1PR0201MB09689BB9A1645396B16AE810E2940@BY1PR0201MB0968.namprd02.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971545314ED91FB7F233AC1E2900@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB288450EF460A39FEA9E1D724C2940@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<BY1PR0201MB09689BB9A1645396B16AE810E2940@BY1PR0201MB0968.namprd02.prod.outlook.com>
Message-ID: <ME2PR01MB2884F01278DC1CD5F7831DFEC26A0@ME2PR01MB2884.ausprd01.prod.outlook.com>

Hi Adam,

Thanks for that.

> you're having issues connecting to Bono because the service/pod is not exposed

What do you mean by that? I've run `kubectl apply -f bono-svc.yaml`. The service is exposed, isn't it?

$ kubectl get services
NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                               AGE
bono             ClusterIP   None         <none>        3478/TCP,5060/TCP,5062/TCP            1d

or after the pcscf changes:

NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                        AGE
bono             NodePort    10.108.162.246   <none>        3478:30078/TCP,5060:30060/TCP,5062:30062/TCP   41m

I've cc'ed in my colleague (Kieran) who set up the cluster for me. He said it's using weave net. Contrary to what I thought, nothing will be externally routable. So that's an issue. After adding the nodePorts I can view ellis in a browser (http://10.3.1.76:30080/login.html), and ` nc 10.3.1.76 30080 -v` shows that I can connect to the kubernetes cluster on port 30080. ` nc 10.3.1.76 30060 -v` says connection refused. When that happens I see a packet in tcpdump on the bono pod. I see nothing on tcpdump in bono on the relevant ports when I run the rake tests.

I'm curious about the ports for Bono. You keep mentioning port 5060. Is that the only port bono uses? What about 3478 and 5062?

I tried your suggestion for the pcscf thing. It didn't work.
The stdio for the rake test is below. How can I figure out whether the 403 error is for ellis or bono?
The last line of the rake test output says: "Error logs, including Call-IDs of failed calls, are in the 'logfiles' directory". But the logfiles directory is empty. Where can I find more verbose logs?

I ran tcpdump inside the bono and ellis pods during the test. If I filter by source IP address I see nothing, so I have to filter by port. The tcpdump command I'm using in bono is `tcpdump -a -vnni any port 5060 or port 5062 or port 3478 or port 30078 or port 30060 or port 30062`

Literally all ports are now open on the firewall.

Here is my bono-svc.yaml file:

```
apiVersion: v1
kind: Service
metadata:
  name: bono
spec:
  type: NodePort
  ports:
  - name: "3478"
    port: 3478
    nodePort: 30078
  - name: "5060"
    port: 5060
    nodePort: 30060
  - name: "5062"
    port: 5062
    nodePort: 30062
  selector:
    service: bono
```

Here is my bono-depl.yaml file:

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: bono
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bono
  template:
    metadata:
      labels:
        service: bono
        snmp: enabled
    spec:
      containers:
      - image: "mlda065/bono:latest"
        imagePullPolicy: Always
        name: bono
        ports:
        - containerPort: 22
        - containerPort: 3478
        - containerPort: 5060
        - containerPort: 5062
        - containerPort: 5060
          protocol: "UDP"
        - containerPort: 5062
          protocol: "UDP"
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PUBLIC_IP
          value: 10.3.1.76 #:6443
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      - image: busybox
        name: tailer
        command: [ "tail", "-F", "/var/log/bono/bono_current.txt" ]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      volumes:
      - name: bonologs
        emptyDir: {}
      imagePullSecrets:
      - name: myregistrykey
      restartPolicy: Always
```

Here is my env-vars:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
default.svc.cluster.local
Events:  <none>
```

Here are my services once deployed:

```
$ kubectl get service
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                        AGE
astaire          ClusterIP   None             <none>        11311/TCP                                      22m
bono             NodePort    10.108.162.246   <none>        3478:30078/TCP,5060:30060/TCP,5062:30062/TCP   22m
cassandra        ClusterIP   None             <none>        7001/TCP,7000/TCP,9042/TCP,9160/TCP            22m
chronos          ClusterIP   None             <none>        7253/TCP                                       22m
ellis            NodePort    10.97.76.168     <none>        80:30080/TCP                                   22m
etcd             ClusterIP   None             <none>        2379/TCP,2380/TCP,4001/TCP                     22m
homer            ClusterIP   None             <none>        7888/TCP                                       22m
homestead        ClusterIP   None             <none>        8888/TCP                                       22m
homestead-prov   ClusterIP   None             <none>        8889/TCP                                       22m
kubernetes       ClusterIP   10.96.0.1        <none>        443/TCP                                        2d
ralf             ClusterIP   None             <none>        10888/TCP                                      22m
sprout           ClusterIP   None             <none>        5052/TCP,5054/TCP                              22m
```


When I restarted the bono service there was a couple of warnings:

```
Defaulting container name to bono.
Use 'kubectl describe pod/bono-6dfc579b5-bdgzj -n default' to see all of the containers in this pod.
* Restarting Bono SIP Edge Proxy bono
/etc/init.d/bono: line 63: ulimit: open files: cannot modify limit: Operation not permitted
/etc/init.d/bono: line 64: ulimit: open files: cannot modify limit: Invalid argument
23-05-2018 06:21:52.570 UTC [7f2b793ec7c0] Status utils.cpp:651: Switching to daemon mode
   ...done.
```

Here's the result of the test:

```
$rake test[default.svc.cw-k8s.test] PROXY=10.3.1.76 PROXY_PORT=30060 SIGNUP_CODE='secret' ELLIS=10.3.1.76:30080 TESTS="Basic call - mainline"
Basic Call - Mainline (TCP) - Failed
  RestClient::Forbidden thrown:
   - 403 Forbidden
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/abstract_response.rb:74:in `return
!'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:495:in `process_result'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:421:in `block in transm
it'
     - /usr/lib/ruby/2.3.0/net/http.rb:853:in `start'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:413:in `transmit'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:176:in `execute'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient.rb:69:in `post'
```

Here is /var/log/bono/bono_current.txt in the bono pod:

```
24-05-2018 05:17:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
24-05-2018 05:17:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1012.3 alarm
24-05-2018 05:17:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1013.3 alarm
24-05-2018 05:17:33.337 UTC [7f2b4b7ee700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 11
24-05-2018 05:17:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
24-05-2018 05:17:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1012.3 alarm
24-05-2018 05:17:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1013.3 alarm
24-05-2018 05:18:21.346 UTC [7f2b4b7ee700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 36
24-05-2018 05:18:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
24-05-2018 05:18:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1012.3 alarm
24-05-2018 05:18:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1013.3 alarm
24-05-2018 05:18:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
24-05-2018 05:18:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1012.3 alarm
24-05-2018 05:18:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1013.3 alarm
24-05-2018 05:18:57.350 UTC [7f2b4b7ee700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 35
```

Thanks,


Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Wednesday, 23 May 2018 1:21 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com>; clearwater at lists.projectclearwater.org
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

Sounds like interesting progress on the Azure side. I'd be really interested in the details of how you get it up and running if this approach works, and we'd be really happy to be able to integrate some steps into our docs to expand the platforms we can easily get running on. Do update with any progress you make there :)

As for your vanilla Kubernetes setup, as far as I can see you're having issues connecting to Bono because the service/pod is not exposed; you're trying to connect to port 5060 on your Kubernetes host, without any service set up on the port. This is why your tests and sip client are failing, and nothing is hitting the Bono process.
Can you give some more information on the network setup of your Kubernetes rig? What network plugin/CNI have you configured it with? And are the pod networks purely internal to the cluster? Or have you set them up as externally routable?

As documented in the clearwater-docker README<https://github.com/Metaswitch/clearwater-docker/blob/master/README.md#alternative-configuration-for-exposing-bono-and-ellis-services>, exposing Bono presents a few challenges. In our internal setups, we have in the past either ended up using GRE tunnels to route traffic directly into pod networks (not simple, and a fair pain to set up), or have deployed Kubernetes using fairly basic network infrastructure, and making the pod networks directly routable using the kubenet plugin and some simple forwarding rules. This means that we are able to route directly into the bono pod.
In your case, you may well have deployed with an overlay network, and an internal only pod network, which makes things a bit harder.

The right solution here is to find a way to route traffic through on port 5060 direct into your bono pod. As in our README, GKE provides a load balancer that is able to do this, and internally we've used the above methods. I'm not sure what the best approach available on current Kubernetes is.

However, as a stop gap to get you a bit further forward for now, I have played around and have a suggestion for something that may work for now. With a few manual tweaks we should be able to set bono up to listen as the P-CSCF on a different port, expose that as a nodeport, and have bono view itself as recording the same IP as the Kubernetes host.
To do this, try the following steps:

  *   Set up the bono service with a NodePort (similar to ellis). This will be something like port 30060.
  *   Modify the bono-depl file:
     *   add `name: PUBLIC_IP` and `value: <k8s host IP>` to the `env:` section.
     *   Remove the liveness and readiness probes relating to port 5060. We will be changing the port bono listens on manually, and without this change the containers will be killed and restarted under our feet
  *   Deploy all the pods and services
  *   Connect to the bono pod, and modify `/etc/init.d/bono`
     *   Change the value of the line `--pcscf=5060,5058` to `--pcscf=<nodePort>,5058` , with the same nodeport you set in bono-svc
  *   Restart the bono service using `service bono restart` from within the container

This should leave your bono pod able to receive traffic through a nodePort connection on the Kubernetes host IP. You should be able to run the live tests with the command below, and should be able to get a GUI SIP client registering as well.

rake test[default.svc.cw-k8s.test] PROXY=10.3.1.76 PROXY_PORT=<bono nodeport> SIGNUP_CODE='secret' ELLIS=10.3.1.76:30080

If you get more connection errors, as in your below error output, try tcpdumping in different locations along the call path to find out where traffic is getting dropped. The test failure output should give a reasonable clue as to what is going wrong.

Keep us updated. Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 22 May 2018 09:15
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,

I heard back from a contact we have in Microsoft. They seemed to suggest that selecting 'advanced networking' instead of 'basic' (which is what I was using) might make each pod directly addressable, but from within an azure vnet (so I'd have to find a way to expose that to the public internet for my SIP clients). I'm halfway through trying that at the moment. I'll update that thread once I run some rake tests.

As far as a vanilla kubernetes installation goes, I tried that today and couldn't get it working. Can you help me with that?
I'm trying to just get the simplest setup possible. For now my goal is just to get something working somewhere. (Incidentally this is why I initially tried Azure Kubernetes Service with Application Routing)

So in my vanilla installation (i.e. kubernetes on openstack) I just used 1 bono pod. (Once I get this simple install working, then I'll try more complicated setups)

How am I supposed to run the rake tests?

My cluster is at 10.3.1.76. I changed ellis-svc to a nodePort type.

I'm trying:

rake test[default.svc.cluster.local] PROXY="10.3.1.76" SIGNUP_CODE=secret ELLIS=10.3.1.76:30080 TESTS="Basic call - mainline"

The error message is:

Basic Call - Mainline (TCP) - Failed
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2) for "10.3.1.76" port 5060
     - /home/ubuntu/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     - /home/ubuntu/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     - /home/ubuntu/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     - /home/ubuntu/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
     - /home/ubuntu/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new_source'

The logs in bono in /var/log/bono/bono_current.txt did not show any additional lines during the test. The logs are currently:

22-05-2018 08:11:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1012.3 alarm
22-05-2018 08:11:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1013.3 alarm
22-05-2018 08:12:02.410 UTC [7f59f0ff1700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 20
22-05-2018 08:12:04.667 UTC [7f5a15023700] Status alarm.cpp:244: Reraising all alarms with a known state
22-05-2018 08:12:04.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1012.3 alarm
22-05-2018 08:12:04.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1013.3 alarm
22-05-2018 08:12:26.414 UTC [7f59f0ff1700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 5
22-05-2018 08:12:34.667 UTC [7f5a15023700] Status alarm.cpp:244: Reraising all alarms with a known state
22-05-2018 08:12:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1012.3 alarm
22-05-2018 08:12:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1013.3 alarm

When I try manually in a GUI SIP client I get a 503 error.

I haven't done any DNS stuff for this deployment. As I said, I'm trying to keep it simple, just so that I have something working. My understanding is that I only need the DNS server if I want SIP clients to access the server via a domain name instead of an IP address. Is that correct?

When I run `nc 10.3.1.76 30080 -v` it connects successfully.
When I run `nc 10.3.1.76 5060 -v`  fails (connection refused), both from my laptop and from one of the pods which isn't bono. Since I get 'connection refused' when running that from within the cluster, that means it's not a firewall issue. (The firewall surrounds the whole cluster, and doesn't get between pods).

Thanks,
Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Saturday, 19 May 2018 3:46 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

No worries. Glad we're making headway on this. It will be useful to work out any of the kinks in using platforms like AKS so that we can continue to deploy easily in as many situations as possible.

>From the info you've sent below, it looks like there's an issue in the AKS configuration/ingress rules meaning there isn't anything in place for Bono. Without a rule to pass traffic through port 5060 to Bono, we're going to have a tough time getting calls running.
>From my experience with them, I think ingress rules should work on any port combinations, not just port 80. I would however suggest, if possible, clearing out the existing rules and then re-configuring the Ellis and Bono ingress rules from scratch, so that we can eliminate anything unusual that might have got in place while running through the other issues.
If you do this, and things still seem out of place, send over some more details on what the setup you're following is, and some diagnostics/info on what it all looks like.

To answer your direct questions:

  *   The port number 30080 came from the fact that some platforms have a restricted set of external ports available to expose. I believe we first saw this using GKE, but essentially, some platforms reserve low values in the port ranges for platform system processes, or exposing their own web UIs, and so we can't always guarantee that port 80 will be usable as the external facing port. Mapping port 30080 on the host through to 80 on the Ellis service however has worked well on other platforms, and just means you need to tweak the web address you access it over.
However, this may clash with some of the AKS behaviour, and if you are able to map directly through on port 80, that will work just fine. I suggest you lean more on AKS documentation on this side of things, as the Azure provided DNS etc may have special requirements or behaviours.
  *   The nodeport yaml configuration should look something like the below, taking Ellis as an example, with the 'nodePort' entry simply in addition to the existing 'name' and 'port' entries. i.e.:
apiVersion: v1
kind: Service
metadata:
  name: ellis
spec:
  type: NodePort
  ports:
  - name: "http"
    port: 80
    NodePort: 30080
  selector:
    service: ellis


  *   Bono may prove more challenging in the AKS environment, as it currently requires having a single static IP address. As the docs have it, we found the solution in GKE to be using a LoadBalancer type exposure, with static IP address. You will hopefully be able to do something very similar in AKS, where the IP of the Kubernetes host, already exposed by the AKS DNS records, is probably the right choice to start working with. I would suggest searching up a bit in the AKS documentation to see if there's anything in there about exposing services with a static IP, or potentially there are other support channels available for the Azure side of things.

I don't think we've quite reached an impasse on getting running with AKS here, but if you're keen to set up your own Kubernetes deployment that is definitely a possibility. As you've noted, there will be some limitations, particularly centred around networking. However, this is something we have definitely done before, so once those limitations are dealt with it should work as expected.
A few potentially helpful guiding points:

  *   If you are able to set up your Kubernetes cluster using a service network that is routable on the wider system network, potentially using simpler CNI network plugins, you can build a solution where your pod IPs are directly accessible, and so the need for any special NodePort or LoadBalancer options can be removed. This is very much a tech interest set up though, and not necessarily how we see things happening in the wider context of cloud native containerisation.
  *   You would potentially have to manage your own external DNS service if you want external components to be able to resolve your Kubernetes services. However, this should be as simple as running a simple DNS server on a box somewhere, and delegating your Kubernetes domain down to the Kubernetes DNS service. Again, there are likely to be many solutions to this problem, with lots of different information and guidance available. This will probably make it harder to get started, but should mean you're able to set up the solution you want.

Potentially, if you're able to let us know more about what sort of set up you're after, and what your requirements and available kit are, we may be able to help steer towards the right solution.

Hope this helps,
Adam


From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 18 May 2018 09:14
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,

Thanks for that.


  *   From the machine I'm running tests on, I can't ping bono.<ask-domain> or ellis.<aks-domain>. But I can open ellis.<aks-domain> in a browser (or with wget). I suspect this is because ping has no port number, so kubernetes doesn't know which ingress it should hand the packet to. `dig ellis.<aks-domain>` gives the ip address listed with `kubectl get ingress`
  *   Curl:
     *   `curl ellis.<ask-domain>:80` it returns a blank string immediately.
     *   `curl ellis.<ask-domain>:81` - times out
     *   `curl bono.<ask-domain>:5060` - times out
        *   Nothing in tcpdump
  *   When I run `kubectl get ingress` I only see port 80, even though I configured them to look on other ports. I suspect that ingresses only accept connections from port 80, and then on the internal side of the ingress they can work on any port. I don't know how to verify this.

Edit: It seems like this setup is temperamental. Yesterday and this morning I could open ellis in my browser and create an account. After tinkering around I've gone back to that setup, but it doesn't work. (timeout). Hmm.

My next step will be to install on a vanilla kubernetes environment. But I expect that will come with it's own challenges and even more limitations. (I won't have something exposing Kubernetes' DNS entries externally, and I won't be able to run tests on any real devices because this cluster will be in our corporate sandpit which is fenced off from everything).

I've tried to follow the alternative for DNS: https://github.com/Metaswitch/clearwater-docker#alternative-configuration-for-exposing-bono-and-ellis-services
I don't understand that part of the instructions.

  *   Where do I add the 'nodePort' line.

  *   Where did the number 30080 come from?

Regards,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 18 May 2018 3:05 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

I think we're making some solid progress, and I have a decent idea what was going on here. I think I have a better idea of what the AKS platform is doing, and should hopefully be able to explain why you were seeing the issues you were. Next steps on resolving your current issues below :)

So, first thing is the confusion over domains. I think there are two separate 'domains' here, as well as two separate DNS solutions doing slightly different things, and so the terms are a bit overloaded. As I see it there you have:

  *   AKS
     *   AKS domain, which is a globally routable web domain, '2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io', specific to your AKS account
     *   AKS DNS, which automatically creates entries based on Kubernetes ingress rules to provide access to specific pods at this domain, as you have set up for Ellis and Bono.
     *   This domain is what you originally passed in to the Project Clearwater pods, through the config map, and so is what our processes were attempting to connect to


  *   Kubernetes
     *   Kubernetes cluster domain, default.svc.cluster.local, which is a domain internal to the kubernetes cluster (the default value)
     *   Kubernetes DNS (probably kube-dns), where Kubernetes will automatically create records for all deployed pods and services, based on the configuration files you used in deploying.
     *   This domain is what is set in e.g. resolv.conf in your containers, and so is what you connect to if you run e.g. `nc cassandra 9160`

So, I think the way the issue you had getting the pods running manifested in such a hard to diagnose way is:

  *   You are able to contact the cassandra pods internally, e.g. using `nc cassandra 9160`, as this is going via the Kubernetes domain based DNS. i.e. cassandra.default.svc.cluster.local resolves to the internal IP of your cassandra pods.
  *   The clearwater processes were trying to use the AKS domain provided in `ZONE`, i.e. "cassandra.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io". As you noted, all the AKS DNS addresses resolve to the same IP. I believe this is the IP of the Kubernetes host, where the ingress rules you set up direct traffic for specific services through via the host IP.
  *   Because this resolved to an IP, we didn't see a DNS lookup failure, but it wasn't an IP at which we could connect to cassandra, because there are no ingress rules set up.

I think there's a simple solution here, based on the assumption that you want to be able to use the AKS domain as your subscriber home domain. We should simply be able to set that up as an additional home domain in the system.
To do this, we simply need to set up your configmap with the ZONE key set as `default.svc.cluster.local`, and the additional argument `--from-literal=ADDITIONAL_SHARED_CONFIG=additional_home_domains=2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io`

However, as you are currently provisioning subscribers through the Ellis web UI, and not integrating with an external HSS, I don't think we need to do this at the moment.
We can add it in if we see it become an issue down the line.


Next steps - Live tests
Good to see you've got the live test suite set up and running, though not successfully at the moment. In terms of the right command, I think the attempt you highlighted as taking a long time is the correct one. That is:
rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

You don't need to be running within the cluster for it to work; as long as your Bono and Ellis services are accessible from your test box, that will be fine.
While we're hitting errors, you can add ` TESTS="Basic call - mainline"` onto the command to simply run the first test only. This will limit the amount of error output you see on each run, and make it a bit easier to go about debugging.

The fact that the test command returned the error `Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060` suggests to me that there is potentially an issue in the ingress rules here. It is not returning a `getaddrinfo` error, so I believe the DNS is resolving to an IP, but that Bono is not reachable via that IP on port 5060.
I think your best next step here is to run some networking checks on this connection:

  *   From the box you are running the live tests on, make sure the bono.<aks domain> address resolves, the IP is reachable, and then try using `nc` to connect to port 5060.
  *   Run some tcpdumps in the bono pod to see if you are seeing any traffic reaching it on port 5060 when running the live tests

Hopefully this should help you track down at what point the traffic is getting stuck.

>From the Bono logs you've included, it looks like there's just no traffic reaching the process at all, and so I would suspect there's something unusual in the ingress rules setup. I don't know enough about the Azure http controller behaviour to be able to spot anything that looks particularly out of place at the moment though.
Equally, I am slightly worried that given that it is called 'http-application-routing' there may be some logic in place in the controller that will be interfering with non-http traffic. If we don't see any SIP traffic reaching the bono pod, we could potentially test if this is the case by using something like `curl` to send an http message to `bono.<aks domain>:5060`, and see if we see it reach the bono pod. If HTTP passes through, but SIP is blocked, I think we would need to dig some more into AKS configuration to work out what is going on there.

If you want to do testing with a sip client, I think you have most of the configuration correct, but I haven't used the Twinkle softclient. We have some docs on using sofclients, at http://clearwater.readthedocs.io/en/stable/Making_your_first_call.html, that you've probably already seen. Hopefully should be a decent guide. For the proxy, you'll want to use bono.<aks domain>, as that is what will resolve to the right IP address. However, if the  live tests are failing to connect to Bono, I think you will see similar issues down this path. We should focus on working out what is causing that problem first.

Hopefully once the connectivity issue to Bono is sorted, we should be fully up and running. Let us know how it goes, or if you have any questions on the distinction between the AKS domain and the Kubernetes/clearwater deployment domain.

Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 17 May 2018 09:32
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,


  *   `netstat -planut` shows that Cassandra is listening on 9160 from all ports
  *   `nc cassandra 9160` did not exit immediately. It just hangs without any output
  *   `ping cassandra` works, but `dig cassandra` shows no IP addresses

I tried changing my env-vars configmap from 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io to default.svc.cluster.local. This means that the pods do spin up successfully, and that error message goes away. What is env-vars supposed to be set to?
The docs say: "a ZONE key set to the domain of your Kubernetes cluster". So I set it to the domain of my cluster, and that seems to be the cause of all my problems. If I set it to something which isn't the domain of the cluster, the pods come up. So now I'm confused, is it supposed to be set to the domain of my cluster or not?
Is there some extra step I need to do to configure kubernetes to know that it's domain is 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io? I'm pretty sure it already knows, but how can I check?

Now all the pods are up. I can't make a call or run tests successfully. I wonder whether this is because my deployment has no idea what that it's domain is actually 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io.

In a browser, ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io takes me to the sign up page. I successfully created an account. I think this is good proof that the dns, ingresses and firewalls are set up properly. (albeit only for port 80, but I've checked the other ports. And by 'properly' I mean 'probably too loosely')

I've noticed that all my dns records (set up by Azure) point to the same ip address. Is this right, or not? (I still get confused by how kubernetes shares ip addresses around).
Here's my ellis ingress in kubernetes (required for Azure to set up dns for me)

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: ellis
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: ellis
            servicePort: 80
```

And for bono

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: bono
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: bono
            servicePort: 3478
        - backend:
            serviceName: bono
            servicePort: 5060
        - backend:
            serviceName: bono
            servicePort: 5062
```

I don't understand ingresses well. I'm not certain that this is right. But records appear in the dns server with names 'bono' and 'ellis' pointing to the ip address of something.
The example in the Azure docs<https://docs.microsoft.com/en-us/azure/aks/http-application-routing> has 'path: /`. I'm not sure whether I need that or not in my ingresses. With or without that line, the result is the same.

I've tried with ingresses for just bono and ellis, and also ingresses for every module (except etcd). The result is the same. (If I don't have ingresses for a module, it has no DNS record)



I have no idea what parameters to use when running the tests. The documentation doesn't say. Similarly I have no idea what to use when configuring my SIP client. What's the "domain"? What's the "proxy"?
Here's a bunch of permutations of test arguments which I ran, and the associated error messages:


  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
  *   rake test[default.svc.cluster.local] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   This one took a really long time before failing. So maybe it's the closest?
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'


When I try to register with a SIP client I get a bunch of 404 and 408 (timeout) error codes. Here's what I'm trying to use. I'm basing this off what worked for the AIO installation with this SIP client (Twinkle, one of the ones on Ubuntu)

  *   Your Name: Matthew
  *   User Name: 6505550375
     *   Not sip:6505550375
     *   Not 6505550375 at ...
  *   Domain: default.svc.cluster.local, or is it example.com? The Clearwater kubernetes readme says you should make a call like an AIO installation, which is example.com.
  *   Authentication name: 6505550375 at default.svc.cluster.local<mailto:6505550375 at default.svc.cluster.local>
  *   Password: the one generated by the web page. Not the one you use to sign into the web page.
  *   Proxy server: not sure. What should I use?
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   example.com
     *   default.svc.cluster.local

After running a bunch of failed tests and trying manually to register on a SIP client, here are the logs I see:


  *   /var/log/bono/bono_current.txt on the bono pod:
     *   17-05-2018 08:00:18.268 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 25
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:00:54.278 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 32
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:01:17.285 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 20
  *    Ellis: /var/log/ellis/ellis_20180517T070000Z.txt
     *
     *   17-05-2018 07:44:01.833 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.848 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/
     *   17-05-2018 07:44:01.857 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets/e74929ec-21ea-48ac-95b6-68115894313d
     *   17-05-2018 07:44:01.876 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:144: {"associated_implicit_registration_sets": ["e74929ec-21ea-48ac-95b6-68115894313d"]}
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles
     *   17-05-2018 07:44:01.913 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/public_ids/sip%3A6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.966 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/public/sip%3A6505550400%40default.svc.cluster.local/service_profile
     *   17-05-2018 07:44:01.976 UTC INFO homestead.py:255: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/filter_criteria
     *   17-05-2018 07:44:01.980 UTC INFO xdm.py:29: Sending HTTP PUT request to http://homer.default.svc.cluster.local:7888/org.etsi.ngn.simservs/users/sip%3A6505550400%40default.svc.cluster.local/simservs.xml
     *   17-05-2018 07:44:02.028 UTC INFO web.py:1447: 200 POST /accounts/live.tests at example.com/numbers/<mailto:/accounts/live.tests at example.com/numbers/> (0.0.0.0) 209.96ms

Here are my firewall rules. My understanding is that these are the rules for a firewall between pods and the outside world. I think that communications between pods don't get filtered by these rules, but I added all the inter-pod ports just in case.

Inbound and Outbound TCP: 22,2380,4000,80,443,5060,5062,3478,5058,5054,5052,9888,8888,8889,10888,7888,11211,7000,7253,11311,9160
Inbound and Outbound UDP: 161,162,5060,32768-65535,3478

Any ideas where to go from here? My main question is what command should I run for rake tests?
Should I try to run the rake tests from a test pod within the cluster?

Thanks,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV
M  0415 762 868  | E  Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Thursday, 17 May 2018 3:05 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

I've had a look into your issues deploying under kubernetes, and think I've narrowed down the search a good amount. Details below.
First though I'll take a run through the smaller questions you've raised below, as I think we can knock some of them off fairly quickly.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
[AJL] You shouldn't need it in the containers. In our VMs we found that some operations were fairly heavy on DNS queries, and so provided dnsmasq as a means of providing caching. The need for this has decreased quite a bit now, and as we move forward in the microservice space it makes less sense to have it around.

     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
This will likely be down to how the container networking works, but it won't cause any problems in running under kubernetes.

     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
[AJL] This is a benign error in this case, so we haven't dug down into it. It shouldn't impact your deployment at all.

  *   There are many other red error messages during build. Does that matter?
[AJL] No, these don't matter. I see a number of error messages during the build. A lot of these are dependencies of some of our packages raising error messages when being installed in this environment

  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
[AJL] The AIO has a number of different requirements that we don't have in the container environment, and vice versa, so the fact that there is a different set doesn't mean you have issues there
As a couple of examples for the differences you pulled out below:

  *   The auto config packages are used to enable specific setup in different environments. E.g. setting up required config files that other deployments do not rely on. In the docker case, we need to read environment variables and pass them through to the system configuration files
  *   Clearwater-etcd is our interface into an etcd cluster, and is used for sharing common configuration around a cluster. On the AIO this simply increases complexity, as we have no need for it, but in docker/kubernetes, we want the services to share configuration through an underlying etcd cluster.
I don't believe any of the differences you pulled out below would be leading to the issues you're seeing, but it's a nice analysis :)
The missing dns.json config file is also currently just a benign error in the containers, raised by some expectations we have when running in VMs.


So, on to the main issue:
In getting your deployment up and running, the key issue is definitely going to be the 'Failed to initialize the Cassandra store' one. This indicates the Homestead and Homestead-prov services have been unable to reach the cassandra cluster, and without that they won't be able to get up and running.
As you've already found, getting logs out and doing more detailed debugging in containers presents its own set of problems. To temporarily prevent kubernetes from automatically restarting pods, you can (as you may have already) remove the liveness checking sections from the generated deployment files. This isn't ideal, but it should allow you time to do some more detailed debugging. You may also want to reduce the number of replicas of the pods down to one for now, to make it easier to follow traffic. Should just be a case of changing the .depl files and redeploying.

The log indicates you're getting error code 3 back from attempting to setup the Cassandra store, which is a 'connection error', as opposed to what you would get if the dns was simply unresolvable (I have tested that on our internal setup, and the logs would be clearly stating it was unresolvable). This is definitely leading me to suspect network connectivity issues into the cassandra pods. The fact that you can ping between the two however suggests that it's more complex than simply being unable to send any traffic between them.

To test this, I tweaked the cassandra configuration (running just a single pod for simplicity) so that it was listening on a port that wasn't 9160, which is the port that both homestead and homestead prov attempt to connect to. With this setup in place, I saw both of these pods repeatedly failing with the same error code as yours.
Because of this I would suggest the following next steps in tracking down the problem:

  *   Double check that your networking configuration allows traffic through to the cassandra pods on port 9160. The homestead and homestead prov processes will require this.
  *   Connect to the Cassandra pods, and verify that they are indeed listening for traffic on port 9160
     *   `netstat -planut` is my default for this, but to each their own
     *   /var/log/cassandra/system.log may have more information if something strange is going on here
  *   Manually check the connectivity between the pods, where you could try
     *   Connect to e.g. a homestead pod, and try running `nc <cassandra hostname> 9160`. I'm seeing this instantly return if the port is not open/listened on.
     *   Install tcpdump on both sides, and see whether you see traffic arriving in the cassandra pods at all. This isn't a very container, but getting a dump from both cassandra and homestead sides, to see what point in the flow is failing would be a decent way to work out where the issue lies.

Sadly, I don't have access to an AKS setup right now, so can't investigate the ingress rules component at all. I suspect that there's something slightly unusual happening there that is leading to traffic being unable to reach port 9160.
Hopefully this should give you a better idea of what the source of the issue is here. If this doesn't show anything up, and you're seeing traffic from homestead arrive in the cassandra pods and going back out we may have to run some more testing

Cheers,
Adam


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 16 May 2018 04:35
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Richard,

Tldr: I've made some progress since I sent that email. I've found and fixed bugs to do with dnsmasq, but now I'm stuck.
The systemd logs for homestead still say: "Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."
I'm not even sure whether the dnsmasq bugs were related.

First, to answer your questions:

Yes this happens every time I deploy.

My configmap:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
Events:  <none>
```

I'm deploying on Azure's kubernetes service (AKS). I've enabled HTTP Application Routing<https://docs.microsoft.com/en-us/azure/aks/http-application-routing>, which means that Azure manages DNS for me. The only unusual step is that Azure requires that I add ingresses to do so. The DNS records point to each service (bono, ellis, sprout etc). I can ping "cassandra" from the homestead pod.

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:22:21Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.6", GitCommit:"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1", GitTreeState:"clean", BuildDate:"2018-03-21T15:13:31Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}

Getting the logs is a race against time, since kubernetes keeps terminating the pod while I'm in there.
After several attempts, I managed to get the homestead logs (same as homestead-prov logs)

```
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:123: Creating Cached Resolver using servers:
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:133:     10.0.0.10
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Error static_dns_cache.cpp:89: DNS config file /etc/clearwater/dns.json mis
sing
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status main.cpp:653: Using local impu store: astaire.2991678f-f9dd-4098-8fe
8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.129 UTC [7f6379c8e7c0] Status http_connection_pool.cpp:37: Connection pool will use calculated res
ponse timeout of 550ms
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:35: Configuring HTTP Connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:36:   Connection created for server sprout.2991678f
-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io:9888
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status main.cpp:1013: No HSS configured - using Homestead-prov
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:266: Configuring store connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:267:   Hostname:  cassandra.2991678f-f9dd-4098-8
fe8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:268:   Port:      9160
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:296: Configuring store worker pool
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:297:   Threads:   10
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:298:   Max Queue: 0
15-05-2018 07:05:40.131 UTC [7f6366ffd700] Status alarm.cpp:244: Reraising all alarms with a known state
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Error main.cpp:1056: Failed to initialize the Cassandra store with error co
de 3.
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Status main.cpp:1057: Homestead is shutting down
```

The full logs are just this chunk repeating over and over until everything shuts down.

I think the crucial error message is DNS config file /etc/clearwater/dns.json missing

In the all-in-one installation, that file is just:

{
  "hostnames": [
  ]
}

Is it ok to manually insert that using the Dockerfile? Or should there be other entries in there for a non-AIO installation?

dns.json is created by the dnsmasq service.

During the container build, there is a red error message saying "dnsmasq: setting capabilities failed: Operation not permitted"
(There are a lot of red error messages during build. Is that an issue?)

I noticed that dnsmasq was not running in homestead docker, yet it is running in my working all-in-one installation. (There are also a bunch of other services which are halted. Is that an issue?)

I tried starting dnsmasq manually, it wouldn't start. I looked online and the solution is to add user=root to the file /etc/dnsmasq.conf.

I tried modifying the Dockerfile to manually append that string to the file.  (I'm assuming that the order of lines in /etc/dnsmasq.conf does not matter)

Then dnsmasq still doesn't come up. I get an error about how the port it wants is already taken.

So then I tried modifying the Dockerfile to paste in dns.conf from the AIO installation. (The one with an empty list of hosts)

Now the missing file error is gone, but I still get the other error:

"Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."

How can I debug this?

All this time I thought that error was caused by dns issues. Is it because dnsmasq still won't start, so dns.json is missing hosts? Or is that unrelated?
"ping cassandra" works from the homestead pod.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
  *   There are many other red error messages during build. Does that matter?
  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
     *   Services running in AIO:
        *   [ ? ]  clearwater-auto-config-generic
        *    [ + ]  clearwater-cluster-manager
        *    [ + ]  clearwater-config-manager
        *    [ + ]  clearwater-diags-monitor
        *    [ - ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ ? ]  clearwater-memcached
        *    [ + ]  clearwater-queue-manager
     *   Services running (or not) in docker homestead:
        *   [ ? ]  clearwater-auto-config-docker
        *    [ + ]  clearwater-cluster-manager
        *    [ - ]  clearwater-config-manager
        *    [ - ]  clearwater-diags-monitor
        *    [ + ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ - ]  clearwater-queue-manager


Thanks,

Matthew Davis
Telstra | CTO | Cloud SDN NFV

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Wednesday, 16 May 2018 3:24 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Matthew,

Sorry to hear you are having problems deploying Clearwater on Docker.

I think that the socket factory error is actually fairly benign.

Does this happen every time you create your deployment?

Can you provide the config map that you are providing when deploying on Kubernetes?

Can you connect into one of the pods that is failing to run homestead-prov and grab the logs. The relevant ones will be under /var/log/homestead-prov inside the container.

We cannot reproduce your issue on our kubernetes setup - can you provide details of your kubernetes setup - what version of k8s and what networking configuration you are using?

>From the diagnostics we have at the moment, the most likely scenario is that the Homestead and Homestead Prov containers are unable to contact Cassandra - if that's the case they will continually restart to attempt to resolve the problem.



Richard


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 14 May 2018 07:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi everyone,
I've fixed the issue I had with homestead-prov and submitted the patch as this pull request: https://github.com/Metaswitch/clearwater-docker/pull/89

However now I just run into another issue, so homestead-prov is giving me the same error as homestead. The error message is not verbose. Does anyone know how to make it more verbose? I have no idea what's happening.


```
2018-05-14 05:47:41,369 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:41,370 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-sig entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-mgmt entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,477 INFO exited: snmpd (exit status 0; expected)
2018-05-14 05:47:42,478 CRIT reaped unknown pid 61)
2018-05-14 05:47:44,572 CRIT reaped unknown pid 65)
2018-05-14 05:47:44,576 CRIT reaped unknown pid 66)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 235)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 236)
2018-05-14 05:47:45,902 CRIT reaped unknown pid 257)
2018-05-14 05:47:48,153 CRIT reaped unknown pid 294)
2018-05-14 05:47:48,226 INFO spawned: 'homestead-prov' with pid 297
2018-05-14 05:47:48,237 INFO spawned: 'nginx' with pid 298
2018-05-14 05:47:48,747 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:49,750 INFO spawned: 'homestead-prov' with pid 324
2018-05-14 05:47:49,750 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:50,089 INFO exited: clearwater-infrastructure (exit status 0; expected)
2018-05-14 05:47:50,335 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:52,339 INFO spawned: 'homestead-prov' with pid 335
2018-05-14 05:47:53,324 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:56,330 INFO spawned: 'homestead-prov' with pid 346
2018-05-14 05:47:56,902 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:57,904 INFO gave up: homestead-prov entered FATAL state, too many start retries too quickly
```

Regards,
Matt Davis
Telstra | Graduate Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180524/e7e8b047/attachment.html>

From rajapaue at yahoo.com  Thu May 24 06:47:57 2018
From: rajapaue at yahoo.com (rajapaue at yahoo.com)
Date: Thu, 24 May 2018 10:47:57 +0000 (UTC)
Subject: [Project Clearwater] Stress testing failed in registration.
References: <791797139.676341.1527158877095.ref@mail.yahoo.com>
Message-ID: <791797139.676341.1527158877095@mail.yahoo.com>

Hi,?
I am trying to do a stress test on clearwater in AWS. I used chef to automate install.?
I have two problems during testing.?
When I want to install sipp using the command?
knife?box?create?-V?-E?ENVIRONMENT?sipp?--index?1

It says? ?knife encountered an unexpected errorThis may be a bug in the 'box create' knife command or pluginPlease collect the output of this command with the `-VV` option before filing a bug report.Exception: NoMethodError: undefined method `>' for nil:NilClass

Any ways if I delete the --index 1 from the command I can install it. but when I run a test I get the following error
sipp: The following events occurred:2018-05-23 13:24:18.329759 1527081858.329759: Aborting call on unexpected message for Call-Id '1-4325 at 172.31.42.209': while expecting '401' (index 1), received 'SIP/2.0 403 ForbiddenVia: SIP/2.0/TCP 172.31.42.209:36173;received=172.31.42.209;branch=z9hG4bK-4325-1-0Call-ID: 1-4325 at 172.31.42.209From: <sip:2010000000 at clearwater.amir.test.com>;tag=4325SIPpTag001To: <sip:2010000000 at clearwater.amir.test.com>;tag=z9hG4bKPjz9AXA2brIljLxzEMQ9Akc1jTESHj18SmCSeq: 1 REGISTERP-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"Content-Length:? 0

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180524/03310268/attachment.html>

From kapil.g at hcl.com  Thu May 24 08:28:54 2018
From: kapil.g at hcl.com (Kapil Gupta)
Date: Thu, 24 May 2018 12:28:54 +0000
Subject: [Project Clearwater] Calls are failure with SIP 503 Service
 Unavailable
Message-ID: <PS1PR04MB1676CA0CD326253C770A319EE46A0@PS1PR04MB1676.apcprd04.prod.outlook.com>

Hello,

I have deployed Clearwater IMS using https://github.com/Metaswitch/clearwater-heat with master branch on Vanilla OpenStack.
It deployed 7 VM's (Vellum, homer, sprout, bono, dime, ellis and ns).   All services are running fine from, check using "Monit summary"
We are able to create subscriber using Ellis and Registered  2 user (using Zoiper client 3.1)
But Calls are failling with SIP error 503 Service Unavailable
Please help us to debug it further.  Here i attached configuration files from /etc/clearwater for all VMs.
Please Note:--We are not able to capture detailed traces. However, I have set log_level=5, in user_setting
Logs:-
sprout/analytics.log:2018-05-24T11:41:58.979+00:00 Registration: USER_URI=sip:6505550138 at example.com BINDING_ID=sip:6505550138 at 192.168.0.11:58703;transport=UDP;rinstance=6bc106cd00583afa CONTACT_URI=sip:6505550138 at 192.168.0.11:58703;transport=UDP;rinstance=6bc106cd00583afa EXPIRES=3600
sprout/analytics.log:2018-05-24T11:46:51.899+00:00 Registration: USER_URI=sip:6505550398 at example.com BINDING_ID=sip:6505550398 at 192.168.0.9:39194;transport=UDP;rinstance=d83f28f400ac2933 CONTACT_URI=sip:6505550398 at 192.168.0.9:39194;transport=UDP;rinstance=d83f28f400ac2933 EXPIRES=3600
syslog:7095:May 24 11:47:35 bono-0 bono[10699]: <analytics> 2018-05-24T11:47:35.106+00:00 Call-Not-Connected: FROM=sip:6505550138 at example.com TO=sip:6505550398 at example.com CALL_ID=N2M1MjhlNDUzNDZkZjg5MWE1ODhhZjYyZDkxYTU3N2U. REASON=503
Thanks
Kapil Gupta

::DISCLAIMER::
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only. E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents (with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates. Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification, distribution and / or publication of this message without the prior written consent of authorized representative of HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately. Before opening any email and/or attachments, please check them for viruses and other defects.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180524/b55f727c/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: VM logs.zip
Type: application/x-zip-compressed
Size: 10866 bytes
Desc: VM logs.zip
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180524/b55f727c/attachment.bin>

From rajapaue at yahoo.com  Thu May 24 08:42:56 2018
From: rajapaue at yahoo.com (rajapaue at yahoo.com)
Date: Thu, 24 May 2018 12:42:56 +0000 (UTC)
Subject: [Project Clearwater] Stress testing failed in registration.
References: <574672071.719294.1527165776452.ref@mail.yahoo.com>
Message-ID: <574672071.719294.1527165776452@mail.yahoo.com>

Hi,?
I am trying to do a stress test on clearwater in AWS. I used chef to automate install.?
I have two problems during testing.?
When I want to install sipp using the command?
knife?box?create?-V?-E?ENVIRONMENT?sipp?--index?1

It says??knife encountered an unexpected errorThis may be a bug in the 'box create' knife command or pluginPlease collect the output of this command with the `-VV` option before filing a bug report.Exception: NoMethodError: undefined method `>' for nil:NilClass

Any ways if I delete the --index 1 from the command I can install it. but when I run a test I get the following error
sipp: The following events occurred:2018-05-23 13:24:18.329759 1527081858.329759: Aborting call on unexpected message for Call-Id '1-4325 at 172.31.42.209': while expecting '401' (index 1), received 'SIP/2.0 403 ForbiddenVia: SIP/2.0/TCP 172.31.42.209:36173;received=172.31.42.209;branch=z9hG4bK-4325-1-0Call-ID: 1-4325 at 172.31.42.209From: <sip:2010000000 at clearwater.amir.test.com>;tag=4325SIPpTag001To: <sip:2010000000 at clearwater.amir.test.com>;tag=z9hG4bKPjz9AXA2brIljLxzEMQ9Akc1jTESHj18SmCSeq: 1 REGISTERP-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"Content-Length:? 0


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180524/44da23db/attachment.html>

From Adam.Lindley at metaswitch.com  Thu May 24 13:04:31 2018
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Thu, 24 May 2018 17:04:31 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <ME2PR01MB2884F01278DC1CD5F7831DFEC26A0@ME2PR01MB2884.ausprd01.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971545314ED91FB7F233AC1E2900@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB288450EF460A39FEA9E1D724C2940@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<BY1PR0201MB09689BB9A1645396B16AE810E2940@BY1PR0201MB0968.namprd02.prod.outlook.com>
	<ME2PR01MB2884F01278DC1CD5F7831DFEC26A0@ME2PR01MB2884.ausprd01.prod.outlook.com>
Message-ID: <CY1PR0201MB09710B4690B23BC5F7E8F786E26A0@CY1PR0201MB0971.namprd02.prod.outlook.com>

Hey Matthew,

Sorry, I think I was a bit unclear, and have also noticed I missed out one step in my previous message. I'll try to clarify a bit here.

> you're having issues connecting to Bono because the service/pod is not exposed

By this, I meant that your Bono service is not exposed outside the kubernetes cluster. Other pods would have been able to reach it just fine, but your test setup was trying to reach into it from outside the host, and there was no config in place mapping it through to allow that to happen (e.g. nodePort, LoadBalancer type configuration).

The step I missed out was that we want to change the bono service port on the inside of the pod as well, to match the one we set up on the host. This is because Bono will be record-routing itself using the PUBLIC_IP and pcscf port, so if a SIP client tried to respond using this route, it would by default be sent back to port 5060. Because we can't open that up as a nodePort on the Kubernetes host, this would cause issues in the SIP flows.
As for why we are specifically interested in port 5060; this is the external P-CSCF port. I.e. this is the interface we want to send all our mainline SIP flows through. The others are used for webrtc and restund, neither of which we are particularly interested in at the moment, and neither of which are critical for getting calls through.

So, I think we just want to make a couple of tweaks to the bono-svc.yaml file. I'm going to copy bono yaml files in at the bottom. Simply we want the values of `port` and `nodePort` under the `-name: "5060"` section to match.
With the configuration as in my yaml files, and changing the `--pcscf` option in the bono pod, I see my live tests passing under the following command:
`rake test[default.svc.cw-k8s.test] PROXY=<k8s-host-IP PROXY_PORT=32060 ELLIS=<k8s-host-IP:30080`

Hopefully you see the same. From the Bono logs you've got below, I think the issue is simply that the above misconfiguration meant traffic couldn't reach the bono service correctly. I think this should be the last step in this set of hurdles; hopefully you see tests and calls working :)

Unfortunately, I don't have much experience using weave networking, so can't give much guidance there on how to open the pods up more to the wider network to make this of more use. And if in your testing you are seeing no packets hit any of the pods, that's going to be the first thing we need to debug, as we've probably missed a different piece of network configuration somewhere.
On the other thread, have you made any further progress in the Azure Kubernetes Service setup? I would still be very interested to see that up and working too.

Hope this helps. Cheers,
Adam


bono-svc.yaml:
```
apiVersion: v1
kind: Service
metadata:
  name: bono
spec:
  type: NodePort
  ports:
  - name: "3478"
    port: 3478
  - name: "5060"
    port: 32060
    nodePort: 32060
  - name: "5062"
    port: 5062
  selector:
    service: bono
```

bono-depl.yaml
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: bono
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bono
  template:
    metadata:
      labels:
        service: bono
        snmp: enabled
    spec:
      containers:
      - image:
        imagePullPolicy: Always
        name: bono
        ports:
        - containerPort: 22
        - containerPort: 3478
        - containerPort: 5060
        - containerPort: 5062
        - containerPort: 5060
          protocol: "UDP"
        - containerPort: 5062
          protocol: "UDP"
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PUBLIC_IP
          value: 10.230.16.1
        livenessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
          initialDelaySeconds: 30
        readinessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      - image: busybox
        name: tailer
        command: [ "tail", "-F", "/var/log/bono/bono_current.txt" ]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      volumes:
      - name: bonologs
        emptyDir: {}
      restartPolicy: Always
```


From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 24 May 2018 06:20
To: Adam Lindley <Adam.Lindley at metaswitch.com>; clearwater at lists.projectclearwater.org
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org>; kieran at aptira.com
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,

Thanks for that.

> you're having issues connecting to Bono because the service/pod is not exposed

What do you mean by that? I've run `kubectl apply -f bono-svc.yaml`. The service is exposed, isn't it?

$ kubectl get services
NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                               AGE
bono             ClusterIP   None         <none>        3478/TCP,5060/TCP,5062/TCP            1d

or after the pcscf changes:

NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                        AGE
bono             NodePort    10.108.162.246   <none>        3478:30078/TCP,5060:30060/TCP,5062:30062/TCP   41m

I've cc'ed in my colleague (Kieran) who set up the cluster for me. He said it's using weave net. Contrary to what I thought, nothing will be externally routable. So that's an issue. After adding the nodePorts I can view ellis in a browser (http://10.3.1.76:30080/login.html), and ` nc 10.3.1.76 30080 -v` shows that I can connect to the kubernetes cluster on port 30080. ` nc 10.3.1.76 30060 -v` says connection refused. When that happens I see a packet in tcpdump on the bono pod. I see nothing on tcpdump in bono on the relevant ports when I run the rake tests.

I'm curious about the ports for Bono. You keep mentioning port 5060. Is that the only port bono uses? What about 3478 and 5062?

I tried your suggestion for the pcscf thing. It didn't work.
The stdio for the rake test is below. How can I figure out whether the 403 error is for ellis or bono?
The last line of the rake test output says: "Error logs, including Call-IDs of failed calls, are in the 'logfiles' directory". But the logfiles directory is empty. Where can I find more verbose logs?

I ran tcpdump inside the bono and ellis pods during the test. If I filter by source IP address I see nothing, so I have to filter by port. The tcpdump command I'm using in bono is `tcpdump -a -vnni any port 5060 or port 5062 or port 3478 or port 30078 or port 30060 or port 30062`

Literally all ports are now open on the firewall.

Here is my bono-svc.yaml file:
```
apiVersion: v1
kind: Service
metadata:
  name: bono
spec:
  type: NodePort
  ports:
  - name: "3478"
    port: 3478
    nodePort: 30078
  - name: "5060"
    port: 5060
    nodePort: 30060
  - name: "5062"
    port: 5062
    nodePort: 30062
  selector:
    service: bono
```

Here is my bono-depl.yaml file:

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: bono
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bono
  template:
    metadata:
      labels:
        service: bono
        snmp: enabled
    spec:
      containers:
      - image: "mlda065/bono:latest"
        imagePullPolicy: Always
        name: bono
        ports:
        - containerPort: 22
        - containerPort: 3478
        - containerPort: 5060
        - containerPort: 5062
        - containerPort: 5060
          protocol: "UDP"
        - containerPort: 5062
          protocol: "UDP"
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PUBLIC_IP
          value: 10.3.1.76 #:6443
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      - image: busybox
        name: tailer
        command: [ "tail", "-F", "/var/log/bono/bono_current.txt" ]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      volumes:
      - name: bonologs
        emptyDir: {}
      imagePullSecrets:
      - name: myregistrykey
      restartPolicy: Always
```

Here is my env-vars:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
default.svc.cluster.local
Events:  <none>
```

Here are my services once deployed:

```
$ kubectl get service
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                        AGE
astaire          ClusterIP   None             <none>        11311/TCP                                      22m
bono             NodePort    10.108.162.246   <none>        3478:30078/TCP,5060:30060/TCP,5062:30062/TCP   22m
cassandra        ClusterIP   None             <none>        7001/TCP,7000/TCP,9042/TCP,9160/TCP            22m
chronos          ClusterIP   None             <none>        7253/TCP                                       22m
ellis            NodePort    10.97.76.168     <none>        80:30080/TCP                                   22m
etcd             ClusterIP   None             <none>        2379/TCP,2380/TCP,4001/TCP                     22m
homer            ClusterIP   None             <none>        7888/TCP                                       22m
homestead        ClusterIP   None             <none>        8888/TCP                                       22m
homestead-prov   ClusterIP   None             <none>        8889/TCP                                       22m
kubernetes       ClusterIP   10.96.0.1        <none>        443/TCP                                        2d
ralf             ClusterIP   None             <none>        10888/TCP                                      22m
sprout           ClusterIP   None             <none>        5052/TCP,5054/TCP                              22m
```


When I restarted the bono service there was a couple of warnings:

```
Defaulting container name to bono.
Use 'kubectl describe pod/bono-6dfc579b5-bdgzj -n default' to see all of the containers in this pod.
* Restarting Bono SIP Edge Proxy bono
/etc/init.d/bono: line 63: ulimit: open files: cannot modify limit: Operation not permitted
/etc/init.d/bono: line 64: ulimit: open files: cannot modify limit: Invalid argument
23-05-2018 06:21:52.570 UTC [7f2b793ec7c0] Status utils.cpp:651: Switching to daemon mode
   ...done.
```

Here's the result of the test:

```
$rake test[default.svc.cw-k8s.test] PROXY=10.3.1.76 PROXY_PORT=30060 SIGNUP_CODE='secret' ELLIS=10.3.1.76:30080 TESTS="Basic call - mainline"
Basic Call - Mainline (TCP) - Failed
  RestClient::Forbidden thrown:
   - 403 Forbidden
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/abstract_response.rb:74:in `return
!'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:495:in `process_result'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:421:in `block in transm
it'
     - /usr/lib/ruby/2.3.0/net/http.rb:853:in `start'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:413:in `transmit'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:176:in `execute'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient.rb:69:in `post'
```

Here is /var/log/bono/bono_current.txt in the bono pod:

```
24-05-2018 05:17:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
24-05-2018 05:17:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1012.3 alarm
24-05-2018 05:17:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1013.3 alarm
24-05-2018 05:17:33.337 UTC [7f2b4b7ee700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 11
24-05-2018 05:17:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
24-05-2018 05:17:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1012.3 alarm
24-05-2018 05:17:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1013.3 alarm
24-05-2018 05:18:21.346 UTC [7f2b4b7ee700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 36
24-05-2018 05:18:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
24-05-2018 05:18:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1012.3 alarm
24-05-2018 05:18:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1013.3 alarm
24-05-2018 05:18:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
24-05-2018 05:18:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1012.3 alarm
24-05-2018 05:18:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1013.3 alarm
24-05-2018 05:18:57.350 UTC [7f2b4b7ee700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 35
```

Thanks,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Wednesday, 23 May 2018 1:21 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

Sounds like interesting progress on the Azure side. I'd be really interested in the details of how you get it up and running if this approach works, and we'd be really happy to be able to integrate some steps into our docs to expand the platforms we can easily get running on. Do update with any progress you make there :)

As for your vanilla Kubernetes setup, as far as I can see you're having issues connecting to Bono because the service/pod is not exposed; you're trying to connect to port 5060 on your Kubernetes host, without any service set up on the port. This is why your tests and sip client are failing, and nothing is hitting the Bono process.
Can you give some more information on the network setup of your Kubernetes rig? What network plugin/CNI have you configured it with? And are the pod networks purely internal to the cluster? Or have you set them up as externally routable?

As documented in the clearwater-docker README<https://github.com/Metaswitch/clearwater-docker/blob/master/README.md#alternative-configuration-for-exposing-bono-and-ellis-services>, exposing Bono presents a few challenges. In our internal setups, we have in the past either ended up using GRE tunnels to route traffic directly into pod networks (not simple, and a fair pain to set up), or have deployed Kubernetes using fairly basic network infrastructure, and making the pod networks directly routable using the kubenet plugin and some simple forwarding rules. This means that we are able to route directly into the bono pod.
In your case, you may well have deployed with an overlay network, and an internal only pod network, which makes things a bit harder.

The right solution here is to find a way to route traffic through on port 5060 direct into your bono pod. As in our README, GKE provides a load balancer that is able to do this, and internally we've used the above methods. I'm not sure what the best approach available on current Kubernetes is.

However, as a stop gap to get you a bit further forward for now, I have played around and have a suggestion for something that may work for now. With a few manual tweaks we should be able to set bono up to listen as the P-CSCF on a different port, expose that as a nodeport, and have bono view itself as recording the same IP as the Kubernetes host.
To do this, try the following steps:

  *   Set up the bono service with a NodePort (similar to ellis). This will be something like port 30060.
  *   Modify the bono-depl file:
     *   add `name: PUBLIC_IP` and `value: <k8s host IP>` to the `env:` section.
     *   Remove the liveness and readiness probes relating to port 5060. We will be changing the port bono listens on manually, and without this change the containers will be killed and restarted under our feet
  *   Deploy all the pods and services
  *   Connect to the bono pod, and modify `/etc/init.d/bono`
     *   Change the value of the line `--pcscf=5060,5058` to `--pcscf=<nodePort>,5058` , with the same nodeport you set in bono-svc
  *   Restart the bono service using `service bono restart` from within the container

This should leave your bono pod able to receive traffic through a nodePort connection on the Kubernetes host IP. You should be able to run the live tests with the command below, and should be able to get a GUI SIP client registering as well.

rake test[default.svc.cw-k8s.test] PROXY=10.3.1.76 PROXY_PORT=<bono nodeport> SIGNUP_CODE='secret' ELLIS=10.3.1.76:30080

If you get more connection errors, as in your below error output, try tcpdumping in different locations along the call path to find out where traffic is getting dropped. The test failure output should give a reasonable clue as to what is going wrong.

Keep us updated. Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 22 May 2018 09:15
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,

I heard back from a contact we have in Microsoft. They seemed to suggest that selecting 'advanced networking' instead of 'basic' (which is what I was using) might make each pod directly addressable, but from within an azure vnet (so I'd have to find a way to expose that to the public internet for my SIP clients). I'm halfway through trying that at the moment. I'll update that thread once I run some rake tests.

As far as a vanilla kubernetes installation goes, I tried that today and couldn't get it working. Can you help me with that?
I'm trying to just get the simplest setup possible. For now my goal is just to get something working somewhere. (Incidentally this is why I initially tried Azure Kubernetes Service with Application Routing)

So in my vanilla installation (i.e. kubernetes on openstack) I just used 1 bono pod. (Once I get this simple install working, then I'll try more complicated setups)

How am I supposed to run the rake tests?

My cluster is at 10.3.1.76. I changed ellis-svc to a nodePort type.

I'm trying:

rake test[default.svc.cluster.local] PROXY="10.3.1.76" SIGNUP_CODE=secret ELLIS=10.3.1.76:30080 TESTS="Basic call - mainline"

The error message is:

Basic Call - Mainline (TCP) - Failed
  Errno::ECONNREFUSED thrown:
   - Connection refused - connect(2) for "10.3.1.76" port 5060
     - /home/ubuntu/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     - /home/ubuntu/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     - /home/ubuntu/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     - /home/ubuntu/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
     - /home/ubuntu/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new_source'

The logs in bono in /var/log/bono/bono_current.txt did not show any additional lines during the test. The logs are currently:

22-05-2018 08:11:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1012.3 alarm
22-05-2018 08:11:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1013.3 alarm
22-05-2018 08:12:02.410 UTC [7f59f0ff1700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 20
22-05-2018 08:12:04.667 UTC [7f5a15023700] Status alarm.cpp:244: Reraising all alarms with a known state
22-05-2018 08:12:04.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1012.3 alarm
22-05-2018 08:12:04.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1013.3 alarm
22-05-2018 08:12:26.414 UTC [7f59f0ff1700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 5
22-05-2018 08:12:34.667 UTC [7f5a15023700] Status alarm.cpp:244: Reraising all alarms with a known state
22-05-2018 08:12:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1012.3 alarm
22-05-2018 08:12:34.667 UTC [7f5a15023700] Status alarm.cpp:37: sprout issued 1013.3 alarm

When I try manually in a GUI SIP client I get a 503 error.

I haven't done any DNS stuff for this deployment. As I said, I'm trying to keep it simple, just so that I have something working. My understanding is that I only need the DNS server if I want SIP clients to access the server via a domain name instead of an IP address. Is that correct?

When I run `nc 10.3.1.76 30080 -v` it connects successfully.
When I run `nc 10.3.1.76 5060 -v`  fails (connection refused), both from my laptop and from one of the pods which isn't bono. Since I get 'connection refused' when running that from within the cluster, that means it's not a firewall issue. (The firewall surrounds the whole cluster, and doesn't get between pods).

Thanks,
Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Saturday, 19 May 2018 3:46 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

No worries. Glad we're making headway on this. It will be useful to work out any of the kinks in using platforms like AKS so that we can continue to deploy easily in as many situations as possible.

>From the info you've sent below, it looks like there's an issue in the AKS configuration/ingress rules meaning there isn't anything in place for Bono. Without a rule to pass traffic through port 5060 to Bono, we're going to have a tough time getting calls running.
>From my experience with them, I think ingress rules should work on any port combinations, not just port 80. I would however suggest, if possible, clearing out the existing rules and then re-configuring the Ellis and Bono ingress rules from scratch, so that we can eliminate anything unusual that might have got in place while running through the other issues.
If you do this, and things still seem out of place, send over some more details on what the setup you're following is, and some diagnostics/info on what it all looks like.

To answer your direct questions:

  *   The port number 30080 came from the fact that some platforms have a restricted set of external ports available to expose. I believe we first saw this using GKE, but essentially, some platforms reserve low values in the port ranges for platform system processes, or exposing their own web UIs, and so we can't always guarantee that port 80 will be usable as the external facing port. Mapping port 30080 on the host through to 80 on the Ellis service however has worked well on other platforms, and just means you need to tweak the web address you access it over.
However, this may clash with some of the AKS behaviour, and if you are able to map directly through on port 80, that will work just fine. I suggest you lean more on AKS documentation on this side of things, as the Azure provided DNS etc may have special requirements or behaviours.
  *   The nodeport yaml configuration should look something like the below, taking Ellis as an example, with the 'nodePort' entry simply in addition to the existing 'name' and 'port' entries. i.e.:
apiVersion: v1
kind: Service
metadata:
  name: ellis
spec:
  type: NodePort
  ports:
  - name: "http"
    port: 80
    NodePort: 30080
  selector:
    service: ellis


  *   Bono may prove more challenging in the AKS environment, as it currently requires having a single static IP address. As the docs have it, we found the solution in GKE to be using a LoadBalancer type exposure, with static IP address. You will hopefully be able to do something very similar in AKS, where the IP of the Kubernetes host, already exposed by the AKS DNS records, is probably the right choice to start working with. I would suggest searching up a bit in the AKS documentation to see if there's anything in there about exposing services with a static IP, or potentially there are other support channels available for the Azure side of things.

I don't think we've quite reached an impasse on getting running with AKS here, but if you're keen to set up your own Kubernetes deployment that is definitely a possibility. As you've noted, there will be some limitations, particularly centred around networking. However, this is something we have definitely done before, so once those limitations are dealt with it should work as expected.
A few potentially helpful guiding points:

  *   If you are able to set up your Kubernetes cluster using a service network that is routable on the wider system network, potentially using simpler CNI network plugins, you can build a solution where your pod IPs are directly accessible, and so the need for any special NodePort or LoadBalancer options can be removed. This is very much a tech interest set up though, and not necessarily how we see things happening in the wider context of cloud native containerisation.
  *   You would potentially have to manage your own external DNS service if you want external components to be able to resolve your Kubernetes services. However, this should be as simple as running a simple DNS server on a box somewhere, and delegating your Kubernetes domain down to the Kubernetes DNS service. Again, there are likely to be many solutions to this problem, with lots of different information and guidance available. This will probably make it harder to get started, but should mean you're able to set up the solution you want.

Potentially, if you're able to let us know more about what sort of set up you're after, and what your requirements and available kit are, we may be able to help steer towards the right solution.

Hope this helps,
Adam


From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 18 May 2018 09:14
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,

Thanks for that.


  *   From the machine I'm running tests on, I can't ping bono.<ask-domain> or ellis.<aks-domain>. But I can open ellis.<aks-domain> in a browser (or with wget). I suspect this is because ping has no port number, so kubernetes doesn't know which ingress it should hand the packet to. `dig ellis.<aks-domain>` gives the ip address listed with `kubectl get ingress`
  *   Curl:
     *   `curl ellis.<ask-domain>:80` it returns a blank string immediately.
     *   `curl ellis.<ask-domain>:81` - times out
     *   `curl bono.<ask-domain>:5060` - times out
        *   Nothing in tcpdump
  *   When I run `kubectl get ingress` I only see port 80, even though I configured them to look on other ports. I suspect that ingresses only accept connections from port 80, and then on the internal side of the ingress they can work on any port. I don't know how to verify this.

Edit: It seems like this setup is temperamental. Yesterday and this morning I could open ellis in my browser and create an account. After tinkering around I've gone back to that setup, but it doesn't work. (timeout). Hmm.

My next step will be to install on a vanilla kubernetes environment. But I expect that will come with it's own challenges and even more limitations. (I won't have something exposing Kubernetes' DNS entries externally, and I won't be able to run tests on any real devices because this cluster will be in our corporate sandpit which is fenced off from everything).

I've tried to follow the alternative for DNS: https://github.com/Metaswitch/clearwater-docker#alternative-configuration-for-exposing-bono-and-ellis-services
I don't understand that part of the instructions.

  *   Where do I add the 'nodePort' line.

  *   Where did the number 30080 come from?

Regards,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 18 May 2018 3:05 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

I think we're making some solid progress, and I have a decent idea what was going on here. I think I have a better idea of what the AKS platform is doing, and should hopefully be able to explain why you were seeing the issues you were. Next steps on resolving your current issues below :)

So, first thing is the confusion over domains. I think there are two separate 'domains' here, as well as two separate DNS solutions doing slightly different things, and so the terms are a bit overloaded. As I see it there you have:

  *   AKS
     *   AKS domain, which is a globally routable web domain, '2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io', specific to your AKS account
     *   AKS DNS, which automatically creates entries based on Kubernetes ingress rules to provide access to specific pods at this domain, as you have set up for Ellis and Bono.
     *   This domain is what you originally passed in to the Project Clearwater pods, through the config map, and so is what our processes were attempting to connect to


  *   Kubernetes
     *   Kubernetes cluster domain, default.svc.cluster.local, which is a domain internal to the kubernetes cluster (the default value)
     *   Kubernetes DNS (probably kube-dns), where Kubernetes will automatically create records for all deployed pods and services, based on the configuration files you used in deploying.
     *   This domain is what is set in e.g. resolv.conf in your containers, and so is what you connect to if you run e.g. `nc cassandra 9160`

So, I think the way the issue you had getting the pods running manifested in such a hard to diagnose way is:

  *   You are able to contact the cassandra pods internally, e.g. using `nc cassandra 9160`, as this is going via the Kubernetes domain based DNS. i.e. cassandra.default.svc.cluster.local resolves to the internal IP of your cassandra pods.
  *   The clearwater processes were trying to use the AKS domain provided in `ZONE`, i.e. "cassandra.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io". As you noted, all the AKS DNS addresses resolve to the same IP. I believe this is the IP of the Kubernetes host, where the ingress rules you set up direct traffic for specific services through via the host IP.
  *   Because this resolved to an IP, we didn't see a DNS lookup failure, but it wasn't an IP at which we could connect to cassandra, because there are no ingress rules set up.

I think there's a simple solution here, based on the assumption that you want to be able to use the AKS domain as your subscriber home domain. We should simply be able to set that up as an additional home domain in the system.
To do this, we simply need to set up your configmap with the ZONE key set as `default.svc.cluster.local`, and the additional argument `--from-literal=ADDITIONAL_SHARED_CONFIG=additional_home_domains=2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io`

However, as you are currently provisioning subscribers through the Ellis web UI, and not integrating with an external HSS, I don't think we need to do this at the moment.
We can add it in if we see it become an issue down the line.


Next steps - Live tests
Good to see you've got the live test suite set up and running, though not successfully at the moment. In terms of the right command, I think the attempt you highlighted as taking a long time is the correct one. That is:
rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io

You don't need to be running within the cluster for it to work; as long as your Bono and Ellis services are accessible from your test box, that will be fine.
While we're hitting errors, you can add ` TESTS="Basic call - mainline"` onto the command to simply run the first test only. This will limit the amount of error output you see on each run, and make it a bit easier to go about debugging.

The fact that the test command returned the error `Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060` suggests to me that there is potentially an issue in the ingress rules here. It is not returning a `getaddrinfo` error, so I believe the DNS is resolving to an IP, but that Bono is not reachable via that IP on port 5060.
I think your best next step here is to run some networking checks on this connection:

  *   From the box you are running the live tests on, make sure the bono.<aks domain> address resolves, the IP is reachable, and then try using `nc` to connect to port 5060.
  *   Run some tcpdumps in the bono pod to see if you are seeing any traffic reaching it on port 5060 when running the live tests

Hopefully this should help you track down at what point the traffic is getting stuck.

>From the Bono logs you've included, it looks like there's just no traffic reaching the process at all, and so I would suspect there's something unusual in the ingress rules setup. I don't know enough about the Azure http controller behaviour to be able to spot anything that looks particularly out of place at the moment though.
Equally, I am slightly worried that given that it is called 'http-application-routing' there may be some logic in place in the controller that will be interfering with non-http traffic. If we don't see any SIP traffic reaching the bono pod, we could potentially test if this is the case by using something like `curl` to send an http message to `bono.<aks domain>:5060`, and see if we see it reach the bono pod. If HTTP passes through, but SIP is blocked, I think we would need to dig some more into AKS configuration to work out what is going on there.

If you want to do testing with a sip client, I think you have most of the configuration correct, but I haven't used the Twinkle softclient. We have some docs on using sofclients, at http://clearwater.readthedocs.io/en/stable/Making_your_first_call.html, that you've probably already seen. Hopefully should be a decent guide. For the proxy, you'll want to use bono.<aks domain>, as that is what will resolve to the right IP address. However, if the  live tests are failing to connect to Bono, I think you will see similar issues down this path. We should focus on working out what is causing that problem first.

Hopefully once the connectivity issue to Bono is sorted, we should be fully up and running. Let us know how it goes, or if you have any questions on the distinction between the AKS domain and the Kubernetes/clearwater deployment domain.

Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 17 May 2018 09:32
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,


  *   `netstat -planut` shows that Cassandra is listening on 9160 from all ports
  *   `nc cassandra 9160` did not exit immediately. It just hangs without any output
  *   `ping cassandra` works, but `dig cassandra` shows no IP addresses

I tried changing my env-vars configmap from 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io to default.svc.cluster.local. This means that the pods do spin up successfully, and that error message goes away. What is env-vars supposed to be set to?
The docs say: "a ZONE key set to the domain of your Kubernetes cluster". So I set it to the domain of my cluster, and that seems to be the cause of all my problems. If I set it to something which isn't the domain of the cluster, the pods come up. So now I'm confused, is it supposed to be set to the domain of my cluster or not?
Is there some extra step I need to do to configure kubernetes to know that it's domain is 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io? I'm pretty sure it already knows, but how can I check?

Now all the pods are up. I can't make a call or run tests successfully. I wonder whether this is because my deployment has no idea what that it's domain is actually 2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io.

In a browser, ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io takes me to the sign up page. I successfully created an account. I think this is good proof that the dns, ingresses and firewalls are set up properly. (albeit only for port 80, but I've checked the other ports. And by 'properly' I mean 'probably too loosely')

I've noticed that all my dns records (set up by Azure) point to the same ip address. Is this right, or not? (I still get confused by how kubernetes shares ip addresses around).
Here's my ellis ingress in kubernetes (required for Azure to set up dns for me)

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: ellis
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: ellis
            servicePort: 80
```

And for bono

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
   name: bono
   annotations:
      kubernetes.io/ingress.class: addon-http-application-routing
spec:
   rules:
   - host: bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     http:
        paths:
        - backend:
            serviceName: bono
            servicePort: 3478
        - backend:
            serviceName: bono
            servicePort: 5060
        - backend:
            serviceName: bono
            servicePort: 5062
```

I don't understand ingresses well. I'm not certain that this is right. But records appear in the dns server with names 'bono' and 'ellis' pointing to the ip address of something.
The example in the Azure docs<https://docs.microsoft.com/en-us/azure/aks/http-application-routing> has 'path: /`. I'm not sure whether I need that or not in my ingresses. With or without that line, the result is the same.

I've tried with ingresses for just bono and ellis, and also ingresses for every module (except etcd). The result is the same. (If I don't have ingresses for a module, it has no DNS record)



I have no idea what parameters to use when running the tests. The documentation doesn't say. Similarly I have no idea what to use when configuring my SIP client. What's the "domain"? What's the "proxy"?
Here's a bunch of permutations of test arguments which I ran, and the associated error messages:


  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - Failed to open TCP connection to ellis.default.svc.cluster.local:80 (getaddrinfo: Name or service not known)
     *        - /usr/lib/ruby/2.3.0/net/http.rb:882:in `rescue in block in connect'
     *        - /usr/lib/ruby/2.3.0/net/http.rb:879:in `block in connect'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
     *        - /usr/lib/ruby/2.3.0/timeout.rb:101:in `timeout'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[default.svc.cluster.local] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[default.svc.cluster.local] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:175:in `execute'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.default.svc.cluster.local" SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY="bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io " SIGNUP_CODE=secret ELLIS="ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io"
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     URI::InvalidURIError thrown:
     *      - URI must be ascii only "http://\u{201d}ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io\u{201d}/accounts<http://u%7b201d%7dellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io/u%7b201d%7d/accounts>"
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:21:in `split'
     *        - /usr/lib/ruby/2.3.0/uri/rfc3986_parser.rb:73:in `parse'
     *        - /usr/lib/ruby/2.3.0/uri/common.rb:227:in `parse'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:276:in `parse_url'
     *        - /home/matthew/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:280:in `parse_url_with_auth'
  *   rake test[default.svc.cluster.local] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - ESC[37;41mFailedESC[37;0m
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
  *   rake test[default.svc.cluster.local] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   This one took a really long time before failing. So maybe it's the closest?
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.default.svc.cluster.local SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     SocketError thrown:
     *      - getaddrinfo: Name or service not known
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'
  *   rake test[2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io] PROXY=bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io SIGNUP_CODE=secret ELLIS=ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   Basic Call - Mainline (TCP) - Failed
     *     Errno::ETIMEDOUT thrown:
     *      - Connection timed out - connect(2) for "bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io" port 5060
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `new'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/sources.rb:41:in `initialize'
     *        - /media/sf_MyOneDrive/SDN/clearwater/k8s-test/clearwater-live-test/quaff/lib/endpoint.rb:290:in `new'


When I try to register with a SIP client I get a bunch of 404 and 408 (timeout) error codes. Here's what I'm trying to use. I'm basing this off what worked for the AIO installation with this SIP client (Twinkle, one of the ones on Ubuntu)

  *   Your Name: Matthew
  *   User Name: 6505550375
     *   Not sip:6505550375
     *   Not 6505550375 at ...
  *   Domain: default.svc.cluster.local, or is it example.com? The Clearwater kubernetes readme says you should make a call like an AIO installation, which is example.com.
  *   Authentication name: 6505550375 at default.svc.cluster.local<mailto:6505550375 at default.svc.cluster.local>
  *   Password: the one generated by the web page. Not the one you use to sign into the web page.
  *   Proxy server: not sure. What should I use?
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   ellis-manual.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   bono.2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
     *   example.com
     *   default.svc.cluster.local

After running a bunch of failed tests and trying manually to register on a SIP client, here are the logs I see:


  *   /var/log/bono/bono_current.txt on the bono pod:
     *   17-05-2018 08:00:18.268 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 25
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:26.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:00:54.278 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 32
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:244: Reraising all alarms with a known state
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1012.3 alarm
     *   17-05-2018 08:00:56.632 UTC [7fc5a88b9700] Status alarm.cpp:37: sprout issued 1013.3 alarm
     *   17-05-2018 08:01:17.285 UTC [7fc5847f0700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 20
  *    Ellis: /var/log/ellis/ellis_20180517T070000Z.txt
     *
     *   17-05-2018 07:44:01.833 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.848 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/
     *   17-05-2018 07:44:01.857 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets/e74929ec-21ea-48ac-95b6-68115894313d
     *   17-05-2018 07:44:01.876 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/private/6505550400%40default.svc.cluster.local/associated_implicit_registration_sets
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:144: {"associated_implicit_registration_sets": ["e74929ec-21ea-48ac-95b6-68115894313d"]}
     *   17-05-2018 07:44:01.883 UTC INFO homestead.py:268: Sending HTTP POST request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles
     *   17-05-2018 07:44:01.913 UTC INFO homestead.py:268: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/public_ids/sip%3A6505550400%40default.svc.cluster.local
     *   17-05-2018 07:44:01.966 UTC INFO homestead.py:268: Sending HTTP GET request to http://homestead-prov.default.svc.cluster.local:8889/public/sip%3A6505550400%40default.svc.cluster.local/service_profile
     *   17-05-2018 07:44:01.976 UTC INFO homestead.py:255: Sending HTTP PUT request to http://homestead-prov.default.svc.cluster.local:8889/irs/e74929ec-21ea-48ac-95b6-68115894313d/service_profiles/10dfc27a-9941-4425-b644-bd89c0b32607/filter_criteria
     *   17-05-2018 07:44:01.980 UTC INFO xdm.py:29: Sending HTTP PUT request to http://homer.default.svc.cluster.local:7888/org.etsi.ngn.simservs/users/sip%3A6505550400%40default.svc.cluster.local/simservs.xml
     *   17-05-2018 07:44:02.028 UTC INFO web.py:1447: 200 POST /accounts/live.tests at example.com/numbers/<mailto:/accounts/live.tests at example.com/numbers/> (0.0.0.0) 209.96ms

Here are my firewall rules. My understanding is that these are the rules for a firewall between pods and the outside world. I think that communications between pods don't get filtered by these rules, but I added all the inter-pod ports just in case.

Inbound and Outbound TCP: 22,2380,4000,80,443,5060,5062,3478,5058,5054,5052,9888,8888,8889,10888,7888,11211,7000,7253,11311,9160
Inbound and Outbound UDP: 161,162,5060,32768-65535,3478

Any ideas where to go from here? My main question is what command should I run for rake tests?
Should I try to run the rake tests from a test pod within the cluster?

Thanks,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV
M  0415 762 868  | E  Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Thursday, 17 May 2018 3:05 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

I've had a look into your issues deploying under kubernetes, and think I've narrowed down the search a good amount. Details below.
First though I'll take a run through the smaller questions you've raised below, as I think we can knock some of them off fairly quickly.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
[AJL] You shouldn't need it in the containers. In our VMs we found that some operations were fairly heavy on DNS queries, and so provided dnsmasq as a means of providing caching. The need for this has decreased quite a bit now, and as we move forward in the microservice space it makes less sense to have it around.

     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
This will likely be down to how the container networking works, but it won't cause any problems in running under kubernetes.

     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
[AJL] This is a benign error in this case, so we haven't dug down into it. It shouldn't impact your deployment at all.

  *   There are many other red error messages during build. Does that matter?
[AJL] No, these don't matter. I see a number of error messages during the build. A lot of these are dependencies of some of our packages raising error messages when being installed in this environment

  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
[AJL] The AIO has a number of different requirements that we don't have in the container environment, and vice versa, so the fact that there is a different set doesn't mean you have issues there
As a couple of examples for the differences you pulled out below:

  *   The auto config packages are used to enable specific setup in different environments. E.g. setting up required config files that other deployments do not rely on. In the docker case, we need to read environment variables and pass them through to the system configuration files
  *   Clearwater-etcd is our interface into an etcd cluster, and is used for sharing common configuration around a cluster. On the AIO this simply increases complexity, as we have no need for it, but in docker/kubernetes, we want the services to share configuration through an underlying etcd cluster.
I don't believe any of the differences you pulled out below would be leading to the issues you're seeing, but it's a nice analysis :)
The missing dns.json config file is also currently just a benign error in the containers, raised by some expectations we have when running in VMs.


So, on to the main issue:
In getting your deployment up and running, the key issue is definitely going to be the 'Failed to initialize the Cassandra store' one. This indicates the Homestead and Homestead-prov services have been unable to reach the cassandra cluster, and without that they won't be able to get up and running.
As you've already found, getting logs out and doing more detailed debugging in containers presents its own set of problems. To temporarily prevent kubernetes from automatically restarting pods, you can (as you may have already) remove the liveness checking sections from the generated deployment files. This isn't ideal, but it should allow you time to do some more detailed debugging. You may also want to reduce the number of replicas of the pods down to one for now, to make it easier to follow traffic. Should just be a case of changing the .depl files and redeploying.

The log indicates you're getting error code 3 back from attempting to setup the Cassandra store, which is a 'connection error', as opposed to what you would get if the dns was simply unresolvable (I have tested that on our internal setup, and the logs would be clearly stating it was unresolvable). This is definitely leading me to suspect network connectivity issues into the cassandra pods. The fact that you can ping between the two however suggests that it's more complex than simply being unable to send any traffic between them.

To test this, I tweaked the cassandra configuration (running just a single pod for simplicity) so that it was listening on a port that wasn't 9160, which is the port that both homestead and homestead prov attempt to connect to. With this setup in place, I saw both of these pods repeatedly failing with the same error code as yours.
Because of this I would suggest the following next steps in tracking down the problem:

  *   Double check that your networking configuration allows traffic through to the cassandra pods on port 9160. The homestead and homestead prov processes will require this.
  *   Connect to the Cassandra pods, and verify that they are indeed listening for traffic on port 9160
     *   `netstat -planut` is my default for this, but to each their own
     *   /var/log/cassandra/system.log may have more information if something strange is going on here
  *   Manually check the connectivity between the pods, where you could try
     *   Connect to e.g. a homestead pod, and try running `nc <cassandra hostname> 9160`. I'm seeing this instantly return if the port is not open/listened on.
     *   Install tcpdump on both sides, and see whether you see traffic arriving in the cassandra pods at all. This isn't a very container, but getting a dump from both cassandra and homestead sides, to see what point in the flow is failing would be a decent way to work out where the issue lies.

Sadly, I don't have access to an AKS setup right now, so can't investigate the ingress rules component at all. I suspect that there's something slightly unusual happening there that is leading to traffic being unable to reach port 9160.
Hopefully this should give you a better idea of what the source of the issue is here. If this doesn't show anything up, and you're seeing traffic from homestead arrive in the cassandra pods and going back out we may have to run some more testing

Cheers,
Adam


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 16 May 2018 04:35
To: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Richard,

Tldr: I've made some progress since I sent that email. I've found and fixed bugs to do with dnsmasq, but now I'm stuck.
The systemd logs for homestead still say: "Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."
I'm not even sure whether the dnsmasq bugs were related.

First, to answer your questions:

Yes this happens every time I deploy.

My configmap:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
2991678f-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io
Events:  <none>
```

I'm deploying on Azure's kubernetes service (AKS). I've enabled HTTP Application Routing<https://docs.microsoft.com/en-us/azure/aks/http-application-routing>, which means that Azure manages DNS for me. The only unusual step is that Azure requires that I add ingresses to do so. The DNS records point to each service (bono, ellis, sprout etc). I can ping "cassandra" from the homestead pod.

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:22:21Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.6", GitCommit:"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1", GitTreeState:"clean", BuildDate:"2018-03-21T15:13:31Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}

Getting the logs is a race against time, since kubernetes keeps terminating the pod while I'm in there.
After several attempts, I managed to get the homestead logs (same as homestead-prov logs)

```
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:123: Creating Cached Resolver using servers:
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status dnscachedresolver.cpp:133:     10.0.0.10
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Error static_dns_cache.cpp:89: DNS config file /etc/clearwater/dns.json mis
sing
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.126 UTC [7f6379c8e7c0] Status main.cpp:653: Using local impu store: astaire.2991678f-f9dd-4098-8fe
8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.129 UTC [7f6379c8e7c0] Status http_connection_pool.cpp:37: Connection pool will use calculated res
ponse timeout of 550ms
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:35: Configuring HTTP Connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status httpconnection.h:36:   Connection created for server sprout.2991678f
-f9dd-4098-8fe8-1395c572a307.westeurope.aksapp.io:9888
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status main.cpp:1013: No HSS configured - using Homestead-prov
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status a_record_resolver.cpp:29: Created ARecordResolver
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:266: Configuring store connection
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:267:   Hostname:  cassandra.2991678f-f9dd-4098-8
fe8-1395c572a307.westeurope.aksapp.io
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:268:   Port:      9160
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:296: Configuring store worker pool
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:297:   Threads:   10
15-05-2018 07:05:40.130 UTC [7f6379c8e7c0] Status cassandra_store.cpp:298:   Max Queue: 0
15-05-2018 07:05:40.131 UTC [7f6366ffd700] Status alarm.cpp:244: Reraising all alarms with a known state
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Error main.cpp:1056: Failed to initialize the Cassandra store with error co
de 3.
15-05-2018 07:05:40.636 UTC [7f6379c8e7c0] Status main.cpp:1057: Homestead is shutting down
```

The full logs are just this chunk repeating over and over until everything shuts down.

I think the crucial error message is DNS config file /etc/clearwater/dns.json missing

In the all-in-one installation, that file is just:

{
  "hostnames": [
  ]
}

Is it ok to manually insert that using the Dockerfile? Or should there be other entries in there for a non-AIO installation?

dns.json is created by the dnsmasq service.

During the container build, there is a red error message saying "dnsmasq: setting capabilities failed: Operation not permitted"
(There are a lot of red error messages during build. Is that an issue?)

I noticed that dnsmasq was not running in homestead docker, yet it is running in my working all-in-one installation. (There are also a bunch of other services which are halted. Is that an issue?)

I tried starting dnsmasq manually, it wouldn't start. I looked online and the solution is to add user=root to the file /etc/dnsmasq.conf.

I tried modifying the Dockerfile to manually append that string to the file.  (I'm assuming that the order of lines in /etc/dnsmasq.conf does not matter)

Then dnsmasq still doesn't come up. I get an error about how the port it wants is already taken.

So then I tried modifying the Dockerfile to paste in dns.conf from the AIO installation. (The one with an empty list of hosts)

Now the missing file error is gone, but I still get the other error:

"Error main.cpp:1056: Failed to initialize the Cassandra store with error code 3."

How can I debug this?

All this time I thought that error was caused by dns issues. Is it because dnsmasq still won't start, so dns.json is missing hosts? Or is that unrelated?
"ping cassandra" works from the homestead pod.

Other Remaining questions:

  *   Do I need dnsmasq? The dns.json file is an empty list of 'hosts'. Can I just paste that in?
     *   {  "hostnames": [ ]}
     *   Why won't dnsmasq start? ("dnsmasq: failed to create listening socket for 127.0.0.1: Address already in use") Does that even matter?
     *   During image build there's an error "dnsmasq: setting capabilities failed: Operation not permitted". What's the source of that error?
  *   There are many other red error messages during build. Does that matter?
  *   There are also a bunch of services (mostly clearwater ones) which are halted, but are running in the AIO build. Is that an issue?
     *   Services running in AIO:
        *   [ ? ]  clearwater-auto-config-generic
        *    [ + ]  clearwater-cluster-manager
        *    [ + ]  clearwater-config-manager
        *    [ + ]  clearwater-diags-monitor
        *    [ - ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ ? ]  clearwater-memcached
        *    [ + ]  clearwater-queue-manager
     *   Services running (or not) in docker homestead:
        *   [ ? ]  clearwater-auto-config-docker
        *    [ + ]  clearwater-cluster-manager
        *    [ - ]  clearwater-config-manager
        *    [ - ]  clearwater-diags-monitor
        *    [ + ]  clearwater-etcd
        *    [ + ]  clearwater-infrastructure
        *    [ - ]  clearwater-queue-manager


Thanks,

Matthew Davis
Telstra | CTO | Cloud SDN NFV

From: Richard Whitehouse (projectclearwater.org) [mailto:richard.whitehouse at projectclearwater.org]
Sent: Wednesday, 16 May 2018 3:24 AM
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>; Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Matthew,

Sorry to hear you are having problems deploying Clearwater on Docker.

I think that the socket factory error is actually fairly benign.

Does this happen every time you create your deployment?

Can you provide the config map that you are providing when deploying on Kubernetes?

Can you connect into one of the pods that is failing to run homestead-prov and grab the logs. The relevant ones will be under /var/log/homestead-prov inside the container.

We cannot reproduce your issue on our kubernetes setup - can you provide details of your kubernetes setup - what version of k8s and what networking configuration you are using?

>From the diagnostics we have at the moment, the most likely scenario is that the Homestead and Homestead Prov containers are unable to contact Cassandra - if that's the case they will continually restart to attempt to resolve the problem.



Richard


From: Clearwater [mailto:clearwater-bounces at lists.projectclearwater.org] On Behalf Of Davis, Matthew
Sent: 14 May 2018 07:05
To: clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: Re: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi everyone,
I've fixed the issue I had with homestead-prov and submitted the patch as this pull request: https://github.com/Metaswitch/clearwater-docker/pull/89

However now I just run into another issue, so homestead-prov is giving me the same error as homestead. The error message is not verbose. Does anyone know how to make it more verbose? I have no idea what's happening.


```
2018-05-14 05:47:41,369 INFO success: snmpd entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:41,370 INFO success: clearwater-infrastructure entered RUNNING state, process has stayed up for > than 0 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-sig entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,476 INFO success: socket-factory-mgmt entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:42,477 INFO exited: snmpd (exit status 0; expected)
2018-05-14 05:47:42,478 CRIT reaped unknown pid 61)
2018-05-14 05:47:44,572 CRIT reaped unknown pid 65)
2018-05-14 05:47:44,576 CRIT reaped unknown pid 66)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 235)
2018-05-14 05:47:45,883 CRIT reaped unknown pid 236)
2018-05-14 05:47:45,902 CRIT reaped unknown pid 257)
2018-05-14 05:47:48,153 CRIT reaped unknown pid 294)
2018-05-14 05:47:48,226 INFO spawned: 'homestead-prov' with pid 297
2018-05-14 05:47:48,237 INFO spawned: 'nginx' with pid 298
2018-05-14 05:47:48,747 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:49,750 INFO spawned: 'homestead-prov' with pid 324
2018-05-14 05:47:49,750 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2018-05-14 05:47:50,089 INFO exited: clearwater-infrastructure (exit status 0; expected)
2018-05-14 05:47:50,335 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:52,339 INFO spawned: 'homestead-prov' with pid 335
2018-05-14 05:47:53,324 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:56,330 INFO spawned: 'homestead-prov' with pid 346
2018-05-14 05:47:56,902 INFO exited: homestead-prov (exit status 0; not expected)
2018-05-14 05:47:57,904 INFO gave up: homestead-prov entered FATAL state, too many start retries too quickly
```

Regards,
Matt Davis
Telstra | Graduate Engineer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180524/2f19ec4c/attachment.html>

From kapil.g at hcl.com  Fri May 25 01:35:08 2018
From: kapil.g at hcl.com (Kapil Gupta)
Date: Fri, 25 May 2018 05:35:08 +0000
Subject: [Project Clearwater] Calls are failure with SIP 503 Service
 Unavailable
In-Reply-To: <PS1PR04MB1676CA0CD326253C770A319EE46A0@PS1PR04MB1676.apcprd04.prod.outlook.com>
References: <PS1PR04MB1676CA0CD326253C770A319EE46A0@PS1PR04MB1676.apcprd04.prod.outlook.com>
Message-ID: <PS1PR04MB16765E1301EE430244CA9F44E4690@PS1PR04MB1676.apcprd04.prod.outlook.com>

Hello,

My queries not responded by community please help us and include me in list.


Thanks
Kapil

From: Kapil Gupta
Sent: 24 May 2018 17:59
To: clearwater at lists.projectclearwater.org
Cc: Gaurav Gupta <gaurav.g at hcl.com>
Subject: Calls are failure with SIP 503 Service Unavailable

Hello,

I have deployed Clearwater IMS using https://github.com/Metaswitch/clearwater-heat with master branch on Vanilla OpenStack.
It deployed 7 VM's (Vellum, homer, sprout, bono, dime, ellis and ns).   All services are running fine from, check using "Monit summary"
We are able to create subscriber using Ellis and Registered  2 user (using Zoiper client 3.1)
But Calls are failling with SIP error 503 Service Unavailable
Please help us to debug it further.  Here i attached configuration files from /etc/clearwater for all VMs.
Please Note:--We are not able to capture detailed traces. However, I have set log_level=5, in user_setting
Logs:-
sprout/analytics.log:2018-05-24T11:41:58.979+00:00 Registration: USER_URI=sip:6505550138 at example.com BINDING_ID=sip:6505550138 at 192.168.0.11:58703;transport=UDP;rinstance=6bc106cd00583afa CONTACT_URI=sip:6505550138 at 192.168.0.11:58703;transport=UDP;rinstance=6bc106cd00583afa EXPIRES=3600
sprout/analytics.log:2018-05-24T11:46:51.899+00:00 Registration: USER_URI=sip:6505550398 at example.com BINDING_ID=sip:6505550398 at 192.168.0.9:39194;transport=UDP;rinstance=d83f28f400ac2933 CONTACT_URI=sip:6505550398 at 192.168.0.9:39194;transport=UDP;rinstance=d83f28f400ac2933 EXPIRES=3600
syslog:7095:May 24 11:47:35 bono-0 bono[10699]: <analytics> 2018-05-24T11:47:35.106+00:00 Call-Not-Connected: FROM=sip:6505550138 at example.com TO=sip:6505550398 at example.com CALL_ID=N2M1MjhlNDUzNDZkZjg5MWE1ODhhZjYyZDkxYTU3N2U. REASON=503
Thanks
Kapil Gupta

::DISCLAIMER::
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only. E-mail transmission is not guaranteed to be secure or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or may contain viruses in transmission. The e mail and its contents (with or without referred errors) shall therefore not attach any liability on the originator or HCL or its affiliates. Views or opinions, if any, presented in this email are solely those of the author and may not necessarily reflect the views or opinions of HCL or its affiliates. Any form of reproduction, dissemination, copying, disclosure, modification, distribution and / or publication of this message without the prior written consent of authorized representative of HCL is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately. Before opening any email and/or attachments, please check them for viruses and other defects.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180525/f93067d0/attachment.html>

From Adam.Lindley at metaswitch.com  Fri May 25 04:42:12 2018
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Fri, 25 May 2018 08:42:12 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <ME2PR01MB2884BAB776270D40A8B06347C2690@ME2PR01MB2884.ausprd01.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971545314ED91FB7F233AC1E2900@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB288450EF460A39FEA9E1D724C2940@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<BY1PR0201MB09689BB9A1645396B16AE810E2940@BY1PR0201MB0968.namprd02.prod.outlook.com>
	<ME2PR01MB2884F01278DC1CD5F7831DFEC26A0@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09710B4690B23BC5F7E8F786E26A0@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884BAB776270D40A8B06347C2690@ME2PR01MB2884.ausprd01.prod.outlook.com>
Message-ID: <CY1PR0201MB0971049DFED6D952FFC21DE1E2690@CY1PR0201MB0971.namprd02.prod.outlook.com>

Hi Matthew,

Our Helm support is a recent addition, and came from another external contributor. See the Pull Request at https://github.com/Metaswitch/clearwater-docker/pull/85 for the details :)
As it stands at the moment, the chart is good enough for deploying and re-creating a full standard deployment through Helm, but I don't believe it handles more of the complexities of upgrading a clearwater deployment that it potentially could.

We haven't yet done any significant work in setting up Helm charts, or integrating with them in a more detailed manner, so if that's something you're interested in as well, we'd love to work with you to get some more enhancements in. Especially if you have other expert contacts who know more in this area.

(I'm removing some of the thread in the email below, to keep us below the list limits. The online archives will keep all the info though)

Cheers,
Adam


From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 25 May 2018 08:30
To: Adam Lindley <Adam.Lindley at metaswitch.com>; clearwater at lists.projectclearwater.org
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,

Thanks for that.

I haven't had a chance to try your latest suggestion or do more work on AKS yet.

I just have a quick question. Should the helm charts be mostly empty? I spoke to someone at Microsoft and he was surprised at how short Chart.yaml is.

Chart.yaml:

```
apiVersion: v1
description: A Helm chart for Clearwater
name: clearwater
version: 0.1.0
```

values.yaml

```
# Default values for clearwater.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
image:
  path: {{IMAGE_PATH}}
  tag: {{IMAGE_TAG}}
```
Thanks,


Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 25 May 2018 3:05 AM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>; kieran at aptira.com<mailto:kieran at aptira.com>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hey Matthew,

Sorry, I think I was a bit unclear, and have also noticed I missed out one step in my previous message. I'll try to clarify a bit here.

> you're having issues connecting to Bono because the service/pod is not exposed

By this, I meant that your Bono service is not exposed outside the kubernetes cluster. Other pods would have been able to reach it just fine, but your test setup was trying to reach into it from outside the host, and there was no config in place mapping it through to allow that to happen (e.g. nodePort, LoadBalancer type configuration).

The step I missed out was that we want to change the bono service port on the inside of the pod as well, to match the one we set up on the host. This is because Bono will be record-routing itself using the PUBLIC_IP and pcscf port, so if a SIP client tried to respond using this route, it would by default be sent back to port 5060. Because we can't open that up as a nodePort on the Kubernetes host, this would cause issues in the SIP flows.
As for why we are specifically interested in port 5060; this is the external P-CSCF port. I.e. this is the interface we want to send all our mainline SIP flows through. The others are used for webrtc and restund, neither of which we are particularly interested in at the moment, and neither of which are critical for getting calls through.

So, I think we just want to make a couple of tweaks to the bono-svc.yaml file. I'm going to copy bono yaml files in at the bottom. Simply we want the values of `port` and `nodePort` under the `-name: "5060"` section to match.
With the configuration as in my yaml files, and changing the `--pcscf` option in the bono pod, I see my live tests passing under the following command:
`rake test[default.svc.cw-k8s.test] PROXY=<k8s-host-IP PROXY_PORT=32060 ELLIS=<k8s-host-IP:30080`

Hopefully you see the same. From the Bono logs you've got below, I think the issue is simply that the above misconfiguration meant traffic couldn't reach the bono service correctly. I think this should be the last step in this set of hurdles; hopefully you see tests and calls working :)

Unfortunately, I don't have much experience using weave networking, so can't give much guidance there on how to open the pods up more to the wider network to make this of more use. And if in your testing you are seeing no packets hit any of the pods, that's going to be the first thing we need to debug, as we've probably missed a different piece of network configuration somewhere.
On the other thread, have you made any further progress in the Azure Kubernetes Service setup? I would still be very interested to see that up and working too.

Hope this helps. Cheers,
Adam


bono-svc.yaml:
```
apiVersion: v1
kind: Service
metadata:
  name: bono
spec:
  type: NodePort
  ports:
  - name: "3478"
    port: 3478
  - name: "5060"
    port: 32060
    nodePort: 32060
  - name: "5062"
    port: 5062
  selector:
    service: bono
```

bono-depl.yaml
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: bono
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bono
  template:
    metadata:
      labels:
        service: bono
        snmp: enabled
    spec:
      containers:
      - image:
        imagePullPolicy: Always
        name: bono
        ports:
        - containerPort: 22
        - containerPort: 3478
        - containerPort: 5060
        - containerPort: 5062
        - containerPort: 5060
          protocol: "UDP"
        - containerPort: 5062
          protocol: "UDP"
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PUBLIC_IP
          value: 10.230.16.1
        livenessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
          initialDelaySeconds: 30
        readinessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      - image: busybox
        name: tailer
        command: [ "tail", "-F", "/var/log/bono/bono_current.txt" ]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      volumes:
      - name: bonologs
        emptyDir: {}
      restartPolicy: Always
```


From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 24 May 2018 06:20
To: Adam Lindley <Adam.Lindley at metaswitch.com<mailto:Adam.Lindley at metaswitch.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Cc: Richard Whitehouse (projectclearwater.org) <richard.whitehouse at projectclearwater.org<mailto:richard.whitehouse at projectclearwater.org>>; kieran at aptira.com<mailto:kieran at aptira.com>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,

Thanks for that.

> you're having issues connecting to Bono because the service/pod is not exposed

What do you mean by that? I've run `kubectl apply -f bono-svc.yaml`. The service is exposed, isn't it?

$ kubectl get services
NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                               AGE
bono             ClusterIP   None         <none>        3478/TCP,5060/TCP,5062/TCP            1d

or after the pcscf changes:

NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                        AGE
bono             NodePort    10.108.162.246   <none>        3478:30078/TCP,5060:30060/TCP,5062:30062/TCP   41m

I've cc'ed in my colleague (Kieran) who set up the cluster for me. He said it's using weave net. Contrary to what I thought, nothing will be externally routable. So that's an issue. After adding the nodePorts I can view ellis in a browser (http://10.3.1.76:30080/login.html), and ` nc 10.3.1.76 30080 -v` shows that I can connect to the kubernetes cluster on port 30080. ` nc 10.3.1.76 30060 -v` says connection refused. When that happens I see a packet in tcpdump on the bono pod. I see nothing on tcpdump in bono on the relevant ports when I run the rake tests.

I'm curious about the ports for Bono. You keep mentioning port 5060. Is that the only port bono uses? What about 3478 and 5062?

I tried your suggestion for the pcscf thing. It didn't work.
The stdio for the rake test is below. How can I figure out whether the 403 error is for ellis or bono?
The last line of the rake test output says: "Error logs, including Call-IDs of failed calls, are in the 'logfiles' directory". But the logfiles directory is empty. Where can I find more verbose logs?

I ran tcpdump inside the bono and ellis pods during the test. If I filter by source IP address I see nothing, so I have to filter by port. The tcpdump command I'm using in bono is `tcpdump -a -vnni any port 5060 or port 5062 or port 3478 or port 30078 or port 30060 or port 30062`

Literally all ports are now open on the firewall.

Here is my bono-svc.yaml file:
```
apiVersion: v1
kind: Service
metadata:
  name: bono
spec:
  type: NodePort
  ports:
  - name: "3478"
    port: 3478
    nodePort: 30078
  - name: "5060"
    port: 5060
    nodePort: 30060
  - name: "5062"
    port: 5062
    nodePort: 30062
  selector:
    service: bono
```

Here is my bono-depl.yaml file:

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: bono
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bono
  template:
    metadata:
      labels:
        service: bono
        snmp: enabled
    spec:
      containers:
      - image: "mlda065/bono:latest"
        imagePullPolicy: Always
        name: bono
        ports:
        - containerPort: 22
        - containerPort: 3478
        - containerPort: 5060
        - containerPort: 5062
        - containerPort: 5060
          protocol: "UDP"
        - containerPort: 5062
          protocol: "UDP"
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PUBLIC_IP
          value: 10.3.1.76 #:6443
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      - image: busybox
        name: tailer
        command: [ "tail", "-F", "/var/log/bono/bono_current.txt" ]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      volumes:
      - name: bonologs
        emptyDir: {}
      imagePullSecrets:
      - name: myregistrykey
      restartPolicy: Always
```

Here is my env-vars:

```
$ kubectl describe configmap env-vars
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ZONE:
----
default.svc.cluster.local
Events:  <none>
```

Here are my services once deployed:

```
$ kubectl get service
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                        AGE
astaire          ClusterIP   None             <none>        11311/TCP                                      22m
bono             NodePort    10.108.162.246   <none>        3478:30078/TCP,5060:30060/TCP,5062:30062/TCP   22m
cassandra        ClusterIP   None             <none>        7001/TCP,7000/TCP,9042/TCP,9160/TCP            22m
chronos          ClusterIP   None             <none>        7253/TCP                                       22m
ellis            NodePort    10.97.76.168     <none>        80:30080/TCP                                   22m
etcd             ClusterIP   None             <none>        2379/TCP,2380/TCP,4001/TCP                     22m
homer            ClusterIP   None             <none>        7888/TCP                                       22m
homestead        ClusterIP   None             <none>        8888/TCP                                       22m
homestead-prov   ClusterIP   None             <none>        8889/TCP                                       22m
kubernetes       ClusterIP   10.96.0.1        <none>        443/TCP                                        2d
ralf             ClusterIP   None             <none>        10888/TCP                                      22m
sprout           ClusterIP   None             <none>        5052/TCP,5054/TCP                              22m
```


When I restarted the bono service there was a couple of warnings:

```
Defaulting container name to bono.
Use 'kubectl describe pod/bono-6dfc579b5-bdgzj -n default' to see all of the containers in this pod.
* Restarting Bono SIP Edge Proxy bono
/etc/init.d/bono: line 63: ulimit: open files: cannot modify limit: Operation not permitted
/etc/init.d/bono: line 64: ulimit: open files: cannot modify limit: Invalid argument
23-05-2018 06:21:52.570 UTC [7f2b793ec7c0] Status utils.cpp:651: Switching to daemon mode
   ...done.
```

Here's the result of the test:

```
$rake test[default.svc.cw-k8s.test] PROXY=10.3.1.76 PROXY_PORT=30060 SIGNUP_CODE='secret' ELLIS=10.3.1.76:30080 TESTS="Basic call - mainline"
Basic Call - Mainline (TCP) - Failed
  RestClient::Forbidden thrown:
   - 403 Forbidden
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/abstract_response.rb:74:in `return
!'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:495:in `process_result'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:421:in `block in transm
it'
     - /usr/lib/ruby/2.3.0/net/http.rb:853:in `start'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:413:in `transmit'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:176:in `execute'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:41:in `execute'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient.rb:69:in `post'
```

Here is /var/log/bono/bono_current.txt in the bono pod:

```
24-05-2018 05:17:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
24-05-2018 05:17:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1012.3 alarm
24-05-2018 05:17:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1013.3 alarm
24-05-2018 05:17:33.337 UTC [7f2b4b7ee700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 11
24-05-2018 05:17:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
24-05-2018 05:17:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1012.3 alarm
24-05-2018 05:17:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1013.3 alarm
24-05-2018 05:18:21.346 UTC [7f2b4b7ee700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 36
24-05-2018 05:18:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
24-05-2018 05:18:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1012.3 alarm
24-05-2018 05:18:22.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1013.3 alarm
24-05-2018 05:18:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:244: Reraising all alarms with a known state
24-05-2018 05:18:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1012.3 alarm
24-05-2018 05:18:52.847 UTC [7f2b6b7fe700] Status alarm.cpp:37: sprout issued 1013.3 alarm
24-05-2018 05:18:57.350 UTC [7f2b4b7ee700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 35
```

Thanks,

Matthew Davis
Telstra Graduate Engineer
CTO | Cloud SDN NFV
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180525/b99a02aa/attachment.html>

From rajapaue at yahoo.com  Sat May 26 09:44:26 2018
From: rajapaue at yahoo.com (rajapaue at yahoo.com)
Date: Sat, 26 May 2018 13:44:26 +0000 (UTC)
Subject: [Project Clearwater] getting 403 Forbidden instead of 401 in SIPp
 stress testing. Please help
References: <578442439.1484281.1527342266916.ref@mail.yahoo.com>
Message-ID: <578442439.1484281.1527342266916@mail.yahoo.com>

Hi,?
I am trying to test my IMS core which is ran in AWS. I installed the sipp module using automated chef. But I run the script to test I get the following error. It is a long time I am working to find the reason. Kindly let me know the reason.?
sipp: The following events occurred:2018-05-23 13:24:18.329759 1527081858.329759: Aborting call on unexpected message for Call-Id '1-4325 at 172.31.42.209': while expecting '401' (index 1), received 'SIP/2.0 403 ForbiddenVia: SIP/2.0/TCP 172.31.42.209:36173;received=172.31.42.209;branch=z9hG4bK-4325-1-0Call-ID: 1-4325 at 172.31.42.209From: <sip:2010000000 at clearwater.amir.test.com>;tag=4325SIPpTag001To: <sip:2010000000 at clearwater.amir.test.com>;tag=z9hG4bKPjz9AXA2brIljLxzEMQ9Akc1jTESHj18SmCSeq: 1 REGISTERP-Charging-Vector: icid-value="d4511351a7e24c5ff16243bac827fc3f1"Content-Length:? 0
Thank you in advance.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180526/858b9624/attachment.html>

From Mark.Perryman at metaswitch.com  Tue May 29 04:45:07 2018
From: Mark.Perryman at metaswitch.com (Mark Perryman)
Date: Tue, 29 May 2018 08:45:07 +0000
Subject: [Project Clearwater] getting 403 Forbidden instead of 401 in
 SIPp stress testing. Please help
Message-ID: <CY4PR02MB2518D06D19BECE40B0853BBF8F6D0@CY4PR02MB2518.namprd02.prod.outlook.com>

rajapaue,

A 403 Forbidden when trying to register would often indicate that the username or password are wrong, or that the subscriber has not been provisioned correctly.



sip:2010000000 at clearwater.amir.test.com


How have you added that subscriber?

Mark.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180529/7c15f2ce/attachment.html>

From msc.jaber at gmail.com  Sat May 26 03:36:27 2018
From: msc.jaber at gmail.com (jaber daneshamooz)
Date: Sat, 26 May 2018 12:06:27 +0430
Subject: [Project Clearwater] clearwater QoS
Message-ID: <69566153-E1D8-4C90-A0F8-B1CFCE20C60E@gmail.com>

Hi, 
does clearwater provide QoS by using method such as MPLS and Differentiate Service?
if it does, how it works? 
if it doesn?t , can we add QoS to it?


From Mark.Perryman at metaswitch.com  Tue May 29 06:00:11 2018
From: Mark.Perryman at metaswitch.com (Mark Perryman)
Date: Tue, 29 May 2018 10:00:11 +0000
Subject: [Project Clearwater] Calls are failure with SIP 503 Service
 Unavailable
Message-ID: <CY4PR02MB25184904FF439F918C79D7958F6D0@CY4PR02MB2518.namprd02.prod.outlook.com>

Hi Kapil,

It is hard to debug this without more logging information.  Are you able to reproduce this error and send the log files?

Thanks,

Mark Perryman.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180529/288f68f3/attachment.html>

From Alan.Kwon at interoptechnologies.com  Tue May 29 15:00:20 2018
From: Alan.Kwon at interoptechnologies.com (Kwon, Alan)
Date: Tue, 29 May 2018 15:00:20 -0400
Subject: [Project Clearwater] PSI routing at terminating network
Message-ID: <D7330D28.539E3%Alan.Kwon@interoptechnologies.com>

Hi,

Just following up since I haven?t heard any response? any thoughts?

Thanks,
Alan Kwon

From: <Kwon>, Alan Kwon <Alan.Kwon at interoptechnologies.com<mailto:Alan.Kwon at interoptechnologies.com>>
Reply-To: "clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>" <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>
Date: Wednesday, May 23, 2018 at 3:21 PM
To: "clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>" <clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>>
Subject: [Project Clearwater] PSI routing at terminating network

Hi,

I?m trying to understand the routing logic in Sprout. Basically, it?s a SUBSCRIBE coming from another network (NNI) with ReqURI set to a PSI:

SUBSCRIBE sip:ImService at sc01.ivc.iot1.com<mailto:ImService at sc01.ivc.iot1.com>:5510;as.session=4 SIP/2.0
Via: SIP/2.0/UDP 172.27.0.140;branch=z9hG4bK08B0100666d326454c4
From: <tel:+18152579478>;tag=gK08e-gK00e-492495c9-c6df-4eaf-8016-c126fbac5136
To: <sip:ImService at sc01.ivc.iot1.com<mailto:ImService at sc01.ivc.iot1.com>;as.session=4>
Call-ID: f6fa61ef-a3df-4c30-9069-1ee0a3957f58
CSeq: 2 SUBSCRIBE
Max-Forwards: 61
Expires: 3600
P-Preferred-Identity: <tel:+18152579478>
Contact: <sip:ImService at sc01.sales2.iot1.com<mailto:ImService at sc01.sales2.iot1.com>:5510;as.session=2>;+sip.instance="<urn:gsma:imei:35197909-002681->";+g.oma.sip-im
Event: conference
Accept: application/conference-info+xml
Accept-Contact: *;+g.oma.sip-im
P-Asserted-Identity: <sip:+18152579478 at sales2.iot1.com<mailto:+18152579478 at sales2.iot1.com>>
P-Asserted-Identity: <tel:+18152579478>
Supported: eventlist
Record-Route: <sip:172.27.0.140:5060;lr;transport=udp>
P-Charging-Vector: icid-value=dab7cbc0-4038-1036-00-00-00-50-56-bb-09-00;icid-generated-at=172.27.0.140
Content-Length: 0

>From the Sprout log, I can see I-CSCF performing LIR and getting a successful result:

22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug icscfsproutlet.cpp:551: Terminating request
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer
_sip: true, treat_number_as_phone: false
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:172: Classified URI as 5
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug icscfrouter.cpp:431: Perform LIR - impu sip:ImService at sc01.ivc.iot1.com<mailto:ImService at sc01.ivc.iot1.com>, originating false, auth_type None
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug a_record_resolver.cpp:57: ARecordResolver::resolve_iter for host dime.ivc.iot1.com, port 8888, family 2
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug utils.cpp:446: Attempt to parse dime.ivc.iot1.com as IP address
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Verbose dnscachedresolver.cpp:468: Check cache for dime.ivc.iot1.com type 1
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug dnscachedresolver.cpp:578: Pulling 2 records from cache for dime.ivc.iot1.com A
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug baseresolver.cpp:192: Found 2 A/AAAA records, creating iterator
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug utils.cpp:446: Attempt to parse dime.ivc.iot1.com as IP address
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug baseresolver.cpp:812: Attempting to get 1 targets for host:dime.ivc.iot1.com. allowed_host_state = 3
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug baseresolver.cpp:587: 172.27.0.51:8888;transport=TCP has state: WHITE
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug baseresolver.cpp:587: 172.27.0.52:8888;transport=TCP has state: WHITE
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug baseresolver.cpp:587: 172.27.0.51:8888;transport=TCP has state: WHITE
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug baseresolver.cpp:883: Added a whitelisted server to targets, now have 1 of 1
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug connection_pool.h:207: Request for connection to IP: 172.27.0.51, port: 8888
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug connection_pool.h:229: No existing connection in pool, create one
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug http_connection_pool.cpp:42: Allocated CURL handle 0x7faafc31d9a0
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug connection_pool.h:231: Created new connection 0x7faafc336bb0
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug httpclient.cpp:557: Set CURLOPT_RESOLVE: dime.ivc.iot1.com:8888:172.27.0.51
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug httpclient.cpp:588: Sending HTTP request : http://dime.ivc.iot1.com:8888/impu/sip%3AImService%40sc01.ivc.iot1.com/l
ocation (trying 172.27.0.51)
22-05-2018 21:52:41.173 UTC [7fa9fbf6b700] Debug thread_dispatcher.cpp:117: Pausing stopwatch due to HTTP request to http://dime.ivc.iot1.com:8888/impu/sip%3AImServ
ice%40sc01.ivc.iot1.com/location
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:956: Received header http/1.1200ok with value
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:957: Header pointer: 0x7fa9fbf6a000
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:956: Received header content-length with value 75
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:957: Header pointer: 0x7fa9fbf6a000
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:956: Received header content-type with value text/plain
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:957: Header pointer: 0x7fa9fbf6a000
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:956: Received header  with value
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:957: Header pointer: 0x7fa9fbf6a000
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug thread_dispatcher.cpp:123: Resuming stopwatch after HTTP request to http://dime.ivc.iot1.com:8888/impu/sip%3AImService%40sc01.ivc.iot1.com/location
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug httpclient.cpp:628: Received HTTP response: status=200, doc={"result-code":2001,"mandatory-capabilities":[],"optional-capabilities":[]}
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug baseresolver.cpp:672: Successful response from  172.27.0.51:8888;transport=TCP
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug connection_pool.h:244: Release connection to IP: 172.27.0.51, port: 8888 to pool
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug event_statistic_accumulator.cpp:32: Accumulate 6770 for 0x1f10670
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug event_statistic_accumulator.cpp:32: Accumulate 6770 for 0x1f10718
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug event_statistic_accumulator.cpp:32: Accumulate 6770 for 0x1f19fb0
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug event_statistic_accumulator.cpp:32: Accumulate 6770 for 0x1f19ff8
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug icscfrouter.cpp:246: HSS returned capabilities
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug acr.cpp:621: Storing Server-Capabilities
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug scscfselector.cpp:263: Selected S-CSCF is sip:scscf.sprout.ivc.iot1.com:5054
22-05-2018 21:52:41.179 UTC [7fa9fbf6b700] Debug icscfrouter.cpp:118: SCSCF selected: sip:scscf.sprout.ivc.iot1.com:5054

When the request gets to S-CSCF, I was expecting it to evaluate iFC for the PSI (sip:ImService at sc01.ivc.iot1.com<mailto:ImService at sc01.ivc.iot1.com>), however, it seems to skip the service evaluation:

22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('icscf'->'scscf'/'scscf-proxy')
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug sproutletproxy.cpp:2504: Network function boundary: yes ('icscf'->'scscf'/'scscf-proxy')
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug sproutletproxy.cpp:2517: Internal network function boundary: yes
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug pjutils.cpp:736: Cloned tdta0x7faafc1562c0 to tdta0x7faafc2dac10
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug sproutletproxy.cpp:2517: Internal network function boundary: yes
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug pjutils.cpp:736: Cloned tdta0x7faafc1562c0 to tdta0x7faafc2dac10
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug sproutletproxy.cpp:1450: Remove top Route header Route: <sip:sprout.ivc.iot1.com;lr;service=scscf-proxy>
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug sproutletproxy.cpp:2115: Adding message 0x7faafc2db220 => txdata 0x7faafc2dacb8 mapping
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Verbose sproutletproxy.cpp:1946: scscf-proxy-0x7faafc305ee0 pass initial request Request msg SUBSCRIBE/cseq=2 (tdta0x7faa
fc2dac10) to Sproutlet
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Info scscfsproutlet.cpp:471: S-CSCF received initial request
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: true, is_gruu: false, enforce_user_phone: false, prefer_
sip: true, treat_number_as_phone: false
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:172: Classified URI as 3
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug scscfsproutlet.cpp:945: Route header references this system
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug scscfsproutlet.cpp:991: No ODI token, or invalid ODI token, on request - logging ICID marker dab7cbc0-4038-1036-00-
00-00-50-56-bb-09-00 for B2BUA AS correlation
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug scscfsproutlet.cpp:1004: Got our Route header, session case term, OD=None
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer
_sip: true, treat_number_as_phone: false
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:172: Classified URI as 5
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug scscfsproutlet.cpp:1348: URI is not locally hosted
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug acr.cpp:1797: Create RalfACR for node type S-CSCF with role Terminating
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug acr.cpp:24: Created ACR (0x7faafc168d50)
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug acr.cpp:170: Created S-CSCF Ralf ACR
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug acr.cpp:29: Destroyed ACR (0x7faafc168d50)
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:139: home domain: false, local_to_node: false, is_gruu: false, enforce_user_phone: false, prefer_sip: true, treat_number_as_phone: false
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug uri_classifier.cpp:172: Classified URI as 5
22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Info scscfsproutlet.cpp:656: Route request without applying services

Could you please explain why the iFC evaluation is skipped? 24-229, Section 5.4.3.3 seems to imply that it should build an ordered list of iFC if no original dialog identifier is present in the topmost Route header of the incoming request, which seems to be the case here.

Thank you,



[cid:7F5F1D25-E9CF-4FB6-957D-E4E41857C139]<http://www.interoptechnologies.com/>




ALAN KWON
Senior Software Engineer




T: +1 972-753-1865 (Texas)
F: +1 239-425-6845

Confidentiality Notice: The information in this e-mail and in any attachment may contain information which is legally privileged. It is intended only for the attention and use of the named recipient. If you are not the intended recipient, you are not authorized to retain, disclose, copy or distribute the message and/or any of its attachments. If you received this e-mail in error, please notify me and delete this message.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180529/c9fa23cc/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0E8CB2C0-CC34-4B10-814C-7B349D41FD79[27].png
Type: image/png
Size: 3922 bytes
Desc: 0E8CB2C0-CC34-4B10-814C-7B349D41FD79[27].png
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180529/c9fa23cc/attachment.png>

From Matthew.Davis.2 at team.telstra.com  Wed May 30 03:27:14 2018
From: Matthew.Davis.2 at team.telstra.com (Davis, Matthew)
Date: Wed, 30 May 2018 07:27:14 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <CY1PR0201MB0971049DFED6D952FFC21DE1E2690@CY1PR0201MB0971.namprd02.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971545314ED91FB7F233AC1E2900@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB288450EF460A39FEA9E1D724C2940@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<BY1PR0201MB09689BB9A1645396B16AE810E2940@BY1PR0201MB0968.namprd02.prod.outlook.com>
	<ME2PR01MB2884F01278DC1CD5F7831DFEC26A0@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09710B4690B23BC5F7E8F786E26A0@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884BAB776270D40A8B06347C2690@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971049DFED6D952FFC21DE1E2690@CY1PR0201MB0971.namprd02.prod.outlook.com>
Message-ID: <MEAPR01MB2885D6C8E38E9D4E26723D4EC26C0@MEAPR01MB2885.ausprd01.prod.outlook.com>

Hi Adam,

# Openstack Install

I only mentioned the helm charts just in case the almost empty charts on my machine were the source of the error. I personally have no experience with Helm, so I can't help you with any development.
I applied that latest change to the bono service port number. It still doesn't work.

How can I check whether the rake tests are failing on the bono side, as opposed to failing on the ellis side? Maybe the reason tcpdumps shows nothing in Bono during the rake tests is because the tests failed to create a user in ellis, and never got to the bono part?

Rake output:
```
Basic Call - Mainline (TCP) - Failed
  RestClient::Forbidden thrown:
   - 403 Forbidden
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/abstract_response.rb:74:in `return!'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:495:in `process_result'
...
```

If I go inside the ellis container and run `nc localhost 80 -v` I see that it establishes a connection.
If I go inside the bono container and run `nc localhost 5060 -v` or `nc localhost 30060 -v` it fails to connect. So from within the bono pod I cannot connect to the localhost. To me that suggests that the problem is caused by something inside bono, not the networking between pods. What happens when you try `nc localhost 32060 -v` in your deployment?

The logs inside bono are an echo of an error message from sprout. Does that matter?

```
30-05-2018 06:46:59.432 UTC [7f45527e4700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 4
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:244: Reraising all alarms with a known state
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:37: sprout issued 1012.3 alarm
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:37: sprout issued 1013.3 alarm
```

Those timestamps don't correspond to the rake tests. They just happen every 30 seconds.

When I restart the bono service it says: `63: ulimit: open files: cannot modify limit: Invalid argument`
Does that matter? (I've seen that error message everywhere. I have no idea what it means)

I've appended the yaml files to the end of this email.

# Azure Install

I had a chat to Microsoft. It seems that your hunch was correct. HTTP Application Routing only works on port 80 and 443. Furthermore, I cannot simply route SIP calls through port 443 because the routing does some HTTP specific packet inspection things. So I'll have to give up on that approach and go for a more vanilla, manually configured NodePort approach (either still on AKS but without HTTP Application Routing, or on Openstack). So I'm even more keen to solve the aforementioned issues.


# Yamls and stuff

I'm pasting them again just in case I've forgotten something. 10.3.1.76 is the ip address of my cluster.

Here's a script I'm using to tear down and rebuild everything. (Just incase `kubectl apply -f something.yaml` doesn't actually propagate the change fully) The while loops in this script are just to wait until the previous step has finished.

```
set -x
cd clearwater-docker-master/kubernetes
kubectl delete -f ./
kubectl delete configmap env-vars
set -e
echo 'waiting until old pods are all deleted'
while [ $(kubectl get pods | grep ^NAME -v | wc -l ) -neq 0]
do
   sleep 5
done
echo "creating new pods"
kubectl create configmap env-vars --from-literal=ZONE=default.svc.cluster.local
kubectl apply -f ./
while [ $(kubectl get pods | grep "2/2" | grep bono | wc -l) -neq 1 ]
do
   sleep 5
done
BONO=$(kubectl get pods | grep "2/2" | grep bono | awk '{ print $1 }')
echo "Bono is up as $BONO"
kubectl exec -it $BONO -- apt-get -y install vim
kubectl exec -it $BONO -- sed -i -e 's/--pcscf=5060,5058/--pcscf=30060,5058/g' /etc/init.d/bono
kubectl exec -it $BONO service bono restart
while [ $(kubectl get pods | grep "0/" | wc -l) -neq 0 ]
do
   sleep 5
done
echo "All pods are up now"
kubectl get pods
echo "Done"
```

`kubectl get services`

```
NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                         AGE
astaire          ClusterIP   None           <none>        11311/TCP                                       1h
bono             NodePort    10.0.168.197   <none>        3478:32214/TCP,30060:30060/TCP,5062:30144/TCP   1h
cassandra        ClusterIP   None           <none>        7001/TCP,7000/TCP,9042/TCP,9160/TCP             1h
chronos          ClusterIP   None           <none>        7253/TCP                                        1h
ellis            NodePort    10.0.53.199    <none>        80:30080/TCP                                    1h
etcd             ClusterIP   None           <none>        2379/TCP,2380/TCP,4001/TCP                      1h
homer            ClusterIP   None           <none>        7888/TCP                                        1h
homestead        ClusterIP   None           <none>        8888/TCP                                        1h
homestead-prov   ClusterIP   None           <none>        8889/TCP                                        1h
kubernetes       ClusterIP   10.0.0.1       <none>        443/TCP                                         5d
ralf             ClusterIP   None           <none>        10888/TCP                                       1h
sprout           ClusterIP   None           <none>        5052/TCP,5054/TCP                               1h
```

bono-depl.yaml

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: bono
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bono
  template:
    metadata:
      labels:
        service: bono
        snmp: enabled
    spec:
      containers:
      - image: "mlda065/bono:latest"
        imagePullPolicy: Always
        name: bono
        ports:
        - containerPort: 22
        - containerPort: 3478
        - containerPort: 5060
        - containerPort: 5062
        - containerPort: 5060
          protocol: "UDP"
        - containerPort: 5062
          protocol: "UDP"
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PUBLIC_IP
          value: 10.3.1.76
        livenessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
          initialDelaySeconds: 30
        readinessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      - image: busybox
        name: tailer
        command: [ "tail", "-F", "/var/log/bono/bono_current.txt" ]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      volumes:
      - name: bonologs
        emptyDir: {}
      imagePullSecrets:
      - name: ~
      restartPolicy: Always
```

Bono-svc.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: bono
spec:
  type: NodePort
  ports:
  - name: "3478"
    port: 3478
  - name: "5060"
    port: 30060
    nodePort: 30060
  - name: "5062"
    port: 5062
  selector:
    service: bono
```

Ellis service

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: ellis
spec:
  replicas: 1
  template:
    metadata:
      labels:
        service: ellis
    spec:
      containers:
      - image: "mlda065/ellis:latest"
        imagePullPolicy: Always
        name: ellis
        ports:
        - containerPort: 22
        - containerPort: 80
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        livenessProbe:
          tcpSocket:
            port: 80
          initialDelaySeconds: 30
        readinessProbe:
          tcpSocket:
            port: 80
      imagePullSecrets:
      - name: ~
      restartPolicy: Always
```

Ellis-svc.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: ellis
spec:
  type: NodePort
  ports:
  - name: "http"
    port: 80
    nodePort: 30080
  selector:
    service: ellis
```

`kubectl describe configmap env-vars`

```
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","data":{"ZONE":"default.svc.cluster.local"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"env-vars","namespace":"default"}...

Data
====
ZONE:
----
default.svc.cluster.local
Events:  <none>
```



Thanks,

Matt
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 25 May 2018 6:42 PM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com>; clearwater at lists.projectclearwater.org
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

Our Helm support is a recent addition, and came from another external contributor. See the Pull Request at https://github.com/Metaswitch/clearwater-docker/pull/85 for the details :)
As it stands at the moment, the chart is good enough for deploying and re-creating a full standard deployment through Helm, but I don't believe it handles more of the complexities of upgrading a clearwater deployment that it potentially could.

We haven't yet done any significant work in setting up Helm charts, or integrating with them in a more detailed manner, so if that's something you're interested in as well, we'd love to work with you to get some more enhancements in. Especially if you have other expert contacts who know more in this area.

(I'm removing some of the thread in the email below, to keep us below the list limits. The online archives will keep all the info though)

Cheers,
Adam

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180530/899275e9/attachment.html>

From Mark.Perryman at metaswitch.com  Wed May 30 05:02:53 2018
From: Mark.Perryman at metaswitch.com (Mark Perryman)
Date: Wed, 30 May 2018 09:02:53 +0000
Subject: [Project Clearwater] clearwater QoS
Message-ID: <BN6PR02MB251659AB8887A725F2DB6DCD8F6C0@BN6PR02MB2516.namprd02.prod.outlook.com>

Hi Jaber,

I do not believe that Clearwater currently provides QoS.  However PJSIP (which Clearwater uses) does https://trac.pjsip.org/repos/wiki/QoS, so it should in theory be possible for you to add it.

I suspect it may not be a trivial change though.

Mark Perryman
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180530/dc856488/attachment.html>

From Mark.Perryman at metaswitch.com  Wed May 30 05:09:28 2018
From: Mark.Perryman at metaswitch.com (Mark Perryman)
Date: Wed, 30 May 2018 09:09:28 +0000
Subject: [Project Clearwater] PSI routing at terminating network
Message-ID: <BN6PR02MB251614F84E0E5B9FE9A03EA58F6C0@BN6PR02MB2516.namprd02.prod.outlook.com>

Hi, sorry for the delay.

The key line in the log is:

    22-05-2018 21:52:41.180 UTC [7fa9fbf6b700] Debug scscfsproutlet.cpp:1348: URI is not locally hosted

If you add the URI as a home domain then Sprout should successfully route to it.

    additional_home_domains=sc01.ivc.iot1.com

I hope that helps.

Mark Perryman
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180530/4a873cd8/attachment.html>

From msc.jaber at gmail.com  Thu May 31 04:55:52 2018
From: msc.jaber at gmail.com (jaber daneshamooz)
Date: Thu, 31 May 2018 13:25:52 +0430
Subject: [Project Clearwater] clearwater QoS
Message-ID: <AB79BD02-1602-4654-9DE4-15900417A102@gmail.com>

thank you for your response,
I think QoS is the most important advantage of IMS to VOIP. if cw does not support QoS, why we use it? 
we can easily use a VOIP server(with additional capabilities like video and conferencing) and name it IMS?
and there is another issue. as you said, cw uses pjsip and pjsip provides QoS. based on my research, asterisk also uses pjsip stack, so litterally IMS has the same QoS as VOIP servers like asterisk
and final question: is there any way to test the QoS of IMS by simulation or in a real network?


From Adam.Lindley at metaswitch.com  Thu May 31 13:28:41 2018
From: Adam.Lindley at metaswitch.com (Adam Lindley)
Date: Thu, 31 May 2018 17:28:41 +0000
Subject: [Project Clearwater] Issues with clearwater-docker homestead
 and homestead-prov under Kubernetes
In-Reply-To: <MEAPR01MB2885D6C8E38E9D4E26723D4EC26C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
References: <ME2PR01MB288482217665730202DF424BC2990@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<MEAPR01MB2885A5ED130E79B45AF062A3C29C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
	<BN6PR02MB336224794DE7A1D3CE31E8B9F0930@BN6PR02MB3362.namprd02.prod.outlook.com>
	<ME2PR01MB2884B5A1DECF5FDD45218873C2920@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971F106A46CE86F83CE39B6E2920@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884A19B45AC25AB2289E099C2910@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09711902974D45A75AFB8096E2910@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB28847583A00768B0F905195CC2900@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971545314ED91FB7F233AC1E2900@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB288450EF460A39FEA9E1D724C2940@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<BY1PR0201MB09689BB9A1645396B16AE810E2940@BY1PR0201MB0968.namprd02.prod.outlook.com>
	<ME2PR01MB2884F01278DC1CD5F7831DFEC26A0@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB09710B4690B23BC5F7E8F786E26A0@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<ME2PR01MB2884BAB776270D40A8B06347C2690@ME2PR01MB2884.ausprd01.prod.outlook.com>
	<CY1PR0201MB0971049DFED6D952FFC21DE1E2690@CY1PR0201MB0971.namprd02.prod.outlook.com>
	<MEAPR01MB2885D6C8E38E9D4E26723D4EC26C0@MEAPR01MB2885.ausprd01.prod.outlook.com>
Message-ID: <CY1PR0201MB0971E10C3CE7DB95C3CDBF08E2630@CY1PR0201MB0971.namprd02.prod.outlook.com>

Hey Matthew,

Sorry to hear it isn't working yet. Looking at the output of rake test, you're hitting a forbidden in the RestClient, and so it's something on the Http calls. I think your issue may just be a mismatch in the value of 'SIGNUP_CODE' that you're passing in to the rake test command, and what's on your deployment. This should default to 'secret', and I don't see anything in your configmap that would change that, so can you double check the test command includes " SIGNUP_CODE='secret' ". If you have made any changes to the signup key then obviously it'll need to match those. If you want to double check the value, take a look in the /etc/clearwater/shared_config file in one of the pods.

I have double checked a deployment on our rig, copying your provided yaml files over directly to make sure there isn't anything odd in there, and the live tests were able to run fine, following the deployment steps you're using. I did have some trouble getting the script to work as copied over, but think that's just outlook formatting quotes wrong etc. Manually running the commands it all worked as expected. The only changes I made were:

  *   Changing the PUBLIC IP to match my rig IP.
  *   Changed the image pull source to our internal one
  *   Changed the zone in the config map to my own one, 'ajl.svc.cw-k8s.test'

With this all deployed, the following test command passed with no issue:
    rake test[ajl.svc.cw-k8s.test] PROXY=10.230.16.1 PROXY_PORT=30060 SIGNUP_CODE='secret' ELLIS=10.230.16.1:30080 TESTS="Basic call - mainline"

If the issue isn't the signup key, can we try getting some more diags that we can take a look into? In particular, I think we would benefit from:

  *   A packet capture on the node you are running the live tests on, when you hit the errors below
  *   The bono logs, at debug level, from the same time. To set up debug logging, you need to add 'log_level=5' to /etc/clearwater/user_settings (creating if needed), and restart the bono service
  *   The ellis logs from the same time

Running the tcpdump on the test node should mean we get to see the full set of flows, and you can likely read through that yourself to work out any following issues you find hiding behind this next one.
Any other diagnostics you can gather would obviously also be useful, but with the above, assuming traffic is reaching the pods, we should be able to work out the issue.

On your connectivity tests, you won't be able to connect to the bono service using 'nc localhost 30060', because that attempts to connect using the localhost IP. We have set the bono service up to listen on the 'PUBLIC_IP', i.e. the host IP. If you try running 'nc 10.3.1.76 30060 -v' you should see successful connection. (Or on whichever host IP you have configured it to listen).
The log output you are seeing on restarting Bono is also benign. These are again simply an artefact of some behaviour we want to have in VMs, but that is not needed in these containers. You can safely ignore this output.

Good luck, and let us know where you get with debugging.
Cheers,
Adam

From: Davis, Matthew [mailto:Matthew.Davis.2 at team.telstra.com]
Sent: 30 May 2018 08:27
To: Adam Lindley <Adam.Lindley at metaswitch.com>; clearwater at lists.projectclearwater.org
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Adam,
# Openstack Install

I only mentioned the helm charts just in case the almost empty charts on my machine were the source of the error. I personally have no experience with Helm, so I can't help you with any development.
I applied that latest change to the bono service port number. It still doesn't work.

How can I check whether the rake tests are failing on the bono side, as opposed to failing on the ellis side? Maybe the reason tcpdumps shows nothing in Bono during the rake tests is because the tests failed to create a user in ellis, and never got to the bono part?

Rake output:
```
Basic Call - Mainline (TCP) - Failed
  RestClient::Forbidden thrown:
   - 403 Forbidden
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/abstract_response.rb:74:in `return!'
     - /home/ubuntu/.rvm/gems/ruby-1.9.3-p551/gems/rest-client-1.8.0/lib/restclient/request.rb:495:in `process_result'
...
```
If I go inside the ellis container and run `nc localhost 80 -v` I see that it establishes a connection.
If I go inside the bono container and run `nc localhost 5060 -v` or `nc localhost 30060 -v` it fails to connect. So from within the bono pod I cannot connect to the localhost. To me that suggests that the problem is caused by something inside bono, not the networking between pods. What happens when you try `nc localhost 32060 -v` in your deployment?
The logs inside bono are an echo of an error message from sprout. Does that matter?

```
30-05-2018 06:46:59.432 UTC [7f45527e4700] Status sip_connection_pool.cpp:428: Recycle TCP connection slot 4
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:244: Reraising all alarms with a known state
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:37: sprout issued 1012.3 alarm
30-05-2018 06:47:06.222 UTC [7f4575ae0700] Status alarm.cpp:37: sprout issued 1013.3 alarm
```

Those timestamps don't correspond to the rake tests. They just happen every 30 seconds.

When I restart the bono service it says: `63: ulimit: open files: cannot modify limit: Invalid argument`
Does that matter? (I've seen that error message everywhere. I have no idea what it means)

I've appended the yaml files to the end of this email.

# Azure Install

I had a chat to Microsoft. It seems that your hunch was correct. HTTP Application Routing only works on port 80 and 443. Furthermore, I cannot simply route SIP calls through port 443 because the routing does some HTTP specific packet inspection things. So I'll have to give up on that approach and go for a more vanilla, manually configured NodePort approach (either still on AKS but without HTTP Application Routing, or on Openstack). So I'm even more keen to solve the aforementioned issues.


# Yamls and stuff

I'm pasting them again just in case I've forgotten something. 10.3.1.76 is the ip address of my cluster.

Here's a script I'm using to tear down and rebuild everything. (Just incase `kubectl apply -f something.yaml` doesn't actually propagate the change fully) The while loops in this script are just to wait until the previous step has finished.

```
set -x
cd clearwater-docker-master/kubernetes
kubectl delete -f ./
kubectl delete configmap env-vars
set -e
echo 'waiting until old pods are all deleted'
while [ $(kubectl get pods | grep ^NAME -v | wc -l ) -neq 0]
do
   sleep 5
done
echo "creating new pods"
kubectl create configmap env-vars --from-literal=ZONE=default.svc.cluster.local
kubectl apply -f ./
while [ $(kubectl get pods | grep "2/2" | grep bono | wc -l) -neq 1 ]
do
   sleep 5
done
BONO=$(kubectl get pods | grep "2/2" | grep bono | awk '{ print $1 }')
echo "Bono is up as $BONO"
kubectl exec -it $BONO -- apt-get -y install vim
kubectl exec -it $BONO -- sed -i -e 's/--pcscf=5060,5058/--pcscf=30060,5058/g' /etc/init.d/bono
kubectl exec -it $BONO service bono restart
while [ $(kubectl get pods | grep "0/" | wc -l) -neq 0 ]
do
   sleep 5
done
echo "All pods are up now"
kubectl get pods
echo "Done"
```

`kubectl get services`

```
NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                         AGE
astaire          ClusterIP   None           <none>        11311/TCP                                       1h
bono             NodePort    10.0.168.197   <none>        3478:32214/TCP,30060:30060/TCP,5062:30144/TCP   1h
cassandra        ClusterIP   None           <none>        7001/TCP,7000/TCP,9042/TCP,9160/TCP             1h
chronos          ClusterIP   None           <none>        7253/TCP                                        1h
ellis            NodePort    10.0.53.199    <none>        80:30080/TCP                                    1h
etcd             ClusterIP   None           <none>        2379/TCP,2380/TCP,4001/TCP                      1h
homer            ClusterIP   None           <none>        7888/TCP                                        1h
homestead        ClusterIP   None           <none>        8888/TCP                                        1h
homestead-prov   ClusterIP   None           <none>        8889/TCP                                        1h
kubernetes       ClusterIP   10.0.0.1       <none>        443/TCP                                         5d
ralf             ClusterIP   None           <none>        10888/TCP                                       1h
sprout           ClusterIP   None           <none>        5052/TCP,5054/TCP                               1h
```

bono-depl.yaml

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: bono
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bono
  template:
    metadata:
      labels:
        service: bono
        snmp: enabled
    spec:
      containers:
      - image: "mlda065/bono:latest"
        imagePullPolicy: Always
        name: bono
        ports:
        - containerPort: 22
        - containerPort: 3478
        - containerPort: 5060
        - containerPort: 5062
        - containerPort: 5060
          protocol: "UDP"
        - containerPort: 5062
          protocol: "UDP"
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PUBLIC_IP
          value: 10.3.1.76
        livenessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
          initialDelaySeconds: 30
        readinessProbe:
          exec:
            command: ["/bin/bash", "/usr/share/kubernetes/liveness.sh", "3478 5062"]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      - image: busybox
        name: tailer
        command: [ "tail", "-F", "/var/log/bono/bono_current.txt" ]
        volumeMounts:
        - name: bonologs
          mountPath: /var/log/bono
      volumes:
      - name: bonologs
        emptyDir: {}
      imagePullSecrets:
      - name: ~
      restartPolicy: Always
```

Bono-svc.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: bono
spec:
  type: NodePort
  ports:
  - name: "3478"
    port: 3478
  - name: "5060"
    port: 30060
    nodePort: 30060
  - name: "5062"
    port: 5062
  selector:
    service: bono
```

Ellis service

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: ellis
spec:
  replicas: 1
  template:
    metadata:
      labels:
        service: ellis
    spec:
      containers:
      - image: "mlda065/ellis:latest"
        imagePullPolicy: Always
        name: ellis
        ports:
        - containerPort: 22
        - containerPort: 80
        envFrom:
        - configMapRef:
              name: env-vars
        env:
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        livenessProbe:
          tcpSocket:
            port: 80
          initialDelaySeconds: 30
        readinessProbe:
          tcpSocket:
            port: 80
      imagePullSecrets:
      - name: ~
      restartPolicy: Always
```

Ellis-svc.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: ellis
spec:
  type: NodePort
  ports:
  - name: "http"
    port: 80
    nodePort: 30080
  selector:
    service: ellis
```

`kubectl describe configmap env-vars`

```
Name:         env-vars
Namespace:    default
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","data":{"ZONE":"default.svc.cluster.local"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"env-vars","namespace":"default"}...

Data
====
ZONE:
----
default.svc.cluster.local
Events:  <none>
```



Thanks,

Matt
Telstra Graduate Engineer
CTO | Cloud SDN NFV

From: Adam Lindley [mailto:Adam.Lindley at metaswitch.com]
Sent: Friday, 25 May 2018 6:42 PM
To: Davis, Matthew <Matthew.Davis.2 at team.telstra.com<mailto:Matthew.Davis.2 at team.telstra.com>>; clearwater at lists.projectclearwater.org<mailto:clearwater at lists.projectclearwater.org>
Subject: RE: [Project Clearwater] Issues with clearwater-docker homestead and homestead-prov under Kubernetes

Hi Matthew,

Our Helm support is a recent addition, and came from another external contributor. See the Pull Request at https://github.com/Metaswitch/clearwater-docker/pull/85 for the details :)
As it stands at the moment, the chart is good enough for deploying and re-creating a full standard deployment through Helm, but I don't believe it handles more of the complexities of upgrading a clearwater deployment that it potentially could.

We haven't yet done any significant work in setting up Helm charts, or integrating with them in a more detailed manner, so if that's something you're interested in as well, we'd love to work with you to get some more enhancements in. Especially if you have other expert contacts who know more in this area.

(I'm removing some of the thread in the email below, to keep us below the list limits. The online archives will keep all the info though)

Cheers,
Adam

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.projectclearwater.org/pipermail/clearwater_lists.projectclearwater.org/attachments/20180531/bd469771/attachment.html>

